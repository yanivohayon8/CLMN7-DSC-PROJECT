{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.docx  import read_docx,process_docx,find_content\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from functools import reduce\n",
    "from statistics import mode\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Defining CONSTS'''\n",
    "docx_path = '../data/raw/docx'\n",
    "groundbase_dir = '../data/raw/groundbase'\n",
    "transcripts_dir = os.path.join(groundbase_dir,'transcripts')\n",
    "topic_dataset_path = os.path.join(groundbase_dir,'dataset.csv')\n",
    "transcript_filespath = glob.glob(groundbase_dir + '/transcripts/*.json')\n",
    "videos_ids = list(map(lambda fl: fl.split('\\\\')[-1].split('.')[0],glob.glob(docx_path + '/*')))\n",
    "\n",
    "desired_videos =['7kLHJ-F33GI'] #['zWg7U0OEAoE','tORLeHHtazM'] #['7kLHJ-F33GI','RIawrYLVdIw','7snJ1mx1EMQ']\n",
    "videos_ids = list(filter(lambda x: x in desired_videos,videos_ids))\n",
    "\n",
    "f_read = {\n",
    "    'statbook':'statbook',\n",
    "    'Dsa':'Dsa'\n",
    "}\n",
    "\n",
    "docxs_chapter_keyword = {\n",
    "    'statbook':'Topic',\n",
    "    'Dsa': 'Chapter'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_paper = {}\n",
    "paper_content ={}\n",
    "for vid in videos_ids:\n",
    "    doc_path = glob.glob(os.path.join(docx_path,vid + '/*.docx'))[0]\n",
    "    doc_name = doc_path.split('\\\\')[-1].split('.')[0]\n",
    "    video_to_paper[vid] = doc_name    \n",
    "    if doc_name not in paper_content.keys():    \n",
    "        full_text,font_sizes = read_docx(doc_path)\n",
    "        paper_content[doc_name] = find_content(f_read[doc_name],full_text,font_sizes,main_chapter_keyword=docxs_chapter_keyword[doc_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Find the following for each paper:\n",
    "    1) main chapter title index. for example [10,15....]\n",
    "    2) range of subsection within each main chapter . for example [(0,9),(11,14)...]\n",
    "    \n",
    "    '''\n",
    "\n",
    "paper_mainchapter_indexes = {}\n",
    "paper_sec_within_main_indexes = {}\n",
    "paper_mains_as_one_doc = {}\n",
    "paper_subsec_as_one_doc = {}\n",
    "\n",
    "\n",
    "for doc_name in paper_content.keys():\n",
    "    '''Find the main chapter indexes in the list of the overall titles'''\n",
    "    mainchapter_indexes = [paper_content[doc_name]['titles'].index(ch_title)\n",
    "                           for ch_title in paper_content[doc_name]['main titles']]\n",
    "    '''Find the subsection indexes range within each main chapter '''\n",
    "    subsec_mainchapter_indexes = [range(mainchapter_indexes[index],mainchapter_indexes[index + 1])\n",
    "                                  for index in range(len(mainchapter_indexes) - 1)]\n",
    "    subsec_mainchapter_indexes.append(range(mainchapter_indexes[-1],\n",
    "                                            len(paper_content[doc_name]['titles'])))\n",
    "    paper_mainchapter_indexes[doc_name] = mainchapter_indexes\n",
    "    paper_sec_within_main_indexes[doc_name] = subsec_mainchapter_indexes\n",
    "    \n",
    "    \n",
    "    '''Making each chapter as a one documents'''\n",
    "    \n",
    "    '''Union all the documents in a section into single document'''\n",
    "    paper_subsec_as_one_doc[doc_name] = [list(reduce(lambda doc,acc:doc + acc,sec,[]))\n",
    "                                         for sec in paper_content[doc_name]['corpus']]\n",
    "    '''Union all the sub section in a main chapter into one document'''\n",
    "    paper_mains_as_one_doc[doc_name] = [list(reduce(lambda acc,s_i:\n",
    "                                                    paper_subsec_as_one_doc[doc_name][s_i]+acc,subsec_indexes,[]))\n",
    "                                        for subsec_indexes in paper_sec_within_main_indexes[doc_name]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['answer_select_exercise', 'word', 'increase', 'decrease', 'maximum', 'would', 'like', 'maximize', 'likelihood', 'give', 'number_recapture', 'individual', 'domain', 'nonnegative', 'integer', 'can', 'use', 'calculus', 'look', 'ratio', 'likelihood', 'value', 'successive', 'value', 'total_population', 'ratio', 'large', 'computation', 'show', 'ratio', 'great', 'small', 'value', 'less', 'large', 'value', 'place', 'maximum', 'expand', 'binomial', 'coefficient', 'expression', 'njr', 'simplify', 'technical', 'aspect', 'use', 'concept', 'introduce', 'obtain', 'property', 'maximum_likelihood_estimator', 'choice', 'estimator', 'desirable', 'property', 'maximum', 'estimator', 'question', 'arise', 'would', 'choose', 'method_moment_estimator', 'answer', 'use', 'maximum_likelihood', 'technique', 'rely', 'know', 'density_function', 'form', 'density', 'must', 'amenable', 'analysis', 'find', 'fisher_information', 'less', 'experiment', 'need', 'order', 'compute', 'moment', 'compute', 'consider', 'case', 'determine', 'parameter', 'distribution', 'number', 'protein', 'tissue', 'tissue', 'several', 'cell', 'type', 'would', 'need', 'distribution', 'cell', 'type', 'density_function', 'number', 'protein', 'cell', 'type', 'piece', 'information', 'use', 'calculate', 'mean_variance', 'number', 'cell', 'ease', 'give', 'explicit', 'expression', 'density', 'likelihood_function', 'difficult', 'obtain', 'lead', 'intricate', 'computation', 'carry', 'desire', 'analysis', 'likelihood_function', 'case', 'exponential_family', 'cramer_bind', 'unbiased_estimator', 'case', 'exponential_family', 'form', 'elegant', 'example', 'case', 'maximum_likelihood', 'estimation', 'let', 'write', 'density_function', 'family', 'use', 'natural', 'parameter', 'exercise', 'maximum_likelihood_estimate', 'base', 'independent_observation', 'member', 'family', 'function', 'mean', 'sufficient', 'statistic', 'write', 'cramer', 'bound', 'estimator', 'efficient', 'word', 'independent_observation', 'return', 'space', 'form', 'exponential_family', 'use', 'invariance', 'property', 'maximum_likelihood_estimate', 'say', 'provide', 'function', 'inverse', 'function_exercise', 'use', 'show', 'discussion', 'exponential_family', 'cramer', 'learn', 'approximation', 'equality', 'introduction_science_statistic', 'estimation', 'space', 'take', 'reciprocal', 'obtain', 'estimate', 'variance', 'find', 'inverse', 'standard_normal', 'see', 'dimension', 'replace', 'information', 'observe', 'information', 'science_statistic', 'example', 'obtain', 'maximum_likelihood_estimate', 'gamma', 'family_random', 'variable', 'write', 'likelihood', 'recall', 'mean', 'gamma', 'distribution', 'invariance', 'property', 'maximum_likelihood_estimator', 'sample_estimate', 'distributional_mean', 'substituting', 'solve', 'derivative', 'function', 'introduction_science_statistic', 'likelihood', 'estimation', 'know', 'digamma', 'function', 'call', 'digamma', 'example', 'distribution', 'fitness', 'effect', 'yield', 'determine', 'variance', 'estimator', 'compute', 'fish', 'information', 'take', 'priate', 'derivative', 'find', 'second', 'order', 'derivative', 'constant', 'expect_value', 'use', 'determine', 'entry', 'fisher', 'negative', 'constant', 'second_derivative', 'gamma', 'function', 'know', 'trigamma', 'function', 'call', 'trigamma', 'score', 'eval', 'alpha', 'beta', 'top', 'log', 'likelihood', 'maximum_likelihood_estimator', 'domain', 'bottom', 'graph', 'introduction_science_statistic', 'estimation', 'compare', 'empirical', 'value', 'method_moment', 'give', 'follow', 'standard_deviation', 'maximum_likelihood_estimator', 'method_moment_estimator', 'look', 'impact', 'move', 'next', 'topic', 'estimation', 'confidence', 'comparison', 'estimation', 'procedure', 'independent_observation', 'continuous', 'estimator', 'consistent', 'vector', 'parameter', 'nee', 'perform', 'high', 'dimenstional', 'delta_method', 'introduction_science_statistic', 'estimation', 'method_moment', 'figure', 'distribution', 'estimator', 'large', 'number_observation', 'figure', 'left', 'method_moment_estimator', 'mean', 'determined', 'use', 'delta_method', 'right', 'likelihood', 'estimator', 'consistent', 'mean', 'estimator', 'converge', 'look', 'property', 'detail', 'revisit', 'example', 'distribution', 'fitness', 'effect', 'example', 'parameter', 'gamma', 'distribution', 'want', 'extend', 'property', 'circumstance', 'look', 'estimate', 'parameter', 'asymptotic', 'property', 'much', 'attraction', 'maximum_likelihood_estimator', 'base', 'property', 'large', 'sample_size', 'summarize', 'important', 'property', 'save', 'technical', 'discussion', 'property', 'word', 'number_observation', 'increase', 'distribution', 'maximum_likelihood_estimator', 'become', 'concentrated', 'true', 'state_nature', 'introduction_science_statistic', 'asymptotic', 'efficiency', 'assumption', 'allow', 'several', 'analytical', 'proper', 'tie', 'use', 'central_limit_theorem', 'converge', 'distribution', 'normal_random', 'variable', 'mean_variance', 'low', 'variance', 'possible', 'cramer_bind', 'property', 'call', 'asymptotic', 'efficiency', 'write', 'term', 'score', 'let', 'central_limit_theorem', 'property', 'log', 'likelihood', 'surface', 'large', 'sample_size', 'single', 'parameter', 'reciprocal', 'fisher_information', 'fisher_information', 'approximate', 'observe', 'information', 'base', 'datum', 'negative', 'curvature', 'maximum_likelihood_estimate', 'small', 'maximum_likelihood_estimator', 'nearty', 'flat', 'variance', 'large', 'curvature', 'large', 'likelihood', 'decrease', 'maximum', 'variance', 'small', 'summary', 'estimator', 'look', 'text', 'definition', 'variable', 'example', 'example', 'trial', 'experiment', 'consist', 'success_probability', 'equal', 'exercise_check', 'maximum', 'case', 'likelihood', 'estimator', 'unbiase', 'normal', 'estimation', 'apply', 'vector', 'value', 'simple_random', 'sample', 'normal_random', 'variable', 'use', 'property', 'exponential', 'function', 'function', 'note', 'maximum_likelihood_estimator', 'biased', 'estimator', 'let', 'recall', 'variable', 'mark_recapture', 'introduction_science_statistic', 'maximum_likelihood', 'estimation', 'number', 'capture_tag', 'number_second_capture', 'number_second_capture', 'tag', 'let', 'total_population', 'set', 'experimental_design', 'observation', 'may', 'vary', 'total_population', 'unknown', 'distribution', 'exercise_show', 'maximum_likelihood_estimator', 'mean', 'great', 'integer', 'less', 'estimator', 'case', 'obtain', 'method_moment_estimator', 'next', 'integer', 'let', 'look', 'example', 'mark', 'capture', 'previous', 'topic', 'number', 'fish', 'population', 'unknown', 'tag_fish', 'first', 'capture', 'event', 'obtain', 'fish', 'second_capture', 'vector', 'length', 'one', 'represent', 'tag_fish', 'zero', 'represent', 'untagged', 'fish', 'sum', 'sample', 'fish', 'sample', 'recapture', 'add', 'one', 'obtain', 'simulation', 'number_recapture', 'fish', 'function', 'look', 'range', 'value', 'symmetric', 'maximum_likelihood_estimate', 'dhyper', 'plot', 'type', 'ylab', 'function', 'example', 'show', 'figure', 'example', 'linear_regression', 'datum', 'observation', 'explanatory_variable', 'response_variable', 'model', 'response', 'take', 'introduction_science_statistic', 'function', 'tag_fish', 'second_capture_tag', 'recapture', 'note', 'estimator', 'total', 'fish', 'population', 'joint_density', 'logarithm', 'maximize', 'likelihood_function', 'parameter', 'equivalent', 'minimizing', 'principle', 'maximum_likelihood', 'equivalent', 'least_square_criterion', 'ordinary', 'linear_regression', 'maximum', 'estimator', 'give', 'regression_line', 'introduction_science_statistic', 'estimation', 'measurement', 'length', 'centimeter', 'specimen', 'archeopteryx', 'follow', 'output', 'linear_regression', 'standard_error', 'centimeter', 'obtain', 'square', 'residual', 'divide', 'take', 'square', 'root', 'example', 'weight', 'least_square', 'know', 'relative', 'size', 'variance', 'independent_normal', 'random_variable', 'mean', 'unknown', 'variance', 'word', 'weight', 'proportional', 'variance', 'science_statistic', 'square', 'estimator', 'unbiase', 'compute', 'optimal', 'value', 'use', 'introductory', 'differential', 'calculus', 'maximum', 'occur', 'critical', 'point', 'endpoint', 'next', 'example', 'show', 'maximum_value', 'likelihood', 'occur', 'end', 'point', 'interval', 'example', 'uniform_random', 'variable', 'datum', 'xj', 'joint_density', 'likelihood', 'joint_density', 'interval', 'estimator', 'less', 'parameter_value', 'mean', 'estimate', 'estimator', 'introduction_science_statistic', 'figure', 'function', 'uniform_random', 'variable', 'likelihood', 'suspect', 'biased', 'downward', 'expect_value', 'note', 'random_variable', 'distribution_function', 'distribution_function', 'introduction_science_statistic', 'confirm', 'bias', 'estimator', 'yield', 'unbiased_estimator', 'straightforward', 'state', 'begin', 'observation', 'principle', 'maximum_likelihood', 'yield', 'choice', 'estimator', 'make', 'observed', 'datum', 'probable', 'density_function', 'regard', 'function', 'presume', 'unique', 'global', 'maximum', 'exist', 'learn', 'large', 'sample', 'maximum_likelihood_estimator', 'many', 'desirable', 'property', 'high', 'dimensional', 'datum', 'likelihood', 'many', 'local', 'find', 'global', 'maximum', 'major', 'computational', 'challenge', 'independent_observation', 'likelihood', 'product', 'density_function', 'sum', 'function', 'likelihood', 'easy', 'parameter_value', 'variable', 'interest', 'unusual', 'next', 'look', 'several', 'example', 'likelihood_function', 'introduction_science_statistic', 'figure', 'function', 'top', 'row', 'bottom', 'row', 'trial', 'left', 'column', 'base', 'trial_success', 'right', 'column', 'base', 'trial_success', 'likelihood', 'introduction_science_statistic', 'maximum', 'estimation', 'estimation', 'solution', 'problem', 'calculate', 'sample', 'parameter', 'hypothetical', 'tion', 'put', 'maximum_likelihood', 'consist', 'choose', 'value', 'parameter', 'maximum_likelihood']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bigram = Phrases(paper_mains_as_one_doc['statbook'], min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "bigram_mod = Phraser(bigram)\n",
    "documents_bigrams = [bigram_mod[doc] for doc in paper_mains_as_one_doc['statbook']] \n",
    "\n",
    "#[[print(w) for w in d if '_' in w] for d in documents_bigrams]\n",
    "trigram = Phrases(bigram[paper_mains_as_one_doc['statbook']], threshold=3)\n",
    "print(trigram[bigram[paper_mains_as_one_doc['statbook'][14]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def myprocess_docx(total_corpus):\n",
    "    total_corpus_tokenized =[]\n",
    "\n",
    "    for chapter_corpus in total_corpus:\n",
    "        start_index = 0\n",
    "        documents = []\n",
    "        for m in re.finditer('[a-zA-Z]\\.',chapter_corpus):\n",
    "            chapter_tokenized_doc = chapter_corpus[start_index:m.start(0) + 1]\n",
    "            \n",
    "            doc_text_no_punc = simple_preprocess(chapter_tokenized_doc,deacc=True) \n",
    "            tokenized_text_non_stop_words = [ word for word in doc_text_no_punc \\\n",
    "                                             if word not in stop_words]\n",
    "            \n",
    "            '''Form bigram and trigram'''\n",
    "            \"\"\"bigram = Phrases(tokenized_text_non_stop_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "            bigram_mod = Phraser(bigram)\n",
    "            documents_bigrams = [bigram_mod[doc] for doc in tokenized_text_non_stop_words] \n",
    "            trigram = Phrases(bigram[tokenized_text_non_stop_words], threshold=3)\n",
    "            trigram_documents = [trigram[bigram[doc]] for doc in tokenized_text_non_stop_words]\"\"\"\n",
    "            \n",
    "            #text_non_stop_words = ' '.join(tokenized_text_non_stop_words)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \"\"\"tokenized_lemmas = nlp(text_non_stop_words)\n",
    "            tokenized_lemmas = [token.lemma_ for token in tokenized_lemmas \\\n",
    "                                if token.pos_ in allowed_postags]\"\"\"\n",
    "            \n",
    "            tokenized_stem = [porter.stem(w) for w in tokenized_text_non_stop_words]\n",
    "            \n",
    "            \n",
    "            documents.append(tokenized_stem)\n",
    "            start_index = m.end(0) + 1\n",
    "        total_corpus_tokenized.append(documents)\n",
    "    total_corpus_tokenized = [ [doc for doc in ch if len(doc) > 0 ] for ch in total_corpus_tokenized]\n",
    "    return total_corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load('en',disable=['parser','ner'])\n",
    "allowed_postags=['NOUN', 'ADJ', 'VERB']\n",
    "\n",
    "def myfind_content_statbook(full_text,font_sizes,main_chapter_keyword = 'Topic'):\n",
    "    most_common_font_size = mode(font_sizes)\n",
    "    topic_titles = []\n",
    "    main_chapter_titles = []\n",
    "    p_Start_Section = re.compile('{} \\d?'.format(main_chapter_keyword))\n",
    "    p_subsection_numbering = re.compile(r'^((\\d+\\.)+\\d*)$')\n",
    "    p_words = re.compile('[A-Za-z]+')\n",
    "    \n",
    "    main_chapter_indexes = [i+1 for i in range(len(full_text) - 1)\\\n",
    "                            if p_Start_Section.match(full_text[i]) is not None and font_sizes[i] > most_common_font_size]\n",
    "    sub_chapter_indexes  = [i for i in range(len(full_text))\\\n",
    "                            if p_subsection_numbering.match(full_text[i]) is not None\n",
    "                            and\n",
    "                            font_sizes[i] > most_common_font_size]\n",
    "    all_titles_indexes = main_chapter_indexes + sub_chapter_indexes    \n",
    "    all_titles_indexes.sort()\n",
    "    \n",
    "    \n",
    "    '''Finding titles values'''\n",
    "    topic_titles  = []\n",
    "    for j in all_titles_indexes:\n",
    "        # subsection\n",
    "        if j not in main_chapter_indexes:\n",
    "           # if no label is given while reading the doc\n",
    "           # ..\n",
    "           # 2.1.2\n",
    "           # my corpus and my content\n",
    "            if '.' in full_text[j + 1] or ',' in full_text[j + 1]:\n",
    "                topic_titles.append(full_text[j])\n",
    "            else:\n",
    "                # 1.1\n",
    "                # introduction\n",
    "                topic_titles.append(\"%s %s\" %(full_text[j],full_text[j+1]))\n",
    "        else:\n",
    "            # main chapter\n",
    "            topic_titles.append(full_text[j])\n",
    "\n",
    "    main_chapter_titles = [full_text[j] for j in main_chapter_indexes]\n",
    "\n",
    "    \n",
    "    \n",
    "    total_corpus  = []\n",
    "    for i in range(len(all_titles_indexes) - 1):\n",
    "        sec_as_text = reduce(lambda acc,x: acc+\" \" +x,\n",
    "                             full_text[all_titles_indexes[i]:all_titles_indexes[i+1]],\"\")\n",
    "        # adding dot for finishing the corpus\n",
    "        if len(sec_as_text) == 0:\n",
    "            pass\n",
    "            #sec_as_text = 'section empty.' # planting plain text for non empty\n",
    "        elif sec_as_text[-1] != '.':\n",
    "            sec_as_text = sec_as_text + '.'\n",
    "        total_corpus.append(sec_as_text)\n",
    "        \n",
    "    \n",
    "    # adding the text of the last section\n",
    "    last_sec_text = \"\"\n",
    "    i = all_titles_indexes[-1] + 1\n",
    "    while i < len(full_text) and font_sizes[i] <= most_common_font_size:\n",
    "        last_sec_text = last_sec_text + \" \" + full_text[i]\n",
    "        i+=1\n",
    "    total_corpus.append(last_sec_text)\n",
    "    \n",
    "    \n",
    "    # fill empty subssections with random string\n",
    "     \n",
    "    total_corpus = myprocess_docx(total_corpus)\n",
    "    \n",
    "    for i in range(len(total_corpus)):\n",
    "        if len(total_corpus[i]) == 0:\n",
    "            total_corpus[i] = [['section','empty']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return {'corpus':total_corpus,'titles':topic_titles,'main titles':main_chapter_titles}\n",
    "    #return total_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_paper = {}\n",
    "paper_content ={}\n",
    "for vid in [videos_ids[0]]:\n",
    "    doc_path = glob.glob(os.path.join(docx_path,vid + '/*.docx'))[0]\n",
    "    doc_name = doc_path.split('\\\\')[-1].split('.')[0]\n",
    "    video_to_paper[vid] = doc_name    \n",
    "    if doc_name not in paper_content.keys():    \n",
    "        full_text,font_sizes = read_docx(doc_path)\n",
    "        paper_content[doc_name] = myfind_content_statbook(full_text,font_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "[['cramer', 'rao', 'bound', 'topic', 'somewhat', 'advanc', 'skip', 'first', 'read'], ['section', 'give', 'us', 'introduct', 'log', 'likelihood', 'deriv', 'score', 'function'], ['shall', 'encount', 'function', 'introduc', 'maximum', 'likelihood', 'estim'], ['addit', 'cramer', 'rao', 'bound', 'base', 'varianc', 'score', 'function', 'known', 'fisher', 'inform', 'give', 'lower', 'bound', 'varianc', 'unbias', 'estim'], ['concept', 'necessari', 'describ', 'varianc', 'maximum', 'likelihood', 'estim'], ['among', 'unbias', 'estim', 'one', 'import', 'goal', 'find', 'estim', 'small', 'varianc', 'possibl', 'precis', 'goal', 'would', 'find', 'unbias', 'estim', 'uniform', 'minimum', 'varianc'], ['word', 'effici', 'unbias', 'estim', 'minimum', 'valu', 'ratio', 'var', 'var', 'valu', 'thu', 'effici', 'goal', 'find', 'estim', 'effici', 'near', 'one', 'possibl'], ['unbias', 'estim', 'cramer', 'rao', 'bound', 'tell', 'us', 'small', 'varianc', 'ever', 'possibl'], ['formula', 'bit', 'mysteri', 'first'], ['howev', 'shall', 'soon', 'learn', 'bound', 'consequ', 'bound', 'correl', 'previous', 'learn', 'recal', 'two', 'random', 'variabl', 'correl', 'exercis', 'ez', 'cov', 'ey', 'introduct', 'scienc', 'statist', 'unbias', 'estim', 'begin', 'data', 'xj', 'case', 'data', 'come', 'simpl', 'random', 'sampl', 'joint', 'densiti', 'product', 'margin', 'densiti'], ['continu', 'random', 'variabl', 'two', 'basic', 'properti', 'densiti', 'xj', 'xj', 'dx', 'let', 'unbias', 'estim', 'basic', 'formula', 'comput', 'expect', 'function', 'differenti', 'respect', 'paramet', 'pass', 'deriv', 'integr', 'first', 'differenti', 'side', 'equat', 'use', 'logarithm', 'function', 'write', 'deriv', 'expect', 'random', 'variabl', 'similar', 'calcul', 'use', 'ln', 'xj', 'return', 'review', 'correl', 'unbias', 'estim', 'score', 'function', 'equat', 'call', 'cramer', 'rao', 'lower', 'bound', 'inform', 'inequ', 'state', 'lower', 'bound', 'varianc', 'unbias', 'estim', 'reciproc', 'fisher', 'inform'], ['word', 'higher', 'inform', 'lower', 'possibl', 'valu', 'varianc', 'unbias', 'estim'], ['exercis', 'let', 'uniform', 'interv', 'dx', 'thu', 'case', 'cannot', 'pass', 'deriv', 'integr', 'introduct', 'scienc', 'statist', 'unbias', 'estim', 'return', 'case', 'simpl', 'random', 'sampl', 'take', 'logarithm', 'side', 'equat', 'ln', 'xj', 'ln', 'differenti', 'respect', 'paramet', 'ln', 'random', 'variabl', 'ln', 'ng', 'independ', 'distribut'], ['use', 'fact', 'varianc', 'sum', 'sum', 'varianc', 'independ', 'random', 'variabl', 'see', 'notic', 'correspond'], ['inform', 'linearli', 'proport', 'number', 'observ'], ['estim', 'sampl', 'mean', 'function', 'sampl', 'mean', 'varianc', 'invers', 'proport', 'number', 'observ'], ['exampl', 'independ', 'bernoulli', 'random', 'variabl', 'unknown', 'success', 'probabl', 'densiti', 'two', 'equat', 'show', 'unbias', 'estim', 'uniformli', 'minimum', 'varianc'], ['exercis', 'independ', 'normal', 'random', 'variabl', 'known', 'varianc', 'exercis', 'paramet', 'appear', 'densiti', 'function', 'two', 'form', 'fisher', 'inform'], ['show', 'thu', 'unbias', 'estim', 'varianc', 'cramer', 'rao', 'lower', 'bound', 'varianc', 'uniformli', 'minimum', 'varianc', 'unbias', 'estim'], ['exampl', 'give', 'estim', 'achiev', 'cramer', 'rao', 'bound', 'let', 'pareto', 'distribut', 'varianc', 'unless', 'effici', 'compar', 'cramer', 'rao', 'bound', 'low', 'improv', 'larger', 'introduct', 'scienc', 'statist', 'unbias', 'estim']]\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(paper_content['statbook']['corpus'][116])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Find the following for each paper:\n",
    "    1) main chapter title index. for example [10,15....]\n",
    "    2) range of subsection within each main chapter . for example [(0,9),(11,14)...]\n",
    "    \n",
    "    '''\n",
    "\n",
    "paper_mainchapter_indexes = {}\n",
    "paper_sec_within_main_indexes = {}\n",
    "paper_mains_as_one_doc = {}\n",
    "paper_subsec_as_one_doc = {}\n",
    "\n",
    "\n",
    "for doc_name in paper_content.keys():\n",
    "    '''Find the main chapter indexes in the list of the overall titles'''\n",
    "    mainchapter_indexes = [paper_content[doc_name]['titles'].index(ch_title)\n",
    "                           for ch_title in paper_content[doc_name]['main titles']]\n",
    "    '''Find the subsection indexes range within each main chapter '''\n",
    "    subsec_mainchapter_indexes = [range(mainchapter_indexes[index],mainchapter_indexes[index + 1])\n",
    "                                  for index in range(len(mainchapter_indexes) - 1)]\n",
    "    subsec_mainchapter_indexes.append(range(mainchapter_indexes[-1],\n",
    "                                            len(paper_content[doc_name]['titles'])))\n",
    "    paper_mainchapter_indexes[doc_name] = mainchapter_indexes\n",
    "    paper_sec_within_main_indexes[doc_name] = subsec_mainchapter_indexes\n",
    "    \n",
    "    \n",
    "    '''Making each chapter as a one documents'''\n",
    "    \n",
    "    '''Union all the documents in a section into single document'''\n",
    "    paper_subsec_as_one_doc[doc_name] = [list(reduce(lambda doc,acc:doc + acc,sec,[]))\n",
    "                                         for sec in paper_content[doc_name]['corpus']]\n",
    "    '''Union all the sub section in a main chapter into one document'''\n",
    "    paper_mains_as_one_doc[doc_name] = [list(reduce(lambda acc,s_i:\n",
    "                                                    paper_subsec_as_one_doc[doc_name][s_i]+acc,subsec_indexes,[]))\n",
    "                                        for subsec_indexes in paper_sec_within_main_indexes[doc_name]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['answer_select_exercis', 'word', 'ln', 'pjx', 'increas', 'decreas', 'thu', 'maximum', 'would', 'like', 'maxim_likelihood', 'given', 'number_recaptur', 'individu', 'domain', 'nonneg', 'integ', 'cannot', 'use', 'calculu', 'howev', 'look', 'ratio', 'likelihood', 'valu', 'success', 'valu', 'total_popul', 'njr', 'jr', 'like', 'precis', 'ratio', 'larger', 'one', 'comput', 'show', 'ratio', 'greater', 'small', 'valu', 'less', 'one', 'larg', 'valu', 'thu', 'place', 'middl', 'maximum', 'expand', 'binomi', 'coeffici', 'express', 'njr', 'simplifi', 'technic', 'aspect', 'use', 'concept', 'previous', 'introduc', 'obtain', 'properti', 'maximum_likelihood_estim', 'choic', 'estim', 'desir', 'properti', 'maximum_likelihood_estim', 'question', 'aris', 'would', 'one', 'choos', 'method_moment_estim', 'one', 'answer', 'use', 'maximum_likelihood', 'techniqu', 'reli', 'know', 'densiti_function', 'explicitli', 'moreov', 'form', 'densiti', 'must', 'amen', 'analysi', 'necessari', 'maxim_likelihood', 'find', 'fisher_inform', 'howev', 'much', 'less', 'experi', 'need', 'order', 'comput', 'moment', 'thu', 'far', 'comput', 'howev', 'consid', 'case', 'determin', 'paramet', 'distribut', 'number', 'protein', 'tissu', 'tissu', 'sever', 'cell', 'type', 'would', 'need', 'distribut', 'cell', 'type', 'densiti_function', 'number', 'protein', 'cell', 'type', 'two', 'piec', 'inform', 'use', 'calcul', 'mean_varianc', 'number', 'cell', 'eas', 'howev', 'give', 'explicit', 'express', 'densiti', 'henc', 'likelihood_function', 'difficult', 'obtain', 'lead', 'quit', 'intric', 'comput', 'carri', 'desir', 'analysi', 'likelihood_function', 'case', 'exponenti_famili', 'cramer_rao_bound', 'unbias_estim', 'case', 'exponenti_famili', 'form', 'eleg', 'exampl', 'case', 'maximum_likelihood_estim', 'let', 'first', 'write', 'densiti_function', 'famili', 'use', 'natur', 'paramet', 'exercis', 'maximum_likelihood_estim', 'base', 'independ_observ', 'member', 'exponen', 'tial', 'famili', 'function', 'mean', 'suffici', 'statist', 'write', 'recal', 'discuss', 'cramer_rao_bound', 'estim', 'effici', 'word', 'independ_observ', 'densiti', 'var', 'ni', 'return', 'paramet_space', 'form', 'exponenti_famili', 'use', 'invari', 'properti', 'maximum_likelihood_estim', 'say', 'provid', 'one', 'one', 'function', 'thu', 'invers', 'function', 'exercis', 'use', 'delta_method', 'show', 'discuss', 'exponenti_famili', 'cramer_rao_bound', 'learn', 'approxim', 'equal', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'estim', 'paramet_space', 'rather', 'take', 'reciproc', 'obtain', 'estim', 'varianc', 'find', 'matrix', 'invers', 'th', 'paramet', 'ii', 'approxim_standard', 'normal', 'saw', 'one', 'dimens', 'replac', 'inform', 'matrix', 'observ', 'inform', 'matrix', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'exampl', 'obtain', 'maximum_likelihood_estim', 'gamma', 'famili_random_variabl', 'write', 'likelihood', 'recal', 'mean', 'gamma', 'distribut', 'thu', 'invari', 'properti', 'maximum_likelihood_estim', 'sampl_mean', 'maximum_likelihood_estim', 'distribut', 'mean', 'substitut', 'solv', 'numer', 'deriv', 'logarithm', 'gamma', 'function', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'know', 'digamma', 'function', 'call', 'digamma', 'exampl', 'distribut_fit', 'effect', 'yield', 'determin', 'varianc', 'estim', 'first', 'comput', 'fisher_inform', 'matrix', 'take', 'appro', 'priat', 'deriv', 'find', 'second', 'order', 'deriv', 'constant', 'thu', 'expect_valu', 'use', 'determin', 'entri', 'fisher_inform', 'matrix', 'neg', 'constant', 'second_deriv', 'logarithm', 'gamma', 'function', 'known', 'trigamma', 'function', 'call', 'trigamma', 'invers', 'score', 'eval', 'alpha', 'beta', 'figur', 'top', 'log_likelihood', 'near', 'maximum_likelihood_estim', 'domain', 'bottom', 'graph', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'compar', 'empir', 'valu', 'method_moment', 'give', 'follow', 'thu', 'standard_deviat', 'maximum_likelihood_estim', 'respect', 'method_moment_estim', 'look', 'impact', 'move', 'next', 'topic', 'interv_estim', 'confid', 'comparison', 'estim', 'procedur', 'independ_observ', 'continu', 'estim', 'consist', 'vector', 'paramet', 'need', 'perform', 'higher', 'dimenst', 'delta_method', 'invert', 'fisher_inform', 'matrix', 'estim', 'varianc', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'method_moment', 'maximum_likelihood', 'figur', 'distribut', 'estim', 'suffici_larg', 'number_observ', 'bell_curv', 'figur', 'left', 'method_moment_estim', 'mean', 'nj', 'determin', 'use', 'delta_method', 'right', 'maximum_likelihood_estim', 'consist', 'mean', 'estim', 'converg', 'look', 'properti', 'detail', 'revisit', 'exampl', 'distribut_fit', 'effect', 'exampl', 'two', 'paramet', 'gamma', 'distribut', 'want', 'extend', 'properti', 'circumst', 'look', 'estim', 'one', 'paramet', 'asymptot', 'properti', 'much', 'attract', 'maximum_likelihood_estim', 'base', 'properti', 'larg', 'sampl_size', 'summar', 'import', 'properti', 'save', 'technic', 'discuss', 'properti', 'later', 'word', 'number_observ', 'increas', 'distribut', 'maximum_likelihood_estim', 'becom', 'concentr', 'true', 'state_natur', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'asymptot', 'normal', 'effici', 'assumpt', 'allow', 'among', 'sever', 'analyt', 'proper', 'tie', 'use', 'central_limit_theorem', 'converg', 'distribut', 'normal_random_variabl', 'mean_varianc', 'lowest', 'varianc', 'possibl', 'cramer_rao', 'lower_bound', 'properti', 'call', 'asymptot', 'effici', 'write', 'term', 'score', 'let', 'central_limit_theorem', 'properti', 'log_likelihood', 'surfac', 'larg', 'sampl_size', 'varianc', 'maximum_likelihood', 'estima', 'tor', 'singl', 'paramet', 'approxim', 'reciproc', 'fisher_inform', 'fisher_inform', 'approxim', 'observ', 'inform', 'base', 'data', 'neg', 'curvatur', 'log_likelihood', 'maximum_likelihood_estim', 'small', 'near', 'maximum_likelihood_estim', 'likelihood', 'surfac', 'nearti', 'flat', 'varianc', 'larg', 'curvatur', 'larg', 'likelihood', 'decreas', 'quickli', 'maximum', 'varianc', 'small', 'summari', 'estim', 'look', 'text', 'definit', 'variabl', 'exampl', 'exampl', 'bernoulli_trial', 'experi', 'consist', 'bernoulli_trial_success', 'probabl', 'pjx', 'equal', 'zero', 'exercis_check', 'maximum', 'thu', 'case', 'maximum_likelihood_estim', 'also', 'unbias', 'exampl', 'normal', 'data', 'maximum_likelihood_estim', 'appli', 'vector', 'valu', 'paramet', 'simpl_random', 'sampl', 'normal_random_variabl', 'use', 'properti', 'exponenti', 'function', 'simplifi', 'likelihood_function', 'note', 'maximum_likelihood_estim', 'bias', 'estim', 'exampl', 'lincoln', 'peterson', 'method', 'mark_recaptur', 'let', 'recal', 'variabl', 'mark_recaptur', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'number', 'captur_tag', 'number_second_captur', 'number_second_captur', 'tag', 'let', 'total_popul', 'set', 'experiment_design', 'observ', 'may', 'vari', 'total_popul', 'unknown', 'likelihood_function', 'hypergeometr_distribut', 'njr', 'exercis_show', 'maximum_likelihood_estim', 'tk', 'mean', 'greater', 'integ', 'less', 'thu', 'maximum_likelihood_estim', 'case', 'obtain', 'method_moment_estim', 'round', 'ing', 'next', 'integ', 'let_look', 'exampl', 'mark', 'captur', 'previou', 'topic', 'number', 'fish', 'popul', 'unknown', 'us', 'tag_fish', 'first', 'captur', 'event', 'obtain', 'fish', 'second_captur', 'fish', 'rep_rep', 'creat', 'vector', 'length', 'one', 'repres', 'tag_fish', 'zero', 'repres', 'untag', 'fish', 'sum', 'sampl', 'fish', 'sampl', 'recaptur', 'add', 'one', 'obtain', 'simul', 'number_recaptur', 'fish', 'likelihood_function', 'look', 'rang', 'valu', 'symmetr', 'maximum_likelihood_estim', 'dhyper', 'plot', 'type', 'ylab', 'col', 'green', 'likelihood_function', 'exampl', 'shown', 'figur', 'exampl', 'linear_regress', 'data', 'observ', 'one', 'explanatori_variabl', 'one', 'respons_variabl', 'model', 'respons', 'take', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'likelihood_function', 'mark_recaptur', 'figur', 'likelihood_function', 'nj', 'mark_recaptur', 'tag_fish', 'second_captur_tag', 'thu', 'recaptur', 'note', 'maximum_likelihood_estim', 'total', 'fish', 'popul', 'thu', 'joint_densiti', 'logarithm', 'consequ', 'maxim_likelihood', 'function', 'paramet', 'equival', 'minim', 'thu', 'principl', 'maximum_likelihood', 'equival', 'least_squar_criterion', 'ordinari', 'linear_regress', 'maximum_likelihood_estim', 'give', 'regress_line', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'measur', 'length', 'centimet', 'femur_humeru', 'five', 'specimen', 'archeopteryx', 'follow', 'output', 'linear_regress', 'femur_humeru', 'summari', 'lm', 'humeru', 'femur', 'call_lm', 'formula', 'humeru', 'femur', 'residu_standard', 'error', 'centimet', 'obtain', 'squar', 'residu', 'divid', 'take', 'squar', 'root', 'exampl', 'weight', 'least_squar', 'know', 'rel', 'size', 'varianc', 'independ_normal', 'random_variabl', 'mean', 'unknown', 'varianc', 'word', 'weight', 'invers_proport', 'varianc', 'log_likelihood', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'squar', 'estim', 'unbias', 'comput', 'optim', 'valu', 'use', 'introductori', 'differenti', 'calculu', 'maximum', 'occur', 'either', 'critic', 'point', 'endpoint', 'next', 'exampl', 'show', 'maximum_valu', 'likelihood', 'occur', 'end', 'point', 'interv', 'exampl', 'uniform_random_variabl', 'data', 'xj', 'otherwis', 'therefor', 'joint_densiti', 'likelihood', 'consequ', 'joint_densiti', 'whenev', 'recal', 'notat', 'likelihood', 'interv_estim', 'alway', 'less', 'paramet_valu', 'meant', 'estim', 'estim', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'figur', 'likelihood_function', 'uniform_random_variabl', 'interv', 'likelihood', 'max', 'thu', 'suspect', 'bias', 'downward', 'order', 'comput_expect', 'valu', 'note', 'xgp', 'fx', 'random_variabl_distribut', 'function', 'thu', 'distribut_function', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'confirm', 'bia', 'estim', 'yield', 'unbias_estim', 'principl', 'maximum_likelihood', 'rel', 'straightforward', 'state', 'begin', 'observ', 'principl', 'maximum_likelihood', 'yield', 'choic', 'estim', 'make', 'observ', 'data', 'probabl', 'definit', 'likelihood_function', 'densiti_function', 'regard', 'function', 'thu', 'presum', 'uniqu', 'global', 'maximum', 'exist', 'learn', 'especi', 'larg', 'sampl', 'maximum_likelihood_estim', 'mani', 'desir', 'properti', 'howev', 'especi', 'high', 'dimension', 'data', 'likelihood', 'mani', 'local', 'maxima', 'thu', 'find', 'global', 'maximum', 'major', 'comput', 'challeng', 'independ_observ', 'likelihood', 'product', 'densiti_function', 'logarithm', 'product', 'sum', 'logarithm', 'find', 'zero', 'score', 'function', 'ln', 'jx', 'deriv', 'logarithm', 'likelihood', 'easier', 'paramet_valu', 'variabl', 'interest', 'somewhat', 'unusu', 'next', 'look', 'sever', 'exampl', 'likelihood_function', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'figur', 'likelihood_function', 'top', 'row', 'logarithm', 'bottom', 'row', 'bernoulli_trial', 'left', 'column', 'base', 'trial', 'success', 'right', 'column', 'base', 'trial', 'success', 'notic', 'maximum_likelihood', 'approxim', 'introduct_scienc_statist', 'maximum_likelihood_estim', 'maximum_likelihood_estim', 'solut', 'problem', 'calcul', 'sampl', 'paramet', 'hypothet', 'popula', 'tion', 'put', 'forward', 'method', 'maximum_likelihood', 'consist', 'simpli', 'choos', 'valu', 'paramet', 'maximum_likelihood', 'fisher', 'phil', 'tran', 'royal', 'soc', 'ser']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bigram = Phrases(paper_mains_as_one_doc['statbook'], min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "bigram_mod = Phraser(bigram)\n",
    "documents_bigrams = [bigram_mod[doc] for doc in paper_mains_as_one_doc['statbook']] \n",
    "\n",
    "#[[print(w) for w in d if '_' in w] for d in documents_bigrams]\n",
    "trigram = Phrases(bigram[paper_mains_as_one_doc['statbook']], threshold=3)\n",
    "print(trigram[bigram[paper_mains_as_one_doc['statbook'][14]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
