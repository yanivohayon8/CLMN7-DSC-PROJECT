{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def findInDict(needle, haystack):\n",
    "    for key in haystack.keys():\n",
    "        try:\n",
    "            value=haystack[key]\n",
    "        except:\n",
    "            continue\n",
    "        if key==needle:\n",
    "            return value\n",
    "        if isinstance(value,dict):            \n",
    "            x=findInDict(needle,value)            \n",
    "            if x is not None:\n",
    "                return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfobject=open(\"C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf\",'rb')\n",
    "pdf=pypdf.PdfFileReader(pdfobject)\n",
    "#xfa=findInDict('/XFA',pdf.resolvedObjects)\n",
    "\n",
    "#pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Creator': ' TeX output 2005.03.30:0907',\n",
       " '/Producer': 'dvipdfm 0.13.2c, Copyright © 1998, by Mark A. Wicks',\n",
       " '/CreationDate': \"D:20050330092217+08'00'\",\n",
       " '/rgid': 'PB:228524191_AS:102309094756357@1401403797531'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getDocumentInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[IndirectObject(101, 0), IndirectObject(102, 0), IndirectObject(103, 0), IndirectObject(104, 0)]\n",
      "March30,20059:7WSPC/LectureNotesSeries:9inx6inheg05a\n",
      "TheAprioriAlgorithm{aTutorial\n",
      "MarkusHegland\n",
      "CMA,AustralianNationalUniversity\n",
      "JohnDedmanBuilding,CanberraACT0200,Australia\n",
      "E-mail:Markus.Hegland@anu.edu.au\n",
      "Associationrulesare\"if-thenrules\"withtwomeasureswhichquantify\n",
      "thesupportandoftheruleforagivendataset.Havingtheir\n",
      "origininmarketbaskedanalysis,associationrulesarenowoneofthe\n",
      "mostpopulartoolsindatamining.Thispopularityistoalargepartdue\n",
      "totheavailabilityoftalgorithms.Theandarguablymost\n",
      "tialalgorithmfortassociationrulediscoveryisApriori.\n",
      "Inthefollowingwewillreviewbasicconceptsofassociationruledis-\n",
      "coveryincludingsupport,theaprioriproperty,constraints\n",
      "andparallelalgorithms.Thecoreconsistsofareviewofthemostim-\n",
      "portantalgorithmsforassociationrulediscovery.Somefamiliaritywith\n",
      "conceptslikepredicates,probability,expectationandrandomvariables\n",
      "isassumed.\n",
      "1.Introduction\n",
      "Largeamountsofdatahavebeencollectedroutinelyinthecourseofday-\n",
      "to-daymanagementinbusiness,administration,banking,thedeliveryof\n",
      "socialandhealthservices,environmentalprotection,securityandinpol-\n",
      "itics.Suchdataisprimarilyusedforaccountingandformanagementof\n",
      "thecustomerbase.Typically,managementdatasetsareverylargeand\n",
      "constantlygrowingandcontainalargenumberofcomplexfeatures.While\n",
      "thesedatasetspropertiesofthemanagedsubjectsandrelations,and\n",
      "arethuspotentiallyofsomeusetotheirowner,theyoftenhaverelatively\n",
      "lowinformationdensity.Onerequiresrobust,simpleandcomputationally\n",
      "ttoolstoextractinformationfromsuchdatasets.Thedevelopment\n",
      "andunderstandingofsuchtoolsisthecorebusinessofdatamining.These\n",
      "toolsarebasedonideasfromcomputerscience,mathematicsandstatistics.\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pdf.getPage(1).getXmpMetadata())\n",
    "print(pdf.getPage(1).getContents())\n",
    "print(pdf.getPage(1).extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getPage(1).getContents()[1].getObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \u001b[1;31m# printer registered in self.type_pprinters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m                     \u001b[1;31m# deferred printer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sorted_for_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36m_enumerate\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_enumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;34m\"\"\"like enumerate, but with an upper limit on the number of items\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "pdf.resolvedObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'generation', 'getObject', 'idnum', 'pdf', 'readFromStream', 'writeToStream']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(pdf.getPage(1).getContents()[0]))\n",
    "pdf.getPage(1).getContents()[0].getObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfobject=open('C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf','rb')\n",
    "pdf=pypdf.PdfFileReader(pdfobject)\n",
    "pdf.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ShellError",
     "evalue": "The command `pdftoppm C:\\Users\\yaniv\\Downloads\\The_Apriori_Algorithm-a_Tutorial.pdf C:\\Users\\yaniv\\AppData\\Local\\Temp\\tmp64ep6yj6\\conv` failed with exit code 127\n------------- stdout -------------\n------------- stderr -------------\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mShellError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8e086a99f4d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtextract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtextract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tesseract'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'eng'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\__init__.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(filename, encoding, extension, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiletype_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, filename, encoding, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# output encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# http://nedbatchelder.com/text/unipain/unipain.html#35\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mbyte_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0municode_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0municode_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\pdf_parser.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_pdfminer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tesseract'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_tesseract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnknownMethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\pdf_parser.py\u001b[0m in \u001b[0;36mextract_tesseract\u001b[1;34m(self, filename, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pdftoppm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;31m# This is equivalent to getting exitcode 127 from sh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 raise exceptions.ShellError(\n\u001b[1;32m---> 91\u001b[1;33m                     \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m127\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m                 )\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mShellError\u001b[0m: The command `pdftoppm C:\\Users\\yaniv\\Downloads\\The_Apriori_Algorithm-a_Tutorial.pdf C:\\Users\\yaniv\\AppData\\Local\\Temp\\tmp64ep6yj6\\conv` failed with exit code 127\n------------- stdout -------------\n------------- stderr -------------\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "textract.process('C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf', method='tesseract',language='eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March30,20059:7WSPC/LectureNotesSeries:9inx6inheg05a\n",
      "TheAprioriAlgorithm{aTutorial\n",
      "MarkusHegland\n",
      "CMA,AustralianNationalUniversity\n",
      "JohnDedmanBuilding,CanberraACT0200,Australia\n",
      "E-mail:Markus.Hegland@anu.edu.au\n",
      "Associationrulesare\"if-thenrules\"withtwomeasureswhichquantify\n",
      "thesupportandoftheruleforagivendataset.Havingtheir\n",
      "origininmarketbaskedanalysis,associationrulesarenowoneofthe\n",
      "mostpopulartoolsindatamining.Thispopularityistoalargepartdue\n",
      "totheavailabilityoftalgorithms.Theandarguablymost\n",
      "tialalgorithmfortassociationrulediscoveryisApriori.\n",
      "Inthefollowingwewillreviewbasicconceptsofassociationruledis-\n",
      "coveryincludingsupport,theaprioriproperty,constraints\n",
      "andparallelalgorithms.Thecoreconsistsofareviewofthemostim-\n",
      "portantalgorithmsforassociationrulediscovery.Somefamiliaritywith\n",
      "conceptslikepredicates,probability,expectationandrandomvariables\n",
      "isassumed.\n",
      "1.Introduction\n",
      "Largeamountsofdatahavebeencollectedroutinelyinthecourseofday-\n",
      "to-daymanagementinbusiness,administration,banking,thedeliveryof\n",
      "socialandhealthservices,environmentalprotection,securityandinpol-\n",
      "itics.Suchdataisprimarilyusedforaccountingandformanagementof\n",
      "thecustomerbase.Typically,managementdatasetsareverylargeand\n",
      "constantlygrowingandcontainalargenumberofcomplexfeatures.While\n",
      "thesedatasetspropertiesofthemanagedsubjectsandrelations,and\n",
      "arethuspotentiallyofsomeusetotheirowner,theyoftenhaverelatively\n",
      "lowinformationdensity.Onerequiresrobust,simpleandcomputationally\n",
      "ttoolstoextractinformationfromsuchdatasets.Thedevelopment\n",
      "andunderstandingofsuchtoolsisthecorebusinessofdatamining.These\n",
      "toolsarebasedonideasfromcomputerscience,mathematicsandstatistics.\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF4\n",
    "import re\n",
    "import io\n",
    "\n",
    "pdfFileObj = open(r'C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf', 'rb')\n",
    "pdfReader = PyPDF4.PdfFileReader(pdfFileObj)\n",
    "pageObj = pdfReader.getPage(1)\n",
    "pages_text = pageObj.extractText()\n",
    "\n",
    "for line in pages_text.split('\\n'):\n",
    "    #if re.match(r\"^PDF\", line):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "def read_pdf(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'  # 'utf16','utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    #\"\"    C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password, caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    str_ = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return str_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 3\\n\\nProbability and Information\\nTheory\\n\\nIn this chapter, we describe probability theory and information theory.\\n\\nProbability theory is a mathematical framework for representing uncertain\\nstatements. It provides a means of quantifying uncertainty and axioms for deriving\\nnew uncertain statements. In artiﬁcial intelligence applications, we use probability\\ntheory in two major ways. First, the laws of probability tell us how AI systems\\nshould reason, so we design our algorithms to compute or approximate various\\nexpressions derived using probability theory. Second, we can use probability and\\nstatistics to theoretically analyze the behavior of proposed AI systems.\\n\\nProbability theory is a fundamental tool of many disciplines of science and\\nengineering. We provide this chapter to ensure that readers whose background is\\nprimarily in software engineering with limited exposure to probability theory can\\nunderstand the material in this book.\\n\\nWhile probability theory allows us to make uncertain statements and reason\\nin the presence of uncertainty, information allows us to quantify the amount of\\nuncertainty in a probability distribution.\\n\\nIf you are already familiar with probability theory and information theory,\\nyou may wish to skip all of this chapter except for Sec. 3.14, which describes the\\ngraphs we use to describe structured probabilistic models for machine learning. If\\nyou have absolutely no prior experience with these subjects, this chapter should\\nbe suﬃcient to successfully carry out deep learning research projects, but we do\\nsuggest that you consult an additional resource, such as Jaynes (2003).\\n\\n52\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.1 Why Probability?\\n\\nMany branches of computer science deal mostly with entities that are entirely\\ndeterministic and certain. A programmer can usually safely assume that a CPU will\\nexecute each machine instruction ﬂawlessly. Errors in hardware do occur, but are\\nrare enough that most software applications do not need to be designed to account\\nfor them. Given that many computer scientists and software engineers work in a\\nrelatively clean and certain environment, it can be surprising that machine learning\\nmakes heavy use of probability theory.\\n\\nThis is because machine learning must always deal with uncertain quantities,\\nand sometimes may also need to deal with stochastic (non-deterministic) quantities.\\nUncertainty and stochasticity can arise from many sources. Researchers have made\\ncompelling arguments for quantifying uncertainty using probability since at least\\nthe 1980s. Many of the arguments presented here are summarized from or inspired\\nby Pearl (1988).\\n\\nNearly all activities require some ability to reason in the presence of uncertainty.\\nIn fact, beyond mathematical statements that are true by deﬁnition, it is diﬃcult\\nto think of any proposition that is absolutely true or any event that is absolutely\\nguaranteed to occur.\\n\\nThere are three possible sources of uncertainty:\\n\\n1. Inherent stochasticity in the system being modeled. For example, most\\ninterpretations of quantum mechanics describe the dynamics of subatomic\\nparticles as being probabilistic. We can also create theoretical scenarios that\\nwe postulate to have random dynamics, such as a hypothetical card game\\nwhere we assume that the cards are truly shuﬄed into a random order.\\n\\n2. Incomplete observability. Even deterministic systems can appear stochastic\\nwhen we cannot observe all of the variables that drive the behavior of the\\nsystem. For example, in the Monty Hall problem, a game show contestant is\\nasked to choose between three doors and wins a prize held behind the chosen\\ndoor. Two doors lead to a goat while a third leads to a car. The outcome\\ngiven the contestant’s choice is deterministic, but from the contestant’s point\\nof view, the outcome is uncertain.\\n\\n3. Incomplete modeling. When we use a model that must discard some of\\nthe information we have observed, the discarded information results in\\nuncertainty in the model’s predictions. For example, suppose we build a\\nrobot that can exactly observe the location of every object around it. If the\\n\\n53\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nrobot discretizes space when predicting the future location of these objects,\\nthen the discretization makes the robot immediately become uncertain about\\nthe precise position of objects: each object could be anywhere within the\\ndiscrete cell that it was observed to occupy.\\n\\nIn many cases, it is more practical to use a simple but uncertain rule rather\\nthan a complex but certain one, even if the true rule is deterministic and our\\nmodeling system has the ﬁdelity to accommodate a complex rule. For example, the\\nsimple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a rule\\nof the form, “Birds ﬂy, except for very young birds that have not yet learned to\\nﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\\nincluding the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\\ncommunicate, and after all of this eﬀort is still very brittle and prone to failure.\\n\\nGiven that we need a means of representing and reasoning about uncertainty,\\nit is not immediately obvious that probability theory can provide all of the tools\\nwe want for artiﬁcial intelligence applications. Probability theory was originally\\ndeveloped to analyze the frequencies of events. It is easy to see how probability\\ntheory can be used to study events like drawing a certain hand of cards in a\\ngame of poker. These kinds of events are often repeatable. When we say that\\nan outcome has a probability p of occurring, it means that if we repeated the\\nexperiment (e.g., draw a hand of cards) inﬁnitely many times, then proportion p\\nof the repetitions would result in that outcome. This kind of reasoning does not\\nseem immediately applicable to propositions that are not repeatable. If a doctor\\nanalyzes a patient and says that the patient has a 40% chance of having the ﬂu,\\nthis means something very diﬀerent—we can not make inﬁnitely many replicas of\\nthe patient, nor is there any reason to believe that diﬀerent replicas of the patient\\nwould present with the same symptoms yet have varying underlying conditions. In\\nthe case of the doctor diagnosing the patient, we use probability to represent a\\ndegree of belief, with 1 indicating absolute certainty that the patient has the ﬂu\\nand 0 indicating absolute certainty that the patient does not have the ﬂu. The\\nformer kind of probability, related directly to the rates at which events occur, is\\nknown as frequentist probability, while the latter, related to qualitative levels of\\ncertainty, is known as Bayesian probability.\\n\\nIf we list several properties that we expect common sense reasoning about\\nuncertainty to have, then the only way to satisfy those properties is to treat\\nBayesian probabilities as behaving exactly the same as frequentist probabilities.\\nFor example, if we want to compute the probability that a player will win a poker\\ngame given that she has a certain set of cards, we use exactly the same formulas\\nas when we compute the probability that a patient has a disease given that she\\n\\n54\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nhas certain symptoms. For more details about why a small set of common sense\\nassumptions implies that the same axioms must control both kinds of probability,\\nsee Ramsey (1926).\\n\\nProbability can be seen as the extension of logic to deal with uncertainty. Logic\\nprovides a set of formal rules for determining what propositions are implied to\\nbe true or false given the assumption that some other set of propositions is true\\nor false. Probability theory provides a set of formal rules for determining the\\nlikelihood of a proposition being true given the likelihood of other propositions.\\n\\n3.2 Random Variables\\n\\nA random variable is a variable that can take on diﬀerent values randomly. We\\ntypically denote the random variable itself with a lower case letter in plain typeface,\\nand the values it can take on with lower case script letters. For example, x1 and x2\\nare both possible values that the random variable x can take on. For vector-valued\\nvariables, we would write the random variable as x and one of its values as x. On\\nits own, a random variable is just a description of the states that are possible; it\\nmust be coupled with a probability distribution that speciﬁes how likely each of\\nthese states are.\\n\\nRandom variables may be discrete or continuous. A discrete random variable\\nis one that has a ﬁnite or countably inﬁnite number of states. Note that these\\nstates are not necessarily the integers; they can also just be named states that\\nare not considered to have any numerical value. A continuous random variable is\\nassociated with a real value.\\n\\n3.3 Probability Distributions\\n\\nA probability distribution is a description of how likely a random variable or\\nset of random variables is to take on each of its possible states. The way we\\ndescribe probability distributions depends on whether the variables are discrete or\\ncontinuous.\\n\\n3.3.1 Discrete Variables and Probability Mass Functions\\n\\nA probability distribution over discrete variables may be described using a proba-\\nbility mass function (PMF). We typically denote probability mass functions with a\\ncapital P . Often we associate each random variable with a diﬀerent probability\\n\\n55\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nmass function and the reader must infer which probability mass function to use\\nbased on the identity of the random variable, rather than the name of the function;\\nP\\n\\n( )x is usually not the same as\\n\\n( )y .\\n\\nP\\n\\nThe probability mass function maps from a state of a random variable to\\nthe probability of that random variable taking on that state. The probability\\nthat x = x is denoted as P (x), with a probability of 1 indicating that x = x is\\ncertain and a probability of 0 indicating that x = x is impossible. Sometimes\\nto disambiguate which PMF to use, we write the name of the random variable\\nexplicitly: P (x = x). Sometimes we deﬁne a variable ﬁrst, then use ∼ notation to\\nspecify which distribution it follows later: x ∼ P (x .)\\n\\nProbability mass functions can act on many variables at the same time. Such\\na probability distribution over many variables is known as a joint probability\\ndistribution. P (x = x, y = y) denotes the probability that x = x and y = y\\nsimultaneously. We may also write\\n\\nfor brevity.\\n\\nP x, y\\n\\n(\\n\\n)\\n\\nTo be a probability mass function on a random variable x, a function P must\\n\\nsatisfy the following properties:\\n\\nP\\n\\n• The domain of must be the set of all possible states of x.\\n• ∀x\\n0 \\n\\n,∈ x 0 ≤ P (x) ≤ 1. An impossible event has probability and no state can\\nbe less probable than that. Likewise, an event that is guaranteed to happen\\nhas probability , and no state can have a greater chance of occurring.\\n\\n1\\n\\n• \\ue050x∈x P (x) = 1. We refer to this property as being normalized. Without this\\n\\nproperty, we could obtain probabilities greater than one by computing the\\nprobability of one of many events occurring.\\n\\nFor example, consider a single discrete random variable x with k diﬀerent states.\\nx—that is, make each of its states equally\\n\\nuniform distribution\\n\\nWe can place a\\nlikely—by setting its probability mass function to\\n\\non\\n\\nP\\n\\n( = x\\n\\nx\\ni) =\\n\\n1\\nk\\n\\n(3.1)\\n\\nfor all i. We can see that this ﬁts the requirements for a probability mass function.\\nThe value 1\\n\\nis a positive integer. We also see that\\n\\nk is positive because\\n\\nk\\n\\n\\ue058i\\n\\nP\\n\\n( = x\\n\\nx\\n\\ni) =\\ue058i\\n\\n1\\nk\\n\\n=\\n\\nk\\nk\\n\\n= 1,\\n\\n(3.2)\\n\\nso the distribution is properly normalized.\\n\\n56\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.3.2 Continuous Variables and Probability Density Functions\\n\\nWhen working with continuous random variables, we describe probability dis-\\ntributions using a probability density function (PDF) rather than a probability\\nmass function. To be a probability density function, a function p must satisfy the\\nfollowing properties:\\n\\np\\n\\n• The domain of must be the set of all possible states of x.\\n• ∀x\\n\\n0≥ Note that we do not require ( ) \\np x\\n\\n1≤ .\\n\\n.\\n\\n∈ x ( ) \\n, p x\\n( ) = 1.\\n\\n• \\ue052 p x dx\\n\\nA probability density function p(x) does not give the probability of a speciﬁc\\nstate directly, instead the probability of landing inside an inﬁnitesimal region with\\nvolume\\n\\nis given by\\n\\np x δx\\n\\n( )\\n\\nδx\\n\\n.\\n\\nWe can integrate the density function to ﬁnd the actual probability mass of a\\nset of points. Speciﬁcally, the probability that x lies in some set S is given by the\\nintegral of p(x) over that set. In the univariate example, the probability that x\\nlies in the interval\\n\\nis given by\\n\\n.\\np x dx\\n\\n[\\n]a, b\\n\\n( )\\n\\nFor an example of a probability density function corresponding to a speciﬁc\\nprobability density over a continuous random variable, consider a uniform distribu-\\ntion on an interval of the real numbers. We can do this with a function u (x; a, b),\\nwhere a and b are the endpoints of the interval, with b > a. The “;” notation means\\n“parametrized by”; we consider x to be the argument of the function, while a and\\nb are parameters that deﬁne the function. To ensure that there is no probability\\nmass outside the interval, we say u(x; a, b) = 0 for all x \\ue036∈ [a, b]\\n. Within a, b],\\n. We can see that this is nonnegative everywhere. Additionally, it\\nu x a, b\\n( ;\\nintegrates to 1. We often denote that x follows the uniform distribution on [a, b]\\nby writing x\\n\\n) = 1\\nb a−\\n∼ U a, b\\n)\\n\\n(\\n\\n[\\n\\n.\\n\\n\\ue052[\\n\\n]a,b\\n\\n3.4 Marginal Probability\\n\\nSometimes we know the probability distribution over a set of variables and we want\\nto know the probability distribution over just a subset of them. The probability\\ndistribution over the subset is known as the marginal probability distribution.\\n\\nFor example, suppose we have discrete random variables x and y , and we know\\n\\nP ,(x y . We can ﬁnd\\n\\n)\\n\\nx with the\\n\\nsum rule\\n\\n:\\n\\nx, P\\n\\n( = x\\n\\nx\\n\\nP\\n\\n( = x\\n\\nx,\\n\\ny =  )\\ny .\\n\\n(3.3)\\n\\nP ( )\\n∀ ∈x\\n\\n) =\\ue058y\\n\\n57\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe name “marginal probability” comes from the process of computing marginal\\nprobabilities on paper. When the values of P(x y, ) are written in a grid with\\ndiﬀerent values of x in rows and diﬀerent values of y in columns, it is natural to\\nsum across a row of the grid, then write P( x) in the margin of the paper just to\\nthe right of the row.\\n\\nFor continuous variables, we need to use integration instead of summation:\\n\\np x( ) =\\ue05a p x, y dy.\\n\\n(\\n\\n)\\n\\n(3.4)\\n\\n3.5 Conditional Probability\\n\\nIn many cases, we are interested in the probability of some event, given that some\\nother event has happened. This is called a conditional probability. We denote\\nthe conditional probability that y = y given x = x as P (y = y | x = x ). This\\nconditional probability can be computed with the formula\\n\\nP\\n\\n( = y\\n\\ny\\n\\n| x =  ) =\\n\\nx\\n\\nP\\n\\n( = y\\n\\ny,\\n\\nx =  )\\nx\\nx\\n)\\n\\nP\\n\\n( = x\\n\\n.\\n\\n(3.5)\\n\\nThe conditional probability is only deﬁned when P (x = x) > 0. We cannot compute\\nthe conditional probability conditioned on an event that never happens.\\n\\nIt is important not to confuse conditional probability with computing what\\nwould happen if some action were undertaken. The conditional probability that\\na person is from Germany given that they speak German is quite high, but if\\na randomly selected person is taught to speak German, their country of origin\\ndoes not change. Computing the consequences of an action is called making an\\nintervention query. Intervention queries are the domain of causal modeling, which\\nwe do not explore in this book.\\n\\n3.6 The Chain Rule of Conditional Probabilities\\n\\nAny joint probability distribution over many random variables may be decomposed\\ninto conditional distributions over only one variable:\\n\\nP (x(1), . . . , x ( )n ) = \\n\\n| x(1), . . . , x (\\nproduct rule of probability. It\\nfollows immediately from the deﬁnition of conditional probability in Eq. 3.5. For\\n\\nThis observation is known as the\\n\\n(P x(1) )Πn\\n\\ni=2P (x( )i\\n\\nchain rule\\n\\ni− ).\\n\\n(3.6)\\n\\nor\\n\\n1)\\n\\n58\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nexample, applying the deﬁnition twice, we get\\n\\nP ,\\n\\n(a b c) =\\n\\n,\\n\\nP ,\\n\\n(b c) =\\n\\nP ,\\n\\n(a b c) =\\n\\n,\\n\\nP\\n\\nP\\n\\nP\\n\\n(a b|\\n(b c| )\\n(a b|\\n\\nc)\\n, P ,\\n\\n(b c)\\n\\n( )P c\\n\\n, Pc)\\n\\n(b c| )\\n\\n( )P c .\\n\\n3.7 Independence and Conditional Independence\\n\\nTwo random variables x and y are independent if their probability distribution can\\nbe expressed as a product of two factors, one involving only x and one involving\\nonly y:\\n\\n∀ ∈x\\n\\nx, y\\n\\n∈ y\\n\\n, p\\n\\nx\\n( = \\n\\nx, y\\n\\n= ) =  ( =\\n\\np x\\n\\ny\\n\\nx) ( =  )\\ny .\\n\\np y\\n\\n(3.7)\\n\\nTwo random variables x and y are conditionally independent given a random\\nvariable z if the conditional probability distribution over x and y factorizes in this\\nway for every value of z:\\n\\n∀ ∈x\\n\\nx, y\\n\\n, z∈ y ∈ z, p\\n\\n( =x\\n\\nx,\\n\\ny = \\n\\ny\\n\\n| z =  ) =  ( = x\\n\\np\\n\\nz\\n\\nx\\n\\n| z =  ) ( = y\\n\\nz p\\n\\ny\\n\\n| z = )\\nz .\\n(3.8)\\n\\nWe can denote independence and conditional independence with compact\\nnotation: x y⊥ means that x and y are independent, while x y z⊥ | means that x\\nand y are conditionally independent given z.\\n\\n3.8 Expectation, Variance and Covariance\\n\\nor\\n\\nThe expectation\\nof some function f( x) with respect to a probability\\ndistribution P (x) is the average or mean value that f takes on when x is drawn\\nfrom . For discrete variables this can be computed with a summation:\\n\\nexpected value\\n\\nP\\n\\nEx∼P[ (f x)] =\\ue058x\\n\\nP x f x ,\\n( ) ( )\\n\\n(3.9)\\n\\nwhile for continuous variables, it is computed with an integral:\\n\\nEx∼p[ ( )] =\\n\\nf x\\n\\n\\ue05a p x f x dx.\\n\\n( ) ( )\\n\\n59\\n\\n(3.10)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nWhen the identity of the distribution is clear from the context, we may simply\\nwrite the name of the random variable that the expectation is over, as in Ex[f (x)].\\nIf it is clear which random variable the expectation is over, we may omit the\\nsubscript entirely, as in E[f (x)]. By default, we can assume that E [·] averages over\\nthe values of all the random variables inside the brackets. Likewise, when there is\\nno ambiguity, we may omit the square brackets.\\n\\nExpectations are linear, for example,\\n\\nE x[\\n\\nαf x\\n\\n( ) + ( )] = \\n\\nβg x\\n\\nαEx [ ( )] +\\n\\nf x\\n\\nβEx[ ( )]\\ng x ,\\n\\n(3.11)\\n\\nwhen\\n\\nα\\n\\nand\\n\\nβ\\n\\nare not dependent on .\\nx\\n\\nThe variance gives a measure of how much the values of a function of a random\\nvariable x vary as we sample diﬀerent value of x from its probability distribution:\\n\\nVar( ( )) = \\n\\nf x\\n\\nE\\ue068( ( )\\nf x − E f x 2\\ue069.\\n\\n[ ( )])\\n\\n(3.12)\\n\\nWhen the variance is low, the values of f (x) cluster near their expected value. The\\nsquare root of the variance is known as the standard deviation.\\n\\nThe covariance gives some sense of how much two values are linearly related to\\n\\neach other, as well as the scale of these variables:\\n\\nCov( ( )\\n\\nf x , g y\\n\\n( )) = \\n\\nE f x − E f x\\n[( ( )\\n\\n[ ( )]) ( ( )\\n\\ng y − E g y\\n\\n[ ( )])]\\n\\n.\\n\\n(3.13)\\n\\nHigh absolute values of the covariance mean that the values change very much\\nand are both far from their respective means at the same time. If the sign of the\\ncovariance is positive, then both variables tend to take on relatively high values\\nsimultaneously. If the sign of the covariance is negative, then one variable tends to\\ntake on a relatively high value at the times that the other takes on a relatively low\\nvalue and vice versa. Other measures such as correlation normalize the contribution\\nof each variable in order to measure only how much the variables are related, rather\\nthan also being aﬀected by the scale of the separate variables.\\n\\nThe notions of covariance and dependence are related, but are in fact distinct\\nconcepts. They are related because two variables that are independent have zero\\ncovariance, and two variables that have non-zero covariance are dependent. How-\\never, independence is a distinct property from covariance. For two variables to have\\nzero covariance, there must be no linear dependence between them. Independence\\nis a stronger requirement than zero covariance, because independence also excludes\\nnonlinear relationships. It is possible for two variables to be dependent but have\\nzero covariance. For example, suppose we ﬁrst sample a real number x from a\\nuniform distribution over the interval [−1 ,1]. We next sample a random variable\\n\\n60\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\ns. With probability 1\\n2, we choose the value of s to be 1. Otherwise, we choose\\nthe value of s to be − 1. We can then generate a random variable y by assigning\\ny = sx . Clearly, x and y are not independent, because x completely determines\\nthe magnitude of\\n\\n. However,\\n\\n.\\n) = 0\\n\\nCov(\\n\\nx, y\\n\\ny\\n\\nThe covariance matrix of a random vector x ∈ Rn is an n n× matrix, such that\\n(3.14)\\n\\nCov( )x i,j = Cov(xi, xj ).\\n\\nThe diagonal elements of the covariance give the variance:\\n\\nCov(xi , xi) = Var(xi).\\n\\n(3.15)\\n\\n3.9 Common Probability Distributions\\n\\nSeveral simple probability distributions are useful in many contexts in machine\\nlearning.\\n\\n3.9.1 Bernoulli Distribution\\n\\nBernoulli\\n\\nThe\\ndistribution is a distribution over a single binary random variable.\\nIt is controlled by a single parameter φ ∈ [0, 1], which gives the probability of the\\nrandom variable being equal to 1. It has the following properties:\\n\\nP\\n\\nx\\n( = 1) = \\n\\nφ\\n\\nP\\n\\nx\\n( = 0) = 1\\n) =  x (1\\nx\\n\\nφ\\n\\nP\\n\\n( = x\\n\\nEx [ ] = \\n\\nx\\n\\nφ\\n\\n−\\n)− φ 1−x\\nφ\\n\\nVar x( ) =  (1\\n\\nx\\n\\nφ − φ\\n\\n)\\n\\n(3.16)\\n\\n(3.17)\\n\\n(3.18)\\n\\n(3.19)\\n\\n(3.20)\\n\\n3.9.2 Multinoulli Distribution\\n\\nmultinoulli\\n\\nThe\\ncategorical distribution is a distribution over a single discrete\\nvariable with k diﬀerent states, where k is ﬁnite1 . The multinoulli distribution is\\n\\nor\\n\\n1 “Multinoulli” is a term that was recently coined by Gustavo Lacerdo and popularized by\\nMurphy (2012). The multinoulli distribution is a special case of the\\ndistribution. A\\nmultinomial distribution is the distribution over vectors in {0, . . . , n}k representing how many\\ntimes each of the k categories is visited when n samples are drawn from a multinoulli distribution.\\nMany texts use the term “multinomial” to refer to multinoulli distributions without clarifying\\nthat they refer only to the\\n\\nmultinomial\\n\\nn = 1\\n\\ncase.\\n\\n61\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nparametrized by a vector p ∈ [0, 1]k−1 , where p i gives the probability of the i-th\\nstate. The ﬁnal, k-th state’s probability is given by 1− 1\\ue03e p. Note that we must\\nconstrain 1 \\ue03ep ≤ 1. Multinoulli distributions are often used to refer to distributions\\nover categories of objects, so we do not usually assume that state 1 has numerical\\nvalue 1, etc. For this reason, we do not usually need to compute the expectation\\nor variance of multinoulli-distributed random variables.\\n\\nThe Bernoulli and multinoulli distributions are suﬃcient to describe any distri-\\nbution over their domain. This is because they model discrete variables for which\\nit is feasible to simply enumerate all of the states. When dealing with continuous\\nvariables, there are uncountably many states, so any distribution described by a\\nsmall number of parameters must impose strict limits on the distribution.\\n\\n3.9.3 Gaussian Distribution\\n\\nThe most commonly used distribution over real numbers is the normal distribution,\\nalso known as the Gaussian distribution:\\n\\nN ( ;x µ, σ2) =\\ue072 1\\n\\n2πσ2 exp\\ue012−\\n\\n1\\nx\\n2σ2 (\\n\\nµ− 2\\ue013 .\\n\\n)\\n\\n(3.21)\\n\\nSee Fig. 3.1 for a plot of the density function.\\nThe two parameters µ ∈ R and σ ∈ (0,∞ ) control the normal distribution.\\nThe parameter µ gives the coordinate of the central peak. This is also the mean of\\nthe distribution: E[x] = µ. The standard deviation of the distribution is given by\\nσ, and the variance by σ2.\\n\\nWhen we evaluate the PDF, we need to square and invert σ. When we need to\\nfrequently evaluate the PDF with diﬀerent parameter values, a more eﬃcient way\\nof parametrizing the distribution is to use a parameter β ∈ (0 ,∞) to control the\\nprecision or inverse variance of the distribution:\\n\\nN ( ;x µ, β−1) =\\ue072 β\\n\\n2π\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\nβ x\\n\\n( − )2\\ue013 .\\n\\nµ\\n\\n(3.22)\\n\\nNormal distributions are a sensible choice for many applications. In the absence\\nof prior knowledge about what form a distribution over the real numbers should\\ntake, the normal distribution is a good default choice for two major reasons.\\n\\nFirst, many distributions we wish to model are truly close to being normal\\ndistributions. The central limit theorem shows that the sum of many independent\\nrandom variables is approximately normally distributed. This means that in\\n\\n62\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe normal distribution\\n\\nMaximum at x ¹=\\n\\nInflection points at \\n     x ¹ ¾\\n\\n= §\\n\\n−1.5\\n\\n−1.0\\n\\n−0.5\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n)\\nx\\n(\\np\\n\\n0.40\\n\\n0.35\\n\\n0.30\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.10\\n\\n0.05\\n\\n0.00\\n\\n−2.0\\n\\nFigure 3.1: The normal distribution: The normal distribution N (x;µ, σ 2) exhibits a classic\\n“bell curve” shape, with the x coordinate of its central peak given by µ, and the width\\nof its peak controlled by σ. In this example, we depict the standard normal distribution,\\nwith\\n\\nσ = 1\\n\\nµ = 0\\n\\nand\\n\\n.\\n\\nx\\n\\npractice, many complicated systems can be modeled successfully as normally\\ndistributed noise, even if the system can be decomposed into parts with more\\nstructured behavior.\\n\\nSecond, out of all possible probability distributions with the same variance,\\nthe normal distribution encodes the maximum amount of uncertainty over the\\nreal numbers. We can thus think of the normal distribution as being the one that\\ninserts the least amount of prior knowledge into a model. Fully developing and\\njustifying this idea requires more mathematical tools, and is postponed to Sec.\\n19.4.2.\\n\\nThe normal distribution generalizes to Rn, in which case it is known as the\\nmultivariate normal distribution. It may be parametrized with a positive deﬁnite\\nsymmetric matrix\\n\\n:Σ\\n\\nx µ, Σ \\ue073\\n\\n) =\\n\\nN ( ;\\n\\n1\\n\\n(2 )π ndet(\\n\\n)Σ\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\nx µ− \\ue03eΣ−1 (\\n(\\n\\n)\\n\\nx µ− \\ue013 .\\n\\n)\\n\\n(3.23)\\n\\nThe parameter µ still gives the mean of the distribution, though now it is\\nvector-valued. The parameter Σ gives the covariance matrix of the distribution.\\nAs in the univariate case, when we wish to evaluate the PDF several times for\\n\\n63\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nmany diﬀerent values of the parameters, the covariance is not a computationally\\neﬃcient way to parametrize the distribution, since we need to invert Σ to evaluate\\nthe PDF. We can instead use a precision matrix β:\\n\\nN ( ;x µ β, −1) =\\ue073det( )β\\n\\n(2 )π n\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\n( − )\\ue013 .\\nx µ− \\ue03eβ x µ\\n(\\n\\n)\\n\\n(3.24)\\n\\nWe often ﬁx the covariance matrix to be a diagonal matrix. An even simpler\\nversion is the isotropic Gaussian distribution, whose covariance matrix is a scalar\\ntimes the identity matrix.\\n\\n3.9.4 Exponential and Laplace Distributions\\n\\nIn the context of deep learning, we often want to have a probability distribution\\nwith a sharp point at x = 0. To accomplish this, we can use the exponential\\ndistribution:\\n\\n(3.25)\\nThe exponential distribution uses the indicator function 1x≥0 to assign probability\\nzero to all negative values of\\n\\np x λ\\n( ; ) =  1x≥0exp (\\n\\n)−λx .\\n\\n.x\\n\\nλ\\n\\nA closely related probability distribution that allows us to place a sharp peak\\n\\nof probability mass at an arbitrary point\\n\\nµ\\n\\nis the\\n\\nLaplace distribution\\n\\nLaplace( ;\\n\\nx µ, γ\\n\\n) =\\n\\n1\\n2γ\\n\\nexp\\ue012−| − |\\nγ \\ue013.\\n\\nµ\\n\\nx\\n\\n(3.26)\\n\\n3.9.5 The Dirac Distribution and Empirical Distribution\\n\\nIn some cases, we wish to specify that all of the mass in a probability distribution\\nclusters around a single point. This can be accomplished by deﬁning a PDF using\\nthe Dirac delta function,\\n\\nδ x( )\\n\\n:\\n\\n( ) =  ( − )\\np x\\nµ .\\n\\nδ x\\n\\n(3.27)\\n\\nThe Dirac delta function is deﬁned such that it is zero-valued everywhere except\\n0, yet integrates to 1. The Dirac delta function is not an ordinary function that\\nassociates each value x with a real-valued output, instead it is a diﬀerent kind of\\nmathematical object called a generalized function that is deﬁned in terms of its\\nproperties when integrated. We can think of the Dirac delta function as being the\\nlimit point of a series of functions that put less and less mass on all points other\\nthan .µ\\n\\n64\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nBy deﬁning p(x) to be δ shifted by −µ we obtain an inﬁnitely narrow and\\n\\ninﬁnitely high peak of probability mass where\\n\\nx\\n\\nµ= \\n\\n.\\n\\nA common use of the Dirac delta distribution is as a component of an empirical\\n\\ndistribution,\\n\\nˆp( ) =x\\n\\n1\\nm\\n\\nm\\ue058i=1\\n\\nδ(x x− ( )i )\\n\\n(3.28)\\n\\n1\\nm on each of the m points x(1), . . . , x (\\n\\n)m forming\\nwhich puts probability mass\\na given data set or collection of samples. The Dirac delta distribution is only\\nnecessary to deﬁne the empirical distribution over continuous variables. For discrete\\nvariables, the situation is simpler: an empirical distribution can be conceptualized\\nas a multinoulli distribution, with a probability associated to each possible input\\nvalue that is simply equal to the empirical frequency of that value in the training\\nset.\\n\\nWe can view the empirical distribution formed from a dataset of training\\nexamples as specifying the distribution that we sample from when we train a model\\non this dataset. Another important perspective on the empirical distribution is\\nthat it is the probability density that maximizes the likelihood of the training\\ndata (see Sec. 5.5). Many machine learning algorithms can be conﬁgured to have\\narbitrarily high capacity. If given enough capacity, these algorithms will simply\\nlearn the empirical distribution. This is a bad outcome because the model does not\\ngeneralize at all and assigns inﬁnitesimal probability to any point in space that did\\nnot occur in the training set. A central problem in machine learning is studying\\nhow to limit the capacity of a model in a way that prevents it from simply learning\\nthe empirical distribution while also allowing it to learn complicated functions.\\n\\n3.9.6 Mixtures of Distributions\\n\\nIt is also common to deﬁne probability distributions by combining other simpler\\nprobability distributions. One common way of combining distributions is to\\nconstruct a mixture distribution. A mixture distribution is made up of several\\ncomponent distributions. On each trial, the choice of which component distribution\\ngenerates the sample is determined by sampling a component identity from a\\nmultinoulli distribution:\\n\\nP ( ) =x \\ue058i\\n\\nP\\n\\nc\\n( = \\n\\ni P\\n)\\n\\nx c|\\n(\\n\\n=\\n\\ni\\n)\\n\\n(3.29)\\n\\nwhere\\n\\nP ( )\\n\\nc is the multinoulli distribution over component identities.\\n\\n65\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nWe have already seen one example of a mixture distribution: the empirical\\ndistribution over real-valued variables is a mixture distribution with one Dirac\\ncomponent for each training example.\\n\\nThe mixture model is one simple strategy for combining probability distributions\\nto create a richer distribution. In Chapter 16, we explore the art of building complex\\nprobability distributions from simple ones in more detail.\\n\\nlatent variable\\n\\nThe mixture model allows us to brieﬂy glimpse a concept that will be of\\nparamount importance later—the\\n. A latent variable is a random\\nvariable that we cannot observe directly. The component identity variable c of the\\nmixture model provides an example. Latent variables may be related to x through\\nthe joint distribution, in this case, P (x c, ) = P (x c|\\n)P(c). The distribution P (c)\\nover the latent variable and the distribution P (x c| ) relating the latent variables\\nto the visible variables determines the shape of the distribution P ( x) even though\\nit is possible to describe P (x) without reference to the latent variable. Latent\\nvariables are discussed further in Sec. 16.5.\\n\\nA very powerful and common type of mixture model is the Gaussian mixture\\nmodel, in which the components p (x | c = i) are Gaussians. Each component has\\na separately parametrized mean µ ( )i and covariance Σ ( )i . Some mixtures can have\\nmore constraints. For example, the covariances could be shared across components\\nvia the constraint Σ( )i = Σ∀i. As with a single Gaussian distribution, the mixture\\nof Gaussians might constrain the covariance matrix for each component to be\\ndiagonal or isotropic.\\n\\nIn addition to the means and covariances, the parameters of a Gaussian mixture\\nspecify the prior probability α i = P (c = i) given to each component i. The word\\n“prior” indicates that it expresses the model’s beliefs about c before it has observed\\nx. By comparison, P(c | x) is a posterior probability, because it is computed after\\nobservation of x. A Gaussian mixture model is a universal approximator of\\ndensities, in the sense that any smooth density can be approximated with any\\nspeciﬁc, non-zero amount of error by a Gaussian mixture model with enough\\ncomponents.\\n\\nFig. 3.2 shows samples from a Gaussian mixture model.\\n\\n3.10 Useful Properties of Common Functions\\n\\nCertain functions arise often while working with probability distributions, especially\\nthe probability distributions used in deep learning models.\\n\\n66\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n2\\nx\\n\\nx1\\n\\nFigure 3.2: Samples from a Gaussian mixture model. In this example, there are three\\ncomponents. From left to right, the ﬁrst component has an isotropic covariance matrix,\\nmeaning it has the same amount of variance in each direction. The second has a diagonal\\ncovariane matrix, meaning it can control the variance separately along each axis-aligned\\ndirection. This example has more variance along the x2 axis than along the x1 axis. The\\nthird component has a full-rank covariance matrix, allowing it to control the variance\\nseparately along an abitrary basis of directions.\\n\\nOne of these functions is the logistic sigmoid:\\n\\nσ x( ) =\\n\\n1\\n\\n1 + exp(\\n\\n.\\n)−x\\n\\n(3.30)\\n\\nThe logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli\\ndistribution because its range is (0, 1), which lies within the valid range of values\\nfor the φ parameter. See Fig. 3.3 for a graph of the sigmoid function. The sigmoid\\nfunction saturates when its argument is very positive or very negative, meaning\\nthat the function becomes very ﬂat and insensitive to small changes in its input.\\n\\nAnother commonly encountered function is the\\n\\nsoftplus\\n\\nfunction (Dugas\\n\\net al.,\\n\\n2001):\\n\\nζ x\\nx .\\n( ) = log (1 + exp( ))\\n\\n(3.31)\\n\\nThe softplus function can be useful for producing the β or σ parameter of a normal\\ndistribution because its range is (0,∞). It also arises commonly when manipulating\\nexpressions involving sigmoids. The name of the softplus function comes from the\\nfact that it is a smoothed or “softened” version of\\n\\nx+ = max(0 ), x .\\n\\n(3.32)\\n\\nSee Fig. 3.4 for a graph of the softplus function.\\n\\n67\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe logistic sigmoid function\\n\\n)\\nx\\n(\\n¾\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n−10\\n\\n−5\\n\\n0\\nx\\n\\n5\\n\\n10\\n\\nFigure 3.3: The logistic sigmoid function.\\n\\nThe softplus function\\n\\n10\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n)\\nx\\n(\\n³\\n\\n0\\n−10\\n\\n−5\\n\\n0\\n\\nx\\n\\n5\\n\\n10\\n\\nFigure 3.4: The softplus function.\\n\\n68\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe following properties are all useful enough that you may wish to memorize\\n\\nthem:\\n\\nσ x( ) =\\n\\nexp( )x\\nx\\n\\nexp( ) + exp(0)\\n\\nd\\ndx\\n\\nσ x\\n\\n( ) =  (\\n\\n( ) =  ( )(1 − ( ))\\nσ x\\nσ x\\n− σ x\\n1\\nσ x\\n\\nσ −x\\n)\\n−ζ −x\\n(\\nζ x\\nσ x\\n( ) =  ( )\\n\\nlog ( ) = \\n\\n)\\n\\nd\\ndx\\n\\n1)\\n\\n(0,\\n\\n, σ\\n\\n\\ue012 x\\n1 − x\\ue013\\n∀ ∈x\\n∀x > 0, ζ−1( ) = log (exp( )\\nx −\\n\\n−1 ( ) = log\\n\\n1)\\n\\nx\\n\\nx\\n\\nζ x( ) =\\ue05a x\\n\\n−∞\\nx\\nζ\\n\\nσ y dy\\n\\n( )\\n\\n(3.33)\\n\\n(3.34)\\n\\n(3.35)\\n\\n(3.36)\\n\\n(3.37)\\n\\n(3.38)\\n\\n(3.39)\\n\\n(3.40)\\n\\n(3.41)\\nThe function σ−1(x) is called the logit in statistics, but this term is more rarely\\nused in machine learning. The ﬁnal property provides extra justiﬁcation for the\\nname “softplus,” since x+ − x− = x.\\n\\n( ) − (− ) = \\nζ x\\nx\\n\\n3.11 Bayes’ Rule\\n\\nWe often ﬁnd ourselves in a situation where we know P (y x| ) and need to know\\nP (x y|\\n). Fortunately, if we also know P (x), we can compute the desired quantity\\nusing Bayes’ rule:\\n\\nP (\\n\\nx y|\\n\\n) =\\n\\nP\\n\\nP( )x\\n\\ny x|\\n(\\n\\n)\\n\\n.\\n\\n(3.42)\\n\\nP ( )y\\n\\nP ( ) =y \\ue050x P\\n\\n(y |\\n\\nNote that while P (y) appears in the formula, it is usually feasible to compute\\n\\nx P x\\n\\n( ), so we do not need to begin with knowledge of\\n\\n)\\n\\nP\\n\\n(y .)\\n\\nBayes’ rule is straightforward to derive from the deﬁnition of conditional\\nprobability, but it is useful to know the name of this formula since many texts\\nrefer to it by name. It is named after the Reverend Thomas Bayes, who ﬁrst\\ndiscovered a special case of the formula. The general version presented here was\\nindependently discovered by Pierre-Simon Laplace.\\n\\n69\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.12 Technical Details of Continuous Variables\\n\\nA proper formal understanding of continuous random variables and probability\\ndensity functions requires developing probability theory in terms of a branch of\\nmathematics known as measure theory. Measure theory is beyond the scope of\\nthis textbook, but we can brieﬂy sketch some of the issues that measure theory is\\nemployed to resolve.\\n\\nIn Sec. 3.3.2, we saw that the probability of a continuous vector-valued x lying\\nin some set S is given by the integral of p(x ) over the set S. Some choices of set S\\ncan produce paradoxes. For example, it is possible to construct two sets S1 and\\nS2 such that p(x ∈ S1) + p(x ∈ S 2) > 1 but S1 ∩ S2 = ∅. These sets are generally\\nconstructed making very heavy use of the inﬁnite precision of real numbers, for\\nexample by making fractal-shaped sets or sets that are deﬁned by transforming\\nthe set of rational numbers2 . One of the key contributions of measure theory is to\\nprovide a characterization of the set of sets that we can compute the probability\\nof without encountering paradoxes. In this book, we only integrate over sets with\\nrelatively simple descriptions, so this aspect of measure theory never becomes a\\nrelevant concern.\\n\\nFor our purposes, measure theory is more useful for describing theorems that\\napply to most points in Rn but do not apply to some corner cases. Measure theory\\nprovides a rigorous way of describing that a set of points is negligibly small. Such\\na set is said to have “measure zero.” We do not formally deﬁne this concept in this\\ntextbook. However, it is useful to understand the intuition that a set of measure\\nzero occupies no volume in the space we are measuring. For example, within R2 , a\\nline has measure zero, while a ﬁlled polygon has positive measure. Likewise, an\\nindividual point has measure zero. Any union of countably many sets that each\\nhave measure zero also has measure zero (so the set of all the rational numbers\\nhas measure zero, for instance).\\n\\nAnother useful term from measure theory is “almost everywhere.” A property\\nthat holds almost everywhere holds throughout all of space except for on a set of\\nmeasure zero. Because the exceptions occupy a negligible amount of space, they\\ncan be safely ignored for many applications. Some important results in probability\\ntheory hold for all discrete values but only hold “almost everywhere” for continuous\\nvalues.\\n\\nAnother technical detail of continuous variables relates to handling continuous\\nrandom variables that are deterministic functions of one another. Suppose we have\\ntwo random variables, x and y, such that y = g(x), where g is an invertible, con-\\n\\n2The Banach-Tarski theorem provides a fun example of such sets.\\n\\n70\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\ntinuous, diﬀerentiable transformation. One might expect that py(y ) =p x(g−1(y )).\\nThis is actually not the case.\\n\\nAs a simple example, suppose we have scalar random variables x and y. Suppose\\nIf we use the rule p y(y) = p x(2 y) then py will be 0\\non this interval. This means\\n\\ny = x\\neverywhere except the interval [0 , 1\\n2 ]\\n\\n2 and x ∼ U(0,1).\\n\\n, and it will be\\n\\n1\\n\\n\\ue05a py ( ) =\\n\\ny dy\\n\\n1\\n2\\n\\n,\\n\\n(3.43)\\n\\nwhich violates the deﬁnition of a probability distribution.\\n\\nThis common mistake is wrong because it fails to account for the distortion\\nof space introduced by the function g. Recall that the probability of x lying in\\nan inﬁnitesimally small region with volume δx is given by p( x)δx. Since g can\\nexpand or contract space, the inﬁnitesimal volume surrounding x in x space may\\nhave diﬀerent volume in\\n\\nspace.\\n\\ny\\n\\nTo see how to correct the problem, we return to the scalar case. We need to\\n\\npreserve the property\\n\\nSolving from this, we obtain\\n\\n|py( ( ))\\n\\ng x dy|\\n\\n=\\n\\n|p x( )x dx .|\\n\\nor equivalently\\n\\nIn higher dimensions, the derivative generalizes to the determinant of the Jacobian\\nmatrix—the matrix with J i,j = ∂xi\\n∂yj\\n\\n. Thus, for real-valued vectors\\n\\nand ,\\ny\\n\\nx\\n\\npy( ) = \\n\\ny\\n\\npx ( ) = \\n\\nx\\n\\n∂x\\n\\n∂g x( )\\n\\npy( ( ))\\n\\npx (g−1( ))y\\n\\n\\ue00c\\ue00c\\ue00c\\ue00c\\n∂y\\ue00c\\ue00c\\ue00c\\ue00c\\ng x \\ue00c\\ue00c\\ue00c\\ue00c\\n∂x \\ue00c\\ue00c\\ue00c\\ue00c .\\ng x \\ue00c\\ue00c\\ue00c\\ue00cdet\\ue012∂g( )x\\n∂x \\ue013\\ue00c\\ue00c\\ue00c\\ue00c .\\n\\npx ( ) = \\n\\nx\\n\\np y( ( ))\\n\\n3.13 Information Theory\\n\\nInformation theory is a branch of applied mathematics that revolves around\\nquantifying how much information is present in a signal. It was originally invented\\nto study sending messages from discrete alphabets over a noisy channel, such as\\ncommunication via radio transmission. In this context, information theory tells how\\nto design optimal codes and calculate the expected length of messages sampled from\\n\\n71\\n\\n(3.44)\\n\\n(3.45)\\n\\n(3.46)\\n\\n(3.47)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nspeciﬁc probability distributions using various encoding schemes. In the context of\\nmachine learning, we can also apply information theory to continuous variables\\nwhere some of these message length interpretations do not apply. This ﬁeld is\\nfundamental to many areas of electrical engineering and computer science. In this\\ntextbook, we mostly use a few key ideas from information theory to characterize\\nprobability distributions or quantify similarity between probability distributions.\\nFor more detail on information theory, see Cover and Thomas (2006) or MacKay\\n(2003).\\n\\nThe basic intuition behind information theory is that learning that an unlikely\\nevent has occurred is more informative than learning that a likely event has\\noccurred. A message saying “the sun rose this morning” is so uninformative as\\nto be unnecessary to send, but a message saying “there was a solar eclipse this\\nmorning” is very informative.\\n\\nWe would like to quantify information in a way that formalizes this intuition.\\n\\nSpeciﬁcally,\\n\\n• Likely events should have low information content, and in the extreme case,\\nevents that are guaranteed to happen should have no information content\\nwhatsoever.\\n\\n• Less likely events should have higher information content.\\n• Independent events should have additive information. For example, ﬁnding\\nout that a tossed coin has come up as heads twice should convey twice as\\nmuch information as ﬁnding out that a tossed coin has come up as heads\\nonce.\\n\\nIn order to satisfy all three of these properties, we deﬁne the self-information\\n\\nof an event x\\n\\n= x\\n\\nto be\\n\\nI x\\n( ) = \\n\\nlog−\\n\\nP x .\\n( )\\n\\n(3.48)\\n\\nIn this book, we always use log to mean the natural logarithm, with base e. Our\\ndeﬁnition of I(x) is therefore written in units of\\n. One nat is the amount of\\ninformation gained by observing an event of probability 1\\ne . Other texts use base-2\\nlogarithms and units called\\nshannons\\n; information measured in bits is just\\na rescaling of information measured in nats.\\n\\nnats\\n\\nbits\\n\\nor\\n\\nWhen x is continuous, we use the same deﬁnition of information by analogy,\\nbut some of the properties from the discrete case are lost. For example, an event\\nwith unit density still has zero information, despite not being an event that is\\nguaranteed to occur.\\n\\n72\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n0.4\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\ns\\nt\\na\\nn\\n \\nn\\n\\ni\\n \\ny\\np\\no\\nr\\nt\\nn\\ne\\n \\nn\\no\\nn\\nn\\na\\nh\\nS\\n\\n0.0\\n\\n0.0\\n\\nShannon entropy of a binary random variable\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\np\\n\\n1\\n\\nFigure 3.5: This plot shows how distributions that are closer to deterministic have low\\nShannon entropy while distributions that are close to uniform have high Shannon entropy.\\nOn the horizontal axis, we plot p, the probability of a binary random variable being equal\\nto . The entropy is given by\\nlog . When p is near 0, the distribution\\nis nearly deterministic, because the random variable is nearly always 0. When p is near 1,\\nthe distribution is nearly deterministic, because the random variable is nearly always 1.\\nWhen p = 0 .5, the entropy is maximal, because the distribution is uniform over the two\\noutcomes.\\n\\n(p− 1) log(1− p )− p\\n\\np\\n\\nSelf-information deals only with a single outcome. We can quantify the amount\\n\\nof uncertainty in an entire probability distribution using the Shannon entropy:\\n\\nH( ) = \\n\\nx\\n\\nE\\n\\nI x\\n\\nx∼P[ ( )] = \\n\\n−E\\n\\nP x .\\nx∼P[log ( )]\\n\\n(3.49)\\n\\nalso denoted H( P). In other words, the Shannon entropy of a distribution is the\\nexpected amount of information in an event drawn from that distribution. It gives\\na lower bound on the number of bits (if the logarithm is base 2, otherwise the units\\nare diﬀerent) needed on average to encode symbols drawn from a distribution P.\\nDistributions that are nearly deterministic (where the outcome is nearly certain)\\nhave low entropy; distributions that are closer to uniform have high entropy. See\\nFig. 3.5 for a demonstration. When x is continous, the Shannon entropy is known\\nas the diﬀerential entropy.\\n\\nIf we have two separate probability distributions P(x) and Q(x) over the same\\nrandom variable x, we can measure how diﬀerent these two distributions are using\\nthe Kullback-Leibler (KL) divergence:\\n\\nDKL(\\n\\nP Q\\ue06b\\n\\n) = \\n\\nE x∼P\\ue014log\\n\\nP x( )\\n\\nQ x( )\\ue015 = Ex∼P [log ( )\\n\\nP x −\\n\\n73\\n\\nlog ( )]\\nQ x .\\n\\n(3.50)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nIn the case of discrete variables, it is the extra amount of information (measured\\nin bits if we use the base\\nlogarithm, but in machine learning we usually use nats\\nand the natural logarithm) needed to send a message containing symbols drawn\\nfrom probability distribution P , when we use a code that was designed to minimize\\nthe length of messages drawn from probability distribution\\n\\n.Q\\n\\n2\\n\\nThe KL divergence has many useful properties, most notably that it is non-\\nnegative. The KL divergence is 0 if and only if P and Qare the same distribution in\\nthe case of discrete variables, or equal “almost everywhere” in the case of continuous\\nvariables. Because the KL divergence is non-negative and measures the diﬀerence\\nbetween two distributions, it is often conceptualized as measuring some sort of\\ndistance between these distributions. However, it is not a true distance measure\\nbecause it is not symmetric: DKL(P Q\\ue06b ) \\ue036= DKL( Q P\\ue06b ) for some P and Q. This\\nasymmetry means that there are important consequences to the choice of whether\\nto use DKL(\\n\\n. See Fig. 3.6 for more detail.\\n\\nor DKL (\\n\\n)\\n\\nP Q\\ue06b\\n\\n)Q P\\ue06b\\n\\nA quantity that is closely related to the KL divergence is the cross-entropy\\nH (P, Q ) = H (P) + DKL(P Q\\ue06b ), which is similar to the KL divergence but lacking\\nthe term on the left:\\n(3.51)\\n\\nH P, Q(\\n\\n) = −E\\n\\nx∼P log ( )Q x .\\n\\nMinimizing the cross-entropy with respect to Q is equivalent to minimizing the\\nKL divergence, because\\n\\ndoes not participate in the omitted term.\\n\\nQ\\n\\nWhen computing many of these quantities, it is common to encounter expres-\\nsions of the form 0log 0. By convention, in the context of information theory, we\\ntreat these expressions as limx→0 x\\n\\nlog = 0.\\n\\nx\\n\\n3.14 Structured Probabilistic Models\\n\\nMachine learning algorithms often involve probability distributions over a very\\nlarge number of random variables. Often, these probability distributions involve\\ndirect interactions between relatively few variables. Using a single function to\\ndescribe the entire joint probability distribution can be very ineﬃcient (both\\ncomputationally and statistically).\\n\\nInstead of using a single function to represent a probability distribution, we\\ncan split a probability distribution into many factors that we multiply together.\\nFor example, suppose we have three random variables: a, b and c . Suppose that\\na inﬂuences the value of b and b inﬂuences the value of c, but that a and c are\\nindependent given b. We can represent the probability distribution over all three\\n\\n74\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nq∗ = argminq DKL(\\n\\n)p q\\ue06b\\n\\nq ∗ = argminq DKL (\\n\\nq p\\ue06b\\n\\n)\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np x( )\\nq∗( )x\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np( )x\\nq ∗( )x\\n\\nx\\n\\nx\\n\\nFigure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x ) and\\nwish to approximate it with another distribution q(x). We have the choice of minimizing\\neither DKL( p q\\ue06b ) or DKL( q p\\ue06b ). We illustrate the eﬀect of this choice using a mixture of\\ntwo Gaussians for p, and a single Gaussian for q. The choice of which direction of the\\nKL divergence to use is problem-dependent. Some applications require an approximation\\nthat usually places high probability anywhere that the true distribution places high\\nprobability, while other applications require an approximation that rarely places high\\nprobability anywhere that the true distribution places low probability. The choice of the\\ndirection of the KL divergence reﬂects which of these considerations takes priority for each\\napplication. (Left) The eﬀect of minimizing DKL (p q\\ue06b ). In this case, we select a q that has\\nhigh probability where p has high probability. When p has multiple modes, q chooses to\\nblur the modes together, in order to put high probability mass on all of them. (Right) The\\neﬀect of minimizing DKL (q p\\ue06b ). In this case, we select a q that has low probability where\\np has low probability. When p has multiple modes that are suﬃciently widely separated,\\nas in this ﬁgure, the KL divergence is minimized by choosing a single mode, in order to\\navoid putting probability mass in the low-probability areas between modes of p . Here, we\\nillustrate the outcome when q is chosen to emphasize the left mode. We could also have\\nachieved an equal value of the KL divergence by choosing the right mode. If the modes\\nare not separated by a suﬃciently strong low probability region, then this direction of the\\nKL divergence can still choose to blur the modes.\\n\\n75\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nvariables as a product of probability distributions over two variables:\\n\\np ,\\n\\n(a b c) =  ( )a (\\np\\n\\np\\n\\n,\\n\\nb a|\\n\\n)\\n\\nc b|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.52)\\n\\nThese factorizations can greatly reduce the number of parameters needed\\nto describe the distribution. Each factor uses a number of parameters that is\\nexponential in the number of variables in the factor. This means that we can greatly\\nreduce the cost of representing a distribution if we are able to ﬁnd a factorization\\ninto distributions over fewer variables.\\n\\nWe can describe these kinds of factorizations using graphs. Here we use the\\nword “graph” in the sense of graph theory: a set of vertices that may be connected\\nto each other with edges. When we represent the factorization of a probability\\ndistribution with a graph, we call it a structured probabilistic model\\ngraphical\\nmodel.\\n\\nor\\n\\nThere are two main kinds of structured probabilistic models: directed and\\nundirected. Both kinds of graphical models use a graph G in which each node\\nin the graph corresponds to a random variable, and an edge connecting two\\nrandom variables means that the probability distribution is able to represent direct\\ninteractions between those two random variables.\\n\\nDirected models use graphs with directed edges, and they represent factoriza-\\ntions into conditional probability distributions, as in the example above. Speciﬁcally,\\na directed model contains one factor for every random variable x i in the distribution,\\nand that factor consists of the conditional distribution over xi given the parents of\\nxi, denoted P a G(x i):\\n\\n(3.53)\\n\\np( ) =x \\ue059i\\n\\np (xi | P aG (xi )) .\\n\\nSee Fig. 3.7 for an example of a directed graph and the factorization of probability\\ndistributions it represents.\\n\\nUndirected models use graphs with undirected edges, and they represent fac-\\ntorizations into a set of functions; unlike in the directed case, these functions are\\nusually not probability distributions of any kind. Any set of nodes that are all\\nconnected to each other in G is called a clique. Each clique C ( )i\\nin an undirected\\nmodel is associated with a factor φ( )i (C ( )i ). These factors are just functions, not\\nprobability distributions. The output of each factor must be non-negative, but\\nthere is no constraint that the factor must sum or integrate to 1 like a probability\\ndistribution.\\n\\nThe probability of a conﬁguration of random variables is proportional to the\\nproduct of all of these factors—assignments that result in larger factor values are\\n\\n76\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.7: A directed graphical model over random variables a , b, c, d and e. This graph\\ncorresponds to probability distributions that can be factored as\\n\\n(a b c d e) =  ( )a (\\np ,\\np\\n\\np\\n\\n,\\n\\n,\\n\\n,\\n\\nb a|\\n\\n)\\n\\n(c a|\\np\\n\\n,\\n\\nb) (\\np\\n\\nd b|\\n\\n)\\n\\ne c|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.54)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nmore likely. Of course, there is no guarantee that this product will sum to 1. We\\ntherefore divide by a normalizing constant Z, deﬁned to be the sum or integral\\nover all states of the product of the φ functions, in order to obtain a normalized\\nprobability distribution:\\n\\np( ) =x\\n\\n1\\n\\nZ \\ue059i\\n\\nφ( )i \\ue010C( )i\\ue011 .\\n\\n(3.55)\\n\\nSee Fig. 3.8 for an example of an undirected graph and the factorization of\\nprobability distributions it represents.\\n\\nKeep in mind that these graphical representations of factorizations are a\\nlanguage for describing probability distributions. They are not mutually exclusive\\nfamilies of probability distributions. Being directed or undirected is not a property\\nof a probability distribution; it is a property of a particular\\nof a\\nprobability distribution, but any probability distribution may be described in both\\nways.\\n\\ndescription\\n\\nThroughout Part I and Part II of this book, we will use structured probabilistic\\nmodels merely as a language to describe which direct probabilistic relationships\\ndiﬀerent machine learning algorithms choose to represent. No further understanding\\nof structured probabilistic models is needed until the discussion of research topics,\\nin Part III, where we will explore structured probabilistic models in much greater\\ndetail.\\n\\n77\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.8: An undirected graphical model over random variables a, b, c, d and e . This\\ngraph corresponds to probability distributions that can be factored as\\n\\n(a b c d e) =\\np ,\\n\\n,\\n\\n,\\n\\n,\\n\\n1\\nZ\\n\\nφ (1)(\\n\\na b c\\n,\\n\\n, φ (2) (\\n\\n)\\n\\n)b d, φ(3)(\\n\\n)c e,\\n.\\n\\n(3.56)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nThis chapter has reviewed the basic concepts of probability theory that are\\nmost relevant to deep learning. One more set of fundamental mathematical tools\\nremains: numerical methods.\\n\\n78\\n\\n\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/228524191\n",
      "\n",
      "The Apriori Algorithm–a Tutorial\n",
      "\n",
      "Article · January 2008\n",
      "\n",
      "DOI: 10.1142/9789812709066_0006\n",
      "\n",
      "CITATIONS\n",
      "54\n",
      "\n",
      "1 author:\n",
      "\n",
      "Markus Hegland\n",
      "Australian National University\n",
      "\n",
      "188 PUBLICATIONS   1,925 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "READS\n",
      "3,464\n",
      "\n",
      "Some of the authors of this publication are also working on these related projects:\n",
      "\n",
      "Discrete Thin Plate Spline Smoothing View project\n",
      "\n",
      "Modelling gene regulatory networks View project\n",
      "\n",
      "All content following this page was uploaded by Markus Hegland on 29 May 2014.\n",
      "\n",
      "The user has requested enhancement of the downloaded file.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm – a Tutorial\n",
      "\n",
      "Markus Hegland\n",
      "\n",
      "CMA, Australian National University\n",
      "\n",
      "John Dedman Building, Canberra ACT 0200, Australia\n",
      "\n",
      "E-mail: Markus.Hegland@anu.edu.au\n",
      "\n",
      "Association rules are ”if-then rules” with two measures which quantify\n",
      "the support and conﬁdence of the rule for a given data set. Having their\n",
      "origin in market basked analysis, association rules are now one of the\n",
      "most popular tools in data mining. This popularity is to a large part due\n",
      "to the availability of eﬃcient algorithms. The ﬁrst and arguably most\n",
      "inﬂuential algorithm for eﬃcient association rule discovery is Apriori.\n",
      "\n",
      "In the following we will review basic concepts of association rule dis-\n",
      "covery including support, conﬁdence, the apriori property, constraints\n",
      "and parallel algorithms. The core consists of a review of the most im-\n",
      "portant algorithms for association rule discovery. Some familiarity with\n",
      "concepts like predicates, probability, expectation and random variables\n",
      "is assumed.\n",
      "\n",
      "1. Introduction\n",
      "Large amounts of data have been collected routinely in the course of day-\n",
      "to-day management in business, administration, banking, the delivery of\n",
      "social and health services, environmental protection, security and in pol-\n",
      "itics. Such data is primarily used for accounting and for management of\n",
      "the customer base. Typically, management data sets are very large and\n",
      "constantly growing and contain a large number of complex features. While\n",
      "these data sets reﬂect properties of the managed subjects and relations, and\n",
      "are thus potentially of some use to their owner, they often have relatively\n",
      "low information density. One requires robust, simple and computationally\n",
      "eﬃcient tools to extract information from such data sets. The development\n",
      "and understanding of such tools is the core business of data mining. These\n",
      "tools are based on ideas from computer science, mathematics and statistics.\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "2\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "[?] and,\n",
      "\n",
      "The introduction of association rule mining in 1993 by Agrawal, Imielin-\n",
      "ski and Swami\n",
      "in particular, the development of an eﬃcient\n",
      "algorithm by Agrawal and Srikant [?] and by Mannila, Toivonen and\n",
      "Verkamo [?] marked a shift of the focus in the young discipline of data\n",
      "mining onto rules and data bases. Consequently, besides involving the tra-\n",
      "ditional statistical and machine learning community, data mining now at-\n",
      "tracted researchers with a variety of skills ranging from computer science,\n",
      "mathematics, science, to business and administration. The urgent need for\n",
      "computational tools to extract information from data bases and for man-\n",
      "power to apply these tools has allowed a diverse community to settle in\n",
      "this new area. The data analysis aspect of data mining is more exploratory\n",
      "than in statistics and consequently, the mathematical roots of probability\n",
      "are somewhat less prominent in data mining than in statistics. Computa-\n",
      "tionally, however, data mining frequently requires the solution of large and\n",
      "complex search and optimisation problems and it is here where mathemat-\n",
      "ical methods can assist most. This is particularly the case for association\n",
      "rule mining which requires searching large data bases for complex rules.\n",
      "Mathematical modelling is required in order to generalise the original tech-\n",
      "niques used in market basket analysis to a wide variety of applications.\n",
      "Mathematical analysis provides insights into the performance of the algo-\n",
      "rithms.\n",
      "\n",
      "An association rule is an implication or if-then-rule which is supported\n",
      "by data. The motivation given in [?] for the development of association\n",
      "rules is market basket analysis which deals with the contents of point-of-\n",
      "sale transactions of large retailers. A typical association rule resulting from\n",
      "such a study could be “90 percent of all customers who buy bread and\n",
      "butter also buy milk”. Insights into customer behaviour may also be ob-\n",
      "tained through customer surveys, but the analysis of the transactional data\n",
      "has the advantage of being much cheaper and covering all current cus-\n",
      "tomers. Compared to customer surveys, the analysis of transactional data\n",
      "does have some severe limitations, however. For example, point-of-sale data\n",
      "typically does not contain any information about personal interests, age and\n",
      "occupation of customers. Nonetheless, market basket analysis can provide\n",
      "new insights into customer behaviour and has led to higher proﬁts through\n",
      "better customer relations, customer retention, better product placements,\n",
      "product development and fraud detection.\n",
      "\n",
      "Market basket analysis is not limited to retail shopping but has also\n",
      "\n",
      "been applied in other business areas including\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "3\n",
      "\n",
      "• credit card transactions,\n",
      "• telecommunication service purchases,\n",
      "• banking services,\n",
      "• insurance claims, and\n",
      "• medical patient histories.\n",
      "\n",
      "Association rule mining generalises market basket analysis and is used in\n",
      "many other areas including genomics, text data analysis and Internet in-\n",
      "trusion detection. For motivation we will in the following mostly focus on\n",
      "retail market basket analysis.\n",
      "\n",
      "When a customer passes through a point of sale, the contents of his\n",
      "market basket are registered. This results in large collections of market\n",
      "basket data which provide information about which items were sold and, in\n",
      "particular, which combinations of items were sold. The small toy example in\n",
      "the table of ﬁgure 1 shall illustrate this further. Each row corresponds to a\n",
      "\n",
      "market basket id market basket content\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "\n",
      "orange juice, soda water\n",
      "milk, orange juice, bread\n",
      "orange juice, butter\n",
      "orange juice, bread, soda water\n",
      "bread\n",
      "\n",
      "Fig. 1. Five grocery market baskets\n",
      "\n",
      "market basket or transaction containing popular retail items. An inspection\n",
      "of the table reveals that:\n",
      "\n",
      "• Four of the ﬁve baskets contain orange juice,\n",
      "• two baskets contain soda water\n",
      "• half of the baskets which contain orange juice also contain soda\n",
      "• all the baskets which contain soda also contain juice\n",
      "\n",
      "These rules are very simple as is typical for association rule mining. Sim-\n",
      "ple rules are understandable and ultimately useful. In a large retail shop\n",
      "there are usually more than 10,000 items on sale and the shop may service\n",
      "thousands of customers every day. Thus the size of the collected data is\n",
      "substantial and even the detection of simple rules like the ones above re-\n",
      "quires sophisticated algorithms. The eﬃciency of the algorithms will depend\n",
      "on the particular characteristics of the data sets. An important feature of\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "4\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "many retail data sets is that an average market basket only contains a small\n",
      "subset of all items available.\n",
      "\n",
      "The simplest model for the customers assumes that the customers choose\n",
      "products from the shelves in the shop at random. In this case the choice of\n",
      "each product is independent from any other product. Consequently, asso-\n",
      "ciation rule discovery will simply recover the likelihoods for any item to be\n",
      "chosen. While it is important to compare the performance of other models\n",
      "with this “null-hypothesis” one would usually ﬁnd that shoppers do have\n",
      "a more complex approach when they ﬁll the shopping basket (or trolley).\n",
      "They will buy breakfast items, lunch items, dinner items and snacks, party\n",
      "drinks, and Sunday dinners. They will have preferences for cheap items, for\n",
      "(particular) brand items, for high-quality, for freshness, low-fat, special diet\n",
      "and environmentally safe items. Such goals and preferences of the shopper\n",
      "will inﬂuence the choices but can not be directly observed. In some sense,\n",
      "market basket analysis should provide information about how the shop-\n",
      "pers choose. In order to understand this a bit further consider the case of\n",
      "politicians who vote according to party policy but where we will assume\n",
      "for the moment that the party membership is unknown. Is it possible to\n",
      "see an eﬀect of the party membership in voting data? For a small but real\n",
      "illustrative example consider the US Congress voting records from 1984 [?],\n",
      "see ﬁgure 2. The 16 columns of the displayed bit matrix correspond to the\n",
      "16 votes and the 435 rows to the members of congress. We have simpliﬁed\n",
      "the data slightly so that a matrix element is one (pixel set) in the case of\n",
      "votes which contains “voted for”, “paired for” and “announced for” and the\n",
      "matrix element is zero in all other cases. The left data matrix in ﬁgure 2\n",
      "is the original data where only the rows and columns have been randomly\n",
      "permuted to remove any information introduced through the way the data\n",
      "was collected. The matrix on the right side is purely random such that each\n",
      "entry is independent and only the total number of entries is maintained.\n",
      "Can you see the diﬀerence between the two bit matrices? We found that for\n",
      "most people, the diﬀerence between the two matrices is not obvious from\n",
      "visual inspection alone.\n",
      "\n",
      "Data mining aims to discover patterns in the left bit matrix and thus\n",
      "diﬀerences between the two examples. In particular, we will ﬁnd columns or\n",
      "items which display similar voting patterns and we aim to discover rules re-\n",
      "lating to the items which hold for a large proportion of members of congress.\n",
      "We will see how many of these rules can be explained by underlying mech-\n",
      "anisms (in this case party membership).\n",
      "\n",
      "In this example the selection of what are rows and what columns is\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "5\n",
      "\n",
      "Fig. 2.\n",
      "\n",
      "1984 US House of Representatives Votes, with 16 items voted on\n",
      "\n",
      "somewhat arbitrary. Instead of patterns regarding the items voted on one\n",
      "might be interested in patterns relating the members of Congress. For ex-\n",
      "ample one might be interested in statements like “if member x and member\n",
      "y vote yes then member z votes yes as well. Statements like this may reveal\n",
      "some of the interactions between the members of Congress. The duality of\n",
      "observations and objects occurs in other areas of data mining as well and\n",
      "illustrates that data size and data complexity are really two dual concepts\n",
      "which can be interchanged in many cases. This is in particular exploited in\n",
      "some newer association rule discovery algorithms which are based on formal\n",
      "concept analysis [?].\n",
      "\n",
      "By inspecting the data matrix of the voting example, one ﬁnds that the\n",
      "\n",
      "voting datarandom data\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "6\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "items have received between 34 to 63.5 percent yes votes. Pairs of items\n",
      "have received between 4 and 49 percent yes votes. The pairs with the most\n",
      "yes votes (over 45 percent) are in the columns 2/6, 4/6, 13/15, 13/16 and\n",
      "15/16. Some rules obtained for these pairs are: 92 percent of the yes votes\n",
      "in column 2 are also yes votes in column 6, 86 percent of the yes votes in\n",
      "column 4 are also yes votes in column 6 and, on the other side, 88 percent\n",
      "of the votes in column 13 are also in column 15 and 89 percent of the yes\n",
      "votes in column 16 are also yes votes in column 15. These ﬁgures suggest\n",
      "combinations of items which could be further investigated in terms of causal\n",
      "relationships between the items. Only a careful statistical analysis may\n",
      "provide some certainty on this. This and other issues concerning inference\n",
      "belong to statistics and are beyond the scope of this tutorial which focusses\n",
      "on computational issues.\n",
      "\n",
      "2. Itemsets and Associations\n",
      "In this section a formal mathematical model is derived to describe itemsets\n",
      "and associations to provide a framework for the discussion of the apriori al-\n",
      "gorithm. In order to apply ideas from market basket analysis to other areas\n",
      "one needs a general description of market baskets which can equally describe\n",
      "collections of medical services received by a patient during an episode of\n",
      "care, subsequences of amino acid sequences of a protein, and collections or\n",
      "words or concepts used on web pages. In this general description the items\n",
      "are numbered and a market basket is represented by an indicator vector.\n",
      "\n",
      "2.1. The Datamodel\n",
      "In this subsection a probabilistic model for the data is given along with\n",
      "some simple model examples. For this, we consider the voting data example\n",
      "again.\n",
      "First, the items are enumerated as 0, . . . , d − 1. Often, enumeration\n",
      "is done such that the more frequent items correspond to lower numbers\n",
      "but this is not essential. Itemsets are then sets of integers between 0 and\n",
      "d − 1. The itemsets are represented by bitvectors x ∈ X := {0, 1}d where\n",
      "item j is present in the corresponding itemset iﬀ the j-th bit is set in x.\n",
      "Consider the “micromarket” with the items juice, bread, milk, cheese and\n",
      "potatoes with item numbers 0, 1, 2, 3 and 4, respectively. The market basket\n",
      "containing bread, milk and potatoes is then mapped onto the set {1, 2, 4}\n",
      "and is represented by the bitvector (0, 1, 1, 0, 1). From the bitvector it is\n",
      "clear which elements are in the market basket or itemset and which are\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "7\n",
      "\n",
      "not.\n",
      "\n",
      "The data is a sequence of itemsets which is represented as a bitmatrix\n",
      "where each row corresponds to an itemset and the columns correspond to\n",
      "the items. For the micromarket example a dataset containing the market\n",
      "baskets {juice,bread, milk}, {potato} and {bread, potatoes} would be rep-\n",
      "resented by the matrix\n",
      "\n",
      "1 1 1 0 0\n",
      "\n",
      "0 0 0 0 1\n",
      "0 1 0 1 0\n",
      "\n",
      " .\n",
      "\n",
      "In the congressional voting example mentioned in the previous section the\n",
      "ﬁrst few rows of matrix are\n",
      "\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "\n",
      "0 1 0\n",
      "1 1 1\n",
      "1 0 0\n",
      "1 0 1\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "1 0\n",
      "1 0\n",
      "0 1\n",
      "0 1\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "and they correspond to the following “itemsets” (or sets of yes-votes):\n",
      "\n",
      "4\n",
      "3\n",
      "6\n",
      "6\n",
      "7\n",
      "\n",
      "3\n",
      "2\n",
      "5\n",
      "4\n",
      "3\n",
      "\n",
      "6\n",
      "5\n",
      "8\n",
      "6\n",
      "9\n",
      "8\n",
      "9\n",
      "8\n",
      "9 10\n",
      "\n",
      "10 13 14\n",
      "10 11 13 14\n",
      "\n",
      "8\n",
      "9\n",
      "15\n",
      "11 12 15\n",
      "11 13 16.\n",
      "\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "It is assumed that the data matrix X ∈ {0, 1}n,d is random and thus\n",
      "the elements x(i)\n",
      "j are binary random variables. One would in general have\n",
      "to assume correlations between both rows and columns. The correlations\n",
      "between the columns might relate to the type of shopping and customer,\n",
      "e.g., young family with small kids, weekend shopping or shopping for a\n",
      "speciﬁc dish. Correlations between the rows might relate to special oﬀers of\n",
      "the retailer, time of the day and week. In the following it will be assumed\n",
      "that the rows are drawn independently from a population of market baskets.\n",
      "Thus it is assumed that there is a probability distribution function p :→\n",
      "[0, 1] with\n",
      "\n",
      "where X = {0, 1}d. The probability measure with distribution p is denoted\n",
      "by P and one has P (A) =\n",
      "\n",
      "x∈A p(x).\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "x∈X\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "p(x) = 1\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "8\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "The data can be represented as an empirical distribution with\n",
      "\n",
      "n(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "pemp(x) =\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "δ(x − x(i))\n",
      "\n",
      "where δ(x) is the indicator function where δ(0) = 1 and δ(x) = 0 if\n",
      "x (cid:54)= 0. (For simplicity the empty market basket is denoted by 0 instead\n",
      "of (0, . . . , 0).) All the information derived from the data is stored in pemp\n",
      "but some sort of “smoothing” is required if one would like to generalise in-\n",
      "sights from the empirical distribution of one itemset collection to another to\n",
      "separate the noise from the noise from the signal. Association rule discovery\n",
      "has its own form of smoothing as will be seen in the following.\n",
      "\n",
      "The task of frequent itemset mining consists of ﬁnding itemsets which\n",
      "occur frequently in market baskets. For this one recalls that the itemsets\n",
      "are partially ordered with respect to inclusion (the subset relation) and we\n",
      "write x ≤ y if the set with representation x is a subset of the set with\n",
      "representation y or x = y. With this partial order one deﬁnes the support\n",
      "of an itemset x to be\n",
      "\n",
      "s(x) = P ({z | x ≤ z})\n",
      "\n",
      "(1)\n",
      "\n",
      "which is also called the anticummulative distribution function of the prob-\n",
      "ability P . The support is a function s : X → [0, 1] and s(0) = 1. By\n",
      "construction, the support is antimonotone, i.e., if x ≤ y then p(x) ≥ p(y).\n",
      "This antimonotonicity is the basis for eﬃcient algorithms to ﬁnd all frequent\n",
      "itemsets which are deﬁned as itemsets for which s(x) ≥ σ0 > 0 for some\n",
      "user deﬁned σ0.\n",
      "\n",
      "Equation 1 can be reformulated in terms of p(x) as\n",
      "\n",
      "s(x) =\n",
      "\n",
      "p(z).\n",
      "\n",
      "(2)\n",
      "\n",
      "For given supports s(x), this is a linear system of equations which can be\n",
      "solved recursively using s(e) = p(e) (where e = (1, . . . , 1) is the maximal\n",
      "itemset) and\n",
      "\n",
      "p(x) = s(x) −\n",
      "\n",
      "p(z).\n",
      "\n",
      "z>x\n",
      "\n",
      "It follows that the support function s(x) provides an alternative description\n",
      "of the probability measure P which is equivalent to p. However, for many\n",
      "examples the form of s(x) turns out to be simpler. In the cases of market\n",
      "baskets it is highly unlikely, that market baskets contain large numbers of\n",
      "items and so approximations with s(x) = 0 for x with a large number of\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "z≥x\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "9\n",
      "\n",
      "nonzero components will usually produce good approximations of the item-\n",
      "set distribution p(x). This leads to an eﬀective smoothing mechanism for\n",
      "association rule discovery where the minimal support σ0 acts as a smooth-\n",
      "ing parameter which in principle could be determined from a test data set\n",
      "or with crossvalidation.\n",
      "\n",
      "2.1.1. Example: The Random Shopper\n",
      "In the simplest case all the bits (items) in x are chosen independently\n",
      "with probability p0. We call this the case of the “random shopper” as it\n",
      "corresponds to a shopper who ﬁlls the market basket with random items.\n",
      "The distribution is in this case\n",
      "\n",
      "p(x) = p\n",
      "\n",
      "|x|\n",
      "0 (1 − p0)d−|x|\n",
      "\n",
      "where |x| is the number of bits set in x and d is the total number of items\n",
      "available, i.e., number of components of x. As any z ≥ x has at least all the\n",
      "bits set which are set in x one gets for the support\n",
      "\n",
      "s(x) = p\n",
      "\n",
      "|x|\n",
      "0 .\n",
      "\n",
      "It follows that the frequent itemsets x are exactly the ones with few items,\n",
      "in particular, where\n",
      "\n",
      "|x| ≤ log(σ0)/ log(p0).\n",
      "\n",
      "For example, if one is interested in ﬁnding itemsets which are supported by\n",
      "one percent of the data records and if the probability of choosing any item\n",
      "is p0 = 0.1 the frequent itemsets are the ones with at most two items. For\n",
      "large shops one would typically have p0 much smaller. Note that if p0 < σ0\n",
      "one would not get any frequent itemsets at all.\n",
      "\n",
      "The random shopper is of course not a realistic model for shopping\n",
      "and one would in particular not expect to draw useful conclusions from\n",
      "the frequent itemsets. Basically, the random shopper is the market bas-\n",
      "ket equivalent of noise. The above discussion, however, might be used to\n",
      "guide the choice of σ0 to ﬁlter out the noise in market basket analysis. In\n",
      "particular, one could choose\n",
      "\n",
      "σ0 = min\n",
      "|x|=1\n",
      "\n",
      "s(x).\n",
      "\n",
      "Note that in the random shopper case the rhs is just p0. In this case, all\n",
      "the single items would be frequent.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "10\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "A slight generalisation of the random shopper example above assumes\n",
      "that the items are selected independently but with diﬀerent probabilities\n",
      "pj. In this case one gets\n",
      "\n",
      "d(cid:89)\n",
      "\n",
      "p(x) =\n",
      "\n",
      "j (1 − pj)1−xj\n",
      "pxj\n",
      "\n",
      "and\n",
      "\n",
      "j=1\n",
      "\n",
      "s(x) =\n",
      "\n",
      "d(cid:89)\n",
      "\n",
      "j=1\n",
      "\n",
      "pxj\n",
      "j .\n",
      "\n",
      "In examples one can often ﬁnd that the pj, when sorted, are approximated\n",
      "by Zipf’s law, i.e,\n",
      "\n",
      "pj = α\n",
      "j\n",
      "\n",
      "for some constant α. It follows again that itemsets with few popular items\n",
      "are most likely.\n",
      "\n",
      "However, this type of structure is not really what is of interest in as-\n",
      "sociation rule mining. To illustrate this consider again the case of the US\n",
      "Congress voting data. In ﬁgure ?? the support for single itemsets are dis-\n",
      "played for the case of the actual data matrix and for a random permutation\n",
      "of all the matrix elements. The supports are between 0.32 and 0.62 for the\n",
      "original data where for the randomly permuted case the supports are be-\n",
      "tween 0.46 and 0.56. Note that these supports are computed from the data,\n",
      "in theory, the permuted case should have constant supports somewhere\n",
      "slightly below 0.5. More interesting than the variation of the supports of\n",
      "single items is the case when 2 items are considered. Supports for the votes\n",
      "displayed in ﬁgure ??. are of the form “V2 and Vx”. Note that “V2 and\n",
      "V2” is included for reference, even though this itemset has only one item.\n",
      "In the random data case where the vote for any pair {V2,Vx} (where Vx\n",
      "is not V2) is the square of the vote for the single item V2 as predicted\n",
      "by the “random shopper theory” above. One notes that some pairs have\n",
      "signiﬁcantly higher supports than the random ones and others signiﬁcantly\n",
      "lower supports. This type of behaviour is not captured by the “random\n",
      "shopper” model above even if the case of variable supports for single items\n",
      "are allowed. The following example attempts to model some this behaviour.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "11\n",
      "\n",
      "Fig. 3. Supports for all the votes in the US congress data (split by party).\n",
      "\n",
      "2.1.2. Example: Multiple Shopper and Item Classes\n",
      "Consider now the case of some simple structure. Assume that there are\n",
      "two types of shoppers and two types of items. Assume that the items are\n",
      "x = (x0, x1) where the vectors xi correspond to items of class i. In practice,\n",
      "the type of items might have to be discovered as well. Consider that the\n",
      "shoppers of type i have a probability πi of ﬁlling a market basket where here\n",
      "i = 0, 1. Assume that it is not known to which type of shopper a market\n",
      "basket belongs. Finally, assume that the diﬀerence between the two types\n",
      "of shoppers relates to how likely they are to put items of the two types into\n",
      "their market basket. Let pi,j denote the probability that shopper of type i\n",
      "puts an item of type j into the market basket. In this case the probability\n",
      "\n",
      "V2V8V3V14V15V9V10V12voting datavote numberproportion yes votes0.00.10.20.30.40.50.60.7V2V8V3V14V15V9V10V12random datavote numberproportion yes votes0.00.10.20.30.40.50.60.7\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "12\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Fig. 4. Supports for pairs of votes in the US congress data (split by party).\n",
      "\n",
      "|x1|\n",
      "01 (1−p00)d0−|x0|(1−p01)d1−|x1|+π1 p\n",
      "\n",
      "distribution is\n",
      "|x0|\n",
      "p(x) = π0 p\n",
      "00 p\n",
      "This is a mixture model with two components. Recovery of the parameters\n",
      "from the data of mixture models uses the EM algorithm and is discussed\n",
      "in detail in [?]. Note that π0 + π1 = 1.\n",
      "\n",
      "|x0|\n",
      "10 p\n",
      "\n",
      "For frequent itemset mining, however, the support function is considered\n",
      "\n",
      "|x1|\n",
      "11 (1−p10)d0−|x0|(1−p11)d1−|x1|.\n",
      "\n",
      "and similarly to the random shopper case can be shown to be:\n",
      "\n",
      "s(x) = π0p\n",
      "\n",
      "|x0|\n",
      "00 p\n",
      "\n",
      "|x1|\n",
      "01 + π1p\n",
      "\n",
      "|x0|\n",
      "10 p\n",
      "\n",
      "|x1|\n",
      "11 .\n",
      "\n",
      "Assume that the shopper of type i is unlikely to purchase items of the other\n",
      "type. Thus p00 and p11 are much larger than p10 and p01. In this case the\n",
      "\n",
      "V2V8V3V14V15V9V10V12voting datavote number of second vote in pairproportion yes votes0.00.10.20.30.40.50.60.7V2V8V3V14V15V9V10V12random datavote number of second vote in pairproportion yes votes0.00.10.20.30.40.50.60.7\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "13\n",
      "\n",
      "frequent itemsets are going to be small (as before), moreover, one has either\n",
      "x0 = 0 or x1 = 0, thus, frequent itemsets will only contain items of one\n",
      "type. Thus in this case frequent itemset mining acts as a ﬁlter to retrieve\n",
      "“pure” itemsets.\n",
      "\n",
      "A simple application to the voting data could consider two types of\n",
      "politicians. A question to further study would be how closely these two\n",
      "types correspond with the party lines.\n",
      "\n",
      "One can now consider generalisations of this case by including more\n",
      "than two types, combining with diﬀerent probabilities (Zipf’s law) for the\n",
      "diﬀerent items in the same class and even use itemtypes and customer types\n",
      "which overlap. These generalisations lead to graphical models and Bayesian\n",
      "nets [?,?]. The “association rule approach” in these cases distinguishes itself\n",
      "by using support functions, frequent itemsets and in particular, is based on\n",
      "binary data. A statistical approach to this type of data is “discriminant\n",
      "analysis” [?].\n",
      "\n",
      "2.2. The Size of Itemsets\n",
      "The size of the itemsets is a key factor in determining the performance of\n",
      "association rule discovery algorithms. The size of an itemset is the equal to\n",
      "the number of bits set, one has\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "|x| =\n",
      "\n",
      "xi\n",
      "\n",
      "if the components of x are interpreted as integers 0 or 1 this is a real valued\n",
      "random variable or function deﬁned on X. The expectation of a general real\n",
      "random variable f is\n",
      "\n",
      "The expectation is monotone in the sense that f ≥ g ⇒ E(f) ≥ E(g) and\n",
      "the expectation of a constant function is the function value. The variance\n",
      "of the random variable corresponding to the function f is\n",
      "\n",
      "For an arbitrary but ﬁxed itemset x ∈ X consider the function\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "x∈X\n",
      "\n",
      "E(f) =\n",
      "\n",
      "p(x)f(x).\n",
      "\n",
      "var(f) = E\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(f − E(f))2(cid:1)\n",
      "d(cid:89)\n",
      "\n",
      ".\n",
      "\n",
      "ax(z) =\n",
      "\n",
      "zxi\n",
      "i\n",
      "\n",
      ".\n",
      "\n",
      "Thus function takes values which are either 0 or 1 and ax(z) = 1 iﬀ x ≤ z\n",
      "as in this case all the components zi which occur to power 1 in ax(z)\n",
      "\n",
      "i=1\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "14\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "are one. Note that ax(z) is a monotone function of z and antimonotone\n",
      "function of x. Moreover, one has for the expectation E(ax) = s(x) and\n",
      "variance var(ax) = s(x)(1 − s(x)). The term 1 − s(x) is the support of the\n",
      "complement of the itemset x. The values for the expectation and variance\n",
      "are obtained directly from the deﬁnition of the expectation and the fact\n",
      "that ax(z)2 = ax(z) (holds for any function with values 0 and 1).\n",
      "\n",
      "Our main example of a random variable is the length of an itemset,\n",
      "\n",
      "d(cid:88)\n",
      "(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "f(x) = |x| =\n",
      "\n",
      "xj.\n",
      "\n",
      "E(|x|) =\n",
      "\n",
      "s(z).\n",
      "\n",
      "|z|=1\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "|x||z|=1\n",
      "\n",
      "The average length of itemsets is the expectation of f and one can see that\n",
      "\n",
      "The variance of the length is also expressed in terms of the support as\n",
      "\n",
      "var(|x|) =\n",
      "\n",
      "(s(x ∨ z) − s(x)s(z))\n",
      "\n",
      "where x ∨ z corresponds to the union of the itemsets x and z.\n",
      "\n",
      "With the expectation and using the Markov inequality one gets a simple\n",
      "\n",
      "bound on the probability of large itemsets as\n",
      "\n",
      "P ({x | |x| ≥ m}) ≤ E(|x|)/m.\n",
      "\n",
      "The expected length is easily obtained directly from the data and this bound\n",
      "gives an easy upper bound on probability of large itemsets. Of course one\n",
      "could just as easily get a histogram for the size of itemsets directly from\n",
      "the data. Using the above equation one gets an estimate for the average\n",
      "support of one-itemsets as E(|x|)/d.\n",
      "Consider now the special example of a random shopper discussed pre-\n",
      "viously. In this case one gets E(|x|) = dp0. The distribution of the length\n",
      "is in this case binomial and one has:\n",
      "d\n",
      "r\n",
      "\n",
      "0] (1 − p0)d−r.\n",
      "pr\n",
      "\n",
      "P (|x| = r) =\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "Moreover, for very large d and small p0 one gets a good approximation\n",
      "using the Poisson distribution\n",
      "\n",
      "p(|x| = r) ≈ 1\n",
      "\n",
      "r! e−λλr.\n",
      "\n",
      "with λ = E(|x|).\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "15\n",
      "\n",
      "The apriori algorithm which will be discussed in the following works\n",
      "best when long itemsets are unlikely. Thus in order to choose a suitable\n",
      "algorithm, it is important to check if this is the case, e.g., by using the\n",
      "histogram for the length |x|.\n",
      "\n",
      "2.3. The Itemset Lattice\n",
      "The search for frequent itemsets beneﬁts from a structured search space as\n",
      "the itemsets form a lattice. This lattice is also intuitive and itemsets close\n",
      "to 0 in the lattice are often the ones which are of most interest and lend\n",
      "themselves to interpretation and further discussion.\n",
      "\n",
      "As X consists of sets or bitvectors, one has a natural partial ordering\n",
      "which is induced by the subset relation. In terms of the bitvectors one can\n",
      "deﬁne this component-wise as\n",
      "\n",
      "x ≤ y :⇔ xi ≤ yi.\n",
      "\n",
      "Alternatively,\n",
      "\n",
      "x ≤ y :⇔ (xi = 1 ⇒ yi = 1,\n",
      "\n",
      "i = 1, . . . , d) .\n",
      "\n",
      "If xi = 1 and x ≤ y then it follows that yi = 1. Thus if the corresponding\n",
      "itemset to x contains item i then the itemset corresponding to y has to\n",
      "contain the same item. In other words, the itemset corresponding to x is a\n",
      "subset of the one corresponding to y.\n",
      "and so if x ≤ y then |x| ≤ |y|.\n",
      "\n",
      "Subsets have at most the same number of elements as their supersets\n",
      "\n",
      "Bitvectors also allow a total order by interpreting it as an integer φ(x)\n",
      "\n",
      "by\n",
      "\n",
      "φ(x) =\n",
      "\n",
      "d−1(cid:88)\n",
      "\n",
      "xi2i.\n",
      "\n",
      "Now as φ is a bijection it induces a total order on X deﬁned as x ≺ y iﬀ\n",
      "φ(x) < φ(y). This is the colex order and the colex order extends the partial\n",
      "order as x ≤ y ⇒ x ≺ y.\n",
      "\n",
      "i=0\n",
      "\n",
      "The partial order order has a smallest element which consists of the\n",
      "empty set, corresponding to the bitvector x = (0, . . . , 0) and a largest ele-\n",
      "ment which is just the set of all items Zd, corresponding to the bitvector\n",
      "(1, . . . , 1). Furthermore, for each pair x, y ∈ X there is a greatest lower\n",
      "bound and a least upper bound. These are just\n",
      "\n",
      "x ∨ y = z\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "16\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "where zi = max{xi, yi} for i = 0, . . . , d − 1 and similarly for x ∧ y. Conse-\n",
      "quently, the partially ordered set X forms a Boolean lattice. We denote the\n",
      "maximal and minimal elements of X by e and 0.\n",
      "\n",
      "In general, partial order is deﬁned by\n",
      "\n",
      "Deﬁnition 1: A partially ordered set (X,≤) consists of a set X with a\n",
      "binary relation ≤ such that for all x, x(cid:48), x(cid:48)(cid:48) ∈ X:\n",
      "\n",
      "• x ≤ x\n",
      "• If x ≤ x(cid:48) and x(cid:48) ≤ x then x = x(cid:48)\n",
      "• If x ≤ x(cid:48) and x(cid:48) ≤ x(cid:48)(cid:48) then x ≤ x(cid:48)(cid:48)\n",
      "\n",
      "(reﬂexivity)\n",
      "(antisymmetry)\n",
      "(transitivity)\n",
      "\n",
      "A lattice is a partially ordered set with glb and lub:\n",
      "Deﬁnition 2: A lattice (X,≤) is a partially ordered set such that for each\n",
      "pair of of elements of X there is greatest lower bound and a least upper\n",
      "bound.\n",
      "\n",
      "We will call a lattice distributive if the distributive law holds:\n",
      "\n",
      "x ∧ (x(cid:48) ∨ x(cid:48)(cid:48)) = (x ∧ x(cid:48)) ∨ (x ∧ x(cid:48)(cid:48)).\n",
      "\n",
      "where x∨ y is the maximum of the two elements which contains ones when-\n",
      "ever at least one of the two elements and x ∧ x(cid:48) contains ones where both\n",
      "elements contain a one. Then we can deﬁne:\n",
      "Deﬁnition 3: A lattice (X,≤) is a Boolean lattice if\n",
      "(1) (X,≤) is distributive\n",
      "(2) It has a maximal element e and a minimal element 0 such that for all\n",
      "\n",
      "(3) Each element x as a (unique) complement x(cid:48) such that x ∧ x(cid:48) = 0 and\n",
      "\n",
      "0 ≤ x ≤ e.\n",
      "\n",
      "x ∈ X:\n",
      "\n",
      "x ∨ x(cid:48) = e.\n",
      "\n",
      "The maximal and minimal elements 0 and e satisfy 0∨ x = x and e∧ x = x.\n",
      "In algebra one considers the properties of the conjunctives ∨ and ∧ and a\n",
      "set which has conjunctives which have the properties of a Boolean lattice\n",
      "is called a Boolean algebra. We will now consider some of the properties of\n",
      "Boolean algebras.\n",
      "\n",
      "The smallest nontrivial elements of X are the atoms:\n",
      "\n",
      "Deﬁnition 4: The set of atoms A of a lattice is deﬁned by\n",
      "A = {x ∈ X|x (cid:54)= 0 and if x(cid:48) ≤ x then x(cid:48) = x}.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "17\n",
      "\n",
      "(cid:87)\n",
      "\n",
      "The atoms generate the lattice, in particular, one has:\n",
      "Lemma 5: Let X be a ﬁnite Boolean lattice. Then, for each x ∈ X one has\n",
      "\n",
      "(cid:95)\n",
      "\n",
      "x =\n",
      "\n",
      "{z ∈ A(X) | z ≤ x}.\n",
      "\n",
      "(cid:87)\n",
      "\n",
      "Ax ≤ x.\n",
      "\n",
      "Proof: Let Ax := {z ∈ A(B)|z ≤ x}. Thus x is an upper bound for Ax,\n",
      "i.e.,\n",
      "Ax ≤ y. We need to show\n",
      "Now let y be any upper bound for Ax, i.e.,\n",
      "that x ≤ y.\n",
      "Consider x ∧ y(cid:48). If this is 0 then from distributivity one gets x = (x ∧\n",
      "y) ∨ (x ∧ y(cid:48)) = x ∧ y ≤ y. Conversely, if it is not true that x ≤ y then\n",
      "x ∧ y(cid:48) > 0. This happens if what we would like to show doesn’t hold.\n",
      "In this case there is an atom z ≤ x ∧ y(cid:48) and it follows that z ∈ Ax. As\n",
      "y is an upper bound we have y ≥ z and so 0 = y ∧ y(cid:48) ≥ x ∧ y(cid:48) which is\n",
      "impossible as we assumed x ∧ y(cid:48) > 0. Thus it follows that x ≤ y.\n",
      "\n",
      "The set of atoms associated with any element is unique, and the Boolean\n",
      "lattice itself is isomorph to the powerset of the set of atoms. This is the key\n",
      "structural theorem of Boolean lattices and is the reason why we can talk\n",
      "about sets (itemsets) in general for association rule discovery.\n",
      "\n",
      "Theorem 6: A ﬁnite Boolean algebra X is isomorphic to the power set\n",
      "2A(X) of the set of atoms. The isomorphism is given by\n",
      "\n",
      "η : x ∈ X (cid:55)→ {z ∈ A(X) | z ≤ x}\n",
      "\n",
      "and the inverse is\n",
      "\n",
      "η−1 : S (cid:55)→\n",
      "\n",
      "(cid:95)\n",
      "\n",
      "S.\n",
      "\n",
      "(cid:80)d\n",
      "\n",
      "In our case the atoms are the d basis vectors e1, . . . , ed and any element\n",
      "of X can be represented as a set of basis vectors, in particular x =\n",
      "i=1 ξiei\n",
      "where ξi ∈ {0, 1}. For the proof of the above theorem and further informa-\n",
      "tion on lattices and partially ordered sets see [?]. The signiﬁcance of the\n",
      "theorem lays in the fact that if X is an arbitrary Boolean lattice it is equiva-\n",
      "lent to the powerset of atoms (which can be represented by bitvectors) and\n",
      "so one can ﬁnd association rules on any Boolean lattice which conceptually\n",
      "generalises the association rule algorithms.\n",
      "\n",
      "In ﬁgure ?? we show the lattice of patterns for a simple market basket\n",
      "case which is just a power set. The corresponding lattice for the bitvectors\n",
      "is in ﬁgure ??. We represent the lattice using an undirected graph where the\n",
      "nodes are the elements of X and edges are introduced between any element\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "18\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Fig. 5. Lattice of breakfast itemsets\n",
      "\n",
      "Fig. 6. Lattice of bitvectors.\n",
      "\n",
      "and its covering elements. A covering element of x ∈ X is an x(cid:48) ≥ x such\n",
      "that no element is “in between” x and x(cid:48), i.e., any element x(cid:48)(cid:48) with x(cid:48)(cid:48) ≥ x\n",
      "and x(cid:48) ≥ x(cid:48)(cid:48) is either equal to x or to x(cid:48).\n",
      "\n",
      "In Figure ?? we display the graphs of the ﬁrst few Boolean lattices.\n",
      "We will graph speciﬁc lattices with (Hasse) diagrams [?] and later use the\n",
      "\n",
      "Fig. 7. The ﬁrst Boolean Lattices\n",
      "\n",
      "positive plane R2\n",
      "\n",
      "+ to illustrate general aspects of the lattices.\n",
      "\n",
      "{}{bread} {coffee}{juice}{milk}{bread, coffee} {milk, coffee}{milk, bread}{milk, juice}{bread. juice} {coffee, juice}{milk, coffee, juice}{bread, coffee, juice} {milk, bread, coffee}{milk, bread, juice}{milk, bread, coffee, juice}0000011011001011110101111111010010001010100100110010000101011110\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "19\n",
      "\n",
      "In the following we may sometimes also refer to the elements x of X\n",
      "as item sets, market baskets or even patterns depending on the context.\n",
      "As the data set is a ﬁnite collection of elements of a lattice the closure of\n",
      "this collection with respect to ∧ and ∨ (the inf and sup operators) in the\n",
      "lattice forms again a boolean lattice. The powerset of the set of elements of\n",
      "this lattice is then the sigma algebra which is fundamental to the measure\n",
      "which is deﬁned by the data.\n",
      "\n",
      "The partial order in the set X allows us to introduce the cumulative\n",
      "distribution function as the probability that we observe a bitvector less\n",
      "than a given x ∈ X:\n",
      "\n",
      "F (x) = P ({x(cid:48)|x(cid:48) ≤ x}) .\n",
      "\n",
      "By deﬁnition, the cumulative distribution function is monotone, i.e.\n",
      "\n",
      "x ≤ x(cid:48) ⇒ F (x) ≤ F (x(cid:48)).\n",
      "\n",
      "A second cumulative distribution function is obtained from the dual order\n",
      "as:\n",
      "\n",
      "F ∂(x) = P ({x(cid:48)|x(cid:48) ≤ x}) .\n",
      "\n",
      "It turns out that this dual cumulative distribution function is the one which\n",
      "is more useful in the discussion of association rules and frequent itemsets\n",
      "as one has for the support s(x):\n",
      "\n",
      "s(x) = F ∂(x).\n",
      "\n",
      "From this it follows directly that s(x) is antimonotone, i.e., that\n",
      "\n",
      "x ≤ y ⇒ s(x) ≥ s(y).\n",
      "\n",
      "The aim of frequent itemset mining is to ﬁnd sets of itemsets, i.e., subsets\n",
      "\n",
      "of X. In particular, one aims to determine\n",
      "\n",
      "L = {x | s(x) ≥ σ0}.\n",
      "From the antimonotonicity of s, it follows that\n",
      "\n",
      "x ∈ L and x ≥ y ⇒ y ∈ L.\n",
      "\n",
      "Such a set is called an down-set, decreasing set or order ideal. This algebraic\n",
      "characterisation will turn out to be crucial for the development and analysis\n",
      "of algorithms.\n",
      "\n",
      "The set of downsets is a subset of the power set of X, however, there\n",
      "are still a very large number of possible downsets. For example, in the case\n",
      "of d = 6, the set X has 64 elements and the power set has 264 ≈ 1.81019\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "20\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "elements and the number of downsets is 7,828,354. The simplest downsets\n",
      "are generated by one element and are\n",
      "\n",
      "For example, one has\n",
      "\n",
      "↓ x = {z | z ≤ x}\n",
      "\n",
      "X =↓ e.\n",
      "\n",
      "Consider the set of itemsets for which the (empirical) support as deﬁned by\n",
      "a data base D is nonzero. This is the set of all itemsets which are at least\n",
      "contained in one data record. It is\n",
      "\n",
      "(cid:91)\n",
      "\n",
      "x∈D\n",
      "\n",
      "(cid:91)\n",
      "\n",
      "L(0) =\n",
      "\n",
      "↓ x.\n",
      "\n",
      "L(0) =\n",
      "\n",
      "x∈Dmax\n",
      "\n",
      "↓ x.\n",
      "\n",
      "This formula can be simpliﬁed by considering only the maximal elements\n",
      "Dmax in D:\n",
      "\n",
      "Any general downset can be represented as a union of the “simple\n",
      "downsets” generated by one element. It follows that for some set Z ⊂ X of\n",
      "maximal elements one then has\n",
      "\n",
      "(cid:91)\n",
      "\n",
      "x∈Z\n",
      "\n",
      "L =\n",
      "\n",
      "↓ x.\n",
      "\n",
      "The aim of frequent itemset mining is to determine Z for a given σ0. Al-\n",
      "gorithms to determine such Z will be discussed in the next sections. Note\n",
      "that L ⊂ L(0 and that the representation of L can be considerably more\n",
      "complex than the representation of L(0). As illustration, consider the case\n",
      "where e is in the data base. In this case L(0) = X =↓ e but e is usually not\n",
      "going to be a frequent itemset.\n",
      "\n",
      "2.4. General Search for Itemsets and Search for Rules\n",
      "The previous subsections considered the search for itemsets which were\n",
      "frequently occurring in a database. One might be interested in more general\n",
      "characterisations, maybe searching for itemsets for which the income from\n",
      "their sale amounted to some minimum ﬁgure or which combined only certain\n",
      "items together. Thus one has a criterion or predicate a(x) which is true\n",
      "for items of interest. It is assumed that the evaluation of this criterion is\n",
      "expensive and requires reading the database. One would now like to ﬁnd all\n",
      "“interesting” items and would like to do this without having to consider all\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "21\n",
      "\n",
      "possible itemsets. This search for interesting itemsets is considerably more\n",
      "challenging. In some cases one ﬁnds, however, that the sets to be found\n",
      "are again downsets and then similar algorithms can be employed. Often,\n",
      "however, one has to resort to heuristics and approximate methods.\n",
      "\n",
      "Once frequent itemsets are available one can ﬁnd strong rules. These\n",
      "rules are of the type “if itemset x is in a record than so is itemset y. Such\n",
      "a rule is written as x ⇒ y and is deﬁned by a pair of itemsets (x, y) ∈ X2.\n",
      "The proportion of records for which this rule holds is called the conﬁdence,\n",
      "it is deﬁned formally as\n",
      "\n",
      "c(x ⇒ y) = s(x ∨ y)\n",
      "s(x)\n",
      "\n",
      ".\n",
      "\n",
      "A strong rule is given by a rule x ⇒ y for which x ∨ y is frequent, i.e.,\n",
      "s(x ∨ y) ≥ σ0 for a given σ0 > 0 and for which c(x ⇒ y) ≥ γ0 for a given\n",
      "γ0 > 0. The constants σ0, γ0 are provided by the user and their careful\n",
      "choice is crucial to the detection of sensible rules. From the deﬁnition it\n",
      "follows that the conﬁdence is the conditional anticumulative distribution,\n",
      "i.e.,\n",
      "\n",
      "c(ax ⇒ ay) = F δ(y|x)\n",
      "\n",
      "where F δ(y|x) = F δ(y ∨ x)/F δ(x) is the conditional cumulative distribu-\n",
      "tion function. Now there are several problems with strong association rules\n",
      "which have been addressed in the literature:\n",
      "• The straight-forward interpretation of the rules may lead to wrong in-\n",
      "• The number of strong rules found can be very small.\n",
      "• The number of strong rules can be very large.\n",
      "• Most of the strong rules found can be inferred from domain knowledge\n",
      "• The strong rules found do not lend themselves to any actions and are\n",
      "\n",
      "and do not lead to new insights.\n",
      "\n",
      "ferences.\n",
      "\n",
      "hard to interpret.\n",
      "\n",
      "We will address various of these challenges in the following. At this stage\n",
      "the association rule mining problem consists of the following:\n",
      "\n",
      "Find all strong association rules in a given data set D.\n",
      "\n",
      "A simple procedure would now visit each frequent itemset z and look at\n",
      "all pairs z1, z2 such that z = z1 ∨ z2 and z1 ∧ z2 = 0 and consider all rules\n",
      "az1 ⇒ az2. This procedure can be improved by taking into account that\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "22\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Theorem 7: Let z = z1 ∨ z2 = z3 ∨ z4 and z1 ∧ z2 = z3 ∧ z4 = 0. Then if\n",
      "az1 ⇒ az2 is a strong association rule and z3 ≥ z1 then so is az3 ⇒ az4.\n",
      "\n",
      "This is basically “the apriori property for the rules” and allows pruning\n",
      "the tree of possible rules quite a lot. The theorem is again used as a nec-\n",
      "essary condition. We start the algorithm by considering z = z1 ∨ z2 with\n",
      "1-itemsets for z2 and looking at all strong rules. Then, if we consider a\n",
      "2-itemset for z2 both subsets y < z2 need to be consequents of strong rules\n",
      "in order for z2 to be a candidate of a consequent. By constructing the con-\n",
      "sequents taking into account that all their nearest neighbours (their cover\n",
      "in lattice terminology) need to be consequents as well. Due to the inter-\n",
      "pretability problem one is mostly interested in small consequent itemsets\n",
      "so that this is not really a big consideration. See [?] for eﬃcient algorithms\n",
      "for the direct search for association rules.\n",
      "\n",
      "3. The Apriori Algorithm\n",
      "The aim of association rule discovery is the derivation of if-then-rules based\n",
      "on the itemsets x deﬁned in the previous subsection. An example of such\n",
      "a rule is “if a market basket contains orange juice then it also contains\n",
      "bread”. In this section the Apriori algorithm to ﬁnd all frequent itemsets\n",
      "is discussed. The classical Apriori algorithm as suggested by Agrawal et\n",
      "al. in [?] is one of the most important data mining algorithms. It uses a\n",
      "breadth ﬁrst search approach, ﬁrst ﬁnding all frequent 1-itemsets, and then\n",
      "discovering 2-itemsets and continues by ﬁnding increasingly larger frequent\n",
      "itemsets. The three subsections of this section consider ﬁrst the problem of\n",
      "the determination of the support of any itemset and the storage of the data\n",
      "in memory, then the actual apriori algorithm and ﬁnally the estimation of\n",
      "the size of candidate itemsets which allows the prediction of computational\n",
      "time as the size of the candidates is a determining factor in the complexity\n",
      "of the apriori algorithm.\n",
      "\n",
      "3.1. Time complexity – computing supports\n",
      "(cid:80)n\n",
      "The data is a sequence x(1), . . . , x(n) of binary vectors. We can thus repre-\n",
      "sent the data as a n by d binary matrix. The number of nonzero elements\n",
      "i=1 |x(i)|. This is approximated by the n times expected length, i.e.,\n",
      "is\n",
      "nE(|x|). So the proportion of nonzero elements is E(|x|)/d. This can be very\n",
      "small, especially for the case of market baskets, where out of often more\n",
      "than 10,000 items usually less than 100 items are purchased. Thus less than\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "23\n",
      "\n",
      "one percent of all the items are nonzero. In this case it makes sense to store\n",
      "the matrix in a sparse format. Here we will consider two ways to store\n",
      "the matrix, either by rows or by columns. The matrix corresponding to an\n",
      "earlier example is\n",
      "\n",
      " .\n",
      "\n",
      "\n",
      "\n",
      "1 1 0 0 0\n",
      "1 0 1 1 0\n",
      "1 0 0 0 1\n",
      "1 1 0 1 0\n",
      "0 0 0 0 1\n",
      "\n",
      "First we discuss the horizontal organisation. A row is represented simply\n",
      "by the indices of the nonzero elements. And the matrix is represented as a\n",
      "tuple of rows. For example, the above matrix is represented as\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(1, 2) (1, 3, 4) (1, 5) (1, 2, 4) (5)\n",
      "\n",
      "In practice we also need pointers which tell us where the row starts if\n",
      "contiguous locations are used in memory.\n",
      "Now assume that we have any row x and a az and would like to ﬁnd\n",
      "out if x supports az, i.e., if z ≤ x. If both the vectors are represented in the\n",
      "sparse format this means that we would like to ﬁnd out if the indices of z\n",
      "are a subset of the indices of x. There are several diﬀerent ways to do this\n",
      "and we will choose the one which uses an auxiliary bitvector v ∈ X (in full\n",
      "format) which is initialised to zero. The proposed algorithm has 3 steps:\n",
      "(1) Expand x into a bitvector v: v ← x\n",
      "(2) Extract the value of v for the elements of z, i.e., v[z]. If they are all\n",
      "nonzero, i.e., if v[z] = e then z ≤ x.\n",
      "(3) Set v to zero again, i.e., v ← 0\n",
      "We assume that the time per nonzero element for all the steps is the same\n",
      "τ and we get for the time:\n",
      "\n",
      "T = (2|x| + |z|)τ.\n",
      "\n",
      "In practice we will have to determine if az(x) holds for mk diﬀerent\n",
      "vectors z which have all the same length k. Rather than doing the above\n",
      "algorithm mk times one can extract x once and one so gets the algorithm\n",
      "(1) Extract x into v: v ← x\n",
      "(2) For all j = 1, . . . , mk check if v[z(j)] = e, i.e., if z(j) ≤ x.\n",
      "(3) Set v to zero again, i.e., v ← 0\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "24\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "With the same assumptions as above we get (|z(j)| = k) for the time:\n",
      "\n",
      "T = (2|x| + mkk)τ.\n",
      "\n",
      "Finally, running this algorithm for all the rows x(i) and vectors z(j) of\n",
      "\n",
      "diﬀerent lengths, one gets the total time\n",
      "\n",
      "T =\n",
      "\n",
      "and the expected time is\n",
      "\n",
      "|x(i)| + mkkn)τ\n",
      "\n",
      "E(T ) =\n",
      "\n",
      "(2E(|x|) + mkk)nτ.\n",
      "\n",
      "(cid:88)\n",
      "n(cid:88)\n",
      "(cid:88)\n",
      "\n",
      "(2\n",
      "\n",
      "i=1\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "Note that the sum over k is for k between one and d but only the k for\n",
      "which mk > 0 need to be considered. The complexity has two parts. The\n",
      "ﬁrst part is proportional to E(|x|)n which corresponds to the number of\n",
      "data points times the average complexity of each data point. This part\n",
      "thus encapsulates the data dependency. The second part is proportional to\n",
      "mkkn where the factor mkk refers to the complexity of the search space\n",
      "which has to be visited for each record n. For k = 1 we have m1 = 1 as\n",
      "we need to consider all the components. Thus the second part is larger\n",
      "than dnτ, in fact, we would probably have to consider all the pairs so that\n",
      "it would be larger than d2nτ which is much larger than the ﬁrst part as\n",
      "2E(|x|) ≤ 2d. Thus the major cost is due to the search through the possible\n",
      "patterns and one typically has a good approximation\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "E(T ) ≈\n",
      "\n",
      "mkknτ.\n",
      "\n",
      "An alternative is based on the vertical organisation where the binary\n",
      "matrix (or Boolean relational table) is stored column-wise. This may require\n",
      "slightly less storage as the row wise storage as we only needs pointers to\n",
      "each column and one typically has more rows than columns. In this vertical\n",
      "storage scheme the matrix considered earlier would be represented as\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(1, 2, 3, 4) (1, 4) (2) (2, 4) (3, 5)\n",
      "\n",
      "The storage savings in the vertical format however, are oﬀset by extra\n",
      "storage costs for an auxiliary vector with n elements.\n",
      "\n",
      "For any az the algorithm considers only the columns for which the com-\n",
      "ponents zj are one. The algorithm determines the intersection (or elemen-\n",
      "twise product) of all the columns j with zj = 1. This is done by using the\n",
      "auxiliary array v which holds the current intersection. We initially set it\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "25\n",
      "\n",
      "to the ﬁrst column j with zj = 1, later extract all the values at the points\n",
      "deﬁned by the nonzero elements for the next column j(cid:48) for which zj(cid:48) = 1,\n",
      "then zero the original ones in v and ﬁnally set the extracted values into the\n",
      "v. More concisely, we have the algorithm, where xj stands for the whole\n",
      "column j in the data matrix.\n",
      "\n",
      "(1) Get j such that zj = 1, mark as visited\n",
      "(2) Extract column xj into v: v ← xj\n",
      "(3) Repeat until no nonzero elements in z unvisited:\n",
      "\n",
      "(a) Get unvisited j such that zj = 1, mark as visited\n",
      "(b) Extract elements of v corresponding to xj, i.e., w ← v[xj]\n",
      "(c) Set v to zero, v ← 0\n",
      "(d) Set v to w, v ← w\n",
      "\n",
      "(4) Get the support s(az) = |w|\n",
      "\n",
      "So we access v three times for each column, once for the extraction of\n",
      "elements, once for setting it to zero and once for resetting the elements.\n",
      "Thus for the determination of the support of z in the data base we have\n",
      "the time complexity of\n",
      "\n",
      "d(cid:88)\n",
      "\n",
      "n(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "T = 3τ\n",
      "\n",
      "x(i)\n",
      "j zj.\n",
      "\n",
      "A more careful analysis shows that this is actually an upper bound for the\n",
      "complexity. Now this is done for mk arrays z(s,k) of size k and for all k.\n",
      "Thus we get the total time for the determination of the support of all az\n",
      "to be\n",
      "\n",
      "We can get a simple upper bound for this using x(i)\n",
      "\n",
      "j ≤ 1 as\n",
      "\n",
      "T = 3τ\n",
      "\n",
      "x(i)\n",
      "j z(s,k)\n",
      "\n",
      "j\n",
      "\n",
      ".\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "mk(cid:88)\n",
      "\n",
      "d(cid:88)\n",
      "\n",
      "n(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "s=1\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "T ≤ 3\n",
      "\n",
      "mkknτ\n",
      "\n",
      "(cid:80)d\n",
      "\n",
      "j=1 z(s,k)\n",
      "\n",
      "because\n",
      "= k. This is roughly 3 times what we got for the pre-\n",
      "vious algorithm. However, the x(i)\n",
      "j are random with an expectation E(x(i)\n",
      "j )\n",
      "which is typically much less than one and have an average expected length\n",
      "\n",
      "j\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "26\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "of E(|x|)/d. If we introduce this into the equation for T we get the approx-\n",
      "imation\n",
      "\n",
      "E(T ) ≈ 3E(|x|)\n",
      "\n",
      "d\n",
      "\n",
      "mkknτ\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "which can be substantially smaller than the time for the previous algorithm.\n",
      "Finally, we should point out that there are many other possible al-\n",
      "gorithms and other possible data formats. Practical experience and more\n",
      "careful analysis shows that one method may be more suitable for one data\n",
      "set where the other is better for another data set. Thus one carefully has\n",
      "to consider the speciﬁcs of a data set. Another consideration is also the size\n",
      "k and number mk of the z considered. It is clear from the above that it is\n",
      "essential to carefully choose the “candidates” az for which the support will\n",
      "be determined. This will further be discussed in the next sections. There is\n",
      "one term which occurred in both algorithms above and which characterises\n",
      "the complexity of the search through multiple levels of az, it is:\n",
      "\n",
      "∞(cid:88)\n",
      "\n",
      "C =\n",
      "\n",
      "mkk.\n",
      "\n",
      "We will use this constant later in the discussion of the eﬃciency of the\n",
      "search procedures.\n",
      "\n",
      "k=1\n",
      "\n",
      "3.2. The algorithm\n",
      "Some principles of the apriori algorithm are suggested in [?]. In partic-\n",
      "ular, the authors suggest a breadth-ﬁrst search algorithm and utilise the\n",
      "apriori principle to avoid unnecessary processing. However, the problem\n",
      "with this early algorithm is that it generates candidate itemsets for each\n",
      "record and also cannot make use of the vertical data organisation. Con-\n",
      "sequently, two groups of authors suggested at the same conference a new\n",
      "and faster algorithm which determines candidate itemsets before each data\n",
      "base scan [?,?]. This approach has substantially improved performance and\n",
      "is capable of utilising the vertical data organisation. We will discuss this\n",
      "algorithm using the currently accepted term frequent itemsets. (This was\n",
      "in the earlier literature called “large itemsets” or “covering itemsets”.)\n",
      "\n",
      "The apriori algorithm visits the lattice of itemsets in a level-wise fashion,\n",
      "see Figure ?? and Algorithm ??. Thus it is a breadth-ﬁrst-search or BFS\n",
      "procedure. At each level the data base is scanned to determine the support\n",
      "of items in the candidate itemset Ck. Recall from the last section that\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "27\n",
      "\n",
      "Fig. 8. Level sets of Boolean lattices\n",
      "\n",
      "Algorithm 1 Apriori\n",
      "\n",
      "C1 = A(X) is the set of all one-itemsets, k = 1\n",
      "while Ck (cid:54)= ∅ do\n",
      "\n",
      "scan database to determine support of all ay with y ∈ Ck\n",
      "extract frequent itemsets from Ck into Lk\n",
      "generate Ck+1\n",
      "k := k + 1.\n",
      "\n",
      "end while\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "the major determining parameter for the complexity of the algorithm is\n",
      "C =\n",
      "\n",
      "k mkk where mk = |Ck|.\n",
      "\n",
      "It is often pointed out that much of the time is spent in dealing with\n",
      "pairs of items. We know that m1 = d as one needs to consider all single\n",
      "items. Furthermore, one would not have any items which alone are not\n",
      "frequent and so one has m2 = d(d − 1)/2. Thus we get the lower bound for\n",
      "C:\n",
      "\n",
      "C ≤ m1 + 2m2 = d2.\n",
      "\n",
      "As one sees in practice that this is a large portion of the total computations\n",
      "one has a good approximation C ≈ d2. Including also the dependence on\n",
      "the data size we get for the time complexity of apriori:\n",
      "\n",
      "T = O(d2n).\n",
      "\n",
      "Thus we have scalability in the data size but quadratic dependence on the\n",
      "dimension or number of attributes.\n",
      "Consider the ﬁrst (row-wise) storage where T ≈ d2nτ. If we have\n",
      "d = 10, 000 items and n = 1, 000, 000 data records and the speed of the\n",
      "computations is such that τ = 1ns the apriori algorithm would require 105\n",
      "seconds which is around 30 hours, more than one day. Thus the time spent\n",
      "for the algorithm is clearly considerable.\n",
      "\n",
      "L0L1L2L3L4\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "28\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "In case of the second (column-wise) storage scheme we have T ≈\n",
      "3E(|x|)dnτ. Note that in this case for ﬁxed size of the market baskets the\n",
      "complexity is now\n",
      "\n",
      "T = O(dn).\n",
      "\n",
      "If we take the same data set as before and we assume that the average\n",
      "market basket contains 100 items (E(|x|) = 100) then the apriori algorithm\n",
      "would require only 300 seconds or ﬁve minutes, clearly a big improvement\n",
      "over the row-wise algorithm.\n",
      "\n",
      "3.3. Determination and size of the candidate itemsets\n",
      "The computational complexity of the apriori algorithm is dominated by\n",
      "data scanning, i.e., evaluation of az(x) for data records x. We found earlier\n",
      "k mkk. As mk\n",
      "that the complexity can be described by the constant C =\n",
      "is the number of k itemsets, i.e., bitvectors where exactly k bits are one we\n",
      ". On the other hand the apriori algorithm will ﬁnd the set Lk\n",
      "of the actual frequent itemsets, thus Lk ⊂ Ck and so |Lk| ≤ mk. Thus one\n",
      "gets for the constant C the bounds\n",
      "\n",
      "get mk ≤(cid:0)m\n",
      "(cid:1)\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k|Lk| ≤ C =\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "mkk ≤ d(cid:88)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "d\n",
      "k\n",
      "\n",
      "k.\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "k=0\n",
      "\n",
      "The upper bound is hopeless for any large size d and we need to get better\n",
      "bounds. This depends very much on how the candidate itemsets Ck are\n",
      "chosen. We choose C1 to be the set of all 1-itemsets, and C2 to be the set\n",
      "of all 2-itemsets so that we get m1 = 1 and m2 = d(d − 1)/2.\n",
      "\n",
      "The apriori algorithm determines alternatively Ck and Lk such that\n",
      "\n",
      "successively the following chain of sets is generated:\n",
      "\n",
      "C1 = L1 → C2 → L2 → C3 → L3 → C4 → ···\n",
      "\n",
      "How should we now choose the Ck? We know, that the sequence Lk satisﬁes\n",
      "the apriori property, which can be reformulated as\n",
      "Deﬁnition 8: If y is a frequent k-itemset (i.e., y ∈ Lk) and if z ≤ y then\n",
      "z is a frequent |z|-itemset, i.e., z ∈ L|z|.\n",
      "Thus in extending a sequence L1, L2, . . . , Lk by a Ck+1 we can choose the\n",
      "candidate itemset such that the extended sequence still satisﬁes the apriori\n",
      "property. This still leaves a lot of freedom to choose the candidate itemset.\n",
      "In particular, the empty set would always be admissible. We need to ﬁnd\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "29\n",
      "\n",
      "a set which contains Lk+1. The apriori algorithm chooses the largest set\n",
      "Ck+1 which satisﬁes the apriori condition. But is this really necessary? It\n",
      "is if we can ﬁnd a data set for which the extended sequence is the set of\n",
      "frequent itemsets. This is shown in the next proposition:\n",
      "\n",
      "Proof: Set x(i) ∈(cid:83)\n",
      "sets, i.e., for any z ∈(cid:83)\n",
      "\n",
      "Proposition 9: Let L1, . . . , Lm be any sequence of sets of k-itemsets which\n",
      "satisﬁes the apriori condition. Then there exists a dataset D and a σ > 0\n",
      "such that the Lk are frequent itemsets for this dataset with minimal support\n",
      "σ.\n",
      "\n",
      "k Lk, i = 1, . . . , n to be sequence of all maximal item-\n",
      "k Lk there is an x(i) such that z ≤ x(i) and x(i) (cid:54)≤ x(j)\n",
      "for i (cid:54)= j. Choose σ = 1/n. Then the Lk are the sets of frequent itemsets\n",
      "for this data set.\n",
      "\n",
      "For any collection Lk there might be other data sets as well, the one chosen\n",
      "above is the minimal one. The sequence of the Ck is now characterised by:\n",
      "(1) C1 = L1\n",
      "(2) If y ∈ Ck and z ≤ y then z ∈ Ck(cid:48) where k(cid:48) = |z|.\n",
      "In this case we will say that the sequence Ck satisﬁes the apriori condition.\n",
      "It turns out that this characterisation is strong enough to get good upper\n",
      "bounds for the size of mk = |Ck|.\n",
      "However, before we go any further in the study of bounds for |Ck| we\n",
      "provide a construction of a sequence Ck which satisﬁes the apriori condi-\n",
      "tion. A ﬁrst method uses Lk to construct Ck+1 which it chooses to be the\n",
      "maximal set such that the sequence L1, . . . , Lk, Ck+1 satisﬁes the apriori\n",
      "property. One can see by induction that then the sequence C1, . . . , Ck+1\n",
      "will also satisfy the apriori property. A more general approach constructs\n",
      "Ck+1, . . . , Ck+p such that L1, . . . , Lk,Ck+1, . . . , Ck+p satisﬁes the apriori\n",
      "property. As p increases the granularity gets larger and this method may\n",
      "work well for larger itemsets. However, choosing larger p also amounts to\n",
      "larger Ck and thus some overhead. We will only discuss the case of p = 1\n",
      "here.\n",
      "\n",
      "The generation of Ck+1 is done in two steps. First a slightly larger set is\n",
      "constructed and then all the elements which break the apriori property are\n",
      "removed. For the ﬁrst step the join operation is used. To explain join let the\n",
      "elements of L1 (the atoms) be enumerated as e1, . . . , ed. Any itemset can\n",
      "then be constructed as join of these atoms. We denote a general itemset by\n",
      "\n",
      "e(j1, . . . , jk) = ej1 ∨ ··· ∨ ejk\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "30\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "where j1 < j2 < ··· < jk. The join of any k-itemset with itself is then\n",
      "deﬁned as\n",
      "Lk (cid:111)(cid:110) Lk := {e(j1, . . . , jk+1) | e(j1, . . . , jk) ∈ Lk,\n",
      "Thus Lk (cid:111)(cid:110) Lk is the set of all k + 1 itemsets for which 2 subsets with\n",
      "k items each are frequent. As this condition also holds for all elements in\n",
      "Ck+1 one has Ck+1 ⊂ Lk (cid:111)(cid:110) Lk. The Ck+1 is then obtained by removing\n",
      "elements which contain infrequent subsets.\n",
      "if (1, 0, 1, 0, 0) ∈ L2 and (0, 1, 1, 0, 0) ∈ L2 then\n",
      "(1, 1, 1, 0, 0) ∈ L2 (cid:111)(cid:110) L2. If, in addition, (1, 1, 0, 0, 0) ∈ L2 then (1, 1, 1, 0, 0) ∈\n",
      "C3.\n",
      "\n",
      "For example,\n",
      "\n",
      "e(j1, . . . , jk−1, jk+1) ∈ Lk}.\n",
      "\n",
      "Having developed a construction for Ck we can now determine the\n",
      "bounds for the size of the candidate itemsets based purely on combinatorial\n",
      "considerations. The main tool for our discussion is the Kruskal-Katona the-\n",
      "orem. The proof of this theorem and much of the discussion follows closely\n",
      "the exposition in Chapter 5 of [?]. The bounds developed in this way have\n",
      "ﬁrst been developed in [?].\n",
      "We will denote the set of all possible k-itemsets or bitvectors with ex-\n",
      "actly k bits as Ik. Subsets of this set are sometimes also called hypergraphs\n",
      "in the literature. The set of candidate itemsets Ck ⊂ Ik.\n",
      "k − 1 subsets of the elements of Ck:\n",
      "\n",
      "Given a set of k-itemsets Ck the lower shadow of Ck is the set of all\n",
      "\n",
      "∂(Ck) := {y ∈ Ik−1 | y < z for some z ∈ Ck}.\n",
      "\n",
      "This is the set of bitvectors which have k − 1 bits set at places where some\n",
      "z ∈ Ck has them set. The shadow ∂Ck can be smaller or larger than the\n",
      "Ck. In general, one has for the size |∂Ck| ≥ k independent of the size of Ck.\n",
      "So, for example, if k = 20 then |∂Ck| ≥ 20 even if |Ck| = 1. (In this case we\n",
      "actually have |∂Ck| = 20.) For example, we have ∂C1 = ∅, and |∂C2| ≤ d.\n",
      "It follows now that the sequence of sets of itemsets Ck satisﬁes the\n",
      "\n",
      "apriori condition iﬀ\n",
      "\n",
      "∂(Ck) ⊂ Ck−1.\n",
      "\n",
      "The Kruskal-Katona Theorem provides an estimate of the size of the\n",
      "shadow.\n",
      "\n",
      "First, recall the mapping φ : X → N, deﬁned by:\n",
      "\n",
      "d−1(cid:88)\n",
      "\n",
      "i=0\n",
      "\n",
      "φ(x) =\n",
      "\n",
      "2ixi,\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "31\n",
      "\n",
      "and the induced order\n",
      "\n",
      "y ≺ z :⇔ φ(y) < φ(z)\n",
      "\n",
      "which is the colex (or colexicographic) order. In this order the itemset\n",
      "{3, 5, 6, 9} ≺ {3, 4, 7, 9} as the largest items determine the order. (In the\n",
      "lexicographic ordering the order of these two sets would be reversed.)\n",
      "Let [m] := {0, . . . , m − 1} and [m](k) be the set of all k-itemsets where\n",
      "k bits are set in the ﬁrst m positions and all other bits can be either 0 or 1.\n",
      "In the colex order any z where bits m (and beyond) are set are larger than\n",
      "any of the elements in [m](k). Thus [m]k is just the set of the ﬁrst\n",
      "bitvectors with k bits set.\n",
      "\n",
      "(cid:0)m−1\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "k\n",
      "\n",
      "We will now construct the sequence of the ﬁrst m bitvectors for any m.\n",
      "This corresponds to the ﬁrst numbers, which, in the binary representation\n",
      "have m ones set. Consider, for example the case of d = 5 and k = 2. For this\n",
      "case all the bitvectors are in table ??. (Printed with the lowest signiﬁcant\n",
      "bit on the right hand side for legibility.)\n",
      "\n",
      "(0,0,0,1,1)\n",
      "(0,0,1,0,1)\n",
      "(0,0,1,1,0)\n",
      "(0,1,0,0,1)\n",
      "(0,1,0,1,0)\n",
      "(0,1,1,0,0)\n",
      "(1,0,0,0,1)\n",
      "(1,0,0,1,0)\n",
      "(1,0,1,0,0)\n",
      "(1,1,0,0,0)\n",
      "\n",
      "3\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "12\n",
      "17\n",
      "18\n",
      "20\n",
      "24\n",
      "\n",
      "As before, we denote by ej the j − th atom and by e(j1, . . . , jk) the\n",
      "bitvector with bits j1, . . . , jk set to one. Furthermore, we introduce the\n",
      "element-wise join of a bitvector and a set C of bitvectors as:\n",
      "\n",
      "C ∨ y := {z ∨ y | z ∈ C}.\n",
      "\n",
      "For 0 ≤ s ≤ ms < ms+1 < ··· < mk we introduce the following set of\n",
      "k-itemsets:\n",
      "\n",
      "B(k)(mk, . . . , ms) :=\n",
      "\n",
      "[mj](j) ∨ e(mj+1, . . . , mk)\n",
      "\n",
      "⊂ Ik.\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "k(cid:91)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "j=s\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "32\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "k(cid:88)\n",
      "\n",
      "j=s\n",
      "\n",
      "mj\n",
      "j\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "As only term j does not contain itemsets with item mj (all the others\n",
      "\n",
      "do) the terms are pairwise disjoint and so the union contains\n",
      "\n",
      "|B(k)(mk, . . . , ms)| = b(k)(mk, . . . , ms) :=\n",
      "\n",
      "k-itemsets. This set contains the ﬁrst (in colex order) bitvectors with k bits\n",
      "set. By splitting oﬀ the last term in the union one then sees:\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "B(k−1)(mk−1, . . . , ms) ∨ emk\n",
      "\n",
      "∪ [mk](k)\n",
      "\n",
      "(3)\n",
      "\n",
      "B(k)(mk, . . . , ms) =\n",
      "\n",
      "and consequently\n",
      "\n",
      "b(k)(mk, . . . , ms) = b(k−1)(mk−1, . . . , ms) + b(k)(mk).\n",
      "\n",
      "Consider the example of table ?? of all bitvectors up to (1, 0, 0, 1, 0).\n",
      "There are 8 bitvectors which come earlier in the colex order. The highest\n",
      "bit set for the largest element is bit 5. As we consider all smaller elements\n",
      "we need to have all two-itemsets where the 2 bits are distributed between\n",
      "positions 1 to 4 and there are\n",
      "= 6 such bitvectors. The other cases have\n",
      "the top bit ﬁxed at position 5 and the other bit is either on position 1 or\n",
      "two thus there are\n",
      "= 2 bitvectors for which the top bit is ﬁxed. Thus\n",
      "we get a total of\n",
      "\n",
      "(cid:0)2\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "(cid:0)4\n",
      "(cid:18)\n",
      "\n",
      "4\n",
      "2\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "2\n",
      "1\n",
      "\n",
      "+\n",
      "\n",
      "= 8\n",
      "\n",
      "bitvectors up to (and including) bitvector (1, 0, 0, 1, 0) for which 2 bits are\n",
      "set. This construction is generalised in the following.\n",
      "\n",
      "In the following we will show that b(k)(mk, . . . , ms) provides a unique\n",
      "representation for the integers. We will make frequent use of the identity:\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "t + 1\n",
      "\n",
      "r\n",
      "\n",
      "− 1 =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "r(cid:88)\n",
      "\n",
      "t − r + l\n",
      "\n",
      "l=1\n",
      "\n",
      "l\n",
      "\n",
      ".\n",
      "\n",
      "(4)\n",
      "\n",
      "Lemma 10: For every m, k ∈ N there are numbers ms < ··· < mk such\n",
      "that\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "k(cid:88)\n",
      "\n",
      "j=s\n",
      "\n",
      "mj\n",
      "j\n",
      "\n",
      "m =\n",
      "\n",
      "(5)\n",
      "\n",
      "and the mj are uniquely determined by m.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "33\n",
      "\n",
      "Assume a decomposition of the form ?? is given. Using equation ?? one\n",
      "\n",
      "Proof: The proof is by induction over m. In the case of m = 1 one sees\n",
      "immediately that there can only be one term in the sum of equation (??),\n",
      "thus s = k and the only choice is mk = k.\n",
      "Now assume that equation ?? is true for some m(cid:48) = m − 1. We show\n",
      "uniqueness for m. We only need to show that mk is uniquely determined\n",
      "as the uniqueness of the other mj follows from the induction hypothesis\n",
      "\n",
      "(cid:1)\n",
      "\n",
      ".\n",
      "\n",
      "k\n",
      "\n",
      "gets:\n",
      "\n",
      "applied to m(cid:48) = m − m −(cid:0)mk\n",
      "(cid:18)\n",
      "(cid:19)\n",
      "(cid:18)\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "mk\n",
      "k\n",
      "mk\n",
      "k\n",
      "mk + 1\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "m =\n",
      "\n",
      "≤\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "=\n",
      "\n",
      "k\n",
      "\n",
      "as mk−1 ≤ mk − 1 etc. Thus we get\n",
      "≤ m ≤\n",
      "\n",
      "mk\n",
      "k\n",
      "\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "mk−1\n",
      "k − 1\n",
      "mk − 1\n",
      "k − 1\n",
      "− 1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "ms\n",
      "s\n",
      "mk − k + 1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "\n",
      "+ ··· +\n",
      "\n",
      "+ ··· +\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "mk + 1\n",
      "\n",
      "k\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "− 1.\n",
      "\n",
      "With other words, mk is the largest integer such that\n",
      "provides a unique characterisation of mk which proves uniqueness.\n",
      "\n",
      "k\n",
      "\n",
      "Assume that the mj be constructed according to the method outlined\n",
      "in the ﬁrst part of this proof. One can check that equation ?? holds for\n",
      "these mj using the characterisation.\n",
      "\n",
      "What remains to be shown is mj+1 > mj and using inductions, it is\n",
      "enough to show that mk−1 < mk. If, on the contrary, this does not hold\n",
      "and mk−1 ≥ mk, then one gets from ??:\n",
      "\n",
      "(cid:0)mk\n",
      "\n",
      "(cid:1) ≤ m. This\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "mk\n",
      "k\n",
      "mk\n",
      "k\n",
      "mk\n",
      "k\n",
      "mk\n",
      "k\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "mk−1\n",
      "k − 1\n",
      "mk\n",
      "k − 1\n",
      "mk − 1\n",
      "k − 1\n",
      "mk−1\n",
      "k − 1\n",
      "\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "m ≥\n",
      "\n",
      "≥\n",
      "\n",
      "=\n",
      "\n",
      "≥\n",
      "\n",
      "= m + 1\n",
      "\n",
      "which is not possible.\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "+ 1\n",
      "\n",
      "mk − k + 1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "+ ··· +\n",
      "\n",
      "+ ··· +\n",
      "\n",
      "ms\n",
      "s\n",
      "\n",
      "+ 1 by induction hyp.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "34\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Let N (k) be the set of all k-itemsets of integers. It turns out that the\n",
      "\n",
      "j\n",
      "\n",
      "j=s\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:0)mj\n",
      "\n",
      "B(k) occur as natural subsets of N (k):\n",
      "\n",
      "itemsets of N (k) (in colex order).\n",
      "\n",
      "Theorem 11: The set B(k)(mk, . . . , ms) consists of the ﬁrst m =\n",
      "\n",
      "(cid:80)k\n",
      "elements are still [mk](k). The remaining m −(cid:0)mk\n",
      "\n",
      "Proof: The proof is by induction over k − s. If k = s and thus m =\n",
      "then the ﬁrst elements of N (k) are just [mk](k). If k > s then the ﬁrst\n",
      "\n",
      "elements all contain\n",
      "bit mk. By the induction hypothesis the ﬁrst bk−1(mk−1, . . . , ms) elements\n",
      "containing bit mk are Bk−1(mk−1, . . . , ms)∨ emk and the rest follows from\n",
      "??.\n",
      "\n",
      "(cid:0)mk\n",
      "(cid:0)mk\n",
      "\n",
      "(cid:1)\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "The shadow of the ﬁrst k-itemsets B(k)(mk, . . . , ms) are the ﬁrst k − 1-\n",
      "\n",
      "itemsets, or more precisely:\n",
      "\n",
      "Lemma 12:\n",
      "\n",
      "∂B(k)(mk, . . . , ms) = B(k−1)(mk, . . . , ms).\n",
      "\n",
      "Proof: First we observe that in the case of s = k the shadow is simply set\n",
      "of all k − 1 itemsets:\n",
      "\n",
      "∂[mk]k = [mk](k−1).\n",
      "\n",
      "This can be used as anchor for the induction over k − s. As was shown\n",
      "\n",
      "earlier, one has in general:\n",
      "\n",
      "B(k)(mk, . . . , ms) = [mk]k ∪\n",
      "and, as the shadow is additive, as\n",
      "∂B(k)(mk, . . . , ms) = [mk](k−1)∪\n",
      "Note that B(k−1)(mk−1, . . . , ms) ⊂ [ml](k−1).\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:16)\n",
      "B(k−1)(mk−1, . . . , ms) ∨ emk\n",
      "(cid:17)\n",
      "\n",
      "B(k−2)(mk−1, . . . , ms) ∨ emk\n",
      "\n",
      "= B(k−1)(mk, . . . , ms).\n",
      "\n",
      "The shadow is important for the apriori property and we would thus\n",
      "like to determine the shadow, or at least its size for more arbitrary k-\n",
      "itemsets as they occur in the apriori algorithm. Getting bounds is feasible\n",
      "but one requires special technology to do this. This is going to be developed\n",
      "further in the sequel. We would like to reduce the case of general sets of\n",
      "k-itemsets to the case of the previous lemma, where we know the shadow.\n",
      "So we would like to ﬁnd a mapping which maps the set of k-itemsets to the\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "35\n",
      "\n",
      "ﬁrst k itemsets in colex order without changing the size of the shadow. We\n",
      "will see that this can almost be done in the following. The way to move the\n",
      "itemsets to earlier ones (or to “compress” them) is done by moving later\n",
      "bits to earlier positions.\n",
      "\n",
      "So we try to get the k itemsets close to B(k)(mk, . . . , ms) in some sense,\n",
      "so that the size of the shadow can be estimated. In order to simplify no-\n",
      "tation we will introduce z + ej for z ∨ ej when ej (cid:54)≤ z and the reverse\n",
      "operation (removing the j-th bit) by z − ej when ej ≤ z. Now we introduce\n",
      "compression of a bitvector as\n",
      "\n",
      "(cid:40)\n",
      "\n",
      "Rij(z) =\n",
      "\n",
      "z − ej + ei\n",
      "z\n",
      "\n",
      "if ei (cid:54)≤ z and ej ≤ z\n",
      "else\n",
      "\n",
      "Thus we simply move the bit in position j to position i if there is a bit in\n",
      "position j and position i is empty. If not, then we don’t do anything. So we\n",
      "did not change the number of bits set. Also, if i < j then we move the bit\n",
      "to an earlier position so that Rij(z) ≤ z. For our earlier example, when we\n",
      "number the bits from the right, starting with 0 we get R1,3((0, 1, 1, 0, 0)) =\n",
      "(0, 0, 1, 1, 0) and R31((0, 0, 0, 1, 1)) = (0, 0, 0, 1, 1). This is a “compression”\n",
      "as it moves a collection of k-itemsets closer together and closer to the vector\n",
      "z = 0 in terms of the colex order.\n",
      "\n",
      "The mapping Rij is not injective as\n",
      "\n",
      "Rij(z) = Rij(y)\n",
      "\n",
      "when y = Rij(z) and this is the only case. Now consider for any set C of\n",
      "ij (C) ∩ C. These are those elements of C which stay\n",
      "bitvectors the set R−1\n",
      "in C when compressed by Rij. The compression operator for bitsets is now\n",
      "deﬁned as\n",
      "\n",
      "˜Ri,j(C) = Rij(C) ∪ (C ∩ R−1\n",
      "\n",
      "ij (C)).\n",
      "\n",
      "Thus the points which stay in C under Rij are retained and the points\n",
      "which are mapped outside C are added. Note that by this we have avoided\n",
      "the problem with the non-injectivity as only points which stay in C can\n",
      "be mapped onto each other. The size of the compressed set is thus the\n",
      "same. However, the elements in the ﬁrst part have been mapped to earlier\n",
      "elements in the colex order. In our earlier example, for i, j = 1, 3 we get\n",
      "\n",
      "C = {(0, 0, 0, 1, 1), (0, 1, 1, 0, 0), (1, 1, 0, 0, 0), (0, 1, 0, 1, 0)}\n",
      "\n",
      "we get\n",
      "\n",
      "˜Ri,j(C) = {(0, 0, 0, 1, 1), (0, 0, 1, 1, 0), (1, 1, 0, 0, 0), (0, 1, 0, 1, 0)}.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "36\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Corresponding to this compression of sets we introduce a mapping ˜Ri,j\n",
      "(which depends on C) by ˜Ri,j(y) = y if Ri,j(y) ∈ C and ˜Ri,j(y) = Ri,j(y)\n",
      "else. In our example this maps corresponding elements in the sets onto each\n",
      "other. In preparation, for the next lemma we need the simple little result:\n",
      "Lemma 13: Let C be any set of k itemsets and z ∈ C, ej ≤ z, ei (cid:54)≤ z. Then\n",
      "\n",
      "z − ej + ei ∈ ˜Ri,j(C).\n",
      "\n",
      "Proof: There are two cases to consider:\n",
      "(1) Either z − ej + ei (cid:54)∈ C in which case z − ej + ei = ˜Ri,j(z).\n",
      "(2) Or z − ej + ei ∈ C and as z − ej + ei = ˜Ri,j(z − ej + ei) one gets again\n",
      "\n",
      "z − ej + ei ∈ ˜Ri,j(C).\n",
      "\n",
      "The next result shows that in terms of the shadow, the “compression”\n",
      "˜Ri,j really is a compression as the shadow of a compressed set can never\n",
      "be larger than the shadow of the original set. We suggest therefor to call it\n",
      "compression lemma.\n",
      "\n",
      "Lemma 14: Let C be a set of k itemsets. Then one has\n",
      "\n",
      "∂ ˜Ri,j(C) ⊂ ˜Ri,j(∂C).\n",
      "Proof: Let x ∈ ∂ ˜Ri,j(C). We need to show that\n",
      "\n",
      "x ∈ ˜Ri,j(∂C)\n",
      "\n",
      "and we will enumerate all possible cases.\n",
      "\n",
      "First notice that there exists a ek (cid:54)≤ x such that\n",
      "\n",
      "x + ek ∈ ˜Ri,j(C)\n",
      "\n",
      "so there is an y ∈ C such that\n",
      "\n",
      "x + ek = ˜Ri,j(y).\n",
      "\n",
      "(1) In the ﬁrst two cases ˜Ri,j(y) (cid:54)= y and so one has (by the previous\n",
      "\n",
      "lemma)\n",
      "\n",
      "x + ek = y − ej + ei,\n",
      "\n",
      "for some y ∈ C, ej ≤ y, ei (cid:54)≤ y.\n",
      "\n",
      "(a) First consider i (cid:54)= k. Then there is a bitvector z such that y = z+ek\n",
      "\n",
      "and z ∈ ∂C. Thus we get\n",
      "\n",
      "x = z − ej + ei ∈ ˜Ri,j(∂C)\n",
      "\n",
      "as z ∈ ∂C and with lemma ??.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "37\n",
      "\n",
      "(b) Now consider i = k. In this case x + ei = y − ej + ei and so\n",
      "\n",
      "x = y − ej ∈ ∂C. As ej (cid:54)≤ x one gets\n",
      "\n",
      "x = ˜Ri,j(x) ∈ ˜Ri,j(∂C).\n",
      "\n",
      "(2) In the remaining cases ˜Ri,j(y) = y, i.e., x + ek = ˜Ri,j(x + ek). Thus\n",
      "x + ek = y ∈ C and so x ∈ ∂C. Note that ˜Ri,j actually depends on ∂C!\n",
      "(a) In the case where ej (cid:54)≤ x one has x = ˜Ri,j(x) ∈ ˜Ri,j(∂C).\n",
      "(b) In the other case ej ≤ x. We will show that x = ˜Ri,j(x) and, as\n",
      "\n",
      "x ∈ ∂C one gets x ∈ ˜Ri,j(∂C).\n",
      "i. If k (cid:54)= i then one can only have x + ek = ˜Ri,j(x + ek) if either\n",
      "ei ≤ x, in which case x = ˜Ri,j(x), or x− ej + ei + ek ∈ C in which\n",
      "case x − ej + ei ∈ ∂C and so x = ˜Ri,j(x).\n",
      "ii. Finally, if k = i, then x + ei ∈ C and so x − ej + ei ∈ ∂C thus\n",
      "\n",
      "x = ˜Ri,j(x).\n",
      "\n",
      "The operator ˜Ri,j maps sets of k itemsets onto sets of k itemsets and\n",
      "does not change the number of elements in a set of k itemsets. One now\n",
      "says that a set of k itemsets C is compressed if ˜Ri,j(C) = C for all i < j.\n",
      "This means that for any z ∈ C one has again Rij(z) ∈ C. Now we can move\n",
      "to prove the key theorem:\n",
      "Theorem 15: Let k ≥ 1, A ⊂ N (k), s ≤ ms < ··· < mk and\n",
      "\n",
      "then\n",
      "\n",
      "|A| ≥ b(k)(mk, . . . , ms)\n",
      "\n",
      "|∂A| ≥ b(k−1)(mk, . . . , ms).\n",
      "\n",
      "Proof: First we note that the shadow is a monotone function of the un-\n",
      "derlying set, i.e., if A1 ⊂ A2 then ∂A1 ⊂ ∂A2. From this it follows that it\n",
      "is enough to show that the bound holds for |A| = b(k)(mk, . . . , ms).\n",
      "\n",
      "Furthermore, it is suﬃcient to show this bound for compressed A as\n",
      "compression at most reduces the size of the shadow and we are looking for\n",
      "a lower bound. Thus we will assume A to be compressed in the following.\n",
      "The proof uses double induction over k and m = |A|. First we show\n",
      "that the theorem holds for the cases of k = 1 for any m and m = 1 for any\n",
      "k. In the induction step we show that if the theorem holds for 1, . . . , k − 1\n",
      "and any m and for 1, . . . , m − 1 and k then it also holds for k and m, see\n",
      "ﬁgure (??).\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "38\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Fig. 9. Double induction\n",
      "\n",
      "In the case of k = 1 (as A is compressed) one has:\n",
      "A = B(1)(m) = {e0, . . . , em−1}\n",
      "\n",
      "and so\n",
      "\n",
      "∂A = ∂B(1)(m) = {0}\n",
      "\n",
      "hence |∂A| = 1 = b(0)(m).\n",
      "\n",
      "In the case of m = |A| = 1 one has:\n",
      "\n",
      "and so:\n",
      "\n",
      "A = B(k)(k) = {e(0, . . . , k − 1)}\n",
      "\n",
      "∂A = ∂B(k)(k) = [k](k−1)\n",
      "\n",
      "hence |∂A| = k = b(k−1)(k).\n",
      "The key step of the proof is a partition of A into bitvectors with bit 0 set\n",
      "and such for which bit 0 is not set: A = A0∪A1 where A0 = {x ∈ A|x0 = 0}\n",
      "and A1 = {x ∈ A|x0 = 1}.\n",
      "(1) If x ∈ ∂A0 then x + ej ∈ A0 for some j > 0. As A is compressed it\n",
      "must also contain x + e0 = R0j(x + ej) ∈ A1 and so x ∈ A1 − e0 thus\n",
      "\n",
      "|∂A0| ≤ |A1 − e0| = |A1|.\n",
      "\n",
      "(2) A special case is A = B(k)(mk, . . . , ms) where one has |A0| = b(k)(mk−\n",
      "\n",
      "1, . . . , ms − 1) and |A1| = b(k−1)(mk − 1, . . . , ms − 1) and thus\n",
      "m = b(k)(mk, . . . , ms) = b(k)(mk−1, . . . , ms−1)+b(k−1)(mk−1, . . . , ms−1)\n",
      "\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u00011m−1mkk−1\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "39\n",
      "\n",
      "(3) Now partition ∂A1 into 2 parts:\n",
      "\n",
      "∂A1 = (A1 − e0) ∪ (∂(A1 − e0) + e0).\n",
      "\n",
      "It follows from previous inequalities and the induction hypothesis that\n",
      "|∂A1| = |A1−e0|+|∂(A1−e0)+e0| = |A1|+|∂(A1−e0)| ≥ b(k−1)(mk−\n",
      "1, . . . , ms − 1) + b(k−2)(mk − 1, . . . , ms − 1) = b(k−1)(mk, . . . , ms) and\n",
      "hence\n",
      "\n",
      "|∂A| ≥ |∂A1| ≥ b(k−1)(mk, . . . , ms)\n",
      "\n",
      "This theorem is the tool to derive the bounds for the size of future candidate\n",
      "itemsets based on a current itemset and the apriori principle.\n",
      "\n",
      "Theorem 16: Let the sequence Ck satisfy the apriori property and let\n",
      "\n",
      "|Ck| = b(k)(mk, . . . , mr).\n",
      "\n",
      "|Ck+p| ≤ b(k+p)(mk, . . . , mr)\n",
      "\n",
      "Then\n",
      "\n",
      "for all p ≤ r.\n",
      "\n",
      "Proof: The reason for the condition on p is that the shadows are well\n",
      "deﬁned.\n",
      "First, we choose r such that mr ≤ r + p − 1, mr+1 ≤ r + 1 + p − 1, . . .,\n",
      "ms−1 ≤ s − 1 + p − 1 and ms ≥ s + p − 1. Note that s = r and s = k + 1\n",
      "may be possible.\n",
      "\n",
      "Now we get an upper bound for the size |Ck|:\n",
      "\n",
      "|Ck| = b(k)(mk, . . . , mr)\n",
      "\n",
      "≤ b(k)(mk, . . . , ms) +\n",
      "\n",
      "= b(k)(mk, . . . , ms) +\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "s−1(cid:88)\n",
      "(cid:18)\n",
      "\n",
      "j=1\n",
      "\n",
      "j + p − 1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "j\n",
      "s + p − 1\n",
      "s − 1\n",
      "\n",
      "− 1\n",
      "\n",
      "according to a previous lemma.\n",
      "\n",
      "If the theorem does not hold then |Ck+p| > b(k+p)(mj, . . . , mr) and thus\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "|Ck+p| ≥ b(k+p)(mj, . . . , mr) + 1\n",
      "≥ b(k+p)(mk, . . . , ms) +\n",
      "= b(k+p)(mk, . . . , ms, s + p − 1).\n",
      "\n",
      "s + p − 1\n",
      "s + p − 1\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "40\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Here we can apply the previous theorem to get a lower bound for Ck:\n",
      "\n",
      "|Ck| ≥ b(k)(mk, . . . , ms, s + p − 1).\n",
      "\n",
      "This, however is contradicting the higher upper bound we got previously\n",
      "and so we have to have |Ck+p| ≤ b(k+p)(mj, . . . , mr).\n",
      "\n",
      "As a simple consequence one also gets tightness:\n",
      "\n",
      "Corollary 17: For any m and k there exists a Ck with |Ck| = m =\n",
      "b(k+p)(mk, . . . , ms+1). such that\n",
      "\n",
      "|Ck+p| = b(k+p)(mk, . . . , ms+1).\n",
      "\n",
      "Proof: The Ck consists of the ﬁrst m k-itemsets in the colexicographic\n",
      "ordering.\n",
      "\n",
      "by the theory. A consequence of the theorem is that for Lk with |Lk| ≤(cid:0)mk\n",
      "one has |Ck+p| ≤(cid:0) mk\n",
      "\n",
      "In practice one would know not only the size but also the contents of any\n",
      "Ck and from that one can get a much better bound than the one provided\n",
      ". In particular, one has Ck+p = ∅ for k > mp − p.\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "k\n",
      "\n",
      "k+p\n",
      "\n",
      "4. Extensions\n",
      "\n",
      "4.1. Apriori Tid\n",
      "One variant of the apriori algorithm discussed above computes supports\n",
      "of itemsets by doing intersections of columns. Some of these intersections\n",
      "are repeated over time and, in particular, entries of the Boolean matrix\n",
      "are revisited which have no impact on the support. The Apriori TID [?]\n",
      "algorithm provides a solution to some of these problems. For computing\n",
      "the supports for larger itemsets it does not revisit the original table but\n",
      "transforms the table as it goes along. The new columns correspond to the\n",
      "candidate itemsets. In this way each new candidate itemset only requires\n",
      "the intersection of two old ones.\n",
      "\n",
      "The following demonstrates with an example how this works. The exam-\n",
      "ple is adapted from [?]. In the ﬁrst row the itemsets from Ck are depicted.\n",
      "The minimal support is 50 percent or 2 rows. The initial matrix of the tid\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "41\n",
      "\n",
      "algorithm is equal to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 2 3 4 5\n",
      "1 0 1 1 0\n",
      "0 1 1 0 1\n",
      "1 1 1 0 1\n",
      "0 1 0 0 1\n",
      "\n",
      "Note that the column (or item) four is not frequent and is not considered\n",
      "for Ck. After one step of the Apriori tid one gets the matrix:\n",
      "\n",
      "(1, 2) (1, 3) (1, 5) (2, 3) (2, 5) (3, 5)\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Here one can see directly that the itemsets (1, 2) and (1, 5) are not frequent.\n",
      "It follows that there remains only one candidate itemset with three items,\n",
      "\n",
      "namely (2, 3, 5) and the matrix is\n",
      "\n",
      "\n",
      "\n",
      "(2, 3, 5)\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "Let z(j1, . . . , jk) denote the elements of Ck. Then the elements in the trans-\n",
      "formed Boolean matrix are az(j1,...,jk)(xi).\n",
      "We will again use an auxiliary array v ∈ {0, 1}n. The apriori tid algo-\n",
      "rithm uses the join considered earlier in order to construct a matrix for\n",
      "the frequent itemsets Lk+1 from Lk. (As in the previous algorithms it is\n",
      "assumed that all matrices are stored in memory. The case of very large data\n",
      "sets which do not ﬁt into memory will be discussed later.) The key part of\n",
      "the algorithm, i.e., the step from k to k + 1 is then:\n",
      "\n",
      "(1) Select a pair of frequent k-itemsets (y, z), mark as read\n",
      "(2) expand xy =\n",
      "(3) extract elements using xz, i.e., w ← v[xz]\n",
      "(4) compress result and reset v to zero, v ← 0\n",
      "\n",
      "i , i.e., v ← xy\n",
      "\n",
      "i xyi\n",
      "\n",
      "There are three major steps where the auxiliary vector v is accessed. The\n",
      "\n",
      "\n",
      "\n",
      "(cid:86)\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "42\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "time complexity for this is\n",
      "\n",
      "T =\n",
      "\n",
      "n(cid:88)\n",
      "\n",
      "(2|(x(i))y| + |(x(i))z|)τ.\n",
      "\n",
      "This has to be done for all elements y∨ z where y, z ∈ Lk. Thus the average\n",
      "complexity is\n",
      "\n",
      "i=1\n",
      "\n",
      "E(T ) =\n",
      "\n",
      "3nmkE(xy)τ\n",
      "\n",
      "for some “average” y and xy =\n",
      "i . Now for all elements in Lk the\n",
      "support is larger than σ, thus E(xy) ≥ σ. So we get a lower bound for the\n",
      "complexity:\n",
      "\n",
      "i xyi\n",
      "\n",
      "E(T ) ≥\n",
      "\n",
      "3nmkστ.\n",
      "\n",
      "We can also obtain a simple upper bound if we observe that E(xy) ≤\n",
      "E(|x|)/d which is true “on average”. From this we get\n",
      "\n",
      "(cid:88)\n",
      "(cid:86)\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "E(T ) ≤\n",
      "\n",
      "3nmk\n",
      "\n",
      "E(|x|)\n",
      "\n",
      "τ.\n",
      "\n",
      "d\n",
      "\n",
      "Another approximation (typically a lower bound) is obtained if we assume\n",
      "that the components of x are independent. In this case E(xy) ≈ (E(|x|)/d)k\n",
      "and thus\n",
      "\n",
      "E(T ) ≥\n",
      "\n",
      "3nmk(E(|x|)/d)kτ.\n",
      "\n",
      "From this we would expect that for some rk ∈ [1, k] we get the approxima-\n",
      "tion\n",
      "\n",
      "E(T ) ≈\n",
      "\n",
      "3nmk(E(|x|)/d)rk τ.\n",
      "\n",
      "Now recall that the original column-wise apriori implementation required\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "and so the “speedup” we can achieve by using this new algorithm is around\n",
      "\n",
      "E(T ) ≈\n",
      "\n",
      "3nmkk(E(|x|)/d)τ\n",
      "\n",
      "k\n",
      "\n",
      "(cid:80)\n",
      "(cid:80)\n",
      "k(E(|x|)/d)rk−1mk\n",
      "\n",
      "k kmk\n",
      "\n",
      ".\n",
      "\n",
      "S ≈\n",
      "\n",
      "which can be substantial as both k ≥ 1 and E(|x|)/d)rk−1 < 1. We can see\n",
      "that there are two reasons for the decrease in work: First we have reused\n",
      "earlier computations of xj1 ∧ ··· ∧ xjk and second we are able to make use\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "43\n",
      "\n",
      "of the lower support of the k-itemsets for larger k. While this second eﬀect\n",
      "does strongly depend on rk and thus the data, the ﬁrst eﬀect always holds,\n",
      "so we get a speedup of at least\n",
      "\n",
      "(cid:80)\n",
      "k kmk(cid:80)\n",
      "\n",
      "k mk\n",
      "\n",
      ",\n",
      "\n",
      "S ≥\n",
      "\n",
      "i.e., the average size of the k-itemsets. Note that the role of the number mk\n",
      "of candidate itemsets maybe slightly diminished but this is still the core\n",
      "parameter which determines the complexity of the algorithm and the need\n",
      "to reduce the size of the frequent itemsets is not diminished.\n",
      "\n",
      "4.2. Constrained association rules\n",
      "The number of frequent itemsets found by the apriori algorithm will of-\n",
      "ten be too large or too small. While the prime mechanism of controlling\n",
      "the discovered itemsets is the minimal support σ, this may often not be\n",
      "enough. Small collections of frequent itemsets may often contain mostly\n",
      "well known associations whereas large collections may reﬂect mostly ran-\n",
      "dom ﬂuctuations. There are eﬀective other ways to control the amount of\n",
      "itemsets obtained. First, in the case of too many itemsets one can use con-\n",
      "straints to ﬁlter out trivial or otherwise uninteresting itemsets. In the case\n",
      "of too few frequent itemsets one can also change the attributes or features\n",
      "which deﬁne the vector x. In particular, one can introduce new “more gen-\n",
      "eral” attributes. For example, one might ﬁnd that rules including the item\n",
      "“ginger beer” are not frequent. However, rules including “soft drinks” will\n",
      "have much higher support and may lead to interesting new rules. Thus one\n",
      "introduces new more general items. However, including more general items\n",
      "while maintaining the original special items leads to duplications in the\n",
      "itemsets, in our example the itemset containing ginger beer and soft drinks\n",
      "is identical to the set which only contains ginger beer. In order to avoid this\n",
      "one can again introduce constraints, which, in our example would identify\n",
      "the itemset containing ginger beer only with the one containing softdrink\n",
      "and ginger beer.\n",
      "\n",
      "Constraints are conditions for the frequent itemsets of interest. These\n",
      "\n",
      "conditions take the form “predicate = true” with some predicates\n",
      "\n",
      "b1(z), . . . , bs(z).\n",
      "\n",
      "Thus one is looking for frequent k-itemsets L∗\n",
      "i.e.,\n",
      "\n",
      "k for which the bj are true,\n",
      "\n",
      "k := {z ∈ Lk | bj(z) = 1}.\n",
      "L∗\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "44\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "These constraints will reduce the amount of frequent itemsets which need\n",
      "to be further processed, but can they also assist in making the algorithms\n",
      "more eﬃcient? This will be discussed next after we have considered some\n",
      "examples. Note that the constraints are not necessarily simple conjunctions!\n",
      "Examples:\n",
      "• We have mentioned the rule that any frequent itemset should not con-\n",
      "tain an item and its generalisation, e.g., it should not contain both soft\n",
      "drinks and ginger beer as this is identical to ginger beer. The constraint\n",
      "is of the form b(x) = ¬ay(x) where y is the itemset where the “softdrink\n",
      "and ginger beer bits” are set.\n",
      "• In some cases, frequent itemsets have been well established earlier. An\n",
      "example are crisps and soft drinks. There is no need to rediscover this\n",
      "association. Here the constraint is of the form b(x) = ¬δy(x) where y\n",
      "denotes the itemset “softdrinks and chips”.\n",
      "• In some cases, the domain knowledge tells us that some itemsets are pre-\n",
      "scribed, like in the case of a medical schedule which prescribes certain\n",
      "procedures to be done jointly but others should not be jointly. Finding\n",
      "these rules is not interesting. Here the constraint would exclude certain\n",
      "z, i.e., b(z) = ¬δy(z) where y is the element to exclude.\n",
      "• In some cases, the itemsets are related by deﬁnition. For example the\n",
      "predicates deﬁned by |z| > 2 is a consequence of |z| > 4. Having discov-\n",
      "ered the second one relieves us of the need to discover the ﬁrst one. This,\n",
      "however, is a diﬀerent type of constraint which needs to be considered\n",
      "when deﬁning the search space.\n",
      "A general algorithm for the determination of the L∗\n",
      "\n",
      "k determines at every\n",
      "step the Lk (which are required for the continuation) and from those out-\n",
      "puts the elements of L∗\n",
      "k. The algorithm is exactly the same as apriori or apri-\n",
      "ori tid except that not all frequent itemsets are output. See Algorithm ??.\n",
      "The work is almost exactly the same as for the original apriori algorithm.\n",
      "Now we would like to understand how the constraints can impact the\n",
      "computational performance, after all, one will require less rules in the\n",
      "end and the discovery of less rules should be faster. This, however, is not\n",
      "straight-forward as the constrained frequent itemsets L∗\n",
      "k do not necessar-\n",
      "ily satisfy the apriori property. There is, however an important class of\n",
      "constraints for which the apriori property holds:\n",
      "\n",
      "Theorem 18: If the constraints bj, j = 1, . . . , m are anti-monotone then\n",
      "k} satisﬁes the apriori condition.\n",
      "the set of constrained frequent itemsets {L∗\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "45\n",
      "\n",
      "Algorithm 2 Apriori with general constraints\n",
      "C1 = A(X) is the set of all one-itemsets, k = 1\n",
      "while Ck (cid:54)= ∅ do\n",
      "\n",
      "scan database to determine support of all z ∈ Ck\n",
      "extract frequent itemsets from Ck into Lk\n",
      "use the constraints to extract the constrained frequent itemsets in L∗\n",
      "generate Ck+1\n",
      "k := k + 1.\n",
      "\n",
      "k\n",
      "\n",
      "end while\n",
      "\n",
      "Proof: Let y ∈ L∗\n",
      "itemsets) Lk satisfy the apriori condition one has z ∈ Lsize(z).\n",
      "\n",
      "k ⊂ Lk and the (unconstrained frequent\n",
      "\n",
      "k and z ≤ y. As L∗\n",
      "\n",
      "As the bj are antimonotone and y ∈ L∗\n",
      "\n",
      "k one has\n",
      "\n",
      "bj(z) ≥ bj(y) = 1\n",
      "\n",
      "and so bj(z) = 1 from which it follows that z ∈ L∗\n",
      "\n",
      "size(z).\n",
      "\n",
      "When the apriori condition holds one can generate the candidate item-\n",
      "sets Ck in the (constrained) apriori algorithm from the sets L∗\n",
      "k instead of\n",
      "from the larger Lk. However, the constraints need to be anti-monotone. We\n",
      "know that constraints of the form az(j) are monotone and thus constraints\n",
      "of the form bj = ¬az(j) are antimonotone. Such constraints say that a cer-\n",
      "tain combination of items should not occur in the itemset. An example of\n",
      "this is the case of ginger beer and soft drinks. Thus we will have simpler\n",
      "frequent itemsets in general if we apply such a rule. Note that itemsets have\n",
      "played three diﬀerent roles so far:\n",
      "\n",
      "(1) as data points x(i)\n",
      "(2) as potentially frequent itemsets z and\n",
      "(3) to deﬁne constraints ¬az(j).\n",
      "\n",
      "The constraints of the kind bj = ¬az(j) are now used to reduce the\n",
      "candidate itemsets Ck prior to the data scan (this is how we save most).\n",
      "Even better, it turns out that the conditions only need to be checked for\n",
      "level k = |z(j)| where k is the size of the itemset deﬁning the constraint.\n",
      "(This gives a minor saving.) This is summarised in the next theorem:\n",
      "Theorem 19: Let the constraints be bj = ¬az(j) for j = 1, . . . , s. Further-\n",
      "more let the candidate k-itemsets for L∗\n",
      "\n",
      "k be sets of k-itemsets such that\n",
      "\n",
      "k = {y ∈ Ik |\n",
      "C∗\n",
      "\n",
      "if z < y then z ∈ L|z| and bj(y) = 1}\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "46\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "and a further set deﬁned by\n",
      "˜Ck = {y ∈ Ik | if z < y then z ∈ L|z| and if |z(j)| = k then bj(y) = 1 }.\n",
      "Then ˜Ck = C∗\n",
      "k .\n",
      "Proof: We need to show that every element y ∈ ˜Ck satisﬁes the constraints\n",
      "bj(y) = 1. Remember that |y| = k. There are three cases:\n",
      "• If |z(j)| = |y| then the constraint is satisﬁed by deﬁnition\n",
      "• If |z(j)| > |y| then z(j) (cid:54)≤ y and so bj(y) = 1\n",
      "• Consider the case |z(j)| < |y|. If bj(y) = 0 then az(j)(y) = 1 and\n",
      "so z(j) ≤ y. As |z(j)| < |y| it follows z(j) < y. Thus it follows that\n",
      "z(j) ∈ L∗\n",
      "z(j) and, consequently, bj(z(j)) = 1 or z(j) (cid:54)≤ z(j) which is not\n",
      "true. It follows that in this case we have bj(y) = 1.\n",
      "\n",
      "From this it follows that ˜Ck ⊂ C∗\n",
      "the deﬁnition of the sets.\n",
      "\n",
      "k. The converse is a direct consequence of\n",
      "\n",
      "Thus we get a variant of the apriori algorithm which checks the constraints\n",
      "only for one level, and moreover, this is done to reduce the number of\n",
      "candidate itemsets. This is Algorithm ??.\n",
      "\n",
      "Algorithm 3 Apriori with antimonotone constraints\n",
      "\n",
      "C1 = A(X) is the set of all one-itemsets, k = 1\n",
      "while Ck (cid:54)= ∅ do\n",
      "\n",
      "extract elements of Ck which satisfy the constraints az(j)(x) = 0 for\n",
      "|z(j)| = k and put into C∗\n",
      "scan database to determine support of all y ∈ C∗\n",
      "extract frequent itemsets from C∗\n",
      "generate Ck+1 (as per ordinary apriori)\n",
      "k := k + 1.\n",
      "\n",
      "k into L∗\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "end while\n",
      "\n",
      "4.3. Partitioned algorithms\n",
      "The previous algorithms assumed that all the data was able to ﬁt into\n",
      "main memory and was resident in one place. Also, the algorithm was for\n",
      "one processor. We will look here into partitioned algorithms which lead to\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "47\n",
      "\n",
      "parallel, distributed and out-of-core algorithms with few synchronisation\n",
      "points and little disk access. The algorithms have been suggested in [?].\n",
      "\n",
      "We assume that the data is partitioned into equal parts as\n",
      "\n",
      "D = [D1, D2, . . . , Dp]\n",
      "\n",
      "where D1 = (x(1), . . . , x(n/p)), D2 = (x(n/p+1), . . . , x(2n/p)), etc. While we\n",
      "assume equal distribution it is simple to generalise the discussions below to\n",
      "non-equal distributions.\n",
      "\n",
      "In each partition Dj an estimate for the support s(a) of a predicate\n",
      "can be determined and we will call this ˆsj(a). If ˆs(a) is the estimate of the\n",
      "support in D then one has\n",
      "\n",
      "p(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "ˆs(a) =\n",
      "\n",
      "1\n",
      "p\n",
      "\n",
      "ˆsj(a).\n",
      "\n",
      "This leads to a straight-forward parallel implementation of the apriori al-\n",
      "gorithm: The extraction of the Lk can either be done on all the processors\n",
      "\n",
      "Algorithm 4 Parallel Apriori\n",
      "\n",
      "C1 = A(X) is the set of all one-itemsets, k = 1\n",
      "while Ck (cid:54)= ∅ do\n",
      "\n",
      "scan database to determine support of all z ∈ Ck on each Dj and sum\n",
      "up the results\n",
      "extract frequent itemsets from Ck into Lk\n",
      "generate Ck+1\n",
      "k := k + 1.\n",
      "\n",
      "end while\n",
      "\n",
      "redundantly or on one master processor and the result can then be com-\n",
      "municated. The parallel algorithm also leads to an out-of-core algorithm\n",
      "which does the counting of the supports in blocks. One can equally develop\n",
      "an apriori-tid variant as well.\n",
      "\n",
      "There is a disadvantage of this straight-forward approach, however. It\n",
      "does require many synchronisation points, respectively, many scans of the\n",
      "disk, one for each level. As the disks are slow and synchronisation expensive\n",
      "this will cost some time. We will not discuss and algorithm suggested by [?]\n",
      "which substantially reduces disk scans or synchronisation points at the cost\n",
      "of some redundant computations. First we observe that\n",
      "\n",
      "min\n",
      "\n",
      "k\n",
      "\n",
      "ˆsk(a) ≤ ˆs(a) ≤ max\n",
      "\n",
      "k\n",
      "\n",
      "ˆsk(a)\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "48\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "which follows from the summation formula above. A consequence of this is\n",
      "\n",
      "Theorem 20: Each a which is frequent in D is at least frequent in one\n",
      "Dj.\n",
      "\n",
      "Proof: If for some frequent a this would not hold then one would get\n",
      "\n",
      "max ˆsj(a) < σ0\n",
      "\n",
      "if σ0 is the threshold for frequent a. By the observation above ˆs(a) < σ0\n",
      "which contradicts the assumption that a is frequent.\n",
      "\n",
      "Using this one gets an algorithm which generates in a ﬁrst step frequent k-\n",
      "itemsets Lk,j for each Dj and each k. This requires one scan of the data, or\n",
      "can be done on one processor, respectively. The union of all these frequent\n",
      "itemset is then used as a set of candidate itemsets and the supports of all\n",
      "these candidates is found in a second scan of the data. The parallel variant of\n",
      "the algorithm is then Algorithm ??. Note that the supports for all the levels\n",
      "\n",
      "(cid:83)p\n",
      "\n",
      "Algorithm 5 Parallel Association Rules\n",
      "\n",
      "j=1 Lk,j and broadcast\n",
      "\n",
      "determine the frequent k-itemsets Lk,j for all Dj in parallel\n",
      "C p\n",
      "k :=\n",
      "determine supports ˆsk for all candidates and all partitions in parallel\n",
      "collect all the supports, sum up and extract the frequent elements from\n",
      "C p\n",
      "k.\n",
      "\n",
      "k are collected simultaneously thus they require only two synchronisation\n",
      "points. Also, the apriori property holds for the C p\n",
      "k:\n",
      "\n",
      "Proposition 21: The sequence C p\n",
      "\n",
      "k satisﬁes the apriori property, i.e.,\n",
      "\n",
      "z ∈ C p\n",
      "\n",
      "k & y ≤ z ⇒ y ∈ C p|y|.\n",
      "\n",
      "Proof: If z ∈ C p\n",
      "the apriori property on Dj one has y ∈ L|y|,j and so y ∈ C p|y|.\n",
      "\n",
      "k & y ≤ z then there exists a j such that z ∈ Lk,j. By\n",
      "\n",
      "In order to understand the eﬃciency of the algorithm one needs to esti-\n",
      "mate the size of the C p\n",
      "k. In the (computationally best case, all the frequent\n",
      "itemsets are identiﬁed on the partitions and thus\n",
      "\n",
      "C p\n",
      "k = Lk,j = Lk.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "49\n",
      "\n",
      "We can use any algorithm to determine the frequent itemsets on one par-\n",
      "tition, and, if we assume that the algorithm is scalable in the data size the\n",
      "time to determine the frequent itemsets on all processors is equal to 1/p of\n",
      "the time required to determine the frequent itemsets on one processor as\n",
      "the data is 1/p on each processor. In addition we require to reads of the\n",
      "data base which has an expectation of nλτDisk/p where λ is the average\n",
      "size of the market baskets and τDisk is the time for one disk access. There\n",
      "is also some time required for the communication which is proportional to\n",
      "the size of the frequent itemsets. We will leave the further analysis which\n",
      "follows the same lines as our earlier analysis to the reader at this stage.\n",
      "\n",
      "As the partition is random, one can actually get away with the determi-\n",
      "nation of the supports for a small subset of C p\n",
      "k, as we only need to determine\n",
      "the support for az for which the supports have not been determined in the\n",
      "ﬁrst scan. One may also wish to choose the minimal support σ for the ﬁrst\n",
      "scan slightly lower in order to further reduce the amount of second scans\n",
      "required.\n",
      "\n",
      "4.4. Mining Sequences\n",
      "The following is an example of how one may construct more complex struc-\n",
      "tures from the market baskets. We consider here a special case of sequences,\n",
      "see [?,?]. Let the data be of the form\n",
      "\n",
      "(x1, . . . , xm)\n",
      "\n",
      "where each xi is an itemset (not a component as in our earlier notation.\n",
      "Examples of sequences correspond to the shopping behaviour of customers\n",
      "of retailers over time, or the sequence of services a patient receives over time.\n",
      "The focus is thus not on individual market-baskets but on the customers.\n",
      "We do not discuss the temporal aspects, just the sequential ones.\n",
      "\n",
      "In deﬁning our space of features we include the empty sequence () but\n",
      "\n",
      "not components of the sequences are 0, i.e.,\n",
      "\n",
      "xi (cid:54)= 0.\n",
      "\n",
      "The rationale for this is that sequences correspond to actions which occur in\n",
      "some order and 0 would correspond to a non-action. We are not interested\n",
      "in the times when a shopper went to the store and didn’t buy anything at\n",
      "all. Any empty component itemsets in the data will also be removed.\n",
      "\n",
      "The sequences also have an intrinsic partial ordering\n",
      "\n",
      "x ≤ y\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "50\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "which holds for (x1, . . . , xm) and (y1, . . . , yk) when ever there is a sequence\n",
      "1 ≤ i1 < i2 < ··· < im ≤ k such that\n",
      "\n",
      "xi ≤ yis,\n",
      "\n",
      "s = 1, . . . , m.\n",
      "\n",
      "One can now verify that this deﬁnes a partial order on the set of se-\n",
      "quences introduced above. However, the set of sequences does not form\n",
      "a lattice as there are not necessarily unique lowest upper or greatest\n",
      "lower bounds. For example, the two sequences ((0, 1), (1, 1), (1, 0)) and\n",
      "((0, 1), (0, 1)) have the two (joint) upper bounds ((0, 1), (1, 1), (1, 0), (0, 1))\n",
      "and ((0, 1), (0, 1), (1, 1), (1, 0) which have now common lower bound which\n",
      "is still an upper bound for both original sequences. This makes the search\n",
      "for frequent itemsets somewhat harder.\n",
      "Another diﬀerence is that the complexity of the mining tasks has grown\n",
      "considerably, with |I| items one has 2|I| market-baskets and thus 2|I|m\n",
      "diﬀerent sequences of length ≤ m. Thus it is essential to be able to deal\n",
      "with the computational complexity of this problem. Note in particular, that\n",
      "the probability of any particular sequence is going to be extremely small.\n",
      "However, one will be able to make statements about the support of small\n",
      "subsequences which correspond to shopping or treatment patterns.\n",
      "\n",
      "Based on the ordering, the support of a sequence x is the set of all\n",
      "\n",
      "sequences larger than x is\n",
      "\n",
      "s(x) = P ({x|x ≤ y}) .\n",
      "\n",
      "This is estimated by the number of sequences in the data base which are in\n",
      "the support. Note that the itemsets now occur as length 1 sequences and\n",
      "thus the support of the itemsets can be identiﬁed with the support of the\n",
      "corresponding 1 sequence. As our focus is now on sequences this is diﬀerent\n",
      "from the support we get if we look just at the distribution of the itemsets.\n",
      "The length of a sequence is the number of non-empty components. Thus\n",
      "we can now deﬁne an apriori algorithm as before. This would start with the\n",
      "determination of all the frequent 1 sequences which correspond to all the\n",
      "frequent itemsets. Thus the ﬁrst step of the sequence mining algorithm is\n",
      "just the ordinary apriori algorithm. Then the apriori algorithm continues\n",
      "as before, where the candidate generation step is similar but now we join\n",
      "any two sequences which have all components identical except for the last\n",
      "(non-empty) one. Then one gets a sequence of length m + 1 from two such\n",
      "sequences of length m by concatenating the last component of the second\n",
      "sequence on to the ﬁrst one. After that one still needs to check if all subse-\n",
      "quences are frequent to do some pruning.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "51\n",
      "\n",
      "There has been some arbitrariness in some of the choices. Alternatives\n",
      "choose the size of a sequence as the sum of the sizes of the itemsets. In this\n",
      "case the candidate generation procedure becomes slightly more complex,\n",
      "see [?].\n",
      "\n",
      "4.5. The FP tree algorithm\n",
      "The Apriori algorithm is very eﬀective for discovering a reasonable num-\n",
      "ber of small frequent itemsets. However it does show severe performance\n",
      "problems for the discovery of large numbers of frequent itemsets. If, for\n",
      "example, there are 106 frequent items then the set of candidate 2-itemsets\n",
      "contains 5 · 1011 itemsets which all require testing. In addition, the Apriori\n",
      "algorithm has problems with the discovery of very long frequent itemsets.\n",
      "For the discovery of an itemset with 100 items the algorithm requires scan-\n",
      "ning the data for all the 2100 subsets in 100 scans. The bottleneck in the\n",
      "algorithm is the creation of the candidate itemsets, more precisely, the num-\n",
      "ber of candidate itemsets which need to be created during the mining. The\n",
      "reason for this large number is that the candidate itemsets are visited in a\n",
      "breadth-ﬁrst way.\n",
      "\n",
      "The FP tree algorithm addresses these issues and scans the data in a\n",
      "depth-ﬁrst way. The data is only scanned twice. In a ﬁrst scan, the frequent\n",
      "items (or 1-itemsets) are determined. The data items are then ordered based\n",
      "on their frequency and the infrequent items are removed. In the second scan,\n",
      "the data base is mapped onto a tree structure. Except for the root all the\n",
      "nodes are labelled with items, each item can correspond to multiple nodes.\n",
      "We will explain the algorithm with the help of an example, see table ?? for\n",
      "the original data and the records with the frequent itemsets only (here we\n",
      "look for support > 0.5).\n",
      "\n",
      "items\n",
      "\n",
      "f, a, c, d, g, i, m, p\n",
      "\n",
      "a, b, c, f, l, m, o\n",
      "b, f, h, j, o, w\n",
      "\n",
      "b, c, k, s, p\n",
      "\n",
      "s > 0.5\n",
      "\n",
      "f, c, a, m, p\n",
      "f, c, a, b, m\n",
      "\n",
      "f, b\n",
      "c, b, p\n",
      "\n",
      "a, f, c, e, l, p, m, n\n",
      "\n",
      "f, c, a, m, p\n",
      "\n",
      "Initially the tree consists only of the root. Then the ﬁrst record is read\n",
      "and a path is attached to the root such that the node labelled with the\n",
      "ﬁrst item of the record (items are ordered by their frequency) is adjacent to\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "52\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "the root, the second item labels the next neighbour and so on. In addition\n",
      "to the item, the label also contains the number 1, see Step 1 in ﬁgure ??.\n",
      "Then the second record is included such that any common preﬁx (in the\n",
      "\n",
      "Fig. 10. Construction of the FP-Tree\n",
      "\n",
      "example the items f,c,a is shared with the previous record and the remaining\n",
      "items are added in a splitted path. The numeric parts of the labels of the\n",
      "shared preﬁx nodes are increased by one, see Step 2 in the ﬁgure. This is\n",
      "then done with all the other records until the whole data base is stored\n",
      "in the tree. As the most common items were ordered ﬁrst, there is a big\n",
      "likelihood that many preﬁxes will be shared which results in substantial\n",
      "saving or compression of the data base. Note that no information is lost\n",
      "with respect to the supports. The FP tree structure is completed by adding\n",
      "a header table which contains all items together with pointers to their ﬁrst\n",
      "occurrence in the tree. The other occurrences are then linked together so\n",
      "that all occurrences of an item can easily be retrieved, see ﬁgure ??.\n",
      "\n",
      "The FP tree does never break a long pattern into smaller patterns the\n",
      "way the Apriori algorithm does. Long patterns can be directly retrieved\n",
      "from the FP tree. The FP tree also contains the full relevant information\n",
      "about the data base. It is compact, as all infrequent items are removed and\n",
      "the highly frequent items share nodes in the tree. The number of nodes is\n",
      "never less than the size of the data base measured in the sum of the sizes\n",
      "of the records but there is anecdotal evidence that compression rates can\n",
      "be over 100.\n",
      "\n",
      "The FP tree is used to ﬁnd all association rules containing particular\n",
      "items. Starting with the least frequent items, all rules containing those items\n",
      "\n",
      "c : 1a : 1m : 1p : 1f : 1c : 2a : 2m : 1p : 1f : 2b : 1m : 1c : 3b : 1b : 1p : 1a : 3m : 2p : 2f : 4c : 1b : 1m : 1Step 1Step 2Step 5\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "53\n",
      "\n",
      "Fig. 11. Final FP-Tree\n",
      "\n",
      "can be found simply by generating for each item the conditional data base\n",
      "which consists for each path which contains the item of those items which\n",
      "are between that item and the root. (The lower items don’t need to be\n",
      "considered, as they are considered together with other items.) These con-\n",
      "ditional pattern bases can then again be put into FP-trees, the conditional\n",
      "FP-trees and for those trees all the rules containing the previously selected\n",
      "and any other item will be extracted. If the conditional pattern base con-\n",
      "tains only one item, that item has to be the itemset. The frequencies of\n",
      "these itemsets can be obtained from the number labels.\n",
      "\n",
      "An additional speed-up is obtained by mining long preﬁx paths sepa-\n",
      "rately and combine the results at the end. Of course any chain does not\n",
      "need to be broken into parts necessarily as all the frequent subsets, together\n",
      "with their frequencies are easily obtained directly.\n",
      "\n",
      "5. Conclusion\n",
      "Data mining deals with the processing of large, complex and noisy data.\n",
      "Robust tools are required to recover weak signals. These tools require highly\n",
      "eﬃcient algorithms which scale with data size and complexity. Association\n",
      "rule discovery is one of the most popular and successful tools in data mining.\n",
      "Eﬃcient algorithms are available. The developments in association rule dis-\n",
      "covery combine concepts and insights from probability and combinatorics.\n",
      "The original algorithm “Apriori” was developed in the early years of data\n",
      "mining and is still widely used. Numerous variants and extensions exist of\n",
      "which a small selection was covered in this tutorial.\n",
      "\n",
      "The most recent work in association rules uses concepts from graph\n",
      "\n",
      "c : 3b : 1b : 1p : 1a : 3m : 2p : 2f : 4{}c : 1b : 1m : 1443333fcabmpitemsupportheader table\f",
      "View publication stats\n",
      "View publication stats\n",
      "\n",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "54\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "theory, formal concept analysis and and statistics and links association\n",
      "rules with graphical models and with hidden Markov models.\n",
      "\n",
      "In this tutorial some of the mathematical basis of association rules was\n",
      "covered but no attempt has been made to cover the vast literature discussing\n",
      "with numerous algorithms.\n",
      "\n",
      "Acknowledgements\n",
      "I would like to thank Zuowei Shen, for his patience and support during the\n",
      "preparation of this manuscript. Much of the work has arisen in discussions\n",
      "and collaboration with John Maindonald, Peter Christen, Ole Nielsen, Steve\n",
      "Roberts and Graham Williams.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#apriori = str_\n",
    "print(apriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 3\n",
      "\n",
      "Probability and Information\n",
      "Theory\n",
      "\n",
      "In this chapter, we describe probability theory and information theory.\n",
      "\n",
      "Probability theory is a mathematical framework for representing uncertain\n",
      "statements. It provides a means of quantifying uncertainty and axioms for deriving\n",
      "new uncertain statements. In artiﬁcial intelligence applications, we use probability\n",
      "theory in two major ways. First, the laws of probability tell us how AI systems\n",
      "should reason, so we design our algorithms to compute or approximate various\n",
      "expressions derived using probability theory. Second, we can use probability and\n",
      "statistics to theoretically analyze the behavior of proposed AI systems.\n",
      "\n",
      "Probability theory is a fundamental tool of many disciplines of science and\n",
      "engineering. We provide this chapter to ensure that readers whose background is\n",
      "primarily in software engineering with limited exposure to probability theory can\n",
      "understand the material in this book.\n",
      "\n",
      "While probability theory allows us to make uncertain statements and reason\n",
      "in the presence of uncertainty, information allows us to quantify the amount of\n",
      "uncertainty in a probability distribution.\n",
      "\n",
      "If you are already familiar with probability theory and information theory,\n",
      "you may wish to skip all of this chapter except for Sec. 3.14, which describes the\n",
      "graphs we use to describe structured probabilistic models for machine learning. If\n",
      "you have absolutely no prior experience with these subjects, this chapter should\n",
      "be suﬃcient to successfully carry out deep learning research projects, but we do\n",
      "suggest that you consult an additional resource, such as Jaynes (2003).\n",
      "\n",
      "52\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "3.1 Why Probability?\n",
      "\n",
      "Many branches of computer science deal mostly with entities that are entirely\n",
      "deterministic and certain. A programmer can usually safely assume that a CPU will\n",
      "execute each machine instruction ﬂawlessly. Errors in hardware do occur, but are\n",
      "rare enough that most software applications do not need to be designed to account\n",
      "for them. Given that many computer scientists and software engineers work in a\n",
      "relatively clean and certain environment, it can be surprising that machine learning\n",
      "makes heavy use of probability theory.\n",
      "\n",
      "This is because machine learning must always deal with uncertain quantities,\n",
      "and sometimes may also need to deal with stochastic (non-deterministic) quantities.\n",
      "Uncertainty and stochasticity can arise from many sources. Researchers have made\n",
      "compelling arguments for quantifying uncertainty using probability since at least\n",
      "the 1980s. Many of the arguments presented here are summarized from or inspired\n",
      "by Pearl (1988).\n",
      "\n",
      "Nearly all activities require some ability to reason in the presence of uncertainty.\n",
      "In fact, beyond mathematical statements that are true by deﬁnition, it is diﬃcult\n",
      "to think of any proposition that is absolutely true or any event that is absolutely\n",
      "guaranteed to occur.\n",
      "\n",
      "There are three possible sources of uncertainty:\n",
      "\n",
      "1. Inherent stochasticity in the system being modeled. For example, most\n",
      "interpretations of quantum mechanics describe the dynamics of subatomic\n",
      "particles as being probabilistic. We can also create theoretical scenarios that\n",
      "we postulate to have random dynamics, such as a hypothetical card game\n",
      "where we assume that the cards are truly shuﬄed into a random order.\n",
      "\n",
      "2. Incomplete observability. Even deterministic systems can appear stochastic\n",
      "when we cannot observe all of the variables that drive the behavior of the\n",
      "system. For example, in the Monty Hall problem, a game show contestant is\n",
      "asked to choose between three doors and wins a prize held behind the chosen\n",
      "door. Two doors lead to a goat while a third leads to a car. The outcome\n",
      "given the contestant’s choice is deterministic, but from the contestant’s point\n",
      "of view, the outcome is uncertain.\n",
      "\n",
      "3. Incomplete modeling. When we use a model that must discard some of\n",
      "the information we have observed, the discarded information results in\n",
      "uncertainty in the model’s predictions. For example, suppose we build a\n",
      "robot that can exactly observe the location of every object around it. If the\n",
      "\n",
      "53\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "robot discretizes space when predicting the future location of these objects,\n",
      "then the discretization makes the robot immediately become uncertain about\n",
      "the precise position of objects: each object could be anywhere within the\n",
      "discrete cell that it was observed to occupy.\n",
      "\n",
      "In many cases, it is more practical to use a simple but uncertain rule rather\n",
      "than a complex but certain one, even if the true rule is deterministic and our\n",
      "modeling system has the ﬁdelity to accommodate a complex rule. For example, the\n",
      "simple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a rule\n",
      "of the form, “Birds ﬂy, except for very young birds that have not yet learned to\n",
      "ﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\n",
      "including the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\n",
      "communicate, and after all of this eﬀort is still very brittle and prone to failure.\n",
      "\n",
      "Given that we need a means of representing and reasoning about uncertainty,\n",
      "it is not immediately obvious that probability theory can provide all of the tools\n",
      "we want for artiﬁcial intelligence applications. Probability theory was originally\n",
      "developed to analyze the frequencies of events. It is easy to see how probability\n",
      "theory can be used to study events like drawing a certain hand of cards in a\n",
      "game of poker. These kinds of events are often repeatable. When we say that\n",
      "an outcome has a probability p of occurring, it means that if we repeated the\n",
      "experiment (e.g., draw a hand of cards) inﬁnitely many times, then proportion p\n",
      "of the repetitions would result in that outcome. This kind of reasoning does not\n",
      "seem immediately applicable to propositions that are not repeatable. If a doctor\n",
      "analyzes a patient and says that the patient has a 40% chance of having the ﬂu,\n",
      "this means something very diﬀerent—we can not make inﬁnitely many replicas of\n",
      "the patient, nor is there any reason to believe that diﬀerent replicas of the patient\n",
      "would present with the same symptoms yet have varying underlying conditions. In\n",
      "the case of the doctor diagnosing the patient, we use probability to represent a\n",
      "degree of belief, with 1 indicating absolute certainty that the patient has the ﬂu\n",
      "and 0 indicating absolute certainty that the patient does not have the ﬂu. The\n",
      "former kind of probability, related directly to the rates at which events occur, is\n",
      "known as frequentist probability, while the latter, related to qualitative levels of\n",
      "certainty, is known as Bayesian probability.\n",
      "\n",
      "If we list several properties that we expect common sense reasoning about\n",
      "uncertainty to have, then the only way to satisfy those properties is to treat\n",
      "Bayesian probabilities as behaving exactly the same as frequentist probabilities.\n",
      "For example, if we want to compute the probability that a player will win a poker\n",
      "game given that she has a certain set of cards, we use exactly the same formulas\n",
      "as when we compute the probability that a patient has a disease given that she\n",
      "\n",
      "54\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "has certain symptoms. For more details about why a small set of common sense\n",
      "assumptions implies that the same axioms must control both kinds of probability,\n",
      "see Ramsey (1926).\n",
      "\n",
      "Probability can be seen as the extension of logic to deal with uncertainty. Logic\n",
      "provides a set of formal rules for determining what propositions are implied to\n",
      "be true or false given the assumption that some other set of propositions is true\n",
      "or false. Probability theory provides a set of formal rules for determining the\n",
      "likelihood of a proposition being true given the likelihood of other propositions.\n",
      "\n",
      "3.2 Random Variables\n",
      "\n",
      "A random variable is a variable that can take on diﬀerent values randomly. We\n",
      "typically denote the random variable itself with a lower case letter in plain typeface,\n",
      "and the values it can take on with lower case script letters. For example, x1 and x2\n",
      "are both possible values that the random variable x can take on. For vector-valued\n",
      "variables, we would write the random variable as x and one of its values as x. On\n",
      "its own, a random variable is just a description of the states that are possible; it\n",
      "must be coupled with a probability distribution that speciﬁes how likely each of\n",
      "these states are.\n",
      "\n",
      "Random variables may be discrete or continuous. A discrete random variable\n",
      "is one that has a ﬁnite or countably inﬁnite number of states. Note that these\n",
      "states are not necessarily the integers; they can also just be named states that\n",
      "are not considered to have any numerical value. A continuous random variable is\n",
      "associated with a real value.\n",
      "\n",
      "3.3 Probability Distributions\n",
      "\n",
      "A probability distribution is a description of how likely a random variable or\n",
      "set of random variables is to take on each of its possible states. The way we\n",
      "describe probability distributions depends on whether the variables are discrete or\n",
      "continuous.\n",
      "\n",
      "3.3.1 Discrete Variables and Probability Mass Functions\n",
      "\n",
      "A probability distribution over discrete variables may be described using a proba-\n",
      "bility mass function (PMF). We typically denote probability mass functions with a\n",
      "capital P . Often we associate each random variable with a diﬀerent probability\n",
      "\n",
      "55\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "mass function and the reader must infer which probability mass function to use\n",
      "based on the identity of the random variable, rather than the name of the function;\n",
      "P\n",
      "\n",
      "( )x is usually not the same as\n",
      "\n",
      "( )y .\n",
      "\n",
      "P\n",
      "\n",
      "The probability mass function maps from a state of a random variable to\n",
      "the probability of that random variable taking on that state. The probability\n",
      "that x = x is denoted as P (x), with a probability of 1 indicating that x = x is\n",
      "certain and a probability of 0 indicating that x = x is impossible. Sometimes\n",
      "to disambiguate which PMF to use, we write the name of the random variable\n",
      "explicitly: P (x = x). Sometimes we deﬁne a variable ﬁrst, then use ∼ notation to\n",
      "specify which distribution it follows later: x ∼ P (x .)\n",
      "\n",
      "Probability mass functions can act on many variables at the same time. Such\n",
      "a probability distribution over many variables is known as a joint probability\n",
      "distribution. P (x = x, y = y) denotes the probability that x = x and y = y\n",
      "simultaneously. We may also write\n",
      "\n",
      "for brevity.\n",
      "\n",
      "P x, y\n",
      "\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      "To be a probability mass function on a random variable x, a function P must\n",
      "\n",
      "satisfy the following properties:\n",
      "\n",
      "P\n",
      "\n",
      "• The domain of must be the set of all possible states of x.\n",
      "• ∀x\n",
      "0 \n",
      "\n",
      ",∈ x 0 ≤ P (x) ≤ 1. An impossible event has probability and no state can\n",
      "be less probable than that. Likewise, an event that is guaranteed to happen\n",
      "has probability , and no state can have a greater chance of occurring.\n",
      "\n",
      "1\n",
      "\n",
      "• x∈x P (x) = 1. We refer to this property as being normalized. Without this\n",
      "\n",
      "property, we could obtain probabilities greater than one by computing the\n",
      "probability of one of many events occurring.\n",
      "\n",
      "For example, consider a single discrete random variable x with k diﬀerent states.\n",
      "x—that is, make each of its states equally\n",
      "\n",
      "uniform distribution\n",
      "\n",
      "We can place a\n",
      "likely—by setting its probability mass function to\n",
      "\n",
      "on\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      "x\n",
      "i) =\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "(3.1)\n",
      "\n",
      "for all i. We can see that this ﬁts the requirements for a probability mass function.\n",
      "The value 1\n",
      "\n",
      "is a positive integer. We also see that\n",
      "\n",
      "k is positive because\n",
      "\n",
      "k\n",
      "\n",
      "i\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      "x\n",
      "\n",
      "i) =i\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "=\n",
      "\n",
      "k\n",
      "k\n",
      "\n",
      "= 1,\n",
      "\n",
      "(3.2)\n",
      "\n",
      "so the distribution is properly normalized.\n",
      "\n",
      "56\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "3.3.2 Continuous Variables and Probability Density Functions\n",
      "\n",
      "When working with continuous random variables, we describe probability dis-\n",
      "tributions using a probability density function (PDF) rather than a probability\n",
      "mass function. To be a probability density function, a function p must satisfy the\n",
      "following properties:\n",
      "\n",
      "p\n",
      "\n",
      "• The domain of must be the set of all possible states of x.\n",
      "• ∀x\n",
      "\n",
      "0≥ Note that we do not require ( ) \n",
      "p x\n",
      "\n",
      "1≤ .\n",
      "\n",
      ".\n",
      "\n",
      "∈ x ( ) \n",
      ", p x\n",
      "( ) = 1.\n",
      "\n",
      "•  p x dx\n",
      "\n",
      "A probability density function p(x) does not give the probability of a speciﬁc\n",
      "state directly, instead the probability of landing inside an inﬁnitesimal region with\n",
      "volume\n",
      "\n",
      "is given by\n",
      "\n",
      "p x δx\n",
      "\n",
      "( )\n",
      "\n",
      "δx\n",
      "\n",
      ".\n",
      "\n",
      "We can integrate the density function to ﬁnd the actual probability mass of a\n",
      "set of points. Speciﬁcally, the probability that x lies in some set S is given by the\n",
      "integral of p(x) over that set. In the univariate example, the probability that x\n",
      "lies in the interval\n",
      "\n",
      "is given by\n",
      "\n",
      ".\n",
      "p x dx\n",
      "\n",
      "[\n",
      "]a, b\n",
      "\n",
      "( )\n",
      "\n",
      "For an example of a probability density function corresponding to a speciﬁc\n",
      "probability density over a continuous random variable, consider a uniform distribu-\n",
      "tion on an interval of the real numbers. We can do this with a function u (x; a, b),\n",
      "where a and b are the endpoints of the interval, with b > a. The “;” notation means\n",
      "“parametrized by”; we consider x to be the argument of the function, while a and\n",
      "b are parameters that deﬁne the function. To ensure that there is no probability\n",
      "mass outside the interval, we say u(x; a, b) = 0 for all x ∈ [a, b]\n",
      ". Within a, b],\n",
      ". We can see that this is nonnegative everywhere. Additionally, it\n",
      "u x a, b\n",
      "( ;\n",
      "integrates to 1. We often denote that x follows the uniform distribution on [a, b]\n",
      "by writing x\n",
      "\n",
      ") = 1\n",
      "b a−\n",
      "∼ U a, b\n",
      ")\n",
      "\n",
      "(\n",
      "\n",
      "[\n",
      "\n",
      ".\n",
      "\n",
      "[\n",
      "\n",
      "]a,b\n",
      "\n",
      "3.4 Marginal Probability\n",
      "\n",
      "Sometimes we know the probability distribution over a set of variables and we want\n",
      "to know the probability distribution over just a subset of them. The probability\n",
      "distribution over the subset is known as the marginal probability distribution.\n",
      "\n",
      "For example, suppose we have discrete random variables x and y , and we know\n",
      "\n",
      "P ,(x y . We can ﬁnd\n",
      "\n",
      ")\n",
      "\n",
      "x with the\n",
      "\n",
      "sum rule\n",
      "\n",
      ":\n",
      "\n",
      "x, P\n",
      "\n",
      "( = x\n",
      "\n",
      "x\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      "x,\n",
      "\n",
      "y =  )\n",
      "y .\n",
      "\n",
      "(3.3)\n",
      "\n",
      "P ( )\n",
      "∀ ∈x\n",
      "\n",
      ") =y\n",
      "\n",
      "57\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "The name “marginal probability” comes from the process of computing marginal\n",
      "probabilities on paper. When the values of P(x y, ) are written in a grid with\n",
      "diﬀerent values of x in rows and diﬀerent values of y in columns, it is natural to\n",
      "sum across a row of the grid, then write P( x) in the margin of the paper just to\n",
      "the right of the row.\n",
      "\n",
      "For continuous variables, we need to use integration instead of summation:\n",
      "\n",
      "p x( ) = p x, y dy.\n",
      "\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      "(3.4)\n",
      "\n",
      "3.5 Conditional Probability\n",
      "\n",
      "In many cases, we are interested in the probability of some event, given that some\n",
      "other event has happened. This is called a conditional probability. We denote\n",
      "the conditional probability that y = y given x = x as P (y = y | x = x ). This\n",
      "conditional probability can be computed with the formula\n",
      "\n",
      "P\n",
      "\n",
      "( = y\n",
      "\n",
      "y\n",
      "\n",
      "| x =  ) =\n",
      "\n",
      "x\n",
      "\n",
      "P\n",
      "\n",
      "( = y\n",
      "\n",
      "y,\n",
      "\n",
      "x =  )\n",
      "x\n",
      "x\n",
      ")\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      ".\n",
      "\n",
      "(3.5)\n",
      "\n",
      "The conditional probability is only deﬁned when P (x = x) > 0. We cannot compute\n",
      "the conditional probability conditioned on an event that never happens.\n",
      "\n",
      "It is important not to confuse conditional probability with computing what\n",
      "would happen if some action were undertaken. The conditional probability that\n",
      "a person is from Germany given that they speak German is quite high, but if\n",
      "a randomly selected person is taught to speak German, their country of origin\n",
      "does not change. Computing the consequences of an action is called making an\n",
      "intervention query. Intervention queries are the domain of causal modeling, which\n",
      "we do not explore in this book.\n",
      "\n",
      "3.6 The Chain Rule of Conditional Probabilities\n",
      "\n",
      "Any joint probability distribution over many random variables may be decomposed\n",
      "into conditional distributions over only one variable:\n",
      "\n",
      "P (x(1), . . . , x ( )n ) = \n",
      "\n",
      "| x(1), . . . , x (\n",
      "product rule of probability. It\n",
      "follows immediately from the deﬁnition of conditional probability in Eq. 3.5. For\n",
      "\n",
      "This observation is known as the\n",
      "\n",
      "(P x(1) )Πn\n",
      "\n",
      "i=2P (x( )i\n",
      "\n",
      "chain rule\n",
      "\n",
      "i− ).\n",
      "\n",
      "(3.6)\n",
      "\n",
      "or\n",
      "\n",
      "1)\n",
      "\n",
      "58\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "example, applying the deﬁnition twice, we get\n",
      "\n",
      "P ,\n",
      "\n",
      "(a b c) =\n",
      "\n",
      ",\n",
      "\n",
      "P ,\n",
      "\n",
      "(b c) =\n",
      "\n",
      "P ,\n",
      "\n",
      "(a b c) =\n",
      "\n",
      ",\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "(a b|\n",
      "(b c| )\n",
      "(a b|\n",
      "\n",
      "c)\n",
      ", P ,\n",
      "\n",
      "(b c)\n",
      "\n",
      "( )P c\n",
      "\n",
      ", Pc)\n",
      "\n",
      "(b c| )\n",
      "\n",
      "( )P c .\n",
      "\n",
      "3.7 Independence and Conditional Independence\n",
      "\n",
      "Two random variables x and y are independent if their probability distribution can\n",
      "be expressed as a product of two factors, one involving only x and one involving\n",
      "only y:\n",
      "\n",
      "∀ ∈x\n",
      "\n",
      "x, y\n",
      "\n",
      "∈ y\n",
      "\n",
      ", p\n",
      "\n",
      "x\n",
      "( = \n",
      "\n",
      "x, y\n",
      "\n",
      "= ) =  ( =\n",
      "\n",
      "p x\n",
      "\n",
      "y\n",
      "\n",
      "x) ( =  )\n",
      "y .\n",
      "\n",
      "p y\n",
      "\n",
      "(3.7)\n",
      "\n",
      "Two random variables x and y are conditionally independent given a random\n",
      "variable z if the conditional probability distribution over x and y factorizes in this\n",
      "way for every value of z:\n",
      "\n",
      "∀ ∈x\n",
      "\n",
      "x, y\n",
      "\n",
      ", z∈ y ∈ z, p\n",
      "\n",
      "( =x\n",
      "\n",
      "x,\n",
      "\n",
      "y = \n",
      "\n",
      "y\n",
      "\n",
      "| z =  ) =  ( = x\n",
      "\n",
      "p\n",
      "\n",
      "z\n",
      "\n",
      "x\n",
      "\n",
      "| z =  ) ( = y\n",
      "\n",
      "z p\n",
      "\n",
      "y\n",
      "\n",
      "| z = )\n",
      "z .\n",
      "(3.8)\n",
      "\n",
      "We can denote independence and conditional independence with compact\n",
      "notation: x y⊥ means that x and y are independent, while x y z⊥ | means that x\n",
      "and y are conditionally independent given z.\n",
      "\n",
      "3.8 Expectation, Variance and Covariance\n",
      "\n",
      "or\n",
      "\n",
      "The expectation\n",
      "of some function f( x) with respect to a probability\n",
      "distribution P (x) is the average or mean value that f takes on when x is drawn\n",
      "from . For discrete variables this can be computed with a summation:\n",
      "\n",
      "expected value\n",
      "\n",
      "P\n",
      "\n",
      "Ex∼P[ (f x)] =x\n",
      "\n",
      "P x f x ,\n",
      "( ) ( )\n",
      "\n",
      "(3.9)\n",
      "\n",
      "while for continuous variables, it is computed with an integral:\n",
      "\n",
      "Ex∼p[ ( )] =\n",
      "\n",
      "f x\n",
      "\n",
      " p x f x dx.\n",
      "\n",
      "( ) ( )\n",
      "\n",
      "59\n",
      "\n",
      "(3.10)\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "When the identity of the distribution is clear from the context, we may simply\n",
      "write the name of the random variable that the expectation is over, as in Ex[f (x)].\n",
      "If it is clear which random variable the expectation is over, we may omit the\n",
      "subscript entirely, as in E[f (x)]. By default, we can assume that E [·] averages over\n",
      "the values of all the random variables inside the brackets. Likewise, when there is\n",
      "no ambiguity, we may omit the square brackets.\n",
      "\n",
      "Expectations are linear, for example,\n",
      "\n",
      "E x[\n",
      "\n",
      "αf x\n",
      "\n",
      "( ) + ( )] = \n",
      "\n",
      "βg x\n",
      "\n",
      "αEx [ ( )] +\n",
      "\n",
      "f x\n",
      "\n",
      "βEx[ ( )]\n",
      "g x ,\n",
      "\n",
      "(3.11)\n",
      "\n",
      "when\n",
      "\n",
      "α\n",
      "\n",
      "and\n",
      "\n",
      "β\n",
      "\n",
      "are not dependent on .\n",
      "x\n",
      "\n",
      "The variance gives a measure of how much the values of a function of a random\n",
      "variable x vary as we sample diﬀerent value of x from its probability distribution:\n",
      "\n",
      "Var( ( )) = \n",
      "\n",
      "f x\n",
      "\n",
      "E( ( )\n",
      "f x − E f x 2.\n",
      "\n",
      "[ ( )])\n",
      "\n",
      "(3.12)\n",
      "\n",
      "When the variance is low, the values of f (x) cluster near their expected value. The\n",
      "square root of the variance is known as the standard deviation.\n",
      "\n",
      "The covariance gives some sense of how much two values are linearly related to\n",
      "\n",
      "each other, as well as the scale of these variables:\n",
      "\n",
      "Cov( ( )\n",
      "\n",
      "f x , g y\n",
      "\n",
      "( )) = \n",
      "\n",
      "E f x − E f x\n",
      "[( ( )\n",
      "\n",
      "[ ( )]) ( ( )\n",
      "\n",
      "g y − E g y\n",
      "\n",
      "[ ( )])]\n",
      "\n",
      ".\n",
      "\n",
      "(3.13)\n",
      "\n",
      "High absolute values of the covariance mean that the values change very much\n",
      "and are both far from their respective means at the same time. If the sign of the\n",
      "covariance is positive, then both variables tend to take on relatively high values\n",
      "simultaneously. If the sign of the covariance is negative, then one variable tends to\n",
      "take on a relatively high value at the times that the other takes on a relatively low\n",
      "value and vice versa. Other measures such as correlation normalize the contribution\n",
      "of each variable in order to measure only how much the variables are related, rather\n",
      "than also being aﬀected by the scale of the separate variables.\n",
      "\n",
      "The notions of covariance and dependence are related, but are in fact distinct\n",
      "concepts. They are related because two variables that are independent have zero\n",
      "covariance, and two variables that have non-zero covariance are dependent. How-\n",
      "ever, independence is a distinct property from covariance. For two variables to have\n",
      "zero covariance, there must be no linear dependence between them. Independence\n",
      "is a stronger requirement than zero covariance, because independence also excludes\n",
      "nonlinear relationships. It is possible for two variables to be dependent but have\n",
      "zero covariance. For example, suppose we ﬁrst sample a real number x from a\n",
      "uniform distribution over the interval [−1 ,1]. We next sample a random variable\n",
      "\n",
      "60\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "s. With probability 1\n",
      "2, we choose the value of s to be 1. Otherwise, we choose\n",
      "the value of s to be − 1. We can then generate a random variable y by assigning\n",
      "y = sx . Clearly, x and y are not independent, because x completely determines\n",
      "the magnitude of\n",
      "\n",
      ". However,\n",
      "\n",
      ".\n",
      ") = 0\n",
      "\n",
      "Cov(\n",
      "\n",
      "x, y\n",
      "\n",
      "y\n",
      "\n",
      "The covariance matrix of a random vector x ∈ Rn is an n n× matrix, such that\n",
      "(3.14)\n",
      "\n",
      "Cov( )x i,j = Cov(xi, xj ).\n",
      "\n",
      "The diagonal elements of the covariance give the variance:\n",
      "\n",
      "Cov(xi , xi) = Var(xi).\n",
      "\n",
      "(3.15)\n",
      "\n",
      "3.9 Common Probability Distributions\n",
      "\n",
      "Several simple probability distributions are useful in many contexts in machine\n",
      "learning.\n",
      "\n",
      "3.9.1 Bernoulli Distribution\n",
      "\n",
      "Bernoulli\n",
      "\n",
      "The\n",
      "distribution is a distribution over a single binary random variable.\n",
      "It is controlled by a single parameter φ ∈ [0, 1], which gives the probability of the\n",
      "random variable being equal to 1. It has the following properties:\n",
      "\n",
      "P\n",
      "\n",
      "x\n",
      "( = 1) = \n",
      "\n",
      "φ\n",
      "\n",
      "P\n",
      "\n",
      "x\n",
      "( = 0) = 1\n",
      ") =  x (1\n",
      "x\n",
      "\n",
      "φ\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      "Ex [ ] = \n",
      "\n",
      "x\n",
      "\n",
      "φ\n",
      "\n",
      "−\n",
      ")− φ 1−x\n",
      "φ\n",
      "\n",
      "Var x( ) =  (1\n",
      "\n",
      "x\n",
      "\n",
      "φ − φ\n",
      "\n",
      ")\n",
      "\n",
      "(3.16)\n",
      "\n",
      "(3.17)\n",
      "\n",
      "(3.18)\n",
      "\n",
      "(3.19)\n",
      "\n",
      "(3.20)\n",
      "\n",
      "3.9.2 Multinoulli Distribution\n",
      "\n",
      "multinoulli\n",
      "\n",
      "The\n",
      "categorical distribution is a distribution over a single discrete\n",
      "variable with k diﬀerent states, where k is ﬁnite1 . The multinoulli distribution is\n",
      "\n",
      "or\n",
      "\n",
      "1 “Multinoulli” is a term that was recently coined by Gustavo Lacerdo and popularized by\n",
      "Murphy (2012). The multinoulli distribution is a special case of the\n",
      "distribution. A\n",
      "multinomial distribution is the distribution over vectors in {0, . . . , n}k representing how many\n",
      "times each of the k categories is visited when n samples are drawn from a multinoulli distribution.\n",
      "Many texts use the term “multinomial” to refer to multinoulli distributions without clarifying\n",
      "that they refer only to the\n",
      "\n",
      "multinomial\n",
      "\n",
      "n = 1\n",
      "\n",
      "case.\n",
      "\n",
      "61\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "parametrized by a vector p ∈ [0, 1]k−1 , where p i gives the probability of the i-th\n",
      "state. The ﬁnal, k-th state’s probability is given by 1− 1 p. Note that we must\n",
      "constrain 1 p ≤ 1. Multinoulli distributions are often used to refer to distributions\n",
      "over categories of objects, so we do not usually assume that state 1 has numerical\n",
      "value 1, etc. For this reason, we do not usually need to compute the expectation\n",
      "or variance of multinoulli-distributed random variables.\n",
      "\n",
      "The Bernoulli and multinoulli distributions are suﬃcient to describe any distri-\n",
      "bution over their domain. This is because they model discrete variables for which\n",
      "it is feasible to simply enumerate all of the states. When dealing with continuous\n",
      "variables, there are uncountably many states, so any distribution described by a\n",
      "small number of parameters must impose strict limits on the distribution.\n",
      "\n",
      "3.9.3 Gaussian Distribution\n",
      "\n",
      "The most commonly used distribution over real numbers is the normal distribution,\n",
      "also known as the Gaussian distribution:\n",
      "\n",
      "N ( ;x µ, σ2) = 1\n",
      "\n",
      "2πσ2 exp−\n",
      "\n",
      "1\n",
      "x\n",
      "2σ2 (\n",
      "\n",
      "µ− 2 .\n",
      "\n",
      ")\n",
      "\n",
      "(3.21)\n",
      "\n",
      "See Fig. 3.1 for a plot of the density function.\n",
      "The two parameters µ ∈ R and σ ∈ (0,∞ ) control the normal distribution.\n",
      "The parameter µ gives the coordinate of the central peak. This is also the mean of\n",
      "the distribution: E[x] = µ. The standard deviation of the distribution is given by\n",
      "σ, and the variance by σ2.\n",
      "\n",
      "When we evaluate the PDF, we need to square and invert σ. When we need to\n",
      "frequently evaluate the PDF with diﬀerent parameter values, a more eﬃcient way\n",
      "of parametrizing the distribution is to use a parameter β ∈ (0 ,∞) to control the\n",
      "precision or inverse variance of the distribution:\n",
      "\n",
      "N ( ;x µ, β−1) = β\n",
      "\n",
      "2π\n",
      "\n",
      "exp−\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "β x\n",
      "\n",
      "( − )2 .\n",
      "\n",
      "µ\n",
      "\n",
      "(3.22)\n",
      "\n",
      "Normal distributions are a sensible choice for many applications. In the absence\n",
      "of prior knowledge about what form a distribution over the real numbers should\n",
      "take, the normal distribution is a good default choice for two major reasons.\n",
      "\n",
      "First, many distributions we wish to model are truly close to being normal\n",
      "distributions. The central limit theorem shows that the sum of many independent\n",
      "random variables is approximately normally distributed. This means that in\n",
      "\n",
      "62\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "The normal distribution\n",
      "\n",
      "Maximum at x ¹=\n",
      "\n",
      "Inflection points at \n",
      "     x ¹ ¾\n",
      "\n",
      "= §\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1.0\n",
      "\n",
      "−0.5\n",
      "\n",
      "0.0\n",
      "\n",
      "0.5\n",
      "\n",
      "1.0\n",
      "\n",
      "1.5\n",
      "\n",
      "2.0\n",
      "\n",
      ")\n",
      "x\n",
      "(\n",
      "p\n",
      "\n",
      "0.40\n",
      "\n",
      "0.35\n",
      "\n",
      "0.30\n",
      "\n",
      "0.25\n",
      "\n",
      "0.20\n",
      "\n",
      "0.15\n",
      "\n",
      "0.10\n",
      "\n",
      "0.05\n",
      "\n",
      "0.00\n",
      "\n",
      "−2.0\n",
      "\n",
      "Figure 3.1: The normal distribution: The normal distribution N (x;µ, σ 2) exhibits a classic\n",
      "“bell curve” shape, with the x coordinate of its central peak given by µ, and the width\n",
      "of its peak controlled by σ. In this example, we depict the standard normal distribution,\n",
      "with\n",
      "\n",
      "σ = 1\n",
      "\n",
      "µ = 0\n",
      "\n",
      "and\n",
      "\n",
      ".\n",
      "\n",
      "x\n",
      "\n",
      "practice, many complicated systems can be modeled successfully as normally\n",
      "distributed noise, even if the system can be decomposed into parts with more\n",
      "structured behavior.\n",
      "\n",
      "Second, out of all possible probability distributions with the same variance,\n",
      "the normal distribution encodes the maximum amount of uncertainty over the\n",
      "real numbers. We can thus think of the normal distribution as being the one that\n",
      "inserts the least amount of prior knowledge into a model. Fully developing and\n",
      "justifying this idea requires more mathematical tools, and is postponed to Sec.\n",
      "19.4.2.\n",
      "\n",
      "The normal distribution generalizes to Rn, in which case it is known as the\n",
      "multivariate normal distribution. It may be parametrized with a positive deﬁnite\n",
      "symmetric matrix\n",
      "\n",
      ":Σ\n",
      "\n",
      "x µ, Σ \n",
      "\n",
      ") =\n",
      "\n",
      "N ( ;\n",
      "\n",
      "1\n",
      "\n",
      "(2 )π ndet(\n",
      "\n",
      ")Σ\n",
      "\n",
      "exp−\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x µ− Σ−1 (\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      "x µ−  .\n",
      "\n",
      ")\n",
      "\n",
      "(3.23)\n",
      "\n",
      "The parameter µ still gives the mean of the distribution, though now it is\n",
      "vector-valued. The parameter Σ gives the covariance matrix of the distribution.\n",
      "As in the univariate case, when we wish to evaluate the PDF several times for\n",
      "\n",
      "63\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "many diﬀerent values of the parameters, the covariance is not a computationally\n",
      "eﬃcient way to parametrize the distribution, since we need to invert Σ to evaluate\n",
      "the PDF. We can instead use a precision matrix β:\n",
      "\n",
      "N ( ;x µ β, −1) =det( )β\n",
      "\n",
      "(2 )π n\n",
      "\n",
      "exp−\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "( − ) .\n",
      "x µ− β x µ\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      "(3.24)\n",
      "\n",
      "We often ﬁx the covariance matrix to be a diagonal matrix. An even simpler\n",
      "version is the isotropic Gaussian distribution, whose covariance matrix is a scalar\n",
      "times the identity matrix.\n",
      "\n",
      "3.9.4 Exponential and Laplace Distributions\n",
      "\n",
      "In the context of deep learning, we often want to have a probability distribution\n",
      "with a sharp point at x = 0. To accomplish this, we can use the exponential\n",
      "distribution:\n",
      "\n",
      "(3.25)\n",
      "The exponential distribution uses the indicator function 1x≥0 to assign probability\n",
      "zero to all negative values of\n",
      "\n",
      "p x λ\n",
      "( ; ) =  1x≥0exp (\n",
      "\n",
      ")−λx .\n",
      "\n",
      ".x\n",
      "\n",
      "λ\n",
      "\n",
      "A closely related probability distribution that allows us to place a sharp peak\n",
      "\n",
      "of probability mass at an arbitrary point\n",
      "\n",
      "µ\n",
      "\n",
      "is the\n",
      "\n",
      "Laplace distribution\n",
      "\n",
      "Laplace( ;\n",
      "\n",
      "x µ, γ\n",
      "\n",
      ") =\n",
      "\n",
      "1\n",
      "2γ\n",
      "\n",
      "exp−| − |\n",
      "γ .\n",
      "\n",
      "µ\n",
      "\n",
      "x\n",
      "\n",
      "(3.26)\n",
      "\n",
      "3.9.5 The Dirac Distribution and Empirical Distribution\n",
      "\n",
      "In some cases, we wish to specify that all of the mass in a probability distribution\n",
      "clusters around a single point. This can be accomplished by deﬁning a PDF using\n",
      "the Dirac delta function,\n",
      "\n",
      "δ x( )\n",
      "\n",
      ":\n",
      "\n",
      "( ) =  ( − )\n",
      "p x\n",
      "µ .\n",
      "\n",
      "δ x\n",
      "\n",
      "(3.27)\n",
      "\n",
      "The Dirac delta function is deﬁned such that it is zero-valued everywhere except\n",
      "0, yet integrates to 1. The Dirac delta function is not an ordinary function that\n",
      "associates each value x with a real-valued output, instead it is a diﬀerent kind of\n",
      "mathematical object called a generalized function that is deﬁned in terms of its\n",
      "properties when integrated. We can think of the Dirac delta function as being the\n",
      "limit point of a series of functions that put less and less mass on all points other\n",
      "than .µ\n",
      "\n",
      "64\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "By deﬁning p(x) to be δ shifted by −µ we obtain an inﬁnitely narrow and\n",
      "\n",
      "inﬁnitely high peak of probability mass where\n",
      "\n",
      "x\n",
      "\n",
      "µ= \n",
      "\n",
      ".\n",
      "\n",
      "A common use of the Dirac delta distribution is as a component of an empirical\n",
      "\n",
      "distribution,\n",
      "\n",
      "ˆp( ) =x\n",
      "\n",
      "1\n",
      "m\n",
      "\n",
      "mi=1\n",
      "\n",
      "δ(x x− ( )i )\n",
      "\n",
      "(3.28)\n",
      "\n",
      "1\n",
      "m on each of the m points x(1), . . . , x (\n",
      "\n",
      ")m forming\n",
      "which puts probability mass\n",
      "a given data set or collection of samples. The Dirac delta distribution is only\n",
      "necessary to deﬁne the empirical distribution over continuous variables. For discrete\n",
      "variables, the situation is simpler: an empirical distribution can be conceptualized\n",
      "as a multinoulli distribution, with a probability associated to each possible input\n",
      "value that is simply equal to the empirical frequency of that value in the training\n",
      "set.\n",
      "\n",
      "We can view the empirical distribution formed from a dataset of training\n",
      "examples as specifying the distribution that we sample from when we train a model\n",
      "on this dataset. Another important perspective on the empirical distribution is\n",
      "that it is the probability density that maximizes the likelihood of the training\n",
      "data (see Sec. 5.5). Many machine learning algorithms can be conﬁgured to have\n",
      "arbitrarily high capacity. If given enough capacity, these algorithms will simply\n",
      "learn the empirical distribution. This is a bad outcome because the model does not\n",
      "generalize at all and assigns inﬁnitesimal probability to any point in space that did\n",
      "not occur in the training set. A central problem in machine learning is studying\n",
      "how to limit the capacity of a model in a way that prevents it from simply learning\n",
      "the empirical distribution while also allowing it to learn complicated functions.\n",
      "\n",
      "3.9.6 Mixtures of Distributions\n",
      "\n",
      "It is also common to deﬁne probability distributions by combining other simpler\n",
      "probability distributions. One common way of combining distributions is to\n",
      "construct a mixture distribution. A mixture distribution is made up of several\n",
      "component distributions. On each trial, the choice of which component distribution\n",
      "generates the sample is determined by sampling a component identity from a\n",
      "multinoulli distribution:\n",
      "\n",
      "P ( ) =x i\n",
      "\n",
      "P\n",
      "\n",
      "c\n",
      "( = \n",
      "\n",
      "i P\n",
      ")\n",
      "\n",
      "x c|\n",
      "(\n",
      "\n",
      "=\n",
      "\n",
      "i\n",
      ")\n",
      "\n",
      "(3.29)\n",
      "\n",
      "where\n",
      "\n",
      "P ( )\n",
      "\n",
      "c is the multinoulli distribution over component identities.\n",
      "\n",
      "65\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "We have already seen one example of a mixture distribution: the empirical\n",
      "distribution over real-valued variables is a mixture distribution with one Dirac\n",
      "component for each training example.\n",
      "\n",
      "The mixture model is one simple strategy for combining probability distributions\n",
      "to create a richer distribution. In Chapter 16, we explore the art of building complex\n",
      "probability distributions from simple ones in more detail.\n",
      "\n",
      "latent variable\n",
      "\n",
      "The mixture model allows us to brieﬂy glimpse a concept that will be of\n",
      "paramount importance later—the\n",
      ". A latent variable is a random\n",
      "variable that we cannot observe directly. The component identity variable c of the\n",
      "mixture model provides an example. Latent variables may be related to x through\n",
      "the joint distribution, in this case, P (x c, ) = P (x c|\n",
      ")P(c). The distribution P (c)\n",
      "over the latent variable and the distribution P (x c| ) relating the latent variables\n",
      "to the visible variables determines the shape of the distribution P ( x) even though\n",
      "it is possible to describe P (x) without reference to the latent variable. Latent\n",
      "variables are discussed further in Sec. 16.5.\n",
      "\n",
      "A very powerful and common type of mixture model is the Gaussian mixture\n",
      "model, in which the components p (x | c = i) are Gaussians. Each component has\n",
      "a separately parametrized mean µ ( )i and covariance Σ ( )i . Some mixtures can have\n",
      "more constraints. For example, the covariances could be shared across components\n",
      "via the constraint Σ( )i = Σ∀i. As with a single Gaussian distribution, the mixture\n",
      "of Gaussians might constrain the covariance matrix for each component to be\n",
      "diagonal or isotropic.\n",
      "\n",
      "In addition to the means and covariances, the parameters of a Gaussian mixture\n",
      "specify the prior probability α i = P (c = i) given to each component i. The word\n",
      "“prior” indicates that it expresses the model’s beliefs about c before it has observed\n",
      "x. By comparison, P(c | x) is a posterior probability, because it is computed after\n",
      "observation of x. A Gaussian mixture model is a universal approximator of\n",
      "densities, in the sense that any smooth density can be approximated with any\n",
      "speciﬁc, non-zero amount of error by a Gaussian mixture model with enough\n",
      "components.\n",
      "\n",
      "Fig. 3.2 shows samples from a Gaussian mixture model.\n",
      "\n",
      "3.10 Useful Properties of Common Functions\n",
      "\n",
      "Certain functions arise often while working with probability distributions, especially\n",
      "the probability distributions used in deep learning models.\n",
      "\n",
      "66\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "2\n",
      "x\n",
      "\n",
      "x1\n",
      "\n",
      "Figure 3.2: Samples from a Gaussian mixture model. In this example, there are three\n",
      "components. From left to right, the ﬁrst component has an isotropic covariance matrix,\n",
      "meaning it has the same amount of variance in each direction. The second has a diagonal\n",
      "covariane matrix, meaning it can control the variance separately along each axis-aligned\n",
      "direction. This example has more variance along the x2 axis than along the x1 axis. The\n",
      "third component has a full-rank covariance matrix, allowing it to control the variance\n",
      "separately along an abitrary basis of directions.\n",
      "\n",
      "One of these functions is the logistic sigmoid:\n",
      "\n",
      "σ x( ) =\n",
      "\n",
      "1\n",
      "\n",
      "1 + exp(\n",
      "\n",
      ".\n",
      ")−x\n",
      "\n",
      "(3.30)\n",
      "\n",
      "The logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli\n",
      "distribution because its range is (0, 1), which lies within the valid range of values\n",
      "for the φ parameter. See Fig. 3.3 for a graph of the sigmoid function. The sigmoid\n",
      "function saturates when its argument is very positive or very negative, meaning\n",
      "that the function becomes very ﬂat and insensitive to small changes in its input.\n",
      "\n",
      "Another commonly encountered function is the\n",
      "\n",
      "softplus\n",
      "\n",
      "function (Dugas\n",
      "\n",
      "et al.,\n",
      "\n",
      "2001):\n",
      "\n",
      "ζ x\n",
      "x .\n",
      "( ) = log (1 + exp( ))\n",
      "\n",
      "(3.31)\n",
      "\n",
      "The softplus function can be useful for producing the β or σ parameter of a normal\n",
      "distribution because its range is (0,∞). It also arises commonly when manipulating\n",
      "expressions involving sigmoids. The name of the softplus function comes from the\n",
      "fact that it is a smoothed or “softened” version of\n",
      "\n",
      "x+ = max(0 ), x .\n",
      "\n",
      "(3.32)\n",
      "\n",
      "See Fig. 3.4 for a graph of the softplus function.\n",
      "\n",
      "67\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "The logistic sigmoid function\n",
      "\n",
      ")\n",
      "x\n",
      "(\n",
      "¾\n",
      "\n",
      "1.0\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.0\n",
      "\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "x\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "Figure 3.3: The logistic sigmoid function.\n",
      "\n",
      "The softplus function\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      ")\n",
      "x\n",
      "(\n",
      "³\n",
      "\n",
      "0\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "x\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "Figure 3.4: The softplus function.\n",
      "\n",
      "68\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "The following properties are all useful enough that you may wish to memorize\n",
      "\n",
      "them:\n",
      "\n",
      "σ x( ) =\n",
      "\n",
      "exp( )x\n",
      "x\n",
      "\n",
      "exp( ) + exp(0)\n",
      "\n",
      "d\n",
      "dx\n",
      "\n",
      "σ x\n",
      "\n",
      "( ) =  (\n",
      "\n",
      "( ) =  ( )(1 − ( ))\n",
      "σ x\n",
      "σ x\n",
      "− σ x\n",
      "1\n",
      "σ x\n",
      "\n",
      "σ −x\n",
      ")\n",
      "−ζ −x\n",
      "(\n",
      "ζ x\n",
      "σ x\n",
      "( ) =  ( )\n",
      "\n",
      "log ( ) = \n",
      "\n",
      ")\n",
      "\n",
      "d\n",
      "dx\n",
      "\n",
      "1)\n",
      "\n",
      "(0,\n",
      "\n",
      ", σ\n",
      "\n",
      " x\n",
      "1 − x\n",
      "∀ ∈x\n",
      "∀x > 0, ζ−1( ) = log (exp( )\n",
      "x −\n",
      "\n",
      "−1 ( ) = log\n",
      "\n",
      "1)\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "ζ x( ) = x\n",
      "\n",
      "−∞\n",
      "x\n",
      "ζ\n",
      "\n",
      "σ y dy\n",
      "\n",
      "( )\n",
      "\n",
      "(3.33)\n",
      "\n",
      "(3.34)\n",
      "\n",
      "(3.35)\n",
      "\n",
      "(3.36)\n",
      "\n",
      "(3.37)\n",
      "\n",
      "(3.38)\n",
      "\n",
      "(3.39)\n",
      "\n",
      "(3.40)\n",
      "\n",
      "(3.41)\n",
      "The function σ−1(x) is called the logit in statistics, but this term is more rarely\n",
      "used in machine learning. The ﬁnal property provides extra justiﬁcation for the\n",
      "name “softplus,” since x+ − x− = x.\n",
      "\n",
      "( ) − (− ) = \n",
      "ζ x\n",
      "x\n",
      "\n",
      "3.11 Bayes’ Rule\n",
      "\n",
      "We often ﬁnd ourselves in a situation where we know P (y x| ) and need to know\n",
      "P (x y|\n",
      "). Fortunately, if we also know P (x), we can compute the desired quantity\n",
      "using Bayes’ rule:\n",
      "\n",
      "P (\n",
      "\n",
      "x y|\n",
      "\n",
      ") =\n",
      "\n",
      "P\n",
      "\n",
      "P( )x\n",
      "\n",
      "y x|\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      ".\n",
      "\n",
      "(3.42)\n",
      "\n",
      "P ( )y\n",
      "\n",
      "P ( ) =y x P\n",
      "\n",
      "(y |\n",
      "\n",
      "Note that while P (y) appears in the formula, it is usually feasible to compute\n",
      "\n",
      "x P x\n",
      "\n",
      "( ), so we do not need to begin with knowledge of\n",
      "\n",
      ")\n",
      "\n",
      "P\n",
      "\n",
      "(y .)\n",
      "\n",
      "Bayes’ rule is straightforward to derive from the deﬁnition of conditional\n",
      "probability, but it is useful to know the name of this formula since many texts\n",
      "refer to it by name. It is named after the Reverend Thomas Bayes, who ﬁrst\n",
      "discovered a special case of the formula. The general version presented here was\n",
      "independently discovered by Pierre-Simon Laplace.\n",
      "\n",
      "69\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "3.12 Technical Details of Continuous Variables\n",
      "\n",
      "A proper formal understanding of continuous random variables and probability\n",
      "density functions requires developing probability theory in terms of a branch of\n",
      "mathematics known as measure theory. Measure theory is beyond the scope of\n",
      "this textbook, but we can brieﬂy sketch some of the issues that measure theory is\n",
      "employed to resolve.\n",
      "\n",
      "In Sec. 3.3.2, we saw that the probability of a continuous vector-valued x lying\n",
      "in some set S is given by the integral of p(x ) over the set S. Some choices of set S\n",
      "can produce paradoxes. For example, it is possible to construct two sets S1 and\n",
      "S2 such that p(x ∈ S1) + p(x ∈ S 2) > 1 but S1 ∩ S2 = ∅. These sets are generally\n",
      "constructed making very heavy use of the inﬁnite precision of real numbers, for\n",
      "example by making fractal-shaped sets or sets that are deﬁned by transforming\n",
      "the set of rational numbers2 . One of the key contributions of measure theory is to\n",
      "provide a characterization of the set of sets that we can compute the probability\n",
      "of without encountering paradoxes. In this book, we only integrate over sets with\n",
      "relatively simple descriptions, so this aspect of measure theory never becomes a\n",
      "relevant concern.\n",
      "\n",
      "For our purposes, measure theory is more useful for describing theorems that\n",
      "apply to most points in Rn but do not apply to some corner cases. Measure theory\n",
      "provides a rigorous way of describing that a set of points is negligibly small. Such\n",
      "a set is said to have “measure zero.” We do not formally deﬁne this concept in this\n",
      "textbook. However, it is useful to understand the intuition that a set of measure\n",
      "zero occupies no volume in the space we are measuring. For example, within R2 , a\n",
      "line has measure zero, while a ﬁlled polygon has positive measure. Likewise, an\n",
      "individual point has measure zero. Any union of countably many sets that each\n",
      "have measure zero also has measure zero (so the set of all the rational numbers\n",
      "has measure zero, for instance).\n",
      "\n",
      "Another useful term from measure theory is “almost everywhere.” A property\n",
      "that holds almost everywhere holds throughout all of space except for on a set of\n",
      "measure zero. Because the exceptions occupy a negligible amount of space, they\n",
      "can be safely ignored for many applications. Some important results in probability\n",
      "theory hold for all discrete values but only hold “almost everywhere” for continuous\n",
      "values.\n",
      "\n",
      "Another technical detail of continuous variables relates to handling continuous\n",
      "random variables that are deterministic functions of one another. Suppose we have\n",
      "two random variables, x and y, such that y = g(x), where g is an invertible, con-\n",
      "\n",
      "2The Banach-Tarski theorem provides a fun example of such sets.\n",
      "\n",
      "70\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "tinuous, diﬀerentiable transformation. One might expect that py(y ) =p x(g−1(y )).\n",
      "This is actually not the case.\n",
      "\n",
      "As a simple example, suppose we have scalar random variables x and y. Suppose\n",
      "If we use the rule p y(y) = p x(2 y) then py will be 0\n",
      "on this interval. This means\n",
      "\n",
      "y = x\n",
      "everywhere except the interval [0 , 1\n",
      "2 ]\n",
      "\n",
      "2 and x ∼ U(0,1).\n",
      "\n",
      ", and it will be\n",
      "\n",
      "1\n",
      "\n",
      " py ( ) =\n",
      "\n",
      "y dy\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      ",\n",
      "\n",
      "(3.43)\n",
      "\n",
      "which violates the deﬁnition of a probability distribution.\n",
      "\n",
      "This common mistake is wrong because it fails to account for the distortion\n",
      "of space introduced by the function g. Recall that the probability of x lying in\n",
      "an inﬁnitesimally small region with volume δx is given by p( x)δx. Since g can\n",
      "expand or contract space, the inﬁnitesimal volume surrounding x in x space may\n",
      "have diﬀerent volume in\n",
      "\n",
      "space.\n",
      "\n",
      "y\n",
      "\n",
      "To see how to correct the problem, we return to the scalar case. We need to\n",
      "\n",
      "preserve the property\n",
      "\n",
      "Solving from this, we obtain\n",
      "\n",
      "|py( ( ))\n",
      "\n",
      "g x dy|\n",
      "\n",
      "=\n",
      "\n",
      "|p x( )x dx .|\n",
      "\n",
      "or equivalently\n",
      "\n",
      "In higher dimensions, the derivative generalizes to the determinant of the Jacobian\n",
      "matrix—the matrix with J i,j = ∂xi\n",
      "∂yj\n",
      "\n",
      ". Thus, for real-valued vectors\n",
      "\n",
      "and ,\n",
      "y\n",
      "\n",
      "x\n",
      "\n",
      "py( ) = \n",
      "\n",
      "y\n",
      "\n",
      "px ( ) = \n",
      "\n",
      "x\n",
      "\n",
      "∂x\n",
      "\n",
      "∂g x( )\n",
      "\n",
      "py( ( ))\n",
      "\n",
      "px (g−1( ))y\n",
      "\n",
      "\n",
      "∂y\n",
      "g x \n",
      "∂x  .\n",
      "g x det∂g( )x\n",
      "∂x  .\n",
      "\n",
      "px ( ) = \n",
      "\n",
      "x\n",
      "\n",
      "p y( ( ))\n",
      "\n",
      "3.13 Information Theory\n",
      "\n",
      "Information theory is a branch of applied mathematics that revolves around\n",
      "quantifying how much information is present in a signal. It was originally invented\n",
      "to study sending messages from discrete alphabets over a noisy channel, such as\n",
      "communication via radio transmission. In this context, information theory tells how\n",
      "to design optimal codes and calculate the expected length of messages sampled from\n",
      "\n",
      "71\n",
      "\n",
      "(3.44)\n",
      "\n",
      "(3.45)\n",
      "\n",
      "(3.46)\n",
      "\n",
      "(3.47)\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "speciﬁc probability distributions using various encoding schemes. In the context of\n",
      "machine learning, we can also apply information theory to continuous variables\n",
      "where some of these message length interpretations do not apply. This ﬁeld is\n",
      "fundamental to many areas of electrical engineering and computer science. In this\n",
      "textbook, we mostly use a few key ideas from information theory to characterize\n",
      "probability distributions or quantify similarity between probability distributions.\n",
      "For more detail on information theory, see Cover and Thomas (2006) or MacKay\n",
      "(2003).\n",
      "\n",
      "The basic intuition behind information theory is that learning that an unlikely\n",
      "event has occurred is more informative than learning that a likely event has\n",
      "occurred. A message saying “the sun rose this morning” is so uninformative as\n",
      "to be unnecessary to send, but a message saying “there was a solar eclipse this\n",
      "morning” is very informative.\n",
      "\n",
      "We would like to quantify information in a way that formalizes this intuition.\n",
      "\n",
      "Speciﬁcally,\n",
      "\n",
      "• Likely events should have low information content, and in the extreme case,\n",
      "events that are guaranteed to happen should have no information content\n",
      "whatsoever.\n",
      "\n",
      "• Less likely events should have higher information content.\n",
      "• Independent events should have additive information. For example, ﬁnding\n",
      "out that a tossed coin has come up as heads twice should convey twice as\n",
      "much information as ﬁnding out that a tossed coin has come up as heads\n",
      "once.\n",
      "\n",
      "In order to satisfy all three of these properties, we deﬁne the self-information\n",
      "\n",
      "of an event x\n",
      "\n",
      "= x\n",
      "\n",
      "to be\n",
      "\n",
      "I x\n",
      "( ) = \n",
      "\n",
      "log−\n",
      "\n",
      "P x .\n",
      "( )\n",
      "\n",
      "(3.48)\n",
      "\n",
      "In this book, we always use log to mean the natural logarithm, with base e. Our\n",
      "deﬁnition of I(x) is therefore written in units of\n",
      ". One nat is the amount of\n",
      "information gained by observing an event of probability 1\n",
      "e . Other texts use base-2\n",
      "logarithms and units called\n",
      "shannons\n",
      "; information measured in bits is just\n",
      "a rescaling of information measured in nats.\n",
      "\n",
      "nats\n",
      "\n",
      "bits\n",
      "\n",
      "or\n",
      "\n",
      "When x is continuous, we use the same deﬁnition of information by analogy,\n",
      "but some of the properties from the discrete case are lost. For example, an event\n",
      "with unit density still has zero information, despite not being an event that is\n",
      "guaranteed to occur.\n",
      "\n",
      "72\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      " \n",
      "n\n",
      "\n",
      "i\n",
      " \n",
      "y\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "n\n",
      "e\n",
      " \n",
      "n\n",
      "o\n",
      "n\n",
      "n\n",
      "a\n",
      "h\n",
      "S\n",
      "\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "\n",
      "Shannon entropy of a binary random variable\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1.0\n",
      "\n",
      "p\n",
      "\n",
      "1\n",
      "\n",
      "Figure 3.5: This plot shows how distributions that are closer to deterministic have low\n",
      "Shannon entropy while distributions that are close to uniform have high Shannon entropy.\n",
      "On the horizontal axis, we plot p, the probability of a binary random variable being equal\n",
      "to . The entropy is given by\n",
      "log . When p is near 0, the distribution\n",
      "is nearly deterministic, because the random variable is nearly always 0. When p is near 1,\n",
      "the distribution is nearly deterministic, because the random variable is nearly always 1.\n",
      "When p = 0 .5, the entropy is maximal, because the distribution is uniform over the two\n",
      "outcomes.\n",
      "\n",
      "(p− 1) log(1− p )− p\n",
      "\n",
      "p\n",
      "\n",
      "Self-information deals only with a single outcome. We can quantify the amount\n",
      "\n",
      "of uncertainty in an entire probability distribution using the Shannon entropy:\n",
      "\n",
      "H( ) = \n",
      "\n",
      "x\n",
      "\n",
      "E\n",
      "\n",
      "I x\n",
      "\n",
      "x∼P[ ( )] = \n",
      "\n",
      "−E\n",
      "\n",
      "P x .\n",
      "x∼P[log ( )]\n",
      "\n",
      "(3.49)\n",
      "\n",
      "also denoted H( P). In other words, the Shannon entropy of a distribution is the\n",
      "expected amount of information in an event drawn from that distribution. It gives\n",
      "a lower bound on the number of bits (if the logarithm is base 2, otherwise the units\n",
      "are diﬀerent) needed on average to encode symbols drawn from a distribution P.\n",
      "Distributions that are nearly deterministic (where the outcome is nearly certain)\n",
      "have low entropy; distributions that are closer to uniform have high entropy. See\n",
      "Fig. 3.5 for a demonstration. When x is continous, the Shannon entropy is known\n",
      "as the diﬀerential entropy.\n",
      "\n",
      "If we have two separate probability distributions P(x) and Q(x) over the same\n",
      "random variable x, we can measure how diﬀerent these two distributions are using\n",
      "the Kullback-Leibler (KL) divergence:\n",
      "\n",
      "DKL(\n",
      "\n",
      "P Q\n",
      "\n",
      ") = \n",
      "\n",
      "E x∼Plog\n",
      "\n",
      "P x( )\n",
      "\n",
      "Q x( ) = Ex∼P [log ( )\n",
      "\n",
      "P x −\n",
      "\n",
      "73\n",
      "\n",
      "log ( )]\n",
      "Q x .\n",
      "\n",
      "(3.50)\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "In the case of discrete variables, it is the extra amount of information (measured\n",
      "in bits if we use the base\n",
      "logarithm, but in machine learning we usually use nats\n",
      "and the natural logarithm) needed to send a message containing symbols drawn\n",
      "from probability distribution P , when we use a code that was designed to minimize\n",
      "the length of messages drawn from probability distribution\n",
      "\n",
      ".Q\n",
      "\n",
      "2\n",
      "\n",
      "The KL divergence has many useful properties, most notably that it is non-\n",
      "negative. The KL divergence is 0 if and only if P and Qare the same distribution in\n",
      "the case of discrete variables, or equal “almost everywhere” in the case of continuous\n",
      "variables. Because the KL divergence is non-negative and measures the diﬀerence\n",
      "between two distributions, it is often conceptualized as measuring some sort of\n",
      "distance between these distributions. However, it is not a true distance measure\n",
      "because it is not symmetric: DKL(P Q ) = DKL( Q P ) for some P and Q. This\n",
      "asymmetry means that there are important consequences to the choice of whether\n",
      "to use DKL(\n",
      "\n",
      ". See Fig. 3.6 for more detail.\n",
      "\n",
      "or DKL (\n",
      "\n",
      ")\n",
      "\n",
      "P Q\n",
      "\n",
      ")Q P\n",
      "\n",
      "A quantity that is closely related to the KL divergence is the cross-entropy\n",
      "H (P, Q ) = H (P) + DKL(P Q ), which is similar to the KL divergence but lacking\n",
      "the term on the left:\n",
      "(3.51)\n",
      "\n",
      "H P, Q(\n",
      "\n",
      ") = −E\n",
      "\n",
      "x∼P log ( )Q x .\n",
      "\n",
      "Minimizing the cross-entropy with respect to Q is equivalent to minimizing the\n",
      "KL divergence, because\n",
      "\n",
      "does not participate in the omitted term.\n",
      "\n",
      "Q\n",
      "\n",
      "When computing many of these quantities, it is common to encounter expres-\n",
      "sions of the form 0log 0. By convention, in the context of information theory, we\n",
      "treat these expressions as limx→0 x\n",
      "\n",
      "log = 0.\n",
      "\n",
      "x\n",
      "\n",
      "3.14 Structured Probabilistic Models\n",
      "\n",
      "Machine learning algorithms often involve probability distributions over a very\n",
      "large number of random variables. Often, these probability distributions involve\n",
      "direct interactions between relatively few variables. Using a single function to\n",
      "describe the entire joint probability distribution can be very ineﬃcient (both\n",
      "computationally and statistically).\n",
      "\n",
      "Instead of using a single function to represent a probability distribution, we\n",
      "can split a probability distribution into many factors that we multiply together.\n",
      "For example, suppose we have three random variables: a, b and c . Suppose that\n",
      "a inﬂuences the value of b and b inﬂuences the value of c, but that a and c are\n",
      "independent given b. We can represent the probability distribution over all three\n",
      "\n",
      "74\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "q∗ = argminq DKL(\n",
      "\n",
      ")p q\n",
      "\n",
      "q ∗ = argminq DKL (\n",
      "\n",
      "q p\n",
      "\n",
      ")\n",
      "\n",
      "y\n",
      "t\n",
      "i\n",
      "s\n",
      "n\n",
      "e\n",
      "D\n",
      "y\n",
      "t\n",
      "i\n",
      "l\n",
      "i\n",
      "\n",
      "b\n",
      "a\n",
      "b\n",
      "o\n",
      "r\n",
      "P\n",
      "\n",
      "p x( )\n",
      "q∗( )x\n",
      "\n",
      "y\n",
      "t\n",
      "i\n",
      "s\n",
      "n\n",
      "e\n",
      "D\n",
      "y\n",
      "t\n",
      "i\n",
      "l\n",
      "i\n",
      "\n",
      "b\n",
      "a\n",
      "b\n",
      "o\n",
      "r\n",
      "P\n",
      "\n",
      "p( )x\n",
      "q ∗( )x\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "Figure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x ) and\n",
      "wish to approximate it with another distribution q(x). We have the choice of minimizing\n",
      "either DKL( p q ) or DKL( q p ). We illustrate the eﬀect of this choice using a mixture of\n",
      "two Gaussians for p, and a single Gaussian for q. The choice of which direction of the\n",
      "KL divergence to use is problem-dependent. Some applications require an approximation\n",
      "that usually places high probability anywhere that the true distribution places high\n",
      "probability, while other applications require an approximation that rarely places high\n",
      "probability anywhere that the true distribution places low probability. The choice of the\n",
      "direction of the KL divergence reﬂects which of these considerations takes priority for each\n",
      "application. (Left) The eﬀect of minimizing DKL (p q ). In this case, we select a q that has\n",
      "high probability where p has high probability. When p has multiple modes, q chooses to\n",
      "blur the modes together, in order to put high probability mass on all of them. (Right) The\n",
      "eﬀect of minimizing DKL (q p ). In this case, we select a q that has low probability where\n",
      "p has low probability. When p has multiple modes that are suﬃciently widely separated,\n",
      "as in this ﬁgure, the KL divergence is minimized by choosing a single mode, in order to\n",
      "avoid putting probability mass in the low-probability areas between modes of p . Here, we\n",
      "illustrate the outcome when q is chosen to emphasize the left mode. We could also have\n",
      "achieved an equal value of the KL divergence by choosing the right mode. If the modes\n",
      "are not separated by a suﬃciently strong low probability region, then this direction of the\n",
      "KL divergence can still choose to blur the modes.\n",
      "\n",
      "75\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "variables as a product of probability distributions over two variables:\n",
      "\n",
      "p ,\n",
      "\n",
      "(a b c) =  ( )a (\n",
      "p\n",
      "\n",
      "p\n",
      "\n",
      ",\n",
      "\n",
      "b a|\n",
      "\n",
      ")\n",
      "\n",
      "c b|\n",
      "(\n",
      "p\n",
      "\n",
      ")\n",
      "\n",
      ".\n",
      "\n",
      "(3.52)\n",
      "\n",
      "These factorizations can greatly reduce the number of parameters needed\n",
      "to describe the distribution. Each factor uses a number of parameters that is\n",
      "exponential in the number of variables in the factor. This means that we can greatly\n",
      "reduce the cost of representing a distribution if we are able to ﬁnd a factorization\n",
      "into distributions over fewer variables.\n",
      "\n",
      "We can describe these kinds of factorizations using graphs. Here we use the\n",
      "word “graph” in the sense of graph theory: a set of vertices that may be connected\n",
      "to each other with edges. When we represent the factorization of a probability\n",
      "distribution with a graph, we call it a structured probabilistic model\n",
      "graphical\n",
      "model.\n",
      "\n",
      "or\n",
      "\n",
      "There are two main kinds of structured probabilistic models: directed and\n",
      "undirected. Both kinds of graphical models use a graph G in which each node\n",
      "in the graph corresponds to a random variable, and an edge connecting two\n",
      "random variables means that the probability distribution is able to represent direct\n",
      "interactions between those two random variables.\n",
      "\n",
      "Directed models use graphs with directed edges, and they represent factoriza-\n",
      "tions into conditional probability distributions, as in the example above. Speciﬁcally,\n",
      "a directed model contains one factor for every random variable x i in the distribution,\n",
      "and that factor consists of the conditional distribution over xi given the parents of\n",
      "xi, denoted P a G(x i):\n",
      "\n",
      "(3.53)\n",
      "\n",
      "p( ) =x i\n",
      "\n",
      "p (xi | P aG (xi )) .\n",
      "\n",
      "See Fig. 3.7 for an example of a directed graph and the factorization of probability\n",
      "distributions it represents.\n",
      "\n",
      "Undirected models use graphs with undirected edges, and they represent fac-\n",
      "torizations into a set of functions; unlike in the directed case, these functions are\n",
      "usually not probability distributions of any kind. Any set of nodes that are all\n",
      "connected to each other in G is called a clique. Each clique C ( )i\n",
      "in an undirected\n",
      "model is associated with a factor φ( )i (C ( )i ). These factors are just functions, not\n",
      "probability distributions. The output of each factor must be non-negative, but\n",
      "there is no constraint that the factor must sum or integrate to 1 like a probability\n",
      "distribution.\n",
      "\n",
      "The probability of a conﬁguration of random variables is proportional to the\n",
      "product of all of these factors—assignments that result in larger factor values are\n",
      "\n",
      "76\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "aa\n",
      "\n",
      "bb\n",
      "\n",
      "dd\n",
      "\n",
      "cc\n",
      "\n",
      "ee\n",
      "\n",
      "Figure 3.7: A directed graphical model over random variables a , b, c, d and e. This graph\n",
      "corresponds to probability distributions that can be factored as\n",
      "\n",
      "(a b c d e) =  ( )a (\n",
      "p ,\n",
      "p\n",
      "\n",
      "p\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "b a|\n",
      "\n",
      ")\n",
      "\n",
      "(c a|\n",
      "p\n",
      "\n",
      ",\n",
      "\n",
      "b) (\n",
      "p\n",
      "\n",
      "d b|\n",
      "\n",
      ")\n",
      "\n",
      "e c|\n",
      "(\n",
      "p\n",
      "\n",
      ")\n",
      "\n",
      ".\n",
      "\n",
      "(3.54)\n",
      "\n",
      "This graph allows us to quickly see some properties of the distribution. For example, a\n",
      "and c interact directly, but a and e interact only indirectly via c.\n",
      "\n",
      "more likely. Of course, there is no guarantee that this product will sum to 1. We\n",
      "therefore divide by a normalizing constant Z, deﬁned to be the sum or integral\n",
      "over all states of the product of the φ functions, in order to obtain a normalized\n",
      "probability distribution:\n",
      "\n",
      "p( ) =x\n",
      "\n",
      "1\n",
      "\n",
      "Z i\n",
      "\n",
      "φ( )i C( )i .\n",
      "\n",
      "(3.55)\n",
      "\n",
      "See Fig. 3.8 for an example of an undirected graph and the factorization of\n",
      "probability distributions it represents.\n",
      "\n",
      "Keep in mind that these graphical representations of factorizations are a\n",
      "language for describing probability distributions. They are not mutually exclusive\n",
      "families of probability distributions. Being directed or undirected is not a property\n",
      "of a probability distribution; it is a property of a particular\n",
      "of a\n",
      "probability distribution, but any probability distribution may be described in both\n",
      "ways.\n",
      "\n",
      "description\n",
      "\n",
      "Throughout Part I and Part II of this book, we will use structured probabilistic\n",
      "models merely as a language to describe which direct probabilistic relationships\n",
      "diﬀerent machine learning algorithms choose to represent. No further understanding\n",
      "of structured probabilistic models is needed until the discussion of research topics,\n",
      "in Part III, where we will explore structured probabilistic models in much greater\n",
      "detail.\n",
      "\n",
      "77\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "aa\n",
      "\n",
      "bb\n",
      "\n",
      "dd\n",
      "\n",
      "cc\n",
      "\n",
      "ee\n",
      "\n",
      "Figure 3.8: An undirected graphical model over random variables a, b, c, d and e . This\n",
      "graph corresponds to probability distributions that can be factored as\n",
      "\n",
      "(a b c d e) =\n",
      "p ,\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "1\n",
      "Z\n",
      "\n",
      "φ (1)(\n",
      "\n",
      "a b c\n",
      ",\n",
      "\n",
      ", φ (2) (\n",
      "\n",
      ")\n",
      "\n",
      ")b d, φ(3)(\n",
      "\n",
      ")c e,\n",
      ".\n",
      "\n",
      "(3.56)\n",
      "\n",
      "This graph allows us to quickly see some properties of the distribution. For example, a\n",
      "and c interact directly, but a and e interact only indirectly via c.\n",
      "\n",
      "This chapter has reviewed the basic concepts of probability theory that are\n",
      "most relevant to deep learning. One more set of fundamental mathematical tools\n",
      "remains: numerical methods.\n",
      "\n",
      "78\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "cry = str_\n",
    "print(cry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\yaniv\\\\Downloads\\\\deeplearningbook-prob.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pypdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ed07d9f3ea64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpdfobject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpypdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdfobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pypdf' is not defined"
     ]
    }
   ],
   "source": [
    "pdfobject=open(path,'rb')\n",
    "pdf=pypdf.PdfFileReader(pdfobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d7fc26c9707a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetXmpMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "print(pdf.getPage(1).getXmpMetadata())\n",
    "print(pdf.getPage(1).getContents())\n",
    "print(pdf.getPage(1).extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getPage(3).extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Filter': '/FlateDecode'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getPage(1).getContents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_addTransformationMatrix',\n",
       " '_contentStreamRename',\n",
       " '_mergePage',\n",
       " '_mergeResources',\n",
       " '_pushPopGS',\n",
       " '_rotate',\n",
       " 'addTransformation',\n",
       " 'artBox',\n",
       " 'bleedBox',\n",
       " 'clear',\n",
       " 'compressContentStreams',\n",
       " 'copy',\n",
       " 'createBlankPage',\n",
       " 'cropBox',\n",
       " 'extractText',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'getContents',\n",
       " 'getObject',\n",
       " 'getXmpMetadata',\n",
       " 'indirectRef',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'mediaBox',\n",
       " 'mergePage',\n",
       " 'mergeRotatedPage',\n",
       " 'mergeRotatedScaledPage',\n",
       " 'mergeRotatedScaledTranslatedPage',\n",
       " 'mergeRotatedTranslatedPage',\n",
       " 'mergeScaledPage',\n",
       " 'mergeScaledTranslatedPage',\n",
       " 'mergeTransformedPage',\n",
       " 'mergeTranslatedPage',\n",
       " 'pdf',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'raw_get',\n",
       " 'readFromStream',\n",
       " 'rotateClockwise',\n",
       " 'rotateCounterClockwise',\n",
       " 'scale',\n",
       " 'scaleBy',\n",
       " 'scaleTo',\n",
       " 'setdefault',\n",
       " 'trimBox',\n",
       " 'update',\n",
       " 'values',\n",
       " 'writeToStream',\n",
       " 'xmpMetadata']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pdf.getPage(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_match  = '\\n\\n3.1 Why Probability?\\n\\n'\n",
    "str_to_match = '\\n\\n3.1 why probability\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 3\\n\\nProbability and Information\\nTheory\\n\\nIn this chapter, we describe probability theory and information theory.\\n\\nProbability theory is a mathematical framework for representing uncertain\\nstatements. It provides a means of quantifying uncertainty and axioms for deriving\\nnew uncertain statements. In artiﬁcial intelligence applications, we use probability\\ntheory in two major ways. First, the laws of probability tell us how AI systems\\nshould reason, so we design our algorithms to compute or approximate various\\nexpressions derived using probability theory. Second, we can use probability and\\nstatistics to theoretically analyze the behavior of proposed AI systems.\\n\\nProbability theory is a fundamental tool of many disciplines of science and\\nengineering. We provide this chapter to ensure that readers whose background is\\nprimarily in software engineering with limited exposure to probability theory can\\nunderstand the material in this book.\\n\\nWhile probability theory allows us to make uncertain statements and reason\\nin the presence of uncertainty, information allows us to quantify the amount of\\nuncertainty in a probability distribution.\\n\\nIf you are already familiar with probability theory and information theory,\\nyou may wish to skip all of this chapter except for Sec. 3.14, which describes the\\ngraphs we use to describe structured probabilistic models for machine learning. If\\nyou have absolutely no prior experience with these subjects, this chapter should\\nbe suﬃcient to successfully carry out deep learning research projects, but we do\\nsuggest that you consult an additional resource, such as Jaynes (2003).\\n\\n52\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.1 Why Probability?\\n\\nMany branches of computer science deal mostly with entities that are entirely\\ndeterministic and certain. A programmer can usually safely assume that a CPU will\\nexecute each machine instruction ﬂawlessly. Errors in hardware do occur, but are\\nrare enough that most software applications do not need to be designed to account\\nfor them. Given that many computer scientists and software engineers work in a\\nrelatively clean and certain environment, it can be surprising that machine learning\\nmakes heavy use of probability theory.\\n\\nThis is because machine learning must always deal with uncertain quantities,\\nand sometimes may also need to deal with stochastic (non-deterministic) quantities.\\nUncertainty and stochasticity can arise from many sources. Researchers have made\\ncompelling arguments for quantifying uncertainty using probability since at least\\nthe 1980s. Many of the arguments presented here are summarized from or inspired\\nby Pearl (1988).\\n\\nNearly all activities require some ability to reason in the presence of uncertainty.\\nIn fact, beyond mathematical statements that are true by deﬁnition, it is diﬃcult\\nto think of any proposition that is absolutely true or any event that is absolutely\\nguaranteed to occur.\\n\\nThere are three possible sources of uncertainty:\\n\\n1. Inherent stochasticity in the system being modeled. For example, most\\ninterpretations of quantum mechanics describe the dynamics of subatomic\\nparticles as being probabilistic. We can also create theoretical scenarios that\\nwe postulate to have random dynamics, such as a hypothetical card game\\nwhere we assume that the cards are truly shuﬄed into a random order.\\n\\n2. Incomplete observability. Even deterministic systems can appear stochastic\\nwhen we cannot observe all of the variables that drive the behavior of the\\nsystem. For example, in the Monty Hall problem, a game show contestant is\\nasked to choose between three doors and wins a prize held behind the chosen\\ndoor. Two doors lead to a goat while a third leads to a car. The outcome\\ngiven the contestant’s choice is deterministic, but from the contestant’s point\\nof view, the outcome is uncertain.\\n\\n3. Incomplete modeling. When we use a model that must discard some of\\nthe information we have observed, the discarded information results in\\nuncertainty in the model’s predictions. For example, suppose we build a\\nrobot that can exactly observe the location of every object around it. If the\\n\\n53\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nrobot discretizes space when predicting the future location of these objects,\\nthen the discretization makes the robot immediately become uncertain about\\nthe precise position of objects: each object could be anywhere within the\\ndiscrete cell that it was observed to occupy.\\n\\nIn many cases, it is more practical to use a simple but uncertain rule rather\\nthan a complex but certain one, even if the true rule is deterministic and our\\nmodeling system has the ﬁdelity to accommodate a complex rule. For example, the\\nsimple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a rule\\nof the form, “Birds ﬂy, except for very young birds that have not yet learned to\\nﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\\nincluding the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\\ncommunicate, and after all of this eﬀort is still very brittle and prone to failure.\\n\\nGiven that we need a means of representing and reasoning about uncertainty,\\nit is not immediately obvious that probability theory can provide all of the tools\\nwe want for artiﬁcial intelligence applications. Probability theory was originally\\ndeveloped to analyze the frequencies of events. It is easy to see how probability\\ntheory can be used to study events like drawing a certain hand of cards in a\\ngame of poker. These kinds of events are often repeatable. When we say that\\nan outcome has a probability p of occurring, it means that if we repeated the\\nexperiment (e.g., draw a hand of cards) inﬁnitely many times, then proportion p\\nof the repetitions would result in that outcome. This kind of reasoning does not\\nseem immediately applicable to propositions that are not repeatable. If a doctor\\nanalyzes a patient and says that the patient has a 40% chance of having the ﬂu,\\nthis means something very diﬀerent—we can not make inﬁnitely many replicas of\\nthe patient, nor is there any reason to believe that diﬀerent replicas of the patient\\nwould present with the same symptoms yet have varying underlying conditions. In\\nthe case of the doctor diagnosing the patient, we use probability to represent a\\ndegree of belief, with 1 indicating absolute certainty that the patient has the ﬂu\\nand 0 indicating absolute certainty that the patient does not have the ﬂu. The\\nformer kind of probability, related directly to the rates at which events occur, is\\nknown as frequentist probability, while the latter, related to qualitative levels of\\ncertainty, is known as Bayesian probability.\\n\\nIf we list several properties that we expect common sense reasoning about\\nuncertainty to have, then the only way to satisfy those properties is to treat\\nBayesian probabilities as behaving exactly the same as frequentist probabilities.\\nFor example, if we want to compute the probability that a player will win a poker\\ngame given that she has a certain set of cards, we use exactly the same formulas\\nas when we compute the probability that a patient has a disease given that she\\n\\n54\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nhas certain symptoms. For more details about why a small set of common sense\\nassumptions implies that the same axioms must control both kinds of probability,\\nsee Ramsey (1926).\\n\\nProbability can be seen as the extension of logic to deal with uncertainty. Logic\\nprovides a set of formal rules for determining what propositions are implied to\\nbe true or false given the assumption that some other set of propositions is true\\nor false. Probability theory provides a set of formal rules for determining the\\nlikelihood of a proposition being true given the likelihood of other propositions.\\n\\n3.2 Random Variables\\n\\nA random variable is a variable that can take on diﬀerent values randomly. We\\ntypically denote the random variable itself with a lower case letter in plain typeface,\\nand the values it can take on with lower case script letters. For example, x1 and x2\\nare both possible values that the random variable x can take on. For vector-valued\\nvariables, we would write the random variable as x and one of its values as x. On\\nits own, a random variable is just a description of the states that are possible; it\\nmust be coupled with a probability distribution that speciﬁes how likely each of\\nthese states are.\\n\\nRandom variables may be discrete or continuous. A discrete random variable\\nis one that has a ﬁnite or countably inﬁnite number of states. Note that these\\nstates are not necessarily the integers; they can also just be named states that\\nare not considered to have any numerical value. A continuous random variable is\\nassociated with a real value.\\n\\n3.3 Probability Distributions\\n\\nA probability distribution is a description of how likely a random variable or\\nset of random variables is to take on each of its possible states. The way we\\ndescribe probability distributions depends on whether the variables are discrete or\\ncontinuous.\\n\\n3.3.1 Discrete Variables and Probability Mass Functions\\n\\nA probability distribution over discrete variables may be described using a proba-\\nbility mass function (PMF). We typically denote probability mass functions with a\\ncapital P . Often we associate each random variable with a diﬀerent probability\\n\\n55\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nmass function and the reader must infer which probability mass function to use\\nbased on the identity of the random variable, rather than the name of the function;\\nP\\n\\n( )x is usually not the same as\\n\\n( )y .\\n\\nP\\n\\nThe probability mass function maps from a state of a random variable to\\nthe probability of that random variable taking on that state. The probability\\nthat x = x is denoted as P (x), with a probability of 1 indicating that x = x is\\ncertain and a probability of 0 indicating that x = x is impossible. Sometimes\\nto disambiguate which PMF to use, we write the name of the random variable\\nexplicitly: P (x = x). Sometimes we deﬁne a variable ﬁrst, then use ∼ notation to\\nspecify which distribution it follows later: x ∼ P (x .)\\n\\nProbability mass functions can act on many variables at the same time. Such\\na probability distribution over many variables is known as a joint probability\\ndistribution. P (x = x, y = y) denotes the probability that x = x and y = y\\nsimultaneously. We may also write\\n\\nfor brevity.\\n\\nP x, y\\n\\n(\\n\\n)\\n\\nTo be a probability mass function on a random variable x, a function P must\\n\\nsatisfy the following properties:\\n\\nP\\n\\n• The domain of must be the set of all possible states of x.\\n• ∀x\\n0 \\n\\n,∈ x 0 ≤ P (x) ≤ 1. An impossible event has probability and no state can\\nbe less probable than that. Likewise, an event that is guaranteed to happen\\nhas probability , and no state can have a greater chance of occurring.\\n\\n1\\n\\n• \\ue050x∈x P (x) = 1. We refer to this property as being normalized. Without this\\n\\nproperty, we could obtain probabilities greater than one by computing the\\nprobability of one of many events occurring.\\n\\nFor example, consider a single discrete random variable x with k diﬀerent states.\\nx—that is, make each of its states equally\\n\\nuniform distribution\\n\\nWe can place a\\nlikely—by setting its probability mass function to\\n\\non\\n\\nP\\n\\n( = x\\n\\nx\\ni) =\\n\\n1\\nk\\n\\n(3.1)\\n\\nfor all i. We can see that this ﬁts the requirements for a probability mass function.\\nThe value 1\\n\\nis a positive integer. We also see that\\n\\nk is positive because\\n\\nk\\n\\n\\ue058i\\n\\nP\\n\\n( = x\\n\\nx\\n\\ni) =\\ue058i\\n\\n1\\nk\\n\\n=\\n\\nk\\nk\\n\\n= 1,\\n\\n(3.2)\\n\\nso the distribution is properly normalized.\\n\\n56\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.3.2 Continuous Variables and Probability Density Functions\\n\\nWhen working with continuous random variables, we describe probability dis-\\ntributions using a probability density function (PDF) rather than a probability\\nmass function. To be a probability density function, a function p must satisfy the\\nfollowing properties:\\n\\np\\n\\n• The domain of must be the set of all possible states of x.\\n• ∀x\\n\\n0≥ Note that we do not require ( ) \\np x\\n\\n1≤ .\\n\\n.\\n\\n∈ x ( ) \\n, p x\\n( ) = 1.\\n\\n• \\ue052 p x dx\\n\\nA probability density function p(x) does not give the probability of a speciﬁc\\nstate directly, instead the probability of landing inside an inﬁnitesimal region with\\nvolume\\n\\nis given by\\n\\np x δx\\n\\n( )\\n\\nδx\\n\\n.\\n\\nWe can integrate the density function to ﬁnd the actual probability mass of a\\nset of points. Speciﬁcally, the probability that x lies in some set S is given by the\\nintegral of p(x) over that set. In the univariate example, the probability that x\\nlies in the interval\\n\\nis given by\\n\\n.\\np x dx\\n\\n[\\n]a, b\\n\\n( )\\n\\nFor an example of a probability density function corresponding to a speciﬁc\\nprobability density over a continuous random variable, consider a uniform distribu-\\ntion on an interval of the real numbers. We can do this with a function u (x; a, b),\\nwhere a and b are the endpoints of the interval, with b > a. The “;” notation means\\n“parametrized by”; we consider x to be the argument of the function, while a and\\nb are parameters that deﬁne the function. To ensure that there is no probability\\nmass outside the interval, we say u(x; a, b) = 0 for all x \\ue036∈ [a, b]\\n. Within a, b],\\n. We can see that this is nonnegative everywhere. Additionally, it\\nu x a, b\\n( ;\\nintegrates to 1. We often denote that x follows the uniform distribution on [a, b]\\nby writing x\\n\\n) = 1\\nb a−\\n∼ U a, b\\n)\\n\\n(\\n\\n[\\n\\n.\\n\\n\\ue052[\\n\\n]a,b\\n\\n3.4 Marginal Probability\\n\\nSometimes we know the probability distribution over a set of variables and we want\\nto know the probability distribution over just a subset of them. The probability\\ndistribution over the subset is known as the marginal probability distribution.\\n\\nFor example, suppose we have discrete random variables x and y , and we know\\n\\nP ,(x y . We can ﬁnd\\n\\n)\\n\\nx with the\\n\\nsum rule\\n\\n:\\n\\nx, P\\n\\n( = x\\n\\nx\\n\\nP\\n\\n( = x\\n\\nx,\\n\\ny =  )\\ny .\\n\\n(3.3)\\n\\nP ( )\\n∀ ∈x\\n\\n) =\\ue058y\\n\\n57\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe name “marginal probability” comes from the process of computing marginal\\nprobabilities on paper. When the values of P(x y, ) are written in a grid with\\ndiﬀerent values of x in rows and diﬀerent values of y in columns, it is natural to\\nsum across a row of the grid, then write P( x) in the margin of the paper just to\\nthe right of the row.\\n\\nFor continuous variables, we need to use integration instead of summation:\\n\\np x( ) =\\ue05a p x, y dy.\\n\\n(\\n\\n)\\n\\n(3.4)\\n\\n3.5 Conditional Probability\\n\\nIn many cases, we are interested in the probability of some event, given that some\\nother event has happened. This is called a conditional probability. We denote\\nthe conditional probability that y = y given x = x as P (y = y | x = x ). This\\nconditional probability can be computed with the formula\\n\\nP\\n\\n( = y\\n\\ny\\n\\n| x =  ) =\\n\\nx\\n\\nP\\n\\n( = y\\n\\ny,\\n\\nx =  )\\nx\\nx\\n)\\n\\nP\\n\\n( = x\\n\\n.\\n\\n(3.5)\\n\\nThe conditional probability is only deﬁned when P (x = x) > 0. We cannot compute\\nthe conditional probability conditioned on an event that never happens.\\n\\nIt is important not to confuse conditional probability with computing what\\nwould happen if some action were undertaken. The conditional probability that\\na person is from Germany given that they speak German is quite high, but if\\na randomly selected person is taught to speak German, their country of origin\\ndoes not change. Computing the consequences of an action is called making an\\nintervention query. Intervention queries are the domain of causal modeling, which\\nwe do not explore in this book.\\n\\n3.6 The Chain Rule of Conditional Probabilities\\n\\nAny joint probability distribution over many random variables may be decomposed\\ninto conditional distributions over only one variable:\\n\\nP (x(1), . . . , x ( )n ) = \\n\\n| x(1), . . . , x (\\nproduct rule of probability. It\\nfollows immediately from the deﬁnition of conditional probability in Eq. 3.5. For\\n\\nThis observation is known as the\\n\\n(P x(1) )Πn\\n\\ni=2P (x( )i\\n\\nchain rule\\n\\ni− ).\\n\\n(3.6)\\n\\nor\\n\\n1)\\n\\n58\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nexample, applying the deﬁnition twice, we get\\n\\nP ,\\n\\n(a b c) =\\n\\n,\\n\\nP ,\\n\\n(b c) =\\n\\nP ,\\n\\n(a b c) =\\n\\n,\\n\\nP\\n\\nP\\n\\nP\\n\\n(a b|\\n(b c| )\\n(a b|\\n\\nc)\\n, P ,\\n\\n(b c)\\n\\n( )P c\\n\\n, Pc)\\n\\n(b c| )\\n\\n( )P c .\\n\\n3.7 Independence and Conditional Independence\\n\\nTwo random variables x and y are independent if their probability distribution can\\nbe expressed as a product of two factors, one involving only x and one involving\\nonly y:\\n\\n∀ ∈x\\n\\nx, y\\n\\n∈ y\\n\\n, p\\n\\nx\\n( = \\n\\nx, y\\n\\n= ) =  ( =\\n\\np x\\n\\ny\\n\\nx) ( =  )\\ny .\\n\\np y\\n\\n(3.7)\\n\\nTwo random variables x and y are conditionally independent given a random\\nvariable z if the conditional probability distribution over x and y factorizes in this\\nway for every value of z:\\n\\n∀ ∈x\\n\\nx, y\\n\\n, z∈ y ∈ z, p\\n\\n( =x\\n\\nx,\\n\\ny = \\n\\ny\\n\\n| z =  ) =  ( = x\\n\\np\\n\\nz\\n\\nx\\n\\n| z =  ) ( = y\\n\\nz p\\n\\ny\\n\\n| z = )\\nz .\\n(3.8)\\n\\nWe can denote independence and conditional independence with compact\\nnotation: x y⊥ means that x and y are independent, while x y z⊥ | means that x\\nand y are conditionally independent given z.\\n\\n3.8 Expectation, Variance and Covariance\\n\\nor\\n\\nThe expectation\\nof some function f( x) with respect to a probability\\ndistribution P (x) is the average or mean value that f takes on when x is drawn\\nfrom . For discrete variables this can be computed with a summation:\\n\\nexpected value\\n\\nP\\n\\nEx∼P[ (f x)] =\\ue058x\\n\\nP x f x ,\\n( ) ( )\\n\\n(3.9)\\n\\nwhile for continuous variables, it is computed with an integral:\\n\\nEx∼p[ ( )] =\\n\\nf x\\n\\n\\ue05a p x f x dx.\\n\\n( ) ( )\\n\\n59\\n\\n(3.10)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nWhen the identity of the distribution is clear from the context, we may simply\\nwrite the name of the random variable that the expectation is over, as in Ex[f (x)].\\nIf it is clear which random variable the expectation is over, we may omit the\\nsubscript entirely, as in E[f (x)]. By default, we can assume that E [·] averages over\\nthe values of all the random variables inside the brackets. Likewise, when there is\\nno ambiguity, we may omit the square brackets.\\n\\nExpectations are linear, for example,\\n\\nE x[\\n\\nαf x\\n\\n( ) + ( )] = \\n\\nβg x\\n\\nαEx [ ( )] +\\n\\nf x\\n\\nβEx[ ( )]\\ng x ,\\n\\n(3.11)\\n\\nwhen\\n\\nα\\n\\nand\\n\\nβ\\n\\nare not dependent on .\\nx\\n\\nThe variance gives a measure of how much the values of a function of a random\\nvariable x vary as we sample diﬀerent value of x from its probability distribution:\\n\\nVar( ( )) = \\n\\nf x\\n\\nE\\ue068( ( )\\nf x − E f x 2\\ue069.\\n\\n[ ( )])\\n\\n(3.12)\\n\\nWhen the variance is low, the values of f (x) cluster near their expected value. The\\nsquare root of the variance is known as the standard deviation.\\n\\nThe covariance gives some sense of how much two values are linearly related to\\n\\neach other, as well as the scale of these variables:\\n\\nCov( ( )\\n\\nf x , g y\\n\\n( )) = \\n\\nE f x − E f x\\n[( ( )\\n\\n[ ( )]) ( ( )\\n\\ng y − E g y\\n\\n[ ( )])]\\n\\n.\\n\\n(3.13)\\n\\nHigh absolute values of the covariance mean that the values change very much\\nand are both far from their respective means at the same time. If the sign of the\\ncovariance is positive, then both variables tend to take on relatively high values\\nsimultaneously. If the sign of the covariance is negative, then one variable tends to\\ntake on a relatively high value at the times that the other takes on a relatively low\\nvalue and vice versa. Other measures such as correlation normalize the contribution\\nof each variable in order to measure only how much the variables are related, rather\\nthan also being aﬀected by the scale of the separate variables.\\n\\nThe notions of covariance and dependence are related, but are in fact distinct\\nconcepts. They are related because two variables that are independent have zero\\ncovariance, and two variables that have non-zero covariance are dependent. How-\\never, independence is a distinct property from covariance. For two variables to have\\nzero covariance, there must be no linear dependence between them. Independence\\nis a stronger requirement than zero covariance, because independence also excludes\\nnonlinear relationships. It is possible for two variables to be dependent but have\\nzero covariance. For example, suppose we ﬁrst sample a real number x from a\\nuniform distribution over the interval [−1 ,1]. We next sample a random variable\\n\\n60\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\ns. With probability 1\\n2, we choose the value of s to be 1. Otherwise, we choose\\nthe value of s to be − 1. We can then generate a random variable y by assigning\\ny = sx . Clearly, x and y are not independent, because x completely determines\\nthe magnitude of\\n\\n. However,\\n\\n.\\n) = 0\\n\\nCov(\\n\\nx, y\\n\\ny\\n\\nThe covariance matrix of a random vector x ∈ Rn is an n n× matrix, such that\\n(3.14)\\n\\nCov( )x i,j = Cov(xi, xj ).\\n\\nThe diagonal elements of the covariance give the variance:\\n\\nCov(xi , xi) = Var(xi).\\n\\n(3.15)\\n\\n3.9 Common Probability Distributions\\n\\nSeveral simple probability distributions are useful in many contexts in machine\\nlearning.\\n\\n3.9.1 Bernoulli Distribution\\n\\nBernoulli\\n\\nThe\\ndistribution is a distribution over a single binary random variable.\\nIt is controlled by a single parameter φ ∈ [0, 1], which gives the probability of the\\nrandom variable being equal to 1. It has the following properties:\\n\\nP\\n\\nx\\n( = 1) = \\n\\nφ\\n\\nP\\n\\nx\\n( = 0) = 1\\n) =  x (1\\nx\\n\\nφ\\n\\nP\\n\\n( = x\\n\\nEx [ ] = \\n\\nx\\n\\nφ\\n\\n−\\n)− φ 1−x\\nφ\\n\\nVar x( ) =  (1\\n\\nx\\n\\nφ − φ\\n\\n)\\n\\n(3.16)\\n\\n(3.17)\\n\\n(3.18)\\n\\n(3.19)\\n\\n(3.20)\\n\\n3.9.2 Multinoulli Distribution\\n\\nmultinoulli\\n\\nThe\\ncategorical distribution is a distribution over a single discrete\\nvariable with k diﬀerent states, where k is ﬁnite1 . The multinoulli distribution is\\n\\nor\\n\\n1 “Multinoulli” is a term that was recently coined by Gustavo Lacerdo and popularized by\\nMurphy (2012). The multinoulli distribution is a special case of the\\ndistribution. A\\nmultinomial distribution is the distribution over vectors in {0, . . . , n}k representing how many\\ntimes each of the k categories is visited when n samples are drawn from a multinoulli distribution.\\nMany texts use the term “multinomial” to refer to multinoulli distributions without clarifying\\nthat they refer only to the\\n\\nmultinomial\\n\\nn = 1\\n\\ncase.\\n\\n61\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nparametrized by a vector p ∈ [0, 1]k−1 , where p i gives the probability of the i-th\\nstate. The ﬁnal, k-th state’s probability is given by 1− 1\\ue03e p. Note that we must\\nconstrain 1 \\ue03ep ≤ 1. Multinoulli distributions are often used to refer to distributions\\nover categories of objects, so we do not usually assume that state 1 has numerical\\nvalue 1, etc. For this reason, we do not usually need to compute the expectation\\nor variance of multinoulli-distributed random variables.\\n\\nThe Bernoulli and multinoulli distributions are suﬃcient to describe any distri-\\nbution over their domain. This is because they model discrete variables for which\\nit is feasible to simply enumerate all of the states. When dealing with continuous\\nvariables, there are uncountably many states, so any distribution described by a\\nsmall number of parameters must impose strict limits on the distribution.\\n\\n3.9.3 Gaussian Distribution\\n\\nThe most commonly used distribution over real numbers is the normal distribution,\\nalso known as the Gaussian distribution:\\n\\nN ( ;x µ, σ2) =\\ue072 1\\n\\n2πσ2 exp\\ue012−\\n\\n1\\nx\\n2σ2 (\\n\\nµ− 2\\ue013 .\\n\\n)\\n\\n(3.21)\\n\\nSee Fig. 3.1 for a plot of the density function.\\nThe two parameters µ ∈ R and σ ∈ (0,∞ ) control the normal distribution.\\nThe parameter µ gives the coordinate of the central peak. This is also the mean of\\nthe distribution: E[x] = µ. The standard deviation of the distribution is given by\\nσ, and the variance by σ2.\\n\\nWhen we evaluate the PDF, we need to square and invert σ. When we need to\\nfrequently evaluate the PDF with diﬀerent parameter values, a more eﬃcient way\\nof parametrizing the distribution is to use a parameter β ∈ (0 ,∞) to control the\\nprecision or inverse variance of the distribution:\\n\\nN ( ;x µ, β−1) =\\ue072 β\\n\\n2π\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\nβ x\\n\\n( − )2\\ue013 .\\n\\nµ\\n\\n(3.22)\\n\\nNormal distributions are a sensible choice for many applications. In the absence\\nof prior knowledge about what form a distribution over the real numbers should\\ntake, the normal distribution is a good default choice for two major reasons.\\n\\nFirst, many distributions we wish to model are truly close to being normal\\ndistributions. The central limit theorem shows that the sum of many independent\\nrandom variables is approximately normally distributed. This means that in\\n\\n62\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe normal distribution\\n\\nMaximum at x ¹=\\n\\nInflection points at \\n     x ¹ ¾\\n\\n= §\\n\\n−1.5\\n\\n−1.0\\n\\n−0.5\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n)\\nx\\n(\\np\\n\\n0.40\\n\\n0.35\\n\\n0.30\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.10\\n\\n0.05\\n\\n0.00\\n\\n−2.0\\n\\nFigure 3.1: The normal distribution: The normal distribution N (x;µ, σ 2) exhibits a classic\\n“bell curve” shape, with the x coordinate of its central peak given by µ, and the width\\nof its peak controlled by σ. In this example, we depict the standard normal distribution,\\nwith\\n\\nσ = 1\\n\\nµ = 0\\n\\nand\\n\\n.\\n\\nx\\n\\npractice, many complicated systems can be modeled successfully as normally\\ndistributed noise, even if the system can be decomposed into parts with more\\nstructured behavior.\\n\\nSecond, out of all possible probability distributions with the same variance,\\nthe normal distribution encodes the maximum amount of uncertainty over the\\nreal numbers. We can thus think of the normal distribution as being the one that\\ninserts the least amount of prior knowledge into a model. Fully developing and\\njustifying this idea requires more mathematical tools, and is postponed to Sec.\\n19.4.2.\\n\\nThe normal distribution generalizes to Rn, in which case it is known as the\\nmultivariate normal distribution. It may be parametrized with a positive deﬁnite\\nsymmetric matrix\\n\\n:Σ\\n\\nx µ, Σ \\ue073\\n\\n) =\\n\\nN ( ;\\n\\n1\\n\\n(2 )π ndet(\\n\\n)Σ\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\nx µ− \\ue03eΣ−1 (\\n(\\n\\n)\\n\\nx µ− \\ue013 .\\n\\n)\\n\\n(3.23)\\n\\nThe parameter µ still gives the mean of the distribution, though now it is\\nvector-valued. The parameter Σ gives the covariance matrix of the distribution.\\nAs in the univariate case, when we wish to evaluate the PDF several times for\\n\\n63\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nmany diﬀerent values of the parameters, the covariance is not a computationally\\neﬃcient way to parametrize the distribution, since we need to invert Σ to evaluate\\nthe PDF. We can instead use a precision matrix β:\\n\\nN ( ;x µ β, −1) =\\ue073det( )β\\n\\n(2 )π n\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\n( − )\\ue013 .\\nx µ− \\ue03eβ x µ\\n(\\n\\n)\\n\\n(3.24)\\n\\nWe often ﬁx the covariance matrix to be a diagonal matrix. An even simpler\\nversion is the isotropic Gaussian distribution, whose covariance matrix is a scalar\\ntimes the identity matrix.\\n\\n3.9.4 Exponential and Laplace Distributions\\n\\nIn the context of deep learning, we often want to have a probability distribution\\nwith a sharp point at x = 0. To accomplish this, we can use the exponential\\ndistribution:\\n\\n(3.25)\\nThe exponential distribution uses the indicator function 1x≥0 to assign probability\\nzero to all negative values of\\n\\np x λ\\n( ; ) =  1x≥0exp (\\n\\n)−λx .\\n\\n.x\\n\\nλ\\n\\nA closely related probability distribution that allows us to place a sharp peak\\n\\nof probability mass at an arbitrary point\\n\\nµ\\n\\nis the\\n\\nLaplace distribution\\n\\nLaplace( ;\\n\\nx µ, γ\\n\\n) =\\n\\n1\\n2γ\\n\\nexp\\ue012−| − |\\nγ \\ue013.\\n\\nµ\\n\\nx\\n\\n(3.26)\\n\\n3.9.5 The Dirac Distribution and Empirical Distribution\\n\\nIn some cases, we wish to specify that all of the mass in a probability distribution\\nclusters around a single point. This can be accomplished by deﬁning a PDF using\\nthe Dirac delta function,\\n\\nδ x( )\\n\\n:\\n\\n( ) =  ( − )\\np x\\nµ .\\n\\nδ x\\n\\n(3.27)\\n\\nThe Dirac delta function is deﬁned such that it is zero-valued everywhere except\\n0, yet integrates to 1. The Dirac delta function is not an ordinary function that\\nassociates each value x with a real-valued output, instead it is a diﬀerent kind of\\nmathematical object called a generalized function that is deﬁned in terms of its\\nproperties when integrated. We can think of the Dirac delta function as being the\\nlimit point of a series of functions that put less and less mass on all points other\\nthan .µ\\n\\n64\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nBy deﬁning p(x) to be δ shifted by −µ we obtain an inﬁnitely narrow and\\n\\ninﬁnitely high peak of probability mass where\\n\\nx\\n\\nµ= \\n\\n.\\n\\nA common use of the Dirac delta distribution is as a component of an empirical\\n\\ndistribution,\\n\\nˆp( ) =x\\n\\n1\\nm\\n\\nm\\ue058i=1\\n\\nδ(x x− ( )i )\\n\\n(3.28)\\n\\n1\\nm on each of the m points x(1), . . . , x (\\n\\n)m forming\\nwhich puts probability mass\\na given data set or collection of samples. The Dirac delta distribution is only\\nnecessary to deﬁne the empirical distribution over continuous variables. For discrete\\nvariables, the situation is simpler: an empirical distribution can be conceptualized\\nas a multinoulli distribution, with a probability associated to each possible input\\nvalue that is simply equal to the empirical frequency of that value in the training\\nset.\\n\\nWe can view the empirical distribution formed from a dataset of training\\nexamples as specifying the distribution that we sample from when we train a model\\non this dataset. Another important perspective on the empirical distribution is\\nthat it is the probability density that maximizes the likelihood of the training\\ndata (see Sec. 5.5). Many machine learning algorithms can be conﬁgured to have\\narbitrarily high capacity. If given enough capacity, these algorithms will simply\\nlearn the empirical distribution. This is a bad outcome because the model does not\\ngeneralize at all and assigns inﬁnitesimal probability to any point in space that did\\nnot occur in the training set. A central problem in machine learning is studying\\nhow to limit the capacity of a model in a way that prevents it from simply learning\\nthe empirical distribution while also allowing it to learn complicated functions.\\n\\n3.9.6 Mixtures of Distributions\\n\\nIt is also common to deﬁne probability distributions by combining other simpler\\nprobability distributions. One common way of combining distributions is to\\nconstruct a mixture distribution. A mixture distribution is made up of several\\ncomponent distributions. On each trial, the choice of which component distribution\\ngenerates the sample is determined by sampling a component identity from a\\nmultinoulli distribution:\\n\\nP ( ) =x \\ue058i\\n\\nP\\n\\nc\\n( = \\n\\ni P\\n)\\n\\nx c|\\n(\\n\\n=\\n\\ni\\n)\\n\\n(3.29)\\n\\nwhere\\n\\nP ( )\\n\\nc is the multinoulli distribution over component identities.\\n\\n65\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nWe have already seen one example of a mixture distribution: the empirical\\ndistribution over real-valued variables is a mixture distribution with one Dirac\\ncomponent for each training example.\\n\\nThe mixture model is one simple strategy for combining probability distributions\\nto create a richer distribution. In Chapter 16, we explore the art of building complex\\nprobability distributions from simple ones in more detail.\\n\\nlatent variable\\n\\nThe mixture model allows us to brieﬂy glimpse a concept that will be of\\nparamount importance later—the\\n. A latent variable is a random\\nvariable that we cannot observe directly. The component identity variable c of the\\nmixture model provides an example. Latent variables may be related to x through\\nthe joint distribution, in this case, P (x c, ) = P (x c|\\n)P(c). The distribution P (c)\\nover the latent variable and the distribution P (x c| ) relating the latent variables\\nto the visible variables determines the shape of the distribution P ( x) even though\\nit is possible to describe P (x) without reference to the latent variable. Latent\\nvariables are discussed further in Sec. 16.5.\\n\\nA very powerful and common type of mixture model is the Gaussian mixture\\nmodel, in which the components p (x | c = i) are Gaussians. Each component has\\na separately parametrized mean µ ( )i and covariance Σ ( )i . Some mixtures can have\\nmore constraints. For example, the covariances could be shared across components\\nvia the constraint Σ( )i = Σ∀i. As with a single Gaussian distribution, the mixture\\nof Gaussians might constrain the covariance matrix for each component to be\\ndiagonal or isotropic.\\n\\nIn addition to the means and covariances, the parameters of a Gaussian mixture\\nspecify the prior probability α i = P (c = i) given to each component i. The word\\n“prior” indicates that it expresses the model’s beliefs about c before it has observed\\nx. By comparison, P(c | x) is a posterior probability, because it is computed after\\nobservation of x. A Gaussian mixture model is a universal approximator of\\ndensities, in the sense that any smooth density can be approximated with any\\nspeciﬁc, non-zero amount of error by a Gaussian mixture model with enough\\ncomponents.\\n\\nFig. 3.2 shows samples from a Gaussian mixture model.\\n\\n3.10 Useful Properties of Common Functions\\n\\nCertain functions arise often while working with probability distributions, especially\\nthe probability distributions used in deep learning models.\\n\\n66\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n2\\nx\\n\\nx1\\n\\nFigure 3.2: Samples from a Gaussian mixture model. In this example, there are three\\ncomponents. From left to right, the ﬁrst component has an isotropic covariance matrix,\\nmeaning it has the same amount of variance in each direction. The second has a diagonal\\ncovariane matrix, meaning it can control the variance separately along each axis-aligned\\ndirection. This example has more variance along the x2 axis than along the x1 axis. The\\nthird component has a full-rank covariance matrix, allowing it to control the variance\\nseparately along an abitrary basis of directions.\\n\\nOne of these functions is the logistic sigmoid:\\n\\nσ x( ) =\\n\\n1\\n\\n1 + exp(\\n\\n.\\n)−x\\n\\n(3.30)\\n\\nThe logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli\\ndistribution because its range is (0, 1), which lies within the valid range of values\\nfor the φ parameter. See Fig. 3.3 for a graph of the sigmoid function. The sigmoid\\nfunction saturates when its argument is very positive or very negative, meaning\\nthat the function becomes very ﬂat and insensitive to small changes in its input.\\n\\nAnother commonly encountered function is the\\n\\nsoftplus\\n\\nfunction (Dugas\\n\\net al.,\\n\\n2001):\\n\\nζ x\\nx .\\n( ) = log (1 + exp( ))\\n\\n(3.31)\\n\\nThe softplus function can be useful for producing the β or σ parameter of a normal\\ndistribution because its range is (0,∞). It also arises commonly when manipulating\\nexpressions involving sigmoids. The name of the softplus function comes from the\\nfact that it is a smoothed or “softened” version of\\n\\nx+ = max(0 ), x .\\n\\n(3.32)\\n\\nSee Fig. 3.4 for a graph of the softplus function.\\n\\n67\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe logistic sigmoid function\\n\\n)\\nx\\n(\\n¾\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n−10\\n\\n−5\\n\\n0\\nx\\n\\n5\\n\\n10\\n\\nFigure 3.3: The logistic sigmoid function.\\n\\nThe softplus function\\n\\n10\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n)\\nx\\n(\\n³\\n\\n0\\n−10\\n\\n−5\\n\\n0\\n\\nx\\n\\n5\\n\\n10\\n\\nFigure 3.4: The softplus function.\\n\\n68\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe following properties are all useful enough that you may wish to memorize\\n\\nthem:\\n\\nσ x( ) =\\n\\nexp( )x\\nx\\n\\nexp( ) + exp(0)\\n\\nd\\ndx\\n\\nσ x\\n\\n( ) =  (\\n\\n( ) =  ( )(1 − ( ))\\nσ x\\nσ x\\n− σ x\\n1\\nσ x\\n\\nσ −x\\n)\\n−ζ −x\\n(\\nζ x\\nσ x\\n( ) =  ( )\\n\\nlog ( ) = \\n\\n)\\n\\nd\\ndx\\n\\n1)\\n\\n(0,\\n\\n, σ\\n\\n\\ue012 x\\n1 − x\\ue013\\n∀ ∈x\\n∀x > 0, ζ−1( ) = log (exp( )\\nx −\\n\\n−1 ( ) = log\\n\\n1)\\n\\nx\\n\\nx\\n\\nζ x( ) =\\ue05a x\\n\\n−∞\\nx\\nζ\\n\\nσ y dy\\n\\n( )\\n\\n(3.33)\\n\\n(3.34)\\n\\n(3.35)\\n\\n(3.36)\\n\\n(3.37)\\n\\n(3.38)\\n\\n(3.39)\\n\\n(3.40)\\n\\n(3.41)\\nThe function σ−1(x) is called the logit in statistics, but this term is more rarely\\nused in machine learning. The ﬁnal property provides extra justiﬁcation for the\\nname “softplus,” since x+ − x− = x.\\n\\n( ) − (− ) = \\nζ x\\nx\\n\\n3.11 Bayes’ Rule\\n\\nWe often ﬁnd ourselves in a situation where we know P (y x| ) and need to know\\nP (x y|\\n). Fortunately, if we also know P (x), we can compute the desired quantity\\nusing Bayes’ rule:\\n\\nP (\\n\\nx y|\\n\\n) =\\n\\nP\\n\\nP( )x\\n\\ny x|\\n(\\n\\n)\\n\\n.\\n\\n(3.42)\\n\\nP ( )y\\n\\nP ( ) =y \\ue050x P\\n\\n(y |\\n\\nNote that while P (y) appears in the formula, it is usually feasible to compute\\n\\nx P x\\n\\n( ), so we do not need to begin with knowledge of\\n\\n)\\n\\nP\\n\\n(y .)\\n\\nBayes’ rule is straightforward to derive from the deﬁnition of conditional\\nprobability, but it is useful to know the name of this formula since many texts\\nrefer to it by name. It is named after the Reverend Thomas Bayes, who ﬁrst\\ndiscovered a special case of the formula. The general version presented here was\\nindependently discovered by Pierre-Simon Laplace.\\n\\n69\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.12 Technical Details of Continuous Variables\\n\\nA proper formal understanding of continuous random variables and probability\\ndensity functions requires developing probability theory in terms of a branch of\\nmathematics known as measure theory. Measure theory is beyond the scope of\\nthis textbook, but we can brieﬂy sketch some of the issues that measure theory is\\nemployed to resolve.\\n\\nIn Sec. 3.3.2, we saw that the probability of a continuous vector-valued x lying\\nin some set S is given by the integral of p(x ) over the set S. Some choices of set S\\ncan produce paradoxes. For example, it is possible to construct two sets S1 and\\nS2 such that p(x ∈ S1) + p(x ∈ S 2) > 1 but S1 ∩ S2 = ∅. These sets are generally\\nconstructed making very heavy use of the inﬁnite precision of real numbers, for\\nexample by making fractal-shaped sets or sets that are deﬁned by transforming\\nthe set of rational numbers2 . One of the key contributions of measure theory is to\\nprovide a characterization of the set of sets that we can compute the probability\\nof without encountering paradoxes. In this book, we only integrate over sets with\\nrelatively simple descriptions, so this aspect of measure theory never becomes a\\nrelevant concern.\\n\\nFor our purposes, measure theory is more useful for describing theorems that\\napply to most points in Rn but do not apply to some corner cases. Measure theory\\nprovides a rigorous way of describing that a set of points is negligibly small. Such\\na set is said to have “measure zero.” We do not formally deﬁne this concept in this\\ntextbook. However, it is useful to understand the intuition that a set of measure\\nzero occupies no volume in the space we are measuring. For example, within R2 , a\\nline has measure zero, while a ﬁlled polygon has positive measure. Likewise, an\\nindividual point has measure zero. Any union of countably many sets that each\\nhave measure zero also has measure zero (so the set of all the rational numbers\\nhas measure zero, for instance).\\n\\nAnother useful term from measure theory is “almost everywhere.” A property\\nthat holds almost everywhere holds throughout all of space except for on a set of\\nmeasure zero. Because the exceptions occupy a negligible amount of space, they\\ncan be safely ignored for many applications. Some important results in probability\\ntheory hold for all discrete values but only hold “almost everywhere” for continuous\\nvalues.\\n\\nAnother technical detail of continuous variables relates to handling continuous\\nrandom variables that are deterministic functions of one another. Suppose we have\\ntwo random variables, x and y, such that y = g(x), where g is an invertible, con-\\n\\n2The Banach-Tarski theorem provides a fun example of such sets.\\n\\n70\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\ntinuous, diﬀerentiable transformation. One might expect that py(y ) =p x(g−1(y )).\\nThis is actually not the case.\\n\\nAs a simple example, suppose we have scalar random variables x and y. Suppose\\nIf we use the rule p y(y) = p x(2 y) then py will be 0\\non this interval. This means\\n\\ny = x\\neverywhere except the interval [0 , 1\\n2 ]\\n\\n2 and x ∼ U(0,1).\\n\\n, and it will be\\n\\n1\\n\\n\\ue05a py ( ) =\\n\\ny dy\\n\\n1\\n2\\n\\n,\\n\\n(3.43)\\n\\nwhich violates the deﬁnition of a probability distribution.\\n\\nThis common mistake is wrong because it fails to account for the distortion\\nof space introduced by the function g. Recall that the probability of x lying in\\nan inﬁnitesimally small region with volume δx is given by p( x)δx. Since g can\\nexpand or contract space, the inﬁnitesimal volume surrounding x in x space may\\nhave diﬀerent volume in\\n\\nspace.\\n\\ny\\n\\nTo see how to correct the problem, we return to the scalar case. We need to\\n\\npreserve the property\\n\\nSolving from this, we obtain\\n\\n|py( ( ))\\n\\ng x dy|\\n\\n=\\n\\n|p x( )x dx .|\\n\\nor equivalently\\n\\nIn higher dimensions, the derivative generalizes to the determinant of the Jacobian\\nmatrix—the matrix with J i,j = ∂xi\\n∂yj\\n\\n. Thus, for real-valued vectors\\n\\nand ,\\ny\\n\\nx\\n\\npy( ) = \\n\\ny\\n\\npx ( ) = \\n\\nx\\n\\n∂x\\n\\n∂g x( )\\n\\npy( ( ))\\n\\npx (g−1( ))y\\n\\n\\ue00c\\ue00c\\ue00c\\ue00c\\n∂y\\ue00c\\ue00c\\ue00c\\ue00c\\ng x \\ue00c\\ue00c\\ue00c\\ue00c\\n∂x \\ue00c\\ue00c\\ue00c\\ue00c .\\ng x \\ue00c\\ue00c\\ue00c\\ue00cdet\\ue012∂g( )x\\n∂x \\ue013\\ue00c\\ue00c\\ue00c\\ue00c .\\n\\npx ( ) = \\n\\nx\\n\\np y( ( ))\\n\\n3.13 Information Theory\\n\\nInformation theory is a branch of applied mathematics that revolves around\\nquantifying how much information is present in a signal. It was originally invented\\nto study sending messages from discrete alphabets over a noisy channel, such as\\ncommunication via radio transmission. In this context, information theory tells how\\nto design optimal codes and calculate the expected length of messages sampled from\\n\\n71\\n\\n(3.44)\\n\\n(3.45)\\n\\n(3.46)\\n\\n(3.47)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nspeciﬁc probability distributions using various encoding schemes. In the context of\\nmachine learning, we can also apply information theory to continuous variables\\nwhere some of these message length interpretations do not apply. This ﬁeld is\\nfundamental to many areas of electrical engineering and computer science. In this\\ntextbook, we mostly use a few key ideas from information theory to characterize\\nprobability distributions or quantify similarity between probability distributions.\\nFor more detail on information theory, see Cover and Thomas (2006) or MacKay\\n(2003).\\n\\nThe basic intuition behind information theory is that learning that an unlikely\\nevent has occurred is more informative than learning that a likely event has\\noccurred. A message saying “the sun rose this morning” is so uninformative as\\nto be unnecessary to send, but a message saying “there was a solar eclipse this\\nmorning” is very informative.\\n\\nWe would like to quantify information in a way that formalizes this intuition.\\n\\nSpeciﬁcally,\\n\\n• Likely events should have low information content, and in the extreme case,\\nevents that are guaranteed to happen should have no information content\\nwhatsoever.\\n\\n• Less likely events should have higher information content.\\n• Independent events should have additive information. For example, ﬁnding\\nout that a tossed coin has come up as heads twice should convey twice as\\nmuch information as ﬁnding out that a tossed coin has come up as heads\\nonce.\\n\\nIn order to satisfy all three of these properties, we deﬁne the self-information\\n\\nof an event x\\n\\n= x\\n\\nto be\\n\\nI x\\n( ) = \\n\\nlog−\\n\\nP x .\\n( )\\n\\n(3.48)\\n\\nIn this book, we always use log to mean the natural logarithm, with base e. Our\\ndeﬁnition of I(x) is therefore written in units of\\n. One nat is the amount of\\ninformation gained by observing an event of probability 1\\ne . Other texts use base-2\\nlogarithms and units called\\nshannons\\n; information measured in bits is just\\na rescaling of information measured in nats.\\n\\nnats\\n\\nbits\\n\\nor\\n\\nWhen x is continuous, we use the same deﬁnition of information by analogy,\\nbut some of the properties from the discrete case are lost. For example, an event\\nwith unit density still has zero information, despite not being an event that is\\nguaranteed to occur.\\n\\n72\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n0.4\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\ns\\nt\\na\\nn\\n \\nn\\n\\ni\\n \\ny\\np\\no\\nr\\nt\\nn\\ne\\n \\nn\\no\\nn\\nn\\na\\nh\\nS\\n\\n0.0\\n\\n0.0\\n\\nShannon entropy of a binary random variable\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\np\\n\\n1\\n\\nFigure 3.5: This plot shows how distributions that are closer to deterministic have low\\nShannon entropy while distributions that are close to uniform have high Shannon entropy.\\nOn the horizontal axis, we plot p, the probability of a binary random variable being equal\\nto . The entropy is given by\\nlog . When p is near 0, the distribution\\nis nearly deterministic, because the random variable is nearly always 0. When p is near 1,\\nthe distribution is nearly deterministic, because the random variable is nearly always 1.\\nWhen p = 0 .5, the entropy is maximal, because the distribution is uniform over the two\\noutcomes.\\n\\n(p− 1) log(1− p )− p\\n\\np\\n\\nSelf-information deals only with a single outcome. We can quantify the amount\\n\\nof uncertainty in an entire probability distribution using the Shannon entropy:\\n\\nH( ) = \\n\\nx\\n\\nE\\n\\nI x\\n\\nx∼P[ ( )] = \\n\\n−E\\n\\nP x .\\nx∼P[log ( )]\\n\\n(3.49)\\n\\nalso denoted H( P). In other words, the Shannon entropy of a distribution is the\\nexpected amount of information in an event drawn from that distribution. It gives\\na lower bound on the number of bits (if the logarithm is base 2, otherwise the units\\nare diﬀerent) needed on average to encode symbols drawn from a distribution P.\\nDistributions that are nearly deterministic (where the outcome is nearly certain)\\nhave low entropy; distributions that are closer to uniform have high entropy. See\\nFig. 3.5 for a demonstration. When x is continous, the Shannon entropy is known\\nas the diﬀerential entropy.\\n\\nIf we have two separate probability distributions P(x) and Q(x) over the same\\nrandom variable x, we can measure how diﬀerent these two distributions are using\\nthe Kullback-Leibler (KL) divergence:\\n\\nDKL(\\n\\nP Q\\ue06b\\n\\n) = \\n\\nE x∼P\\ue014log\\n\\nP x( )\\n\\nQ x( )\\ue015 = Ex∼P [log ( )\\n\\nP x −\\n\\n73\\n\\nlog ( )]\\nQ x .\\n\\n(3.50)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nIn the case of discrete variables, it is the extra amount of information (measured\\nin bits if we use the base\\nlogarithm, but in machine learning we usually use nats\\nand the natural logarithm) needed to send a message containing symbols drawn\\nfrom probability distribution P , when we use a code that was designed to minimize\\nthe length of messages drawn from probability distribution\\n\\n.Q\\n\\n2\\n\\nThe KL divergence has many useful properties, most notably that it is non-\\nnegative. The KL divergence is 0 if and only if P and Qare the same distribution in\\nthe case of discrete variables, or equal “almost everywhere” in the case of continuous\\nvariables. Because the KL divergence is non-negative and measures the diﬀerence\\nbetween two distributions, it is often conceptualized as measuring some sort of\\ndistance between these distributions. However, it is not a true distance measure\\nbecause it is not symmetric: DKL(P Q\\ue06b ) \\ue036= DKL( Q P\\ue06b ) for some P and Q. This\\nasymmetry means that there are important consequences to the choice of whether\\nto use DKL(\\n\\n. See Fig. 3.6 for more detail.\\n\\nor DKL (\\n\\n)\\n\\nP Q\\ue06b\\n\\n)Q P\\ue06b\\n\\nA quantity that is closely related to the KL divergence is the cross-entropy\\nH (P, Q ) = H (P) + DKL(P Q\\ue06b ), which is similar to the KL divergence but lacking\\nthe term on the left:\\n(3.51)\\n\\nH P, Q(\\n\\n) = −E\\n\\nx∼P log ( )Q x .\\n\\nMinimizing the cross-entropy with respect to Q is equivalent to minimizing the\\nKL divergence, because\\n\\ndoes not participate in the omitted term.\\n\\nQ\\n\\nWhen computing many of these quantities, it is common to encounter expres-\\nsions of the form 0log 0. By convention, in the context of information theory, we\\ntreat these expressions as limx→0 x\\n\\nlog = 0.\\n\\nx\\n\\n3.14 Structured Probabilistic Models\\n\\nMachine learning algorithms often involve probability distributions over a very\\nlarge number of random variables. Often, these probability distributions involve\\ndirect interactions between relatively few variables. Using a single function to\\ndescribe the entire joint probability distribution can be very ineﬃcient (both\\ncomputationally and statistically).\\n\\nInstead of using a single function to represent a probability distribution, we\\ncan split a probability distribution into many factors that we multiply together.\\nFor example, suppose we have three random variables: a, b and c . Suppose that\\na inﬂuences the value of b and b inﬂuences the value of c, but that a and c are\\nindependent given b. We can represent the probability distribution over all three\\n\\n74\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nq∗ = argminq DKL(\\n\\n)p q\\ue06b\\n\\nq ∗ = argminq DKL (\\n\\nq p\\ue06b\\n\\n)\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np x( )\\nq∗( )x\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np( )x\\nq ∗( )x\\n\\nx\\n\\nx\\n\\nFigure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x ) and\\nwish to approximate it with another distribution q(x). We have the choice of minimizing\\neither DKL( p q\\ue06b ) or DKL( q p\\ue06b ). We illustrate the eﬀect of this choice using a mixture of\\ntwo Gaussians for p, and a single Gaussian for q. The choice of which direction of the\\nKL divergence to use is problem-dependent. Some applications require an approximation\\nthat usually places high probability anywhere that the true distribution places high\\nprobability, while other applications require an approximation that rarely places high\\nprobability anywhere that the true distribution places low probability. The choice of the\\ndirection of the KL divergence reﬂects which of these considerations takes priority for each\\napplication. (Left) The eﬀect of minimizing DKL (p q\\ue06b ). In this case, we select a q that has\\nhigh probability where p has high probability. When p has multiple modes, q chooses to\\nblur the modes together, in order to put high probability mass on all of them. (Right) The\\neﬀect of minimizing DKL (q p\\ue06b ). In this case, we select a q that has low probability where\\np has low probability. When p has multiple modes that are suﬃciently widely separated,\\nas in this ﬁgure, the KL divergence is minimized by choosing a single mode, in order to\\navoid putting probability mass in the low-probability areas between modes of p . Here, we\\nillustrate the outcome when q is chosen to emphasize the left mode. We could also have\\nachieved an equal value of the KL divergence by choosing the right mode. If the modes\\nare not separated by a suﬃciently strong low probability region, then this direction of the\\nKL divergence can still choose to blur the modes.\\n\\n75\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nvariables as a product of probability distributions over two variables:\\n\\np ,\\n\\n(a b c) =  ( )a (\\np\\n\\np\\n\\n,\\n\\nb a|\\n\\n)\\n\\nc b|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.52)\\n\\nThese factorizations can greatly reduce the number of parameters needed\\nto describe the distribution. Each factor uses a number of parameters that is\\nexponential in the number of variables in the factor. This means that we can greatly\\nreduce the cost of representing a distribution if we are able to ﬁnd a factorization\\ninto distributions over fewer variables.\\n\\nWe can describe these kinds of factorizations using graphs. Here we use the\\nword “graph” in the sense of graph theory: a set of vertices that may be connected\\nto each other with edges. When we represent the factorization of a probability\\ndistribution with a graph, we call it a structured probabilistic model\\ngraphical\\nmodel.\\n\\nor\\n\\nThere are two main kinds of structured probabilistic models: directed and\\nundirected. Both kinds of graphical models use a graph G in which each node\\nin the graph corresponds to a random variable, and an edge connecting two\\nrandom variables means that the probability distribution is able to represent direct\\ninteractions between those two random variables.\\n\\nDirected models use graphs with directed edges, and they represent factoriza-\\ntions into conditional probability distributions, as in the example above. Speciﬁcally,\\na directed model contains one factor for every random variable x i in the distribution,\\nand that factor consists of the conditional distribution over xi given the parents of\\nxi, denoted P a G(x i):\\n\\n(3.53)\\n\\np( ) =x \\ue059i\\n\\np (xi | P aG (xi )) .\\n\\nSee Fig. 3.7 for an example of a directed graph and the factorization of probability\\ndistributions it represents.\\n\\nUndirected models use graphs with undirected edges, and they represent fac-\\ntorizations into a set of functions; unlike in the directed case, these functions are\\nusually not probability distributions of any kind. Any set of nodes that are all\\nconnected to each other in G is called a clique. Each clique C ( )i\\nin an undirected\\nmodel is associated with a factor φ( )i (C ( )i ). These factors are just functions, not\\nprobability distributions. The output of each factor must be non-negative, but\\nthere is no constraint that the factor must sum or integrate to 1 like a probability\\ndistribution.\\n\\nThe probability of a conﬁguration of random variables is proportional to the\\nproduct of all of these factors—assignments that result in larger factor values are\\n\\n76\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.7: A directed graphical model over random variables a , b, c, d and e. This graph\\ncorresponds to probability distributions that can be factored as\\n\\n(a b c d e) =  ( )a (\\np ,\\np\\n\\np\\n\\n,\\n\\n,\\n\\n,\\n\\nb a|\\n\\n)\\n\\n(c a|\\np\\n\\n,\\n\\nb) (\\np\\n\\nd b|\\n\\n)\\n\\ne c|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.54)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nmore likely. Of course, there is no guarantee that this product will sum to 1. We\\ntherefore divide by a normalizing constant Z, deﬁned to be the sum or integral\\nover all states of the product of the φ functions, in order to obtain a normalized\\nprobability distribution:\\n\\np( ) =x\\n\\n1\\n\\nZ \\ue059i\\n\\nφ( )i \\ue010C( )i\\ue011 .\\n\\n(3.55)\\n\\nSee Fig. 3.8 for an example of an undirected graph and the factorization of\\nprobability distributions it represents.\\n\\nKeep in mind that these graphical representations of factorizations are a\\nlanguage for describing probability distributions. They are not mutually exclusive\\nfamilies of probability distributions. Being directed or undirected is not a property\\nof a probability distribution; it is a property of a particular\\nof a\\nprobability distribution, but any probability distribution may be described in both\\nways.\\n\\ndescription\\n\\nThroughout Part I and Part II of this book, we will use structured probabilistic\\nmodels merely as a language to describe which direct probabilistic relationships\\ndiﬀerent machine learning algorithms choose to represent. No further understanding\\nof structured probabilistic models is needed until the discussion of research topics,\\nin Part III, where we will explore structured probabilistic models in much greater\\ndetail.\\n\\n77\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.8: An undirected graphical model over random variables a, b, c, d and e . This\\ngraph corresponds to probability distributions that can be factored as\\n\\n(a b c d e) =\\np ,\\n\\n,\\n\\n,\\n\\n,\\n\\n1\\nZ\\n\\nφ (1)(\\n\\na b c\\n,\\n\\n, φ (2) (\\n\\n)\\n\\n)b d, φ(3)(\\n\\n)c e,\\n.\\n\\n(3.56)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nThis chapter has reviewed the basic concepts of probability theory that are\\nmost relevant to deep learning. One more set of fundamental mathematical tools\\nremains: numerical methods.\\n\\n78\\n\\n\\x0c'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'([\\n]{2,2}(\\d\\.)+\\d* [A-Za-z0-9? ]+[\\n]{1,2})', re.UNICODE)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p = re.compile('[\\n]{2,2}(\\d.){1,} [A-Za-z0-9][\\n]{2,2}')\n",
    "#p = re.compile('[a-z]*')\n",
    "\n",
    "#p = re.compile('[\\n]{2,2}[\\d|.]+ [A-Za-z0-9? ]+[\\n]{1,2}')\n",
    "p = re.compile(r'([\\n]{2,2}(\\d\\.)+\\d* [A-Za-z0-9? ]+[\\n]{1,2})')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n3.1 Why Probability?\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.match(string_to_match).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\n3.1 Why Probability?\\n\\n', '3.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.search(cry).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_all = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_all_iter = p.finditer(cry)\n",
    "start_end_sections_titles = [m.span() for m in cry_all_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_all = p.findall(apriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1677, 1701),\n",
       " (7804, 7828),\n",
       " (8773, 8806),\n",
       " (9058, 9117),\n",
       " (11612, 11676),\n",
       " (13399, 13427),\n",
       " (14373, 14404),\n",
       " (15429, 15480),\n",
       " (16105, 16154),\n",
       " (20560, 20600),\n",
       " (20689, 20721),\n",
       " (21118, 21152),\n",
       " (22776, 22807),\n",
       " (26294, 26341),\n",
       " (26895, 26954),\n",
       " (29422, 29457),\n",
       " (32308, 32354),\n",
       " (35989, 36039),\n",
       " (40066, 40093),\n",
       " (46553, 46593)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end_sections_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_section = 0\n",
    "section_text = []\n",
    "for range_ in start_end_sections_titles:\n",
    "    section_text.append(cry[start_section:range_[0]])\n",
    "    start_section = range_[1]\n",
    "    \n",
    "section_text.append(cry[start_section:])\n",
    "section_text = section_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning algorithms often involve probability distributions over a very\\nlarge number of random variables. Often, these probability distributions involve\\ndirect interactions between relatively few variables. Using a single function to\\ndescribe the entire joint probability distribution can be very ineﬃcient (both\\ncomputationally and statistically).\\n\\nInstead of using a single function to represent a probability distribution, we\\ncan split a probability distribution into many factors that we multiply together.\\nFor example, suppose we have three random variables: a, b and c . Suppose that\\na inﬂuences the value of b and b inﬂuences the value of c, but that a and c are\\nindependent given b. We can represent the probability distribution over all three\\n\\n74\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nq∗ = argminq DKL(\\n\\n)p q\\ue06b\\n\\nq ∗ = argminq DKL (\\n\\nq p\\ue06b\\n\\n)\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np x( )\\nq∗( )x\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np( )x\\nq ∗( )x\\n\\nx\\n\\nx\\n\\nFigure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x ) and\\nwish to approximate it with another distribution q(x). We have the choice of minimizing\\neither DKL( p q\\ue06b ) or DKL( q p\\ue06b ). We illustrate the eﬀect of this choice using a mixture of\\ntwo Gaussians for p, and a single Gaussian for q. The choice of which direction of the\\nKL divergence to use is problem-dependent. Some applications require an approximation\\nthat usually places high probability anywhere that the true distribution places high\\nprobability, while other applications require an approximation that rarely places high\\nprobability anywhere that the true distribution places low probability. The choice of the\\ndirection of the KL divergence reﬂects which of these considerations takes priority for each\\napplication. (Left) The eﬀect of minimizing DKL (p q\\ue06b ). In this case, we select a q that has\\nhigh probability where p has high probability. When p has multiple modes, q chooses to\\nblur the modes together, in order to put high probability mass on all of them. (Right) The\\neﬀect of minimizing DKL (q p\\ue06b ). In this case, we select a q that has low probability where\\np has low probability. When p has multiple modes that are suﬃciently widely separated,\\nas in this ﬁgure, the KL divergence is minimized by choosing a single mode, in order to\\navoid putting probability mass in the low-probability areas between modes of p . Here, we\\nillustrate the outcome when q is chosen to emphasize the left mode. We could also have\\nachieved an equal value of the KL divergence by choosing the right mode. If the modes\\nare not separated by a suﬃciently strong low probability region, then this direction of the\\nKL divergence can still choose to blur the modes.\\n\\n75\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nvariables as a product of probability distributions over two variables:\\n\\np ,\\n\\n(a b c) =  ( )a (\\np\\n\\np\\n\\n,\\n\\nb a|\\n\\n)\\n\\nc b|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.52)\\n\\nThese factorizations can greatly reduce the number of parameters needed\\nto describe the distribution. Each factor uses a number of parameters that is\\nexponential in the number of variables in the factor. This means that we can greatly\\nreduce the cost of representing a distribution if we are able to ﬁnd a factorization\\ninto distributions over fewer variables.\\n\\nWe can describe these kinds of factorizations using graphs. Here we use the\\nword “graph” in the sense of graph theory: a set of vertices that may be connected\\nto each other with edges. When we represent the factorization of a probability\\ndistribution with a graph, we call it a structured probabilistic model\\ngraphical\\nmodel.\\n\\nor\\n\\nThere are two main kinds of structured probabilistic models: directed and\\nundirected. Both kinds of graphical models use a graph G in which each node\\nin the graph corresponds to a random variable, and an edge connecting two\\nrandom variables means that the probability distribution is able to represent direct\\ninteractions between those two random variables.\\n\\nDirected models use graphs with directed edges, and they represent factoriza-\\ntions into conditional probability distributions, as in the example above. Speciﬁcally,\\na directed model contains one factor for every random variable x i in the distribution,\\nand that factor consists of the conditional distribution over xi given the parents of\\nxi, denoted P a G(x i):\\n\\n(3.53)\\n\\np( ) =x \\ue059i\\n\\np (xi | P aG (xi )) .\\n\\nSee Fig. 3.7 for an example of a directed graph and the factorization of probability\\ndistributions it represents.\\n\\nUndirected models use graphs with undirected edges, and they represent fac-\\ntorizations into a set of functions; unlike in the directed case, these functions are\\nusually not probability distributions of any kind. Any set of nodes that are all\\nconnected to each other in G is called a clique. Each clique C ( )i\\nin an undirected\\nmodel is associated with a factor φ( )i (C ( )i ). These factors are just functions, not\\nprobability distributions. The output of each factor must be non-negative, but\\nthere is no constraint that the factor must sum or integrate to 1 like a probability\\ndistribution.\\n\\nThe probability of a conﬁguration of random variables is proportional to the\\nproduct of all of these factors—assignments that result in larger factor values are\\n\\n76\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.7: A directed graphical model over random variables a , b, c, d and e. This graph\\ncorresponds to probability distributions that can be factored as\\n\\n(a b c d e) =  ( )a (\\np ,\\np\\n\\np\\n\\n,\\n\\n,\\n\\n,\\n\\nb a|\\n\\n)\\n\\n(c a|\\np\\n\\n,\\n\\nb) (\\np\\n\\nd b|\\n\\n)\\n\\ne c|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.54)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nmore likely. Of course, there is no guarantee that this product will sum to 1. We\\ntherefore divide by a normalizing constant Z, deﬁned to be the sum or integral\\nover all states of the product of the φ functions, in order to obtain a normalized\\nprobability distribution:\\n\\np( ) =x\\n\\n1\\n\\nZ \\ue059i\\n\\nφ( )i \\ue010C( )i\\ue011 .\\n\\n(3.55)\\n\\nSee Fig. 3.8 for an example of an undirected graph and the factorization of\\nprobability distributions it represents.\\n\\nKeep in mind that these graphical representations of factorizations are a\\nlanguage for describing probability distributions. They are not mutually exclusive\\nfamilies of probability distributions. Being directed or undirected is not a property\\nof a probability distribution; it is a property of a particular\\nof a\\nprobability distribution, but any probability distribution may be described in both\\nways.\\n\\ndescription\\n\\nThroughout Part I and Part II of this book, we will use structured probabilistic\\nmodels merely as a language to describe which direct probabilistic relationships\\ndiﬀerent machine learning algorithms choose to represent. No further understanding\\nof structured probabilistic models is needed until the discussion of research topics,\\nin Part III, where we will explore structured probabilistic models in much greater\\ndetail.\\n\\n77\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.8: An undirected graphical model over random variables a, b, c, d and e . This\\ngraph corresponds to probability distributions that can be factored as\\n\\n(a b c d e) =\\np ,\\n\\n,\\n\\n,\\n\\n,\\n\\n1\\nZ\\n\\nφ (1)(\\n\\na b c\\n,\\n\\n, φ (2) (\\n\\n)\\n\\n)b d, φ(3)(\\n\\n)c e,\\n.\\n\\n(3.56)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nThis chapter has reviewed the basic concepts of probability theory that are\\nmost relevant to deep learning. One more set of fundamental mathematical tools\\nremains: numerical methods.\\n\\n78\\n\\n\\x0c'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_text[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load('en',disable=['parser','ner'])\n",
    "allowed_postags=['NOUN', 'ADJ', 'VERB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sc_text in section_text:\n",
    "    documents = []\n",
    "    doc_tokened = sc_text.split(\".\")\n",
    "    for index,doc in enumerate(doc_tokened):\n",
    "        doc_text_no_punc = simple_preprocess(doc,deacc=True) \n",
    "        tokenized_text_non_stop_words = [ word for word in doc_text_no_punc \\\n",
    "                                         if word not in stop_words]\n",
    "        text_non_stop_words = ' '.join(tokenized_text_non_stop_words)\n",
    "        tokenized_lemmas = nlp(text_non_stop_words)\n",
    "        tokenized_lemmas = [token.lemma_ for token in tokenized_lemmas \\\n",
    "                            if token.pos_ in allowed_postags]\n",
    "        documents.append(tokenized_lemmas)\n",
    "    corpus.append(list(filter(lambda x: len(x) >= 2, documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['many',\n",
       "  'branch',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'deal',\n",
       "  'entity',\n",
       "  'deterministic',\n",
       "  'certain'],\n",
       " ['assume', 'execute', 'machine', 'instruction'],\n",
       " ['error',\n",
       "  'hardware',\n",
       "  'occur',\n",
       "  'rare',\n",
       "  'enough',\n",
       "  'software',\n",
       "  'application',\n",
       "  'need',\n",
       "  'design',\n",
       "  'account'],\n",
       " ['give',\n",
       "  'many',\n",
       "  'computer',\n",
       "  'scientist',\n",
       "  'software',\n",
       "  'engineer',\n",
       "  'work',\n",
       "  'clean',\n",
       "  'certain',\n",
       "  'environment',\n",
       "  'surprising',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'make',\n",
       "  'heavy',\n",
       "  'use',\n",
       "  'probability',\n",
       "  'theory'],\n",
       " ['machine',\n",
       "  'must',\n",
       "  'deal',\n",
       "  'uncertain',\n",
       "  'quantity',\n",
       "  'may',\n",
       "  'need',\n",
       "  'deal',\n",
       "  'stochastic',\n",
       "  'non',\n",
       "  'deterministic',\n",
       "  'quantity'],\n",
       " ['uncertainty', 'stochasticity', 'arise', 'many', 'source'],\n",
       " ['researcher',\n",
       "  'make',\n",
       "  'compelling',\n",
       "  'argument',\n",
       "  'quantify',\n",
       "  'uncertainty',\n",
       "  'use',\n",
       "  'probability',\n",
       "  'least'],\n",
       " ['many', 'argument', 'present', 'summarize', 'inspired', 'pearl'],\n",
       " ['activity', 'require', 'ability', 'reason', 'presence', 'uncertainty'],\n",
       " ['fact',\n",
       "  'mathematical',\n",
       "  'statement',\n",
       "  'true',\n",
       "  'deﬁnition',\n",
       "  'diﬃcult',\n",
       "  'think',\n",
       "  'proposition',\n",
       "  'true',\n",
       "  'event',\n",
       "  'guarantee',\n",
       "  'occur'],\n",
       " ['possible', 'source', 'uncertainty'],\n",
       " ['inherent', 'stochasticity', 'system', 'model'],\n",
       " ['example', 'interpretation', 'mechanic', 'describe', 'particle'],\n",
       " ['create',\n",
       "  'theoretical',\n",
       "  'scenario',\n",
       "  'postulate',\n",
       "  'random',\n",
       "  'dynamic',\n",
       "  'hypothetical',\n",
       "  'card',\n",
       "  'game',\n",
       "  'assume',\n",
       "  'card',\n",
       "  'shuﬄed',\n",
       "  'random',\n",
       "  'order'],\n",
       " ['incomplete', 'observability'],\n",
       " ['deterministic',\n",
       "  'system',\n",
       "  'appear',\n",
       "  'stochastic',\n",
       "  'can',\n",
       "  'observe',\n",
       "  'variable',\n",
       "  'drive',\n",
       "  'behavior',\n",
       "  'system'],\n",
       " ['contestant',\n",
       "  'ask',\n",
       "  'choose',\n",
       "  'door',\n",
       "  'win',\n",
       "  'prize',\n",
       "  'hold',\n",
       "  'choose',\n",
       "  'door'],\n",
       " ['door', 'lead', 'lead', 'car'],\n",
       " ['give',\n",
       "  'contestant',\n",
       "  'choice',\n",
       "  'deterministic',\n",
       "  'contestant',\n",
       "  'point',\n",
       "  'view',\n",
       "  'outcome',\n",
       "  'uncertain'],\n",
       " ['incomplete', 'modeling'],\n",
       " ['use',\n",
       "  'model',\n",
       "  'must',\n",
       "  'discard',\n",
       "  'information',\n",
       "  'observe',\n",
       "  'discard',\n",
       "  'information',\n",
       "  'result',\n",
       "  'uncertainty',\n",
       "  'model',\n",
       "  'prediction'],\n",
       " ['example', 'suppose', 'build', 'robot', 'observe', 'location', 'object'],\n",
       " ['robot',\n",
       "  'discretize',\n",
       "  'space',\n",
       "  'predict',\n",
       "  'future',\n",
       "  'location',\n",
       "  'object',\n",
       "  'discretization',\n",
       "  'make',\n",
       "  'robot',\n",
       "  'become',\n",
       "  'uncertain',\n",
       "  'precise',\n",
       "  'position',\n",
       "  'object',\n",
       "  'object',\n",
       "  'could',\n",
       "  'discrete',\n",
       "  'cell',\n",
       "  'observe',\n",
       "  'occupy'],\n",
       " ['many',\n",
       "  'case',\n",
       "  'practical',\n",
       "  'use',\n",
       "  'simple',\n",
       "  'uncertain',\n",
       "  'rule',\n",
       "  'complex',\n",
       "  'certain',\n",
       "  'true',\n",
       "  'rule',\n",
       "  'deterministic',\n",
       "  'modeling',\n",
       "  'system',\n",
       "  'ﬁdelity',\n",
       "  'accommodate',\n",
       "  'complex',\n",
       "  'rule'],\n",
       " ['simple',\n",
       "  'rule',\n",
       "  'bird',\n",
       "  'cheap',\n",
       "  'develop',\n",
       "  'useful',\n",
       "  'rule',\n",
       "  'form',\n",
       "  'bird',\n",
       "  'young',\n",
       "  'bird',\n",
       "  'learn',\n",
       "  'sick',\n",
       "  'injure',\n",
       "  'bird',\n",
       "  'lose',\n",
       "  'ability',\n",
       "  'ﬂightless',\n",
       "  'specie',\n",
       "  'bird',\n",
       "  'include',\n",
       "  'cassowary',\n",
       "  'ostrich',\n",
       "  'kiwi'],\n",
       " ['expensive',\n",
       "  'develop',\n",
       "  'maintain',\n",
       "  'communicate',\n",
       "  'eﬀort',\n",
       "  'brittle',\n",
       "  'prone',\n",
       "  'failure'],\n",
       " ['give',\n",
       "  'need',\n",
       "  'mean',\n",
       "  'represent',\n",
       "  'reason',\n",
       "  'uncertainty',\n",
       "  'obvious',\n",
       "  'probability',\n",
       "  'theory',\n",
       "  'provide',\n",
       "  'tool',\n",
       "  'want',\n",
       "  'artiﬁcial',\n",
       "  'intelligence',\n",
       "  'application'],\n",
       " ['probability', 'theory', 'develop', 'analyze', 'frequency', 'event'],\n",
       " ['see',\n",
       "  'probability',\n",
       "  'theory',\n",
       "  'use',\n",
       "  'study',\n",
       "  'event',\n",
       "  'draw',\n",
       "  'certain',\n",
       "  'hand',\n",
       "  'card',\n",
       "  'game',\n",
       "  'poker'],\n",
       " ['kind', 'event', 'repeatable'],\n",
       " ['say',\n",
       "  'outcome',\n",
       "  'probability',\n",
       "  'occurring',\n",
       "  'mean',\n",
       "  'repeat',\n",
       "  'experiment'],\n",
       " ['draw',\n",
       "  'hand',\n",
       "  'card',\n",
       "  'many',\n",
       "  'time',\n",
       "  'proportion',\n",
       "  'repetition',\n",
       "  'would',\n",
       "  'result',\n",
       "  'outcome'],\n",
       " ['reasoning', 'seem', 'applicable', 'proposition', 'repeatable'],\n",
       " ['doctor',\n",
       "  'analyze',\n",
       "  'say',\n",
       "  'patient',\n",
       "  'chance',\n",
       "  'mean',\n",
       "  'diﬀerent',\n",
       "  'make',\n",
       "  'many',\n",
       "  'replicas',\n",
       "  'patient',\n",
       "  'reason',\n",
       "  'believe',\n",
       "  'replicas',\n",
       "  'patient',\n",
       "  'would',\n",
       "  'present',\n",
       "  'symptom',\n",
       "  'vary',\n",
       "  'underlie',\n",
       "  'condition'],\n",
       " ['case',\n",
       "  'doctor',\n",
       "  'diagnose',\n",
       "  'patient',\n",
       "  'use',\n",
       "  'probability',\n",
       "  'represent',\n",
       "  'degree',\n",
       "  'belief',\n",
       "  'indicate',\n",
       "  'absolute',\n",
       "  'certainty',\n",
       "  'patient',\n",
       "  'indicate',\n",
       "  'absolute',\n",
       "  'certainty',\n",
       "  'patient',\n",
       "  'ﬂu'],\n",
       " ['former',\n",
       "  'kind',\n",
       "  'probability',\n",
       "  'relate',\n",
       "  'rate',\n",
       "  'event',\n",
       "  'occur',\n",
       "  'know',\n",
       "  'frequentist',\n",
       "  'probability',\n",
       "  'relate',\n",
       "  'qualitative',\n",
       "  'level',\n",
       "  'certainty',\n",
       "  'know',\n",
       "  'bayesian',\n",
       "  'probability'],\n",
       " ['list',\n",
       "  'several',\n",
       "  'property',\n",
       "  'expect',\n",
       "  'common',\n",
       "  'sense',\n",
       "  'reason',\n",
       "  'uncertainty',\n",
       "  'way',\n",
       "  'satisfy',\n",
       "  'property',\n",
       "  'treat',\n",
       "  'probability',\n",
       "  'behave',\n",
       "  'frequentist',\n",
       "  'probability'],\n",
       " ['example',\n",
       "  'want',\n",
       "  'compute',\n",
       "  'probability',\n",
       "  'player',\n",
       "  'win',\n",
       "  'poker',\n",
       "  'game',\n",
       "  'give',\n",
       "  'certain',\n",
       "  'set',\n",
       "  'card',\n",
       "  'use',\n",
       "  'formulas',\n",
       "  'compute',\n",
       "  'probability',\n",
       "  'patient',\n",
       "  'disease',\n",
       "  'give',\n",
       "  'chapter'],\n",
       " ['probability', 'information', 'theory', 'certain', 'symptom'],\n",
       " ['detail',\n",
       "  'small',\n",
       "  'set',\n",
       "  'common',\n",
       "  'sense',\n",
       "  'assumption',\n",
       "  'imply',\n",
       "  'axiom',\n",
       "  'must',\n",
       "  'control',\n",
       "  'kind',\n",
       "  'probability',\n",
       "  'see'],\n",
       " ['probability', 'see', 'extension', 'logic', 'deal', 'uncertainty'],\n",
       " ['logic',\n",
       "  'provide',\n",
       "  'set',\n",
       "  'formal',\n",
       "  'rule',\n",
       "  'determine',\n",
       "  'proposition',\n",
       "  'imply',\n",
       "  'true',\n",
       "  'false',\n",
       "  'give',\n",
       "  'assumption',\n",
       "  'set',\n",
       "  'proposition',\n",
       "  'true',\n",
       "  'false'],\n",
       " ['theory',\n",
       "  'provide',\n",
       "  'set',\n",
       "  'formal',\n",
       "  'rule',\n",
       "  'determine',\n",
       "  'likelihood',\n",
       "  'proposition',\n",
       "  'true',\n",
       "  'give',\n",
       "  'likelihood',\n",
       "  'proposition']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x2115c204c88>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "livro_text = read_pdf('../data/raw/pdf/7kLHJ-F33GI/statbook.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Introduction to the Science of Statistics:\\n\\nFrom Theory to Implementation\\n\\nPreliminary Edition\\n\\nc(cid:13)Joseph C. Watkins\\n\\n\\x0cContents\\n\\nI Organizing and Producing Data\\n\\n1 Displaying Data\\n\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n1.1 Types of Data .\\n1.2 Categorical Data\\n1.2.1\\nPie Chart\\n1.2.2 Bar Charts\\n1.3 Two-way Tables .\\n1.4 Histograms and the Empirical Cumulative Distribution Function . . . . . . . . . . . . . . . . . . .\\n.\\n1.5 Scatterplots .\\n1.6 Time Plots .\\n.\\n.\\n1.7 Answers to Selected Exercises .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n2.1.1 Medians .\\n2.1.2 Means .\\n.\\n\\n2.1 Measuring Center .\\n.\\n.\\n2.2 Measuring Spread .\\n\\n2 Describing Distributions with Numbers\\n.\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\nFive Number Summary .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\nSample Variance and Standard Deviation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.3 Quantiles and Standardized Variables\\n2.4 Quantile-Quantile Plots .\\n.\\n2.5 Answers to Selected Exercises .\\n\\n2.2.1\\n2.2.2\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n3 Correlation and Regression\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n.\\n\\n3.1 Covariance and Correlation .\\n3.2 Linear Regression .\\n.\\n\\n.\\n.\\n3.2.1 Transformed Variables .\\n.\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n.\\n.\\n3.3.1 Nonlinear Regression .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3.3.2 Multiple Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n3.4 Answers to Selected Exercises .\\n\\n3.3 Extensions .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n4 Producing Data\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n4.1 Preliminary Steps .\\n.\\n.\\n4.2 Professional Ethics .\\n.\\n4.3 Formal Statistical Procedures .\\n4.3.1 Observational Studies\\n4.3.2 Randomized Controlled Experiments\\n4.3.3 Natural experiments\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n4.4 Case Studies .\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n1\\n\\n3\\n3\\n4\\n4\\n7\\n7\\n10\\n13\\n15\\n18\\n\\n21\\n21\\n21\\n21\\n24\\n24\\n25\\n27\\n28\\n30\\n\\n33\\n33\\n37\\n45\\n50\\n51\\n51\\n56\\n\\n65\\n65\\n66\\n66\\n66\\n67\\n70\\n71\\n\\ni\\n\\n\\x0c4.4.1 Observational Studies\\n4.4.2 Experiments\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n.\\n\\nII Probability\\n\\n5 The Basics of Probability\\n.\\n\\nIntroduction .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n5.1\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5.2 Equally Likely Outcomes and the Axioms of Probability . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5.3 Consequences of the Axioms .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5.4 Counting .\\n.\\n.\\n.\\nFundamental Principle of Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5.4.1\\n.\\n5.4.2\\nPermutations .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n5.4.3 Combinations .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n\\n.\\n.\\n5.5 Answers to Selected Exercises .\\n5.6 Set Theory - Probability Theory Dictionary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n71\\n72\\n\\n79\\n\\n81\\n81\\n82\\n84\\n86\\n86\\n87\\n88\\n91\\n96\\n\\n6 Conditional Probability and Independence\\n\\n97\\n97\\n6.1 Restricting the Sample Space - Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n98\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6.2 The Multiplication Principle .\\n.\\n99\\n6.3 The Law of Total Probability .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n.\\n.\\n.\\n6.4 Bayes formula .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n6.5\\n.\\nIndependence .\\n.\\n6.6 Answers to Selected Exercises .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n7 Random Variables and Distribution Functions\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\nIntroduction .\\n\\n7.4 Mass Functions .\\n.\\n7.5 Density Functions .\\n7.6 Mixtures .\\n.\\n7.7\\n\\n111\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n7.1\\n.\\n7.2 Distribution Functions\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\\n7.3 Properties of the Distribution Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n7.3.1 Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n7.3.2 Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\\nJoint and Conditional Distributions .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n7.7.1 Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\n7.7.2 Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\\n7.7.3\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\\n7.8.1 Discrete Random Variables and the sample Command . . .\\n. . . . . . . . . . . . . . 124\\n7.8.2 Continuous Random Variables and the Probability Transform . . . . . . . . . . . . . . . . . 125\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\\n\\n7.9 Answers to Selected Exercises .\\n\\n7.8 Simulating Random Variables\\n\\nIndependent Random Variables\\n\\n. . .\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n8 The Expected Value\\n\\n.\\n\\n.\\n\\n.\\n.\\n.\\n\\n8.1 Deﬁnition and Properties .\\n.\\n.\\n8.2 Discrete Random Variables .\\n.\\n8.3 Bernoulli Trials .\\n.\\n.\\n8.4 Continuous Random Variables .\\n.\\n8.5 Summary .\\n.\\n.\\n.\\n8.6 Names for Eg(X).\\n.\\n8.7\\n.\\n.\\n.\\n.\\n8.8 Covariance and Correlation .\\n\\nIndependence .\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n137\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\\n.\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\\n.\\n. . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\\n\\n. . .\\n\\nii\\n\\n\\x0c8.8.1 Equivalent Conditions for Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\\n\\n8.9 Quantile Plots and Probability Plots\\n8.10 Answers to Selected Exercises .\\n\\n9 Examples of Mass Functions and Densities\\n\\n157\\n9.1 Examples of Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n9.2 Examples of Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\\n9.3 More on Mixtures .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n9.4 R Commands .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n.\\n.\\n9.5 Summary of Properties of Random Variables\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n9.5.1 Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n9.5.2 Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n\\n9.6 Answers to Selected Exercises .\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n10 The Law of Large Numbers\\n.\\n\\n.\\n10.1 Introduction .\\n.\\n.\\n10.2 Monte Carlo Integration .\\n10.3 Importance Sampling .\\n.\\n.\\n10.4 Answers to Selected Exercises .\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n179\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n11 The Central Limit Theorem\\n.\\n\\n193\\n11.1 Introduction .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\\n11.2 The Classical Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\\n11.2.1 Bernoulli Trials and the Continuity Correction . . . . . . . . . . . . . . . . . . . . . . . . . 197\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\n\\n.\\n.\\n.\\n.\\n11.6 Answers to Selected Exercises .\\n\\n11.3 Propagation of Error\\n11.4 Delta Method .\\n.\\n11.5 Summary of Normal Approximations\\n.\\n\\n.\\n11.5.1 Sample Sum .\\n11.5.2 Sample Mean .\\n.\\n11.5.3 Sample Proportion .\\n.\\n11.5.4 Delta Method .\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\nIII Estimation\\n\\n213\\n\\n12 Overview of Estimation\\n.\\n.\\n.\\n12.1 Introduction .\\n.\\n12.2 Classical Statistics\\n12.3 Bayesian Statistics\\n.\\n12.4 Answers to Selected Exercises .\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n\\n13 Method of Moments\\n.\\n13.1 Introduction .\\n.\\n.\\n13.2 The Procedure .\\n.\\n13.3 Examples .\\n.\\n13.4 Answers to Selected Exercises .\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n\\n215\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\\n.\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\\n\\n231\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\\n\\niii\\n\\n\\x0c.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n14 Unbiased Estimation\\n.\\n\\n241\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\\n.\\n.\\n14.1 Introduction .\\n.\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\\n14.2 Computing Bias .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\\n14.3 Compensating for Bias .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\\n.\\n14.4 Consistency .\\n.\\n14.5 Cram´er-Rao Bound .\\n. .\\n. 250\\n.\\n14.6 A Note on Exponential Families and Efﬁcient Estimators . . . . . . . . . . . . . . . . . . . . . . . . 254\\n14.7 Answers to Selected Exercises .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n15 Maximum Likelihood Estimation\\n.\\n.\\n.\\n.\\n\\n261\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\\n.\\n15.1 Introduction .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\\n15.2 Examples .\\n.\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\\n15.3 Summary of Estimators .\\n15.4 Asymptotic Properties\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\\n.\\n15.5 Comparison of Estimation Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\\n15.6 Multidimensional Estimation .\\n.\\n15.7 The Case of Exponential Families\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\\n.\\n15.8 Choice of Estimators .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\\n15.9 Technical Aspects .\\n.\\n.\\n15.10Answers to Selected Exercises .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n16 Interval Estimation\\n\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n.\\n\\n16.1 Classical Statistics\\n.\\n\\n281\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\\n16.1.1 Means .\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\\n16.1.2 Linear Regression .\\n16.1.3 Sample Proportions .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\\n16.1.4 Summary of Standard Conﬁdence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\\n16.1.5 Interpretation of the Conﬁdence Interval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\\n16.1.6 Extensions on the Use of Conﬁdence Intervals\\n. . . . . . . . . . . . . . . . . . . . . . . . . 292\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\\n\\n.\\n16.2 The Bootstrap .\\n.\\n16.3 Bayesian Statistics\\n.\\n16.4 Answers to Selected Exercises .\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\nIV Hypothesis Testing\\n\\n301\\n\\n17 Simple Hypotheses\\n\\n.\\n\\n.\\n.\\n\\n17.1 Overview and Terminology .\\n17.2 The Neyman-Pearson Lemma\\n\\n303\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\\n.\\n17.2.1 The Receiver Operating Characteristic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\\n17.3 Examples .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\\n17.4 Summary .\\n17.5 Proof of the Neyman-Pearson Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\\n17.6 An Brief Introduction to the Bayesian Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. 316\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\\n17.7 Answers to Selected Exercises .\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n18 Composite Hypotheses\\n\\n323\\n18.1 Partitioning the Parameter Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\\n.\\n18.2 The Power Function .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\\n18.3 The p-value .\\n.\\n18.4 Distribution of p-values and the Receiving Operating Characteristic\\n. . . . . . . . . . . . . . . . . . 334\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n18.5 Multiple Hypothesis Testing .\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\niv\\n\\n\\x0c18.5.1 Familywise Error Rate .\\n18.5.2 False Discovery Rate .\\n.\\n18.6 Answers to Selected Exercises .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\\n\\n19 Extensions on the Likelihood Ratio\\n.\\n.\\n.\\n\\n.\\n19.1 One-Sided Tests .\\n.\\n.\\n19.2 Likelihood Ratio Tests\\n19.3 Chi-square Tests .\\n.\\n.\\n19.4 Answers to Selected Exercises .\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n341\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\\n\\n20 t Procedures\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n. . . .\\n\\n. . . .\\n\\n. . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\n20.1 Guidelines for Using the t Procedures\\n20.2 One Sample t Tests .\\n.\\n20.3 Correspondence between Two-Sided Tests and Conﬁdence Intervals . . . . . . . . . . . . . . . . .\\n.\\n20.4 Matched Pairs Procedures .\\n20.5 Two Sample Procedures .\\n.\\n.\\n20.6 Summary of Tests of Signiﬁcance\\n.\\n\\n361\\n. 361\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\\n. 365\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\\n. 373\\n20.6.1 General Guidelines .\\n20.6.2 Test for Population Proportions\\n. 374\\n20.6.3 Test for Population Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\\n20.7 A Note on the Delta Method .\\n20.8 The t Test as a Likelihood Ratio Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\\n20.9 Non-parametric alternatives .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\\n.\\n.\\n. 378\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\\n\\n20.9.1 Permutation Test\\n20.9.2 Mann-Whitney or Wilcoxon Rank Sum Test . . . . . . . . . . . . . . . . . . . . . . . . . .\\n20.9.3 Wilcoxon Signed-Rank Test\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n20.10Answers to Selected Exercises .\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n21 Goodness of Fit\\n\\n21.1 Fit of a Distribution .\\n21.2 Contingency tables .\\n21.3 Applicability and Alternatives to Chi-squared Tests . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21.4 Answer to Selected Exercise .\\n\\n385\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391\\n. 395\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n22 Analysis of Variance\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n22.1 Overview .\\n.\\n22.2 One Way Analysis of Variance .\\n.\\n22.3 Contrasts\\n.\\n22.4 Two Sample Procedures .\\n.\\n22.5 Kruskal-Wallis Rank-Sum Test .\\n22.6 Answer to Selected Exercises .\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n403\\n.\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414\\n\\nAppendix A: A Sample R Session\\n\\nIndex\\n\\n417\\n\\n423\\n\\nv\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nvi\\n\\n\\x0cPreface\\n\\nStatistical thinking will one day be as necessary a qualiﬁcation for efﬁcient citizenship as the ability to\\nread and write. – Samuel Wilkes, 1951, paraphrasing H. G. Wells from Mankind in the Making\\n\\nThe value of statistical thinking is now accepted by researchers and practitioners from a broad range of endeavors.\\nThis viewpoint has become common wisdom in a world of big data. The challenge for statistics educators is to adapt\\ntheir pedagogy to accommodate the circumstances associated to the information age. This choice of pedagogy should\\nbe attuned to the quantitative capabilities and scientiﬁc background of the students as well as the intended use of their\\nnewly acquired knowledge of statistics.\\n\\nMany university students, presumed to be proﬁcient in college algebra, are taught a variety of procedures and\\nstandard tests under a well-developed pedagogy. This approach is sufﬁciently reﬁned so that students have a good\\nintuitive understanding of the underlying principles presented in the course. However, if the statistical needs presented\\nby a given scientiﬁc question fall outside the battery of methods presented in the standard curriculum, then students\\nare typically at a loss to adjust the procedures to accommodate the additional demand.\\n\\nOn the other hand, undergraduate students majoring in mathematics frequently have a course on the theory of\\nstatistics as a part of their program of study. In this case, the standard curriculum repeatedly ﬁnds itself close to the\\nvery practically minded subject that statistics is. However, the demands of the syllabus provide very little time to\\nexplore these applications with any sustained attention.\\n\\nOur goal is to ﬁnd a middle ground.\\nDespite the fact that calculus is a routine tool in the development of statistics, the beneﬁts to students who have\\nlearned calculus are infrequently employed in the statistics curriculum. The objective of this book is to meet this need\\nwith a one semester course in statistics that moves forward in recognition of the coherent body of knowledge provided\\nby statistical theory having an eye consistently on the application of the subject. Such a course may not be able to\\nachieve the same degree of completeness now presented by the two more standard courses described above. However,\\nit ought to able to achieve some important goals:\\n\\n• leaving students capable of understanding what statistical thinking is and how to integrate this with scientiﬁc\\n\\nprocedures and quantitative modeling and\\n\\n• learning how to ask statistics experts productive questions, and how to implement their ideas using statistical\\n\\nsoftware and other computational tools.\\n\\nInevitably, many important topics are not included in this book. In addition, I have chosen to incorporate abbre-\\nviated introductions of some more advanced topics. Such topics can be skipped in a ﬁrst pass through the material.\\nHowever, one value of a textbook is that it can serve as a reference in future years. The context for some parts of\\nthe exposition will become more clear as students continue their own education in statistics. In these cases, the more\\nadvanced pieces can serve as a bridge from this book to more well developed accounts. My goal is not to compose a\\nstand alone treatise, but rather to build a foundation that allows those who have worked through this book to introduce\\nthemselves to many exciting topics both in statistics and in its areas of application.\\n\\nvii\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nWho Should Use this Book\\nThe major prerequisites are comfort with calculus and a strong interest in questions that can beneﬁt from statistical\\nanalysis. Willingness to engage in explorations utilizing statistical software is an important additional requirement.\\nThe original audience for the course associated to this book are undergraduate students minoring in mathematics.\\nThese student have typically completed a course in multivariate calculus. Many have been exposed to either linear\\nalgebra or differential equations. They enroll in this course because they want to obtain a better understanding of\\ntheir own core subject. Even though we regularly rely on the mechanics of calculus and occasionally need to work\\nwith matrices, this is not a textbook for a mathematics course, but rather a textbook that is dedicated to a higher level\\nof understanding of the concepts and practical applications of statistics. In this regard, it relies on a solid grasp of\\nconcepts and structures in calculus and algebra.\\n\\nWith the advance and adoption of the Common Core State Standards in mathematics, we can anticipate that\\nprimary and secondary school students will experience a broader exposure to statistics through their school years. As\\na consequence, we will need to develop a curriculum for teachers and future teachers so that they can take content in\\nstatistics and turn that into curriculum for their students. This book can serve as a source of that content.\\n\\nIn addition, those engaged both in industry and in scholarly research are experiencing a surge in the need to\\ndesign more complex experiments and analyze more diverse data types. Universities and industry are responding with\\nadvanced educational opportunities to extend statistics education beyond the theory of probability and statistics, linear\\nmodels and design of experiments to more modern approaches that include stochastic processes, machine learning and\\ndata mining, Bayesian statistics, and statistical computing. This book can serve as an entry point for these critical\\ntopics in statistics.\\n\\nAn Annotated Syllabus\\nThe four parts of the course - organizing and collecting data, an introduction to probability, estimation procedures and\\nhypothesis testing - are the building blocks of many statistics courses. We highlight some of the particular features in\\nthis book.\\n\\nOrganizing and Collecting Data\\nMuch of this is standard and essential - organizing categorical and quantitative data, appropriately displayed as contin-\\ngency tables, bar charts, histograms, boxplots, time plots, and scatterplots, and summarized using medians, quartiles,\\nmeans, weighted means, trimmed means, standard deviations, correlations and regression lines. We use this as an\\nopportunity to introduce to the statistical software package R and to add additional summaries like the empirical cu-\\nmulative distribution function and the empirical survival function. One example incorporating the use of this is the\\ncomparison of the lifetimes of wildtype and transgenic mosquitoes and a discussion of the best strategy to display and\\nsummarize data if the goal is to examine the differences in these two genotypes of mosquitoes in their ability to carry\\nand spread malaria. A bit later, we will do an integration by parts exercise to show that the mean of a non-negative\\ncontinuous random variable is the area under its survival function.\\n\\nCollecting data under a good design is introduced early in the text and discussion of the underlying principles of\\nexperimental design is an abiding issue throughout the text. With each new mathematical or statistical concept comes\\nan enhanced understanding of what an experiment might uncover through a more sophisticated design than what was\\npreviously thought possible. The students are given readings on design of experiment and examples using R to create\\na sample under variety of protocols.\\n\\nIntroduction to Probability\\nProbability theory is the analysis of random phenomena. It is built on the axioms of probability and is explored, for\\nexample, through the introduction of random variables. The goal of probability theory is to uncover properties arising\\nfrom the phenomena under study. Statistics is devoted to the analysis of data. One goal of statistical science is to\\n\\nviii\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\narticulate as well as possible what model of random phenomena underlies the production of the data. The focus of this\\nsection of the course is to develop those probabilistic ideas that relate most directly to the needs of statistics.\\n\\nThus, we must study the axioms and basic properties of probability to the extent that the students understand\\nconditional probability and independence. Conditional probability is necessary to develop Bayes formula which we\\nwill later use to give a taste of the Bayesian approach to statistics.\\nIndependence will be needed to describe the\\nlikelihood function in the case of an experimental design that is based on independent observations. Densities for\\ncontinuous random variables and mass function for discrete random variables are necessary to write these likelihood\\nfunctions explicitly. Expectation will be used to standardize a sample sum or sample mean and to perform method of\\nmoments estimates.\\n\\nRandom variables are developed for a variety of reasons. Some, like the binomial, negative binomial, Poisson or\\nthe gamma random variable, arise from considerations based on Bernoulli trials or exponential waiting. The hyperge-\\nometric random variable helps us understand the difference between sampling with and without replacement. The F ,\\nt and chi-square random variables will later become test statistics. Uniform random variables are the ones simulated\\nby random number generators. Because of the central limit theorem, the normal family is the most important among\\nthe list of parametric families of random variables.\\n\\nThe ﬂavor of the text returns to becoming more authentically statistical with the law of large numbers and the\\ncentral limit theorem. These are largely developed using simulation explorations and ﬁrst applied to simple Monte\\nCarlo techniques and importance sampling to estimate the value of an deﬁnite integrals. One cautionary tale is an\\nexample of the failure of these simulation techniques when applied without careful analysis. If one uses, for example,\\nCauchy random variables in the evaluation of some quantity, then the simulated sample means can appear to be\\nconverging only to experience an abrupt and unpredictable jump. The lack of convergence of an improper integral\\nreveals the difﬁculty. The central object of study is, of course, the central limit theorem. It is developed both in terms\\nof sample sums and sample means and proportions and used in relatively standard ways to estimate probabilities.\\nHowever, in this book, we can introduce the delta method which adds ideas associated to the central limit theorem to\\nthe context of propagation of error.\\n\\nEstimation\\nIn the simplest possible terms, the goal of estimation theory is to answer the question: What is that number? An\\nestimate is a statistic, i. e., a function of the data. We look to two types of estimation techniques - method of moments\\nand maximum likelihood and several criteria for an estimator using, for example, variance and bias. Several examples\\nincluding mark and recapture and the distribution of ﬁtness effects from genetic data are developed for both types of\\nestimators. The variance of an estimator is approximated using the delta method for method of moments estimators\\nand using Fisher information for maximum likelihood estimators. An analysis of bias is based on quadratic Taylor\\nseries approximations and the properties of expectations. Both classes of estimators are often consistent. This implies\\nthat the bias decreases towards zero with an increasing number of observations. R is routinely used in simulations to\\ngain insight into the quality of estimators.\\n\\nThe point estimation techniques are followed by interval estimation and, notably, by conﬁdence intervals. This\\nbrings us to the familiar one and two sample t-intervals for population means and one and two sample z-intervals for\\npopulation proportions. In addition, we can return to the delta method and the observed Fisher information to construct\\nconﬁdence intervals associated respectively to method of moment estimators and and maximum likelihood estimators.\\nWe also add a brief introduction on bootstrap conﬁdence intervals and Bayesian credible intervals in order to provide\\na broader introduction to strategies for parameter estimation.\\n\\nHypothesis Testing\\nFor hypothesis testing, we ﬁrst establish the central issues - null and alternative hypotheses, type I and type II errors,\\ntest statistics and critical regions, signiﬁcance and power. We then present the ideas behind the use of likelihood ratio\\ntests as best tests for a simple hypothesis. This is motivated by a game designed to explain the importance of the\\nNeyman Pearson lemma. This approach leads us to well-known diagnostics of an experimental design, notably, the\\nreceiver operating characteristic and power curves.\\n\\nix\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions of the Neyman Pearson lemma form the basis for the t test for means, the chi-square test for goodness\\nof ﬁt, and the F test for analysis of variance. These results follow from the application of optimization techniques\\nfrom calculus, including the use of Lagrange multipliers to develop goodness of ﬁt tests. The Bayesian approach\\nto hypothesis testing is explored for the case of simple hypothesis using morphometric measurements, in this case a\\nbutterﬂy wingspan, to test whether a habitat has been invaded by a mimic species.\\n\\nThe desire of a powerful test is articulated in a variety of ways. In engineering terms, power is called sensitivity.\\nWe illustrate this with a radon detector. An insensitive instrument is a risky purchase. This can be either because\\nthe instrument is substandard in the detection of ﬂuctuations or poor in the statistical basis for the algorithm used to\\ndetermine a change in radon level. An insensitive detector has the undesirable property of not sounding its alarm when\\nthe radon level has indeed risen.\\n\\nThe course ends by looking at the logic of hypotheses testing and the results of different likelihood ratio analyses\\napplied to a variety of experimental designs. The delta method allows us to extend the resulting test statistics to\\nmultivariate nonlinear transformations of the data. The textbook concludes with a practical view of the consequences\\nof this analysis through case studies in a variety of disciplines including, for example, genetics, health, ecology,\\nand bee biology. This will serve to introduce us to the well known t procedure for inference of the mean, both the\\nlikelihood-based G2 test and the traditional chi-square test for discrete distributions and contingency tables, and the\\nF test for one-way analysis of variance. We add short descriptions for the corresponding non-parametric procedures,\\nnamely, permutation, ranked-sum and signed-rank tests for quantitative data, and exact tests for categorical data\\n\\nExercises and Problems\\nOne obligatory statement in the preface of a book such as this is to note the necessity of working problems. The mate-\\nrial can only be mastered by grappling with the issues through the application to engaging and substantive questions.\\nIn this book, we address this imperative through exercises and through problems. The exercises, integrated into the\\ntextbook narrative, are of two basic types. The ﬁrst is largely mathematical or computational exercises that are meant\\nto provide or extend the derivation of a useful identity or data analysis technique. These experiences will prepare the\\nstudent to perform the calculations that routinely occur in investigations that use statistical thinking. The second type\\nform a collection of questions that are meant to afﬁrm the understanding of a particular concept.\\n\\nProblems are collected at the end of each of the four parts of the book. While the ordering of the problems generally\\nfollows the ﬂow of the text, they are designed to be more extensive and integrative. These problems often incorporate\\nseveral concepts and will call on a variety of problem solving strategies combining handwritten work with the use of\\nstatistical software. Without question, the best problems are those that the students chose from their own interests.\\n\\nAcknowledgements\\nThe concept that let to this book grew out of a conversation with the late Michael Wells, Professor of Biochemistry\\nat the University of Arizona. He felt that if we are asking future life scientist researchers to take the time to learn\\ncalculus and differential equations, we should also provide a statistics course that adds value to their abilities to design\\nexperiments and analyze data while reinforcing both the practical and conceptual sides of calculus. As a consequence,\\ncourse development received initial funding from a Howard Hughes Medical Institute grant (52005889). Christopher\\nBergevin, an HHMI postdoctoral fellow, provided a valuable initial collaboration.\\n\\nSince that time, I have had the great fortune to be the teacher of many bright and dedicated students whose future\\ncontribution to our general well-being is beyond dispute. Their cheerfulness and inquisitiveness has been a source\\nof inspiration for me. More practically, their questions and their persistence led to a much clearer exposition and\\nthe addition of many dozens of ﬁgures to the text. Through their end of semester projects, I have been introduced\\nto many interesting questions that are intriguing in their own right, but also have added to the range of applications\\npresented throughout the text. Four of these students - Beryl Jones, Clayton Mosher, Laurel Watkins de Jong, and\\nTaylor Corcoran - have gone on to become assistants in the course. I am particularly thankful to these four for their\\ncontributions to the dynamical atmosphere that characterizes the class experience.\\n\\nx\\n\\n\\x0cPart I\\n\\nOrganizing and Producing Data\\n\\n1\\n\\n\\x0c\\x0cTopic 1\\n\\nDisplaying Data\\n\\nThere are two goals when presenting data: convey your story and establish credibility. - Edward Tufte\\n\\nStatistics is a mathematical science that is concerned with the collection, analysis, interpretation or explanation,\\nand presentation of data. Properly used statistical principles are essential in guiding any inquiry informed by data and,\\nespecially in the phase of data exploration, is routinely a fundamental source for discovery and innovation. Insights\\nfrom data may come from a well conceived visualization of the data, from modern methods of statistical learning and\\nmodel selection as well as from time-honored formal statistical procedures.\\n\\nThe ﬁrst encounters one has to data are through graphical displays and numerical summaries. The goal is to ﬁnd\\nan elegant method for this presentation that is at the same time both objective and informative - making clear with a\\nfew lines or a few numbers the salient features of the data. In this sense, data presentation is at the same time an art, a\\nscience, and an obligation to impartiality.\\n\\nIn the section, we will describe some of the standard presentations of data and at the same time, taking the opportu-\\nnity to introduce some of the commands that the software package R provides to draw ﬁgures and compute summaries\\nof the data.\\n\\n1.1 Types of Data\\nA data set provides information about a group of individuals. These individuals are, typically, representatives chosen\\nfrom a population under study. Data on the individuals are meant, either informally or formally, to allow us to make\\ninferences about the population. We shall later discuss how to deﬁne a population, how to choose individuals in the\\npopulation and how to collect data on these individuals.\\n• Individuals are the objects described by the data.\\n• Variables are characteristics of an individual. In order to present data, we must ﬁrst recognize the types of data\\n\\nunder consideration.\\n\\n– Categorical variables partition the individuals into classes. Other names for categorical variables are\\nlevels or factors. One special type of categorical variables are ordered categorical variables that suggest\\na ranking, say small. medium, large or mild, moderate, severe.\\n\\n– Quantitative variables are those for which arithmetic operations like addition and differences make sense.\\n\\nExample 1.1 (individuals and variables). We consider two populations - the ﬁrst is the nations of the world and the\\nsecond is the people who live in those countries. Below is a collection of variables that might be used to study these\\npopulations.\\n\\n3\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nnations\\n\\npeople\\n\\npopulation size\\n\\ntime zones\\n\\naverage rainfall\\nlife expectancy\\nmean income\\nliteracy rate\\ncapital city\\nlargest river\\n\\nage\\nheight\\ngender\\n\\nethnicities\\n\\nannual income\\n\\nliteracy\\n\\nmother’s maiden name\\n\\nmarital status\\n\\nExercise 1.2. Classify the variables as quantitative or categorical in the example above.\\n\\nThe naming of variables and their classiﬁcation as categorical or quantitative may seem like a simple, even trite,\\nexercise. However, the ﬁrst steps in designing an experiment and deciding on which individuals to include and which\\ninformation to collect are vital to the success of the experiment. For example, if your goal is to measure the time for\\nan animal (insect, bird, mammal) to complete some task under different (genetic, environmental, learning) conditions,\\nthen, you may decide to have a single quantitative variable - the time to complete the task. However, an animal in\\nyour study may not attempt the task, may not complete the task, or may perform the task. As a consequence, your\\ndata analysis will run into difﬁculties if you do not add a categorical variable to include these possible outcomes of an\\nexperiment.\\n\\nExercise 1.3. Give examples of variables for the population of vertebrates, of proteins.\\n\\n1.2 Categorical Data\\n1.2.1 Pie Chart\\nA pie chart is a circular chart divided into sectors, illustrating relative magnitudes in frequencies or percents. In a pie\\nchart, the area is proportional to the quantity it represents.\\n\\nExample 1.4. As the nation debates strategies for delivering health insurance, let’s look at the sources of funds and\\nthe types of expenditures.\\n\\nFigure 1.1: 2008 United States health care (a) expenditures (b) income sources, Source: Centers for Medicare and Medicaid Services, Ofﬁce of\\nthe Actuary, National Health Statistics Group\\n\\n4\\n\\n  \\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nExercise 1.5. How do you anticipate that this pie chart will evolve over the next decade? Which pie slices are likely\\nto become larger? smaller? On what do you base your predictions?\\nExample 1.6. From UNICEF, we read “The proportion of children who reach their ﬁfth birthday is one of the most\\nfundamental indicators of a country’s concern for its people. Child survival statistics are a poignant indicator of the\\npriority given to the services that help a child to ﬂourish: adequate supplies of nutritious food, the availability of high-\\nquality health care and easy access to safe water and sanitation facilities, as well as the family’s overall economic\\ncondition and the health and status of women in the community. ”\\n\\nExample 1.7. Gene Ontology (GO) project is a bioinformatics initiative whose goal is to provide uniﬁed terminology\\nof genes and their products. The project began in 1998 as a collaboration between three model organism databases,\\nDrosophila, yeast, and mouse. The GO Consortium presently includes many databases, spanning repositories for\\nplant, animal and microbial genomes. This project is supported by National Human Genome Research Institute. See\\n\\nhttp://www.geneontology.org/\\n\\nFigure 1.2: The 25 most frequent Biological Process Gene Ontology (GO) terms.\\n\\n5\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nTo make a simple pie chart in R for the proportion of AIDS cases among US males by transmission category.\\n\\n> males<- c(58,18,16,7,1)\\n> pie(males)\\n\\nThis many be sufﬁcient for your own personal use. However, if we want to use a pie chart in a presentation, we\\nwill have to provide some essential details. For a more descriptive pie chart, one has to become accustomed to learning\\nto interact with the software to settle on a graph that is satisfactory to the situation.\\n\\n• Deﬁne some colors ideal for black and white print.\\n\\n> colors <- c(\"white\",\"grey70\",\"grey90\",\"grey50\",\"black\")\\n\\n• Calculate the percentage for each category.\\n\\n> male_labels <- round(males/sum(males)*100, 1)\\n\\nThe number 1 indicates rounded to one decimal place.\\n\\n> male_labels <- paste(male_labels, \"\\\\%\", sep=\" \")\\n\\nThis adds a space and a percent sign.\\n\\n• Create a pie chart with deﬁned heading and custom colors and labels and create a legend.\\n> pie(males, main=\"Proportion of AIDS Cases among Males by\\n+ Diagnosed - USA, 2005\", col=colors, labels=male_labels, cex=0.8)\\n> legend(\"topright\", c(\"Male-male contact\",\"Injection drug use (IDU)\",\\n+ \"High-risk heterosexual contact\",\"Male-male contact and IDU\",\"Other\"),\\n+ cex=0.8,fill=colors)\\n\\nTransmission Category\\n\\nThe entry cex=0.8 indicates that the legend has a type set that is 80% of the font size of the main title.\\n\\n6\\n\\n58 %18 %16 %7 %1 %Proportion of AIDS Cases among Males by Transmission Category Diagnosed − USA, 2005Male−male contactInjection drug use (IDU)High−risk heterosexual contactMale−male contact and IDUOther\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\n1.2.2 Bar Charts\\nBecause the human eye is good at judging linear measures and poor at judging relative areas, a bar chart or bar graph\\nis often preferable to pie charts as a way to display categorical data.\\n\\nTo make a simple bar graph in R,\\n\\n> barplot(males)\\n\\nFor a more descriptive bar chart with information on females:\\n• Enter the data for females and create a 5 × 2 array.\\n\\n> females <- c(0,71,27,0,2)\\n> hiv<-array(c(males,females), dim=c(5,2))\\n\\n• Generate side-by-side bar graphs and create a legend,\\n\\n> barplot(hiv, main=\"Proportion of AIDS Cases by Sex and Transmission Category\\n+ Diagnosed - USA, 2005\", ylab= \"percent\",\\n+ names.arg = c(\"Males\", \"Females\"),col=colors)\\n> legend(\"topright\", c(\"Male-male contact\",\"Injection drug use (IDU)\",\\n+ \"High-risk heterosexual contact\",\"Male-male contact and IDU\",\"Other\"),\\n+ cex=0.8,fill=colors)\\n\\nbeside=TRUE,\\n\\nExample 1.8. Next we examine a segmented bar plot. This shows the ancestral sources of genes for 75 populations\\nthroughout Asia. the data are based on information gathered from 50,000 genetic markers. The designations for the\\ngroups were decided by the software package STRUCTURE.\\n\\n1.3 Two-way Tables\\nRelationships between two categorical variables can be shown through a two-way table (also known as a contingency\\ntable , cross tabulation table or a cross classifying table ).\\n\\n7\\n\\nMalesFemalesProportion of AIDS Cases by Sex and Transmission CategoryDiagnosed − USA, 2005percent010203040506070Male−male contactInjection drug use (IDU)High−risk heterosexual contactMale−male contact and IDUOther\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nFigure 1.3: Dispaying human genetic diversity for 75 populations in Asia. The software program STRUCTURE here infers 14 source populations,\\n10 of them major. The length of each segment in the bar is the estimate by STRUCTURE of the fraction of the genome in the sample that has\\nancestors among the given source population.\\n\\n8\\n\\n(Fig.1andfigs.S1toS13),populationphy-logenies(Fig.1andfigs.S27andS28),andPCAresults(Fig.2)allshowthatpopulationsfromthesamelinguisticgrouptendtoclustertogether.AManteltestconfirmsthecorrelationbetweenlin-guisticandgeneticaffinities(R2=0.253;P<0.0001with10,000permutations),evenaftercontrollingforgeography(partialcorrelation=0.136;P<0.005with10,000permutations).Nevertheless,weidentifiedeightpopulationoutlierswhoselinguisticandgeneticaffinitiesareinconsistent[Affymetrix-Melanesian(AX-ME),Malaysia-Jehai(MY-JH)Fig.1.Maximum-likelihoodtreeof75populations.Ahypotheticalmost-recentcommonancestor(MRCA)composedofancestralallelesasinferredfromthegenotypesofonegorillaand21chimpanzeeswasusedtorootthetree.Brancheswithbootstrapvalueslessthan50%werecondensed.Populationidentificationnumbers(IDs),samplecollectionlocationswithlatitudesandlongitudes,ethnicities,languagespoken,andsizeofpop-ulationsamplesareshowninthetableadjacenttoeachbranchinthetree.Linguisticgroupsareindicatedwithcolorsasshowninthelegend.AllpopulationIDsexceptthefourHapMapsamplesaredenotedbyfourcharacters.Thefirsttwolettersindicatethecountrywherethesampleswerecollectedor(inthecaseofAffymetrix)genotyped,accordingtothefollowingconvention:AX,Affymetrix;CN,China;ID,Indonesia;IN,India;JP,Japan;KR,Korea;MY,Malaysia;PI,thePhilippines;SG,Singapore;TH,Thailand;andTW,Taiwan.ThelasttwolettersareuniqueIDsforthepopulation.Totherightofthetable,anaveragedgraphofresultsfromSTRUCTUREisshownforK=14.11DECEMBER2009VOL326SCIENCEwww.sciencemag.org1542REPORTS on April 28, 2010 www.sciencemag.orgDownloaded from \\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nExample 1.9. In 1964, Surgeon General Dr. Luther Leonidas Terry published a landmark report saying that smoking\\nmay be hazardous to health. This led to many inﬂuential reports on the topic, including the study of the smoking habits\\nof 5375 high school children in Tucson in 1967. Here is a two-way table summarizing some of the results.\\n\\nstudent\\nsmokes\\n\\nstudent\\n\\ndoes not smoke\\n\\n2 parents smoke\\n1 parent smokes\\n0 parents smoke\\ntotal\\n\\n400\\n416\\n188\\n1004\\n\\n1380\\n1823\\n1168\\n4371\\n\\ntotal\\n1780\\n2239\\n1356\\n5375\\n\\n• The row variable is the parents smoking habits.\\n• The column variable is the student smoking habits.\\n• The cells display the counts for each of the categories of row and column variables.\\nA two-way table with r rows and c columns is often called an r by c table (written r × c).\\nThe totals along each of the rows and columns give the marginal distributions. We can create a segmented bar\\n\\ngraph as follows:\\n\\n> smoking<-matrix(c(400,1380,416,1823,188,1168),ncol=3)\\n> colnames(smoking)<-c(\"2 parents\",\"1 parent\", \"0 parents\")\\n> rownames(smoking)<-c(\"smokes\",\"does not smoke\")\\n> smoking\\n\\n2 parents 1 parent 0 parents\\n188\\nsmokes\\n1168\\ndoes not smoke\\n> barplot(smoking,legend=rownames(smoking))\\n\\n400\\n1380\\n\\n416\\n1823\\n\\n9\\n\\n2 parents1 parent0 parentsdoes not smokesmokes0500100015002000\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nExample 1.10. Hemoglobin E is a variant of hemoglobin with a mutation in the β globin gene causing substitution of\\nglutamic acid for lysine at position 26 of the β globin chain. HbE (E is the one letter abbreviation for glutamic acid.)\\nis the second most common abnormal hemoglobin after sickle cell hemoglobin (HbS). HbE is common from India to\\nSoutheast Asia. The β chain of HbE is synthesized at a reduced rate compare to normal hemoglobin (HbA) as the HbE\\nproduces an alternate splicing site within an exon.\\n\\nIt has been suggested that Hemoglobin E provides some protection against malaria virulence when heterozygous,\\nbut is causes anemia when homozygous. The circumstance in which the heterozygotes for the alleles under considera-\\ntion have a higher adaptive value than the homozygote is called balancing selection.\\n\\nThe table below gives the counts of differing hemoglobin genotypes on two Indonesian islands.\\n\\ngenotype AA AE EE\\n0\\nFlores\\nSumba\\n4\\n\\n128\\n119\\n\\n6\\n78\\n\\nBecause the heterozygotes are rare on Flores, it appears malaria is less prevalent there since the heterozygote does\\nnot provide an adaptive advantage.\\n\\nExercise 1.11. Make a segmented barchart of the data on hemoglobin genotypes. Have each bar display the distribu-\\ntion of genotypes on the two Indonesian islands.\\n\\n1.4 Histograms and the Empirical Cumulative Distribution Function\\nHistograms are a common visual representation of a quantitative variable. Histograms summarize the data using\\nrectangles to display either frequencies or proportions as normalized frequencies. In making a histogram, we\\n\\n• Divide the range of data into bins of equal width (usually, but not always).\\n• Count the number of observations in each class.\\n• Draw the histogram rectangles representing frequencies or percents by area.\\nInterpret the histogram by giving\\n• the overall pattern\\n\\n– the center\\n– the spread\\n– the shape (symmetry, skewness, peaks)\\n\\n• and deviations from the pattern\\n\\n– outliers\\n– gaps\\n\\nThe direction of the skewness is the direction of the longer of the two tails (left or right) of the distribution.\\nNo one choice for the number of bins is considered best. One possible choice for larger data sets is Sturges’\\n\\nformula to choose (cid:98)1 + log2 n(cid:99) bins. ((cid:98)·(cid:99), the ﬂoor function, is obtained by rounding down to the next integer.)\\nExercise 1.12. The histograms in Figure 1.4 shows the distribution of lengths of a normal strain and mutant strain of\\nBacillus subtilis. Describe the distributions.\\n\\nExample 1.13. Taking the age of the presidents of the United States at the time of their inauguration and creating its\\nhistogram, empirical cumulative distribution function and boxplot in R is accomplished as follows.\\n\\n10\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nFigure 1.4: Histogram of lengths of Bacillus subtilis. Solid lines indicate wild type and dashed line mutant strain.\\n\\n> age<- c(57,61,57,57,58,57,61,54,68,51,49,64,50,48,65,52,56,46,54,49,51,47,55,55,\\n54,42,51,56,55,51,54,51,60,61,43,55,56,61,52,69,64,46,54,47,70)\\n> par(mfrow=c(1,2))\\n> hist(age)\\n> plot(ecdf(age),xlab=\"age\",main=\"Age of Presidents at the Time of Inauguaration\",\\n\\nsub=\"Empriical Cumulative Distribution Function\")\\n\\nSo the age of presidents at the time of inauguration range from the early forties to the late sixties with the frequency\\nstarting their tenure peaking in the early ﬁfties. The histogram in generally symmetric about 55 years with spread from\\naround 40 to 70 years.\\n\\nThe empirical cumulative distribution function Fn(x) gives, for each value x, the fraction of the data less than\\n\\nor equal to x. If the number of observations is n, then\\n\\nFn(x) =\\n\\n1\\nn\\n\\n#(observations less than or equal to x).\\n\\n11\\n\\nHistogram of ageageFrequency40455055606570051015404550556065700.00.20.40.60.81.0Age of Presidents at InauguarationEmpriical Cumulative Distribution FunctionageFn(x)\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nThus, Fn(x) = 0 for any value of x less than all of the observed values and Fn(x) = 1 for any x greater than\\nall of the observed values. In between, we will see jumps that are multiples of the 1/n. For example, in the empirical\\ncumulative distribution function for the age of the presidents, we will see a jump of size 4/45 at x = 57 to indicate the\\nfact that 4 of the 44 presidents were 57 at the time of their inauguration.\\n\\nFor an alternative method to create a graph of the empirical cumulative distribution function, ﬁrst place the\\nobservations in order from smallest to largest. For the age of presidents data, we can accomplish this in R by writing\\nsort(age). Next match these up with the integral multiples of the 1 over the number of observations. In R, we enter\\n1:length(age)/length(age). Finally, type=\"s\" to give us the steps described above.\\n\\n> plot(sort(age),1:length(age)/length(age),type=\"s\",ylim=c(0,1),\\nmain = c(\"Age of Presidents at the Time of Inauguration\"),\\nsub=(\"Empiricial Cumulative Distribution Function\"),\\nxlab=c(\"age\"),ylab=c(\"cumulative fraction\"))\\n\\nExercise 1.14. Give the fraction of presidents whose age at inauguration was under 60. What is the range for the age\\nat inauguration of the youngest ﬁfth of the presidents?\\n\\nExercise 1.15. The histogram for data on the length of three bacterial strains is shown below. Lengths are given in\\nmicrons. Below the histograms (but not necessarily directly below) are empirical cumulative distribution functions\\ncorresponding to these three histograms.\\n\\nMatch the histograms to their respective empirical cumulative distribution functions.\\n\\nIn looking at life span data, the natural question is “What fraction of the individuals have survived a given length\\nof time?” The survival function Sn(x) gives, for each value x, the fraction of the data greater than or equal to x. If\\nthe number of observations is n, then\\n\\nSn(x) =\\n\\n#(observations greater than x) =\\n\\n1\\nn\\n\\n(n − #(observations less than or equal to x))\\n\\n1\\nn\\n= 1 −\\n\\n1\\nn\\n\\n#(observations less than or equal to x) = 1 − Fn(x)\\n\\n12\\n\\nHistogram of wild1fwild1fFrequency02468051015Histogram of wild2fwild2fFrequency02468051015Histogram of wild3fwild3fFrequency02468051015024680.00.20.40.60.81.0wildafcumulative fraction024680.00.20.40.60.81.0wildbfcumulative fraction024680.00.20.40.60.81.0wildcfcumulative fraction\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\n1.5 Scatterplots\\nWe now consider two dimensional data. The values of the ﬁrst variable x1, x2, . . . , xn are assumed known and in an\\nexperiment and are often set by the experimenter. This variable is called the explanatory, predictor, discriptor, or\\ninput variables and in a two dimensional scatterplot of the data display its values on the horizontal axis. The values\\ny1, y2 . . . , yn, taken from observations with input x1, x2, . . . , xn are called the response or target variable and its\\nvalues are displayed on the vertical axis. In describing a scatterplot, take into consideration\\n\\n• the form, for example,\\n\\n– linear\\n– curved relationships\\n– clusters\\n• the direction,\\n\\n– a positive or negative association\\n\\n• and the strength of the aspects of the scatterplot.\\n\\nExample 1.16. Genetic evolution is based on mutation. Consequently, one fundamental question in evolutionary\\nbiology is the rate of de novo mutations. To investigate this question in humans, Kong et al, sequenced the entire\\ngenomes of 78 Icelandic trios and recorded the age of the parents and the number of de novo mutations in the offspring.\\nThe plot shows a moderate positive linear association, children of older parent have, on average, more mutations.\\nThe number of mutations range from ∼ 40 for children of younger parents to ∼ 100 for children of older parents. We\\nwill later learn that the father is the major source of this difference with age.\\n\\nExample 1.17 (Fossils of the Archeopteryx). The name Archeopteryx derives from the ancient Greek meaning “ancient\\nfeather” or “ancient wing”. Archeopteryx is generally accepted by palaeontologists as being the oldest known bird.\\nArchaeopteryx lived in the Late Jurassic Period around 150 million years ago, in what is now southern Germany\\nduring a time when Europe was an archipelago of islands in a shallow warm tropical sea. The ﬁrst complete specimen\\nof Archaeopteryx was announced in 1861, only two years after Charles Darwin published On the Origin of Species,\\n\\n13\\n\\n2025303540406080100average age of the parentnumber of de novo mutations\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nand thus became a key piece of evidence in the debate over evolution. Below are the lengths in centimeters of the\\nfemur and humerus for the ﬁve specimens of Archeopteryx that have preserved both bones.\\n\\nfemur\\nhumerus\\n\\n38\\n41\\n\\n56\\n63\\n\\n59\\n70\\n\\n64\\n72\\n\\n74\\n84\\n\\n> femur<-c(38,56,59,64,74)\\n> humerus<-c(41,63,70,72,84)\\n> plot(femur, humerus,main=c(\"Bone Lengths for Archeopteryx\"))\\n\\nUnless we have a speciﬁc scientiﬁc question, we have no real reason for a choice of the explanatory variable.\\n\\nDescribe the scatterplot.\\n\\nExample 1.18. This historical data show the 20 largest banks in 1974. Values given in billions of dollars.\\n\\nBank\\nAssets\\nIncome\\nBank\\nAssets\\nIncome\\n\\n1\\n\\n49.0\\n218.8\\n\\n11\\n11.6\\n42.9\\n\\n2\\n\\n42.3\\n265.6\\n\\n12\\n9.5\\n32.4\\n\\n3\\n\\n36.6\\n170.9\\n\\n13\\n9.4\\n68.3\\n\\n4\\n\\n16.4\\n85.9\\n14\\n7.5\\n48.6\\n\\n5\\n\\n14.9\\n88.1\\n15\\n7.2\\n32.2\\n\\n6\\n\\n14.2\\n63.6\\n16\\n6.7\\n42.7\\n\\n7\\n\\n13.5\\n96.9\\n17\\n6.0\\n28.9\\n\\n8\\n\\n13.4\\n60.9\\n18\\n4.6\\n40.7\\n\\n9\\n\\n13.2\\n144.2\\n\\n19\\n3.8\\n13.8\\n\\n10\\n11.8\\n53.6\\n20\\n3.4\\n22.2\\n\\n14\\n\\nlllll40455055606570754050607080Bone Lengths for Archeopteryxfemurhumerus\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nDescribe the scatterplot.\\n\\nIn 1972, Michele Sindona, a banker with close ties to the Maﬁa, along with a purportedly bogus Freemasonic\\nlodge, and the Nixon administration purchased controlling interest in Bank 19, Long Island’s Franklin National Bank.\\nAs a result of his acquisition of a controlling stake in Franklin, Sindona had a money laundering operation to aid his\\nalleged ties to Vatican Bank and the Sicilian drug cartel. Sindona used the bank’s ability to transfer funds, produce\\nletters of credit, and trade in foreign currencies to begin building a banking empire in the United States. In mid-1974,\\nmanagement revealed huge losses and depositors started taking out large withdrawals, causing the bank to have to\\nborrow over $1 billion from the Federal Reserve Bank. On 8 October 1974, the bank was declared insolvent due to\\nmismanagement and fraud, involving losses in foreign currency speculation and poor loan policies.\\n\\nWhat would you expect to be a feature on this scatterplot of a failing bank? Does the Franklin Bank have this\\n\\nfeature?\\n\\n1.6 Time Plots\\n\\nSome data sets come with an order of events, say ordered by time.\\n\\nExample 1.19. The modern history of petroleum began in the 19th century with the reﬁning of kerosene from crude oil.\\nThe world’s ﬁrst commercial oil wells were drilled in the 1850s in Poland and in Romania.The ﬁrst oil well in North\\nAmerica was in Oil Springs, Ontario, Canada in 1858. The US petroleum industry began with Edwin Drake’s drilling\\nof a 69-foot deep oil well in 1859 on Oil Creek near Titusville, Pennsylvania for the Seneca Oil Company. The industry\\ngrew through the 1800s, driven by the demand for kerosene and oil lamps. The introduction of the internal combustion\\nengine in the early part of the 20th century provided a demand that has largely sustained the industry to this day.\\nToday, about 90% of vehicular fuel needs are met by oil. Petroleum also makes up 40% of total energy consumption\\nin the United States, but is responsible for only 2% of electricity generation. Oil use increased exponentially until the\\nworld oil crises of the 1970s.\\n\\n15\\n\\nllllllllllllllllllll102030405050100150200250Income vs. Assets (in billions of dollars)assetsincome\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nWorldwide Oil Production\\n\\nMillion\\nYear Barrels\\n30\\n1880\\n1890\\n77\\n149\\n1900\\n215\\n1905\\n1910\\n328\\n432\\n1915\\n689\\n1920\\n1069\\n1925\\n1930\\n1412\\n1655\\n1935\\n\\nMillion\\nYear Barrels\\n2150\\n1940\\n2595\\n1945\\n3803\\n1950\\n5626\\n1955\\n1960\\n7674\\n8882\\n1962\\n10310\\n1964\\n12016\\n1966\\n1968\\n14014\\n16690\\n1970\\n\\nMillion\\nYear Barrels\\n18584\\n1972\\n20389\\n1974\\n20188\\n1976\\n21922\\n1978\\n1980\\n21722\\n19411\\n1982\\n19837\\n1984\\n20246\\n1986\\n1988\\n21338\\n\\nWith the data given in two columns oil and year, the time plot plot(year,oil,type=\"b\") is given on\\n\\nthe left side of the ﬁgure below. This uses type=\"b\" that puts both lines and circles on the plot.\\n\\nFigure 1.5: Oil production (left) and the logarithm of oil production (right) from 1880 to 1988.\\n\\nSometimes a transformation of the data can reveal the structure of the time series. For example, if we wish to\\nexamine an exponential increase displayed in the oil production plot, then we can take the base 10 logarithm of the\\nproduction and give its time series plot. This is shown in the plot on the right above. (In R, we write log(x) for the\\nnatural logarithm and log(x,10) for the base 10 logarithm.)\\n\\nExercise 1.20. What happened in the mid 1970s that resulted in the long term departure from exponential growth in\\nthe use of oil?\\n\\n16\\n\\nlllllllllllllllllllllllllllll18801900192019401960198005101520World Oil Productionyearbillions of barrelslllllllllllllllllllllllllllll188019001920194019601980−1.5−1.0−0.50.00.51.0World Oil Productionyearlog(billions of barrels)\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nExample 1.21. The Intergovernmental Panel on Climate Change (IPCC) is a scientiﬁc intergovernmental body tasked\\nwith evaluating the risk of climate change caused by human activity. The panel was established in 1988 by the\\nWorld Meteorological Organization and the United Nations Environment Programme, two organizations of the United\\nNations. The IPCC does not perform original research but rather uses three working groups who synthesize research\\nand prepare a report. In addition, the IPCC prepares a summary report. The Fourth Assessment Report (AR4) was\\ncompleted in early 2007. The ﬁfth was released in 2014.\\n\\nBelow is the ﬁrst graph from the 2007 Climate Change Synthesis Report: Summary for Policymakers.\\nThe technique used to draw the curves on the graphs is called local regression. At the risk of discussing concepts\\nthat have not yet been introduced, let’s describe the technique behind local regression. Typically, at each point in the\\ndata set, the goal is to draw a linear or quadratic function. The function is determined using weighted least squares,\\ngiving most weight to nearby points and less weight to points further away. The graphs above show the approximating\\ncurves. The blue regions show areas within two standard deviations of the estimate (called a conﬁdence interval). The\\ngoal of local regression is to provide a smooth approximation to the data and a sense of the uncertainty of the data. In\\npractice, local regression requires a large data set to work well.\\n\\n17\\n\\n \\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\nExample 1.22. The next ﬁgure give a time series plot of a single molecule experiment showing the movement of kinesin\\nalong a microtubule. In this case the kinesin has at its foot a glass bead and its heads are attached to a microtubule.\\nThe position of the glass bead is determined by using a laser beam and the optical properties of the bead to locate the\\nbead and provide a force on the kinesin molecule. In this time plot, the load on the microtubule has a force of 3.5 pN\\nand the concentration of ATP is 100µM. What is the source of ﬂuctuations in this time series plot of bead position?\\nHow would you expect this time plot to change with changes in ATP concentration and with changes in force?\\n\\n1.7 Answers to Selected Exercises\\n1.11. Here are the R commands:\\n\\n> genotypes<-matrix(c(128,6,0,119,78,4),ncol=2)\\n> colnames(genotypes)<-c(\"Flores\",\"Sumba\")\\n> rownames(genotypes)<-c(\"AA\",\"AE\",\"EE\")\\n> genotypes\\n\\nFlores Sumba\\n119\\n78\\n4\\n\\n128\\n6\\n0\\n\\nAA\\nAE\\nEE\\n> barplot(genotypes,legend=rownames(genotypes),args.legend=list(x=\"topleft\"))\\n\\nThe legend was moved to the left side to avoid crowding with the taller bar for the data on Sumba.\\n\\n18\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDisplaying Data\\n\\n1.12. The lengths of the normal strain has its center at 2.5 microns and range from 1.5 to 5 microns. It is somewhat\\nskewed right with no outliers. The mutant strain has its center at 5 or 6 microns. Its range is from 2 to 14 microns and\\nit is slightly skewed right. It has not outliers.\\n1.14. Look at the graph to the point above the value 60 years. Look left from this point to note that it corresponds to a\\nvalue of 0.80.\\n\\nLook at the graph to the point right from the value 0.20. Look down to note that it corresponds to 49 years. .\\n\\n1.15. Match histogram wild1f to wilddaf. Note that both show the range is from 2 to 5 microns and that about half of\\nthe data lies between 2 and 3 microns. Match histogram wild2f with wildcf. The data is relatively uniform from 3.5\\nto 6.5 microns. Finally, match histogram wild3f with wildbf. The range is from 2 to 8 microns with most of the data\\nbetween 3 and 6 microns. .\\n1.22. The ﬂuctuation are due to the many bombardments with other molecules in the cell, most frequently, water\\nmolecules.\\n\\nAs force increases, we expect the velocity to increase - to a point. If the force is too large, then the kinesin is ripped\\naway from the microtubule. As ATP concentration increases, we expect the velocity to increase - again, to a point. If\\nATP concentration is sufﬁciently large, then the biochemical processes are saturated.\\n\\n19\\n\\nFloresSumbaEEAEAA050100150200\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\n20\\n\\n\\x0cTopic 2\\n\\nDescribing Distributions with Numbers\\n\\nThere are three kinds of lies: lies, damned lies, and statistics. - Benjamin Disraeli\\n\\nIt is easy to lie with statistics. It is hard to tell the truth without it. - Andrejs Dunkels\\n\\nWe next look at quantitative data. Recall that in this case, these data can be subject to the operations of arithmetic.\\n\\nIn particular, we can add or subtract observation values, we can sort them and rank them from lowest to highest.\\n\\nWe will look at two fundamental properties of these observations. The ﬁrst is a measure of the center value for\\nthe data, i.e., the median or the mean. Associated to this measure, we add a second value that describes how these\\nobservations are spread or dispersed about this given measure of center.\\n\\nThe median is the central observation of the data after it is sorted from the lowest to highest observations. In\\naddition, to give a sense of the spread in the data, we often give the smallest and largest observations as well as the\\nobserved value that is 1/4 and 3/4 of the way up this list, known at the ﬁrst and third quartiles. This difference, known\\nas the interquartile range is a measure of the spread or the dispersion of the data. For the mean, we commonly use\\nthe standard deviation to describe the spread of the data.\\n\\nThese concepts are described in more detail in this section.\\n\\n2.1 Measuring Center\\n2.1.1 Medians\\nThe median take the middle value for x1, x2, . . . , xn after the data has been sorted from smallest to largest,\\n\\nx(1), x(2), . . . , x(n).\\n\\n(x(k) is called the k-th order statistic. Sorting can be accomplished in R by using the sort command.)\\nIf n is odd, then this is just the value of the middle observation x((n+1)/2). If n is even, then the two values closest\\n\\nto the center are averaged.\\n\\n1\\n2\\n\\n(x(n/2) + x(n/2+1)).\\n\\nIf we store the data in R in a vector x, we can write median(x) to compute the median.\\n\\n2.1.2 Means\\nFor a collection of numeric data, x1, x2, . . . , xn, the sample mean is the numerical average\\n\\n¯x =\\n\\n1\\nn\\n\\n(x1 + x2 + . . . + xn) =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nxi\\n\\n21\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\nAlternatively, if the value x occurs n(x) times in the data, then use the distributive property to see that\\n\\n¯x =\\n\\n1\\n\\nn(cid:88)x\\n\\nxn(x) =(cid:88)x\\n\\nxp(x), where p(x) =\\n\\nn(x)\\n\\nn\\n\\n.\\n\\nSo the mean ¯x depends only on the proportion of observations p(x) for each value of x.\\n\\nExample 2.1. For the data set {1, 2, 2, 2, 3, 3, 4, 4, 4, 5}, we have n = 10 and the sum\\n\\n1 + 2 + 2 + 2 + 3 + 3 + 4 + 4 + 4 + 5 = 1n(1) + 2n(2) + 3n(3) + 4n(4) + 5n(5)\\n\\n= 1(1) + 2(3) + 3(2) + 4(3) + 5(1) = 30\\n\\nThus, ¯x = 30/10 = 3.\\n\\nExample 2.2. For the data on the length in microns of wild type Bacillus subtilis data, we have\\n\\nlength x\\n\\nfrequency n(x)\\n\\nproportion p(x)\\n\\nproduct xp(x)\\n\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\nsum\\n\\n18\\n71\\n48\\n37\\n16\\n6\\n4\\n200\\n\\n0.090\\n0.355\\n0.240\\n0.185\\n0.080\\n0.030\\n0.020\\n\\n1\\n\\n0.135\\n0.710\\n0.600\\n0.555\\n0.280\\n0.120\\n0.090\\n2.490\\n\\nSo the sample mean ¯x = 2.49.\\n\\nIf we store the data in R in a vector x, we can write mean(x) which is equal to sum(x)/length(x) to\\n\\ncompute the mean.\\n\\nthen\\n\\nTo extend this idea a bit, we can take a real-valued function h and instead consider the observations h(x1), h(x2), . . . , h(xn),\\n\\nh(x) =\\n\\n1\\nn\\n\\n(h(x1) + h(x2) + . . . + h(xn)) =\\n\\n1\\nn\\n\\nh(xi) =\\n\\nn(cid:88)i=1\\n\\n1\\n\\nn(cid:88)x\\n\\nh(x)n(x) =(cid:88)x\\n\\nh(x)p(x).\\n\\nExercise 2.3. Let ¯xn be the sample mean for the quantitative data x1, x2, . . . , xn. For an additional observation\\nxn+1, use ¯x to give a formula for ¯xn+1, the mean of n + 1 observations. Generalize this formula for the case of k\\nadditional observations xn+1 . . . , xn+k\\n\\nMany times, we do not want to give the same weight to each observation. For example, in computing a student’s\\ngrade point average, we begin by setting values xi corresponding to grades ( A (cid:55)→ 4, B (cid:55)→ 3 and so on) and giving\\nweights w1, w2, . . . , wn equal to the number of units in a course. We then compute the grade point average as a\\nweighted mean. To do this:\\n\\n• Multiply the value of each course by its weight xiwi. This is called the number of quality points for the course.\\n• Add up the quality points:\\n\\nx1w1 + x2w2 + . . . + xnwn =\\n\\n• Add up the weights, i. e., the number of units attempted:\\n\\nxiwi\\n\\nn(cid:88)i=1\\n\\nw1 + w2 + . . . + wn =\\n\\nwi\\n\\nn(cid:88)i=1\\n\\n22\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\nFigure 2.1: Empirical Survival Function for the Bacterial Data. This ﬁgure displays how the area under the survival function to the right of the\\ny-axis and above the x-axis is the mean value ¯x for non-negative data. For x = 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, and 4.5. This area is the sum of the area\\nof the retangles displayed. The width of each of the rectangles is x and the height is equal to p(x). Thus, the area is the product xp(x). The sum of\\nthese areas are presented in Example 2.2 to compute the sample mean.\\n\\n• Divide the total quality points by the number of units attempted:\\nx1w1 + x2w2 + . . . + xnwn\\n\\nw1 + w2 + . . . + wn\\n\\nIf we let\\n\\npj = wj/\\n\\ni=1 xiwi\\ni=1 wi\\n\\n= (cid:80)n\\n(cid:80)n\\n\\nwi\\n\\nn(cid:88)i=1\\n\\n.\\n\\n(2.1)\\n\\nbe the proportion or fraction of the weight given to the j-th observation, then we can rewrite (2.1) as\\n\\nIf we store the weights in a vector w, then we can compute the weighted mean using weighted.mean(x,w)\\n\\nxipi.\\n\\nn(cid:88)i=1\\n\\nIf an extremely high observation is changed to be even higher, then the mean follows this change while the median\\ndoes not. For this reason, the mean is said to be sensitive to outliers while the median is not. To reduce the impact of\\nextreme outliers on the mean as a measure of center, we can also consider a truncated mean or trimmed mean. The\\np trimmed mean is obtained by discarding both the lower and the upper p× 100% of the data and taking the arithmetic\\nmean of the remaining data.\\nIn R, we write mean(x, trim = p) where p, a number between 0 and 0.5, is the fraction of observations to\\n\\nbe trimmed from each end before the mean is computed.\\n\\nNote that the median can be regarded as the 50% trimmed mean. The median does not change with a changes in\\nthe extreme observations. Such a property is called a resistant measure. On the other hand, the mean is not a resistant\\nmeasure.\\n\\n23\\n\\n00.511.522.533.544.55−0.200.20.40.60.811.5 p(1.5)2.0 p(2.0)2.5 p(2.5)3.0 p(3.0)3.5 p(3.5)4.0 p(4.0)4.5 p(4.5)\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\nExercise 2.4. Give the relationship between the median and the mean for a (a) left skewed, (b) symmetric, or (c) right\\nskewed distribution.\\n\\n2.2 Measuring Spread\\n2.2.1 Five Number Summary\\nThe ﬁrst and third quartile, Q1 and Q3, are, respectively, the median of the lower half and the upper half of the data.\\nThe ﬁve number summary of the data are the values of the minimum, Q1, the median, Q3 and the maximum. These\\nvalues, along with the mean, are given in R using summary(x). Returning to the data set on the age of presidents:\\n\\n> summary(age)\\n\\nMin. 1st Qu. Median\\n55.00\\n\\n51.00\\n\\n42.00\\n\\nMean 3rd Qu.\\n58.00\\n\\n54.98\\n\\nMax.\\n70.00\\n\\nWe can display the ﬁve number summary using a boxplot.\\n\\n> boxplot(age, main = c(\"Age of Presidents at the Time of Inauguration\"))\\n\\nThe value Q3 − Q1 is called the interquartile range and is denoted by IQR. It is found in R with the command IQR.\\nOutliers are somewhat arbitrarily chosen to be those above Q3 + 3\\n2 IQR. With this criterion,\\nthe ages of Ronald Reagan and Donald Trump, considered outliers, are displayed by the two circles at the top of the\\nboxplot. The boxplot command has the default value range = 1.5 in the choice of displaying outliers. This can\\nbe altered to loosen or tighten this criterion.\\n\\n2 IQR and below Q1 − 3\\n\\nExercise 2.5. Use the range command to create a boxplot for the age of the presidents at the time of their inaugu-\\nration using as outliers any value above Q3 + IQR and below Q1 − IQR as the criterion for outliers. How many\\noutliers does this boxplot have?\\n\\n24\\n\\n455055606570Age of Presidents at the Time of Inauguration\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\nExample 2.6. Consider a two column data set. Column 1 - MPH - gives car gas milage. Column 2 - origin - gives\\nthe country of origin for the car. We can create side by side boxplots with the command\\n\\n> boxplot(MPG,Origin)\\n\\nto produce\\n\\n2.2.2 Sample Variance and Standard Deviation\\nThe sample variance averages the square of the differences from the mean\\n\\nvar(x) = s2\\n\\nx =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2.\\n\\nThe sample standard deviation, sx, is the square root of the sample variance. We shall soon learn the rationale for\\nthe decision to divide by n − 1. However, we shall also encounter circumstances in which division by n is preferable.\\nWe will routinely drop the subscript x and write s to denote standard deviation if there is no ambiguity.\\n\\nExample 2.7. For the data set on Bacillus subtilis data, we have ¯x = 498/200 = 2.49\\n\\nlength, x\\n\\nfrequency, n(x)\\n\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\nsum\\n\\n18\\n71\\n48\\n37\\n16\\n6\\n4\\n200\\n\\nx − ¯x\\n-0.99\\n-0.49\\n0.01\\n0.51\\n1.01\\n1.51\\n2.01\\n\\n(x − ¯x)2\\n0.9801\\n0.2401\\n0.0001\\n0.2601\\n1.0201\\n2.2801\\n4.0401\\n\\n(x − ¯x)2n(x)\\n17.6418\\n17.0471\\n0.0048\\n9.6237\\n16.3216\\n13.6806\\n16.1604\\n90.4800\\n\\nSo the sample variance s2\\n\\nx = 90.48/199 = 0.4546734 and standard deviation sx = 0.6742947. To accomplish\\n\\nthis in R\\n\\n25\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\n> bacteria<-c(rep(1.5,18),rep(2.0,71),rep(2.5,48),rep(3,37),rep(3.5,16),rep(4,6),\\n+ rep(4.5,4))\\n> length(bacteria)\\n[1] 200\\n> mean(bacteria)\\n[1] 2.49\\n> var(bacteria)\\n[1] 0.4546734\\n> sd(bacteria)\\n[1] 0.6742947\\n\\nFor quantitative variables that take on positive values, we can take the ratio of the standard deviation to the mean\\n\\ncalled the coefﬁcient of variation as a measure of the relative variability of the observations. Note that cvx is a pure\\nnumber and has no units.\\n\\nFor the data of bacteria lengths, the coefﬁcient of variability is\\n\\ncvx =\\n\\nsx\\n¯x\\n\\n,\\n\\ncvx =\\n\\n0.6742947\\n\\n2.49\\n\\n= 0.2708011,\\n\\nExercise 2.8. Show that(cid:80)n\\nWe now begin to describe the rationale for the division by n − 1 rather than n in the deﬁnition of the variance. To\\n\\ni=1(xi − ¯x) = 0.\\n\\nintroduce the next exercise, deﬁne the sum of squares about the value α,\\n\\nSS(α) =\\n\\n(xi − α)2.\\n\\nn(cid:88)i=1\\n\\nExercise 2.9. Flip a fair coin 16 times, recording the number of heads. Repeat this activity 20 times, giving x1, . . . , x20\\nheads. Our instincts say that the mean should be 8. Compute SS(8). Next ﬁnd ¯x for the data you generated and\\ncompute SS(¯x). Notice that SS(8) > SS(¯x).\\n\\nNote that in repeating the experiment of ﬂipping a fair coin 16 times and recording the number of heads, we would\\nlike to compute the variation about 8, the value that our intuition tells us is the true mean. In many circumstances, we\\ndo not have such intuition. Thus, we doing the best we can by computing ¯x, the mean from the data. In this case, the\\nvariation about the sample mean is smaller than the variation about what may be called a true mean. Thus, division\\nof(cid:80)n\\ni=1(xi − ¯x)2 by n systematically underestimates the variance. The deﬁnition of sample variance is based on the\\nfact that this can be compensated for this by dividing by something small than n. We will learn why the appropriate\\nchoice is n − 1 when we investigate Unbiased Estimation in Topic 13.\\n\\nTo show that the phenomena in Exercise 2.9 is true more broadly, we next perform a little algebra. This is similar\\nto the computation of the parallel axis theorem in physics. The parallel axis theorem is used to determine the moment\\nof inertia of a rigid body about any axis, given the moment of inertia of the object about the parallel axis through the\\nobject’s center of mass (¯x) and the perpendicular distance between the axes. In this case, we a looking at the rigid\\nmotion of a ﬁnite number of equal point masses.\\n\\nIn the formula for SS(α), divide the difference in the value of each observation xi to the value α into the difference\\n\\nto the sample mean ¯x and then the distance from the sample mean to α (i.e. ¯x − α).\\n\\nSS(α) =\\n\\n=\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\n(xi − ¯x)2 +\\n\\nn(cid:88)i=1\\n\\n((xi − ¯x) + (¯x − α))2 =\\n\\n(xi − ¯x)2 + 2\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)(¯x − α) +\\n\\nn(cid:88)i=1\\n\\n(¯x − α)2\\n\\n(¯x − α)2 =\\n\\n(xi − ¯x)2 + n(¯x − α)2.\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\n26\\n\\n\\x0cn(cid:88)i=1\\n\\nn(cid:88)i=1\\ni − n¯x2(cid:33) .\\n\\nx2\\n\\nIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\nBy Exercise 2.8, the cross term above 2(cid:80)n\\n\\ni=1(xi − ¯x)(¯x − α) equals to zero. Thus, we have partitioned the sums of\\nsquares into two levels. The ﬁrst term gives the sums of squares about the sample mean ¯x. The second gives square of\\nthe difference between ¯x and the chosen value α. We shall see this idea of partitioning in other contexts.\\n\\nNote that the minimum value of SS(α) can be obtained by minimizing the second term. This takes place at α = ¯x.\\n\\nThus,\\n\\nmin\\n\\nα\\n\\nSS(α) = SS(¯x) =\\n\\n(xi − ¯x)2.\\n\\nOur second use for this identity provides an alternative method to compute the variance. Take α = 0 to see that\\n\\nSS(0) =\\n\\nx2\\ni =\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2 + n¯x2. Thus,\\n\\n(xi − ¯x)2 =\\n\\nDivide by n − 1 to see that\\n\\ns2 =\\n\\n1\\n\\nn − 1(cid:32) n(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\nx2\\ni − n¯x2.\\n\\n(2.2)\\n\\nExercise 2.10. The following formulas may be useful in aggregating data. Suppose you have data sets collected on\\ntwo consecutive days with the following summary statistics.\\n\\nnumber of\\nobservations mean\\n\\nstandard\\ndeviation\\n\\nn1\\nn2\\n\\n¯x1\\n¯x2\\n\\ns1\\ns2\\n\\nday\\n1\\n2\\n\\nNow combine the observations of the two days and use this to show that the combined mean\\n\\nand the combined variance\\n\\n¯x =\\n\\nn1 ¯x1 + n2 ¯x2\\n\\nn1 + n2\\n\\ns2 =\\n\\n(Hint: Use (2.2)).\\n\\n1\\n\\nn1 + n2 − 1(cid:0)(n1 − 1)s2\\n\\n1 + n1 ¯x2\\n\\n1 + (n2 − 1)s2\\n\\n2 + n2 ¯x2\\n\\n2 − (n1 + n2)¯x2(cid:1) .\\n\\nExercise 2.11. For the data set x1, x2, . . . , xn, let\\n\\nyi = a + bxi.\\n\\nGive the summary statistics for the y data set given the corresponding values of the x data set. (Consider carefully the\\nconsequences of the fact that a might be less than 0.)\\n\\nAmong these, the quadratic identity\\n\\nvar(x + bx) = b2var(x)\\n\\nis one of the most frequently used and useful in all of statistics.\\n\\n2.3 Quantiles and Standardized Variables\\nA single observation, say 87 on a exam, gives little information about the performance on the exam. One way to\\ninclude more about this observation would be to give the value of the empirical cumulative distribution function.\\nThus,\\n\\nFn(87) = 0.7223\\n\\n27\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\ntells us that about 72% of the exam scores were below 87. This is sometimes reported by saying that 87 is the 0.7223\\nquantile for the exam scores.\\n\\nWe can determine this value using the R command quantile. For the ages of presidents at inauguration, we\\n\\nhave that the 72% quantile is 57 year old.\\n\\n> quantile(age,0.72)\\n72%\\n57\\n\\nThus, for example, for the ages of the president, we have that IQR(age) can also be computed using the command\\nquantile(age,3/4) - quantile(age,1/4). R returns the value 7. The quantile command on its own\\nreturns the ﬁve number summary.\\n\\n0%\\n42\\n\\n25% 50% 75% 100%\\n70\\n\\n55\\n\\n51\\n\\n58\\n\\nAnother, and perhaps more common use of the term quantiles is a general term for partitioning ranked data into\\nequal parts. For example, quartiles partitions the data into 4 equal parts. Percentiles partitions the data into 100 equal\\nparts. Thus, the k-th q-tile is the value in the data for which k/q of the values are below the given value. This naturally\\nleads to some rounding issues which leads to a large variety of small differences in the deﬁnition of quantiles.\\n\\nExercise 2.12. For the example above, describe the quintile, decile, and percentile of the observation 87.\\n\\nA second way to evaluate a score of 87 is to related it to the mean. Thus, if the mean ¯x = 76. Then, we might say\\nthat the exam score is 11 points above the mean. If the scores are quite spread out, then 11 points above the mean is\\njust a little above average. If the scores are quite tightly spread, then 11 points is quite a bit above average. Thus, for\\ncomparisons, we will sometimes use the standardized version of xi,\\n\\nzi =\\n\\nxi − ¯x\\nsx\\n\\n.\\n\\nThe observations zi have mean 0 and standard deviation 1. The value zi is also called the standard score , the z-value,\\nthe z-score, and the normal score. An individual z-score, zi, gives the number of standard deviations an observation\\nxi is above (or below) the mean.\\n\\nThe R command scale transforms the data to the standard score. For the ages of the presidents, we use the scale\\ncommand to show the standardized ages. The head command show the ﬁrst 6 rows of the output for presidents from\\nGeorge Washington to John Qunicy Adams.\\n\\n> head(data.frame(scale(age),(age-mean(age))/sd(age)))\\n\\nscale.age. X.age...mean.age...sd.age.\\n0.3076569\\n0.9162091\\n0.3076569\\n0.3076569\\n0.4597950\\n0.3076569\\n\\n0.3076569\\n0.9162091\\n0.3076569\\n0.3076569\\n0.4597950\\n0.3076569\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n\\nExercise 2.13. What are the units of the standard score? What is the relationship of the standard score of an obser-\\nvation xi and yi = axi + b?\\n\\n2.4 Quantile-Quantile Plots\\nIn addition to side by side boxplots or histograms, we can also compare two cumulative distribution function directly\\nwith the quantile-quantile or Q-Q plot. If the quantitative data sets x and y have the same number of observations,\\n\\n28\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\nFigure 2.2: age of ﬁrst seizure (left) side-by-side boxplots, (center) empirical cumulative distribution functions, (right) Q-Q plot with Q1, the\\nmedian, and Q3 indicated by the solid red dots. The solid line on the plot has intercept 0 and slope 1. (missense age=nonsense age)\\n\\nthen this is simply plot(sort(x),sort(y)). In this case the Q-Q plot matches each of the quantiles for the two\\ndata sets. If the data sets have an unequal number of observations, then observations from the larger data are reduced\\nby interpolation to create data sets of equal length and the Q-Q plot is plot(sort(xred),sort(yred)) for the\\nreduced data sets xred and yred.\\n\\nExample 2.14. Dravet syndrome, also known as Severe Myoclonic Epilepsy of Infancy (SMEI), is a rare and catas-\\ntrophic form of intractable epilepsy that begins in infancy. A recent study looks at de novo mutations in the DNA\\nsequence SCN1A that codes for a sodium channel protein. An improperly functioning sodium channel can have severe\\nconsequences for brain function.\\n\\nThe two basic types of mutations under study are point mutations, called missense and nonsense mutations. A\\nmissense mutation results in a change in an amino acid in the SCN1A protein, whereas a nonsense mutation results in\\na truncated, incomplete, and usually nonfunctional protein segment that is degraded.\\n\\nHere is the age of ﬁrst seizure in a study of 264 Japanese children. Age is in months.\\n\\nage\\n\\nmissense\\nnonsense\\n\\n0\\n0\\n1\\n\\n1\\n1\\n1\\n\\n2\\n6\\n7\\n\\n3\\n15\\n18\\n\\n4\\n20\\n30\\n\\n5\\n21\\n27\\n\\n6\\n21\\n24\\n\\n7\\n9\\n16\\n\\n8\\n8\\n11\\n\\n9\\n5\\n3\\n\\n10\\n7\\n2\\n\\n11\\n3\\n0\\n\\n12\\n2\\n2\\n\\n13\\n1\\n1\\n\\n14\\n0\\n0\\n\\n15\\n1\\n1\\n\\nWe enter the data into R using rep, the repeat command and prepare a summary.\\n\\n> missense<-c(1,rep(2,6),rep(3,15),rep(4,20),rep(5,21),rep(6,21),rep(7,9),rep(8,8),\\n\\nrep(9,5),rep(10,7),11,11,11,12,12,13,15)\\n\\n> nonsense<-c(0,1,rep(2,7),rep(3,18),rep(4,30),rep(5,27),rep(6,24),rep(7,16),\\n\\nrep(8,11),rep(9,3),10,10,12,12,13,15)\\n\\n> summary(missense)\\n\\nMin. 1st Qu. Median\\n5.0\\n\\n4.0\\n\\n1.0\\n\\n> summary(nonsense)\\n\\nMin. 1st Qu. Median\\n5.000\\n\\n4.000\\n\\n0.000\\n\\nMean 3rd Qu.\\n7.0\\n\\n5.8\\n\\nMax.\\n15.0\\n\\nMean 3rd Qu.\\n6.250\\n\\n5.326\\n\\nMax.\\n15.000\\n\\n29\\n\\nmissensenonsense0510150510150.00.20.40.60.81.0age in monthsfraction below0510150.00.20.40.60.81.0missensenonsense051015051015missensenonsense051015051015\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\nThe side-by-side boxplots, empirical cumulative distribution functions, and the Q-Q plot . The R code is below.\\n\\n> par(mfrow=c(1,3))\\n> boxplot(missense,nonsense,names=c(\"missense\",\"nonsense\"))\\n> plot(sort(missense),1:length(missense)/length(missense),type=\"s\",\\n\\nxlab=c(\"age in months\"),ylab=c(\"fraction below\"),xlim=c(0,15),\\nylim=c(0,1),col=\"red\")\\n\\n> par(new=TRUE)\\n> plot(sort(nonsense),1:length(nonsense)/length(nonsense),type=\"s\",\\n\\nxlab=c(\"\"), ylab=c(\"\"),xlim=c(0,15),ylim=c(0,1),col=\"blue\")\\n\\n> legend(\"topleft\",c(\"missense\",\"nonsense\"),fill=c(\"red\",\"blue\"))\\n> qqplot(missense,nonsense,xlim=c(0,15),ylim=c(0,15))\\n> abline(a=0,b=1)\\n\\nThe points on the Q-Q plot indicate values having equal quantiles for the age of ﬁrst seizure. The command\\nabline(a=0,b=1) adds the line through the origin of slope 1. If the points on the Q-Q plot are generally above the\\nline, then the vertical axis variable (nonsense) have larger values. Correspondingly, if the points are generally below\\nthe line, then the horizontal axis variable (missense) have larger values.\\n\\nTo see the ﬁrst and third quartiles, Q1 and Q3 as well as the median, we use the points and the quantile\\n\\ncommands.\\n\\n> q<-c(0.25,0.50,0.75)\\n> points(quantile(missense,q),quantile(nonsense,q),col=\"red\",pch=19)\\n\\nThe points command was used to add the three solid red dots. Moving from lower left to upper right, dots\\n\\ncoordinates that are the the values for Q1, (4, 4), the median, (5, 5), and Q3, (7, 6.25), for the two data sets.\\nExercise 2.15. The mean time of ﬁrst seizure is slightly higher for patients with a missense mutation. Explain how\\nthis can be seen for each of the plots that compare the two data sets.\\n\\n2.5 Answers to Selected Exercises\\n2.3. Check the formula\\n\\n¯xn+1 =\\n\\nn\\n\\nn + 1\\n\\n¯xn +\\n\\n1\\n\\nn + 1\\n\\nxn+1.\\n\\nFor k additional observations, write\\n\\n¯xn,k+n =\\n\\nThen the mean of the n + k observations is\\n\\n(xn+1 + ··· + xn+k).\\n\\n1\\nk\\n\\nn\\n\\n¯xn+k =\\n\\n¯xn +\\n\\nk\\n\\nn + k\\n\\n¯xn,k+n.\\n\\nn + k\\n\\n2.4 (a) If the distribution is skewed left, then the mean follows the tail and is less than the median. (b) For a symmetric\\ndistribution, the mean and the median are equal. (c) If the distribution is skewed right, then the mean is greater than\\nthe median.\\n2.5. The boxplot has 5 outliers. Three are above Q3 + IQR and two are below Q3 − IQR.\\n2.8. Divide the sum into 2 terms.\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x) =\\n\\nn(cid:88)i=1\\n\\nxi − n¯x = n(cid:32) 1\\n\\nn\\n\\nxi − ¯x(cid:33) = 0.\\n\\nn(cid:88)i=1\\n\\n30\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\n2.9. This can be obtained by ﬂipping coins. In R, we shall learn that the command to simulate this is rbinom(20,16,0.5).\\nHere are the data. Focusing on the ﬁrst three columns, we see a total of 166 heads in the 20 observations. Thus,\\n¯x = 8.3.\\n\\nheads\\n\\ncounts\\n\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\nsum\\n\\n1\\n1\\n2\\n2\\n5\\n3\\n3\\n2\\n1\\n20\\n\\nheats×counts\\n\\n4\\n5\\n12\\n14\\n40\\n27\\n30\\n22\\n12\\n166\\n\\ncounts×(heads-8)2\\n1 · (4 − 8)2 = 16\\n1 · (5 − 8)2 = 9\\n2 · (6 − 8)2 = 8\\n2 · (7 − 8)2 = 2\\n5 · (8 − 8)2 = 0\\n3 · (9 − 8)2 = 3\\n3 · (10 − 8)2 = 12\\n2 · (11 − 8)2 = 18\\n1 · (12 − 8)2 = 16\\n\\nSS(8) = 84\\n\\ncounts×(heads-¯x)2\\n1 · (4 − 8.3)2 = 18.49\\n1 · (5 − 8.3)2 = 10.89\\n2 · (6 − 8.3)2 = 10.58\\n2 · (7 − 8.3)2 = 5.07\\n5 · (8 − 8.3)2 = 0.45\\n3 · (9 − 8.3)2 = 1.47\\n3 · (10 − 8.3)2 = 8.67\\n2 · (11 − 8.3)2 = 14.58\\n1 · (12 − 8.3)2 = 13.69\\n\\nSS(α) = 82.2\\n\\nNotice that SS(8) > SS(¯x).\\n\\n2.10. Let x1,1, x1,2 . . . , x1,n1 denote the observed values on day 1 and x2,1, x2,2 . . . , x2,n2 denote the observed values\\non day 2. The mean of the combined data\\n\\n¯x =\\n\\nUsing (2.2), we ﬁnd that\\n\\n1\\n\\nx1,i +\\n\\nn1 + n2(cid:32) n1(cid:88)i=1\\nn2(cid:88)i=1\\nn1 + n2 − 1(cid:32) n1(cid:88)i=1\\n\\n1\\n\\ns2 =\\n\\nx2,i(cid:33) =\\n\\n1\\n\\nn1 + n2\\n\\n(n1 ¯x1 + n2 ¯x2)\\n\\nx2\\n1,i +\\n\\n2,i − (n1 + n2)¯x2(cid:33) .\\n\\nx2\\n\\nn2(cid:88)i=1\\n\\nUse (2.2) twice more to see that\\n\\nn1(cid:88)i=1\\n\\nx2\\n1,i = (n1 − 1)s2\\n\\n1 + n1 ¯x2\\n1\\n\\nand\\n\\nn2(cid:88)i=1\\n\\nx2\\n2,i = (n2 − 1)s2\\n\\n2 + n2 ¯x2\\n2\\n\\nNow substitute the sums in the line above into the equation for s2.\\n\\n2.11.\\n\\nstatistic\\nmedian\\nmean\\n\\nIf mx is the median for the x observations, then a + bmx is the median of the y observations.\\n¯y = a + b¯x\\n\\nvariance\\nstandard deviation\\n\\nﬁrst quartile\\n\\nthird quartile\\n\\ninterquartile range\\n\\nvar(y) = b2var(x)\\nsy = |b|sx\\nIf Q1 is the ﬁrst quartile of the x observations and if b > 0, then a + bQ1 is the ﬁrst quartile\\nof the y observations. If b < 0, then a + bQ3 is the ﬁrst quartile of the y observations.\\nIf Q3 is the third quartile of the x observations and if b > 0, then a + bQ3 is the third quartile\\nof the y observations. If b < 0, then a = BQ1 is the third quartile of the y observations.\\nIQR(y) = |b|IQR(x).\\n\\n31\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nDescribing Distributions with Numbers\\n\\nTo verify the quadratic identity for the variance:\\n\\nvar(y) =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(yi − ¯y)2 =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n((a + bxi) − (a + b¯x))2 =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(b(xi − ¯x))2 = b2var(x).\\n\\n2.12.\\n\\nS(α) =\\n\\nn(cid:88)i=1\\n\\n(xi − α)2. Thus, S\\n\\n(cid:48)\\n\\n(α) = −2\\n\\n(xi − α)\\n\\nn(cid:88)i=1\\n\\nand S(cid:48)(¯x) = 0. Next, S(cid:48)(cid:48)(α) = 2n for all α and thus S(cid:48)(cid:48)(¯x) = 2n > 0. Consequently, ¯x is a minimum.\\n2.13. 87 is between the 3-rd and the 4-th quintile, between the 7-th and the 8-th decile and the 72-nd and 73-rd\\npercentile.\\n2.14. Both the numerator and the denominator of the z-score have the same units. Their ratio is thus unitless. The\\nstandard score for y,\\n\\nzy\\ni =\\n\\nyi − ¯y\\nsy\\n\\n(axi + b) − (a¯x + b)\\n\\n=\\n\\na(xi − ¯x)\\n\\n=\\n\\n=\\n\\n|a|sx\\n\\n|a|sx\\n\\nzx\\ni .\\n\\na\\n|a|\\n\\nThus, if a > 0, the two standard scores are the same. If a < 0, the two standard scores are the negative of one another.\\n\\n2.15. (left) The boxplots agree for Q1 and the median, but the missense children show a higher skew to the right,\\ngiving a higher mean.\\n\\n(center) Recall that the area under the survival function and thus the area above the empirical cumulative distribu-\\ntion function is the mean. The cumulative distribution function rises more quickly for the nonsense distribution. Thus,\\nthe area above it is smaller and so the mean is smaller.\\n\\n(right) The points are on or below the line missense age = nonsense age. Thus, for a given quantile, the missense\\nvalue is at least as big ans the nonsense and thus the mean is bigger for the age of ﬁrst seizure for missense children.\\n\\n32\\n\\n\\x0cTopic 3\\n\\nCorrelation and Regression\\n\\nReﬂection soon made it clear to me that not only were the two new problems identical in principle with\\nthe old one of kinship which I had already solved, but that all three of them were no more than special\\ncases of a much more general problem–namely, that of Correlation. - Sir Francis Galton\\n\\nMy object is to place beyond doubt the existence of a simple and far-reaching law that governs the\\nhereditary transmission ... that the mean ﬁlial regression towards mediocrity was directly proportional to\\nthe parental deviation from it. - Sir Francis Galton\\n\\nIn this section, we shall take a careful look at the nature of linear relationships found in the data used to construct a\\nscatterplot. The ﬁrst of these, correlation, examines this relationship in a symmetric manner. The second, regression,\\nconsiders the relationship of a response variable as determined by one or more explanatory variables. Correlation\\nfocuses primarily on association, while regression is designed to help make predictions. Consequently, the ﬁrst does\\nnot attempt to establish any cause and effect. The second is a often used as a tool to establish causality.\\n\\n3.1 Covariance and Correlation\\nThe covariance measures the linear relation-\\nship between a pair of quantitative measures\\n\\nvectors\\n\\nquantitative\\nobservations\\n\\nx1, x2, . . . , xn\\n\\nand\\n\\ny1, y2, . . . , yn\\n\\non the same sample of n individuals. Begin-\\nning with the deﬁnition of variance, the deﬁ-\\nnition of covariance is similar to the relation-\\nship between the square of the norm ||v||2 of\\na vector v and the inner product (cid:104)v, w(cid:105) of two\\nvectors v and w.\\n\\ncov(x, y) =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)(yi − ¯y).\\n\\nA positive covariance means that\\n\\nthe\\n\\nv = (v1, . . . , vn)\\nw = (w1, . . . , wn)\\n\\nnorm-squared\\n\\ni=1 v2\\ni\\n\\nnorm\\n||v||\\n\\n||v||2 =(cid:80)n\\n(cid:104)v, w(cid:105) =(cid:80)n\\n\\ncosine\\n\\ncos θ =\\n\\ninner product\\n\\n(cid:104)v,w(cid:105)\\n||v|| ||w||\\n\\nx = (x1, . . . , xn)\\ny = (y1, . . . , yn)\\n\\nvariance\\n\\nx = 1\\ns2\\n\\nstandard deviation\\n\\ni=1(xi − ¯x)2\\nsx\\n\\ncovariance\\n\\nn−1(cid:80)n\\nn−1(cid:80)n\\n\\ncorrelation\\nr = cov(x,y)\\n\\nsxsy\\n\\ni=1 viwi\\n\\ncov(x, y) = 1\\n\\ni=1(xi − ¯x)(yi − ¯y)\\n\\nTable I: Analogies between vectors and quantitative observations.\\n\\nterms (xi − ¯x)(yi − ¯y) in the sum are more likely to be positive than negative. This occurs whenever the x and\\ny variables are more often both above or below the mean in tandem than not. Just like the situation in which the inner\\nproduct of a vector with itself yields the square of the norm, the covariance of x with itself cov(x, x) = s2\\nx is the\\nvariance of x.\\nExercise 3.1. Explain in words what a negative covariance signiﬁes, and what a covariance near 0 signiﬁes.\\n\\n33\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nWe next look at several exercises that call for algebraic manipulations of the formula for covariance or closely\\n\\nrelated functions.\\n\\nExercise 3.2. Derive the alternative expression for the covariance:\\n\\ncov(x, y) =\\n\\n1\\n\\nn − 1(cid:32) n(cid:88)i=1\\n\\nxiyi − n¯x¯y(cid:33) .\\n\\nExercise 3.3. cov(ax + b, cy + d) = ac· cov(x, y). How does a change in units (say from centimeters to meters) affect\\nthe covariance?\\n\\nThus, covariance as a measure of association has the drawback that its value depends on the units of measurement.\\n\\nThis shortcoming is remedied by using the correlation.\\n\\nDeﬁnition 3.4. The correlation, r, is the covariance of the standardized versions of x and y.\\n\\nr(x, y) =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1(cid:18) xi − ¯x\\n\\nsx (cid:19)(cid:18) yi − ¯y\\n\\nsy (cid:19) =\\n\\ncov(x, y)\\n\\nsxsy\\n\\n.\\n\\nThe observations x and y are called uncorrelated if r(x, y) = 0.\\n\\nExercise 3.5. r(ax + b, cy + d) = ±r(x, y). How does a change in units (say from centimeters to meters) affect the\\ncorrelation? The plus sign occurs if a · c > 0 and the minus sign occurs if a · c < 0.\\n\\nSometimes we will drop (x, y) if there is no ambiguity and simply write r for the correlation.\\n\\nExercise 3.6. Show that\\n\\ns2\\nx+y = s2\\n\\nx + s2\\n\\ny + 2cov(x, y) = s2\\n\\nx + s2\\n\\ny + 2rsxsy.\\n\\n(3.1)\\n\\nGive the analogy between this formula and the law of cosines.\\n\\nIn particular if the two observations are uncorrelated we have the\\n\\nPythagorean identity\\n\\ns2\\nx+y = s2\\n\\nx + s2\\ny.\\n\\n(3.2)\\n\\nWe will now look to uncover some of the properties of correla-\\ntion. The next steps are to show that the correlation is always a num-\\nber between −1 and 1 and to determine the relationship between the\\ntwo variables in the case that the correlation takes on one of the two\\npossible extreme values.\\n\\nExercise 3.7 (Cauchy-Schwarz inequality). For two sequences\\nv1,··· , vn and w1, . . . , wn, show that\\n\\n(cid:32) n(cid:88)i=1\\n\\nviwi(cid:33)2\\n\\n≤(cid:32) n(cid:88)i=1\\n\\nv2\\n\\ni(cid:33)(cid:32) n(cid:88)i=1\\n\\ni(cid:33) .\\n\\nw2\\n\\n(3.3)\\n\\nFigure 3.1: The analogy of the sample standard deviations\\nand the law of cosines in equation (3.1). Here, the corrre-\\nlation r = − cos θ.\\n\\n(Hint:(cid:80)n\\nthe quadratic formula.) If the discriminant is zero, then we have equality in (3.3) and we have that(cid:80)n\\n\\ni=1(vi + wiζ)2 is a non-negative quadratic expression in the variable ζ and consider the discriminant in\\ni=1(vi +wiζ)2 =\\n\\n0 for exactly one value of ζ.\\n\\nWritten in terms of norms and inner products, the Cauchy-Schwarz\\ninequality becomes (cid:104)v, w(cid:105)2 ≤ ||v||2||w||2.\\n\\n34\\n\\n−0.500.511.522.5−0.200.20.40.60.811.2θsxsysx+y\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nFigure 3.2: Scatterplots showing differing levels of the correlation r\\n\\n[t!]\\n\\n35\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−2012−2−10123r=0.9xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−2012−2−10123r=0.7xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−2012−2−10123r=0.3xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−2012−20123r=0.0xzllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−2012−2−10123r=−0.5xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−2012−2−1012r=−0.8xy\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nWe shall use inequality (3.3) by choosing vi = xi − ¯x and wi = yi − ¯y to obtain\\n(yi − ¯y)2(cid:33) ,\\n(yi − ¯y)2(cid:33) ,\\nn(cid:88)i=1\\n\\n(cid:32) n(cid:88)i=1\\n(xi − ¯x)(yi − ¯y)(cid:33)2\\n(xi − ¯x)(yi − ¯y)(cid:33)2\\nn(cid:88)i=1\\n\\n(xi − ¯x)2(cid:33)(cid:32) n(cid:88)i=1\\nn(cid:88)i=1\\n\\n(xi − ¯x)2(cid:33)(cid:32) 1\\n\\n(cid:32) 1\\n\\nn − 1\\n\\nn − 1\\n\\n≤(cid:32) n(cid:88)i=1\\n≤(cid:32) 1\\ncov(x, y)2 ≤ s2\\ncov(x, y)2\\n\\nn − 1\\nxs2\\ny\\n\\n≤ 1\\n\\nConsequently, we ﬁnd that\\n\\ns2\\nxs2\\ny\\n\\nWhen we have |r| = 1, then we have equality in (3.3). In addition, for some value of ζ we have that\\n\\nr2 ≤ 1\\n\\nor − 1 ≤ r ≤ 1.\\n\\nn(cid:88)i=1\\n\\n((xi − ¯x) + (yi − ¯y)ζ)2 = 0.\\n\\nThe only way for a sum of nonnegative terms to add to give zero is for each term in the sum to be zero, i.e.,\\n\\n(xi − ¯x) + (yi − ¯y)ζ = 0,\\n\\nfor all i = 1, . . . , n.\\n\\n(3.4)\\n\\nThus xi and yi are linearly related.\\n\\nyi = α + βxi.\\n\\nIn this case, the sign of r is the same as the sign of β.\\nExercise 3.8. For an alternative derivation that −1 ≤ r ≤ 1. Use equation (3.1) with x and y standardized observa-\\nx−y for\\ntions. Use this to determine ζ in equation (3.4) (Hint: Consider the separate cases s2\\nthe r = 1.)\\n\\nx+y for the r = −1 and s2\\n\\nWe can see how this looks for simulated data. Choose a value for r between −1 and +1.\\n\\n>x<-rnorm(100)\\n>z<-rnorm(100)\\n>y<-r*x + sqrt(1-rˆ2)*z\\n\\nExample of plots of the output of this simulation are given in Figure 3.1. For the moment, the object of this\\nsimulation is to obtain an intuitive feeling for differing values for correlation. We shall soon see that this is the\\nsimulation of pairs of normal random variables with the desired correlation. From the discussion above, we can see\\nthat the scatterplot would lie on a straight line for the values r = ±1.\\nFor the Archeopteryx data on bone lengths, we have the correlation\\n\\n> cor(femur, humerus)\\n[1] 0.9941486\\n\\nThus, the data land very nearly on a line with positive slope.\\nFor the banks in 1974, we have the correlation\\n\\n> cor(income,assets)\\n[1] 0.9325191\\n\\n36\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\n3.2 Linear Regression\\nCovariance and correlation are measures of linear association. For the Archeopteryx measurements, we learn that the\\nrelationship in the length of the femur and the humerus is very nearly linear.\\n\\nWe now turn to situations in which the value of the ﬁrst variable xi will be considered to be explanatory or\\npredictive. The corresponding output observation yi, taken from the input xi, is called the response. For example,\\ncan we explain or predict the income of banks from its assets? In this case, assets is the explanatory variable and\\nincome is the response.\\n\\nIn linear regression, the response variable is linearly related to the explanatory variable (also known as the co-\\n\\nvariate), but is subject to deviation, discrepency, or to error. We write\\n\\nyi = α + βxi + \\x01i.\\n\\n(3.5)\\n\\nOur goal is, given the data, the xi’s and yi’s, to ﬁnd α and β that determines the line having the best ﬁt to the\\ndata. The principle of least squares regression states that the best choice of this linear relationship is the one that\\nminimizes the square in the vertical distance from the y values in the data and the y values on the regression line. This\\nchoice reﬂects the fact that the values of x are set by the experimenter and are thus assumed known. Thus, the “error”\\nappears in the value of the response variable y.\\n\\nThis principle leads to a minimization problem for\\n\\nSS(α, β) =\\n\\n\\x012\\ni =\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\n(yi − (α + βxi))2.\\n\\n(3.6)\\n\\nIn other words, given the data, determine the values for α and β that minimizes the sum of squares SS. Let’s the\\ndenote by ˆα and ˆβ the value for α and β that minimize SS.\\n\\nTake the partial derivative with respect to α.\\n\\n∂\\n∂α\\n\\nSS(α, β) = −2\\n\\n(yi − α − βxi)\\n\\nn(cid:88)i=1\\n\\nAt the values ˆα and ˆβ, this partial derivative is 0. Consequently\\n\\n0 =\\n\\nn(cid:88)i=1\\n(yi − ˆα − ˆβxi)\\n\\nn(cid:88)i=1\\n\\nyi =\\n\\nn(cid:88)i=1\\n(ˆα − ˆβxi).\\n\\nNow, divide by both sides of the equation by n to obtain\\n\\n¯y = ˆα + ˆβ ¯x.\\n\\n(3.7)\\n\\nThus, we see that the center of mass point (¯x, ¯y) is on the regression line. To emphasize this fact, we rewrite (3.5)\\n\\nin slope-point form.\\n\\nWe then apply this to the sums of squares criterion (3.6) to obtain a condition that depends on β,\\n\\nyi − ¯y = β(xi − ¯x) + \\x01i.\\n\\n˜SS(β) =\\n\\n\\x012\\ni =\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\n((yi − ¯y) − β(xi − ¯x))2.\\n\\nNow, differentiate with respect to β and set this equation to zero for the value ˆβ.\\n\\n(3.8)\\n\\n(3.9)\\n\\nd\\ndβ\\n\\n˜SS(β) = −2\\n\\nn(cid:88)i=1\\n\\n((yi − ¯y) − ˆβ(xi − ¯x))(xi − ¯x) = 0.\\n\\n37\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nThus,\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n1\\n\\n(yi − ¯y)(xi − ¯x) = ˆβ\\n\\n(xi − ¯x)2\\n\\n(yi − ¯y)(xi − ¯x) = ˆβ\\n\\nn − 1\\ncov(x, y) = ˆβvar(x)\\n\\nNow solve for ˆβ.\\n\\nIn summary, to determine the regression line.\\n\\nˆβ =\\n\\ncov(x, y)\\nvar(x)\\n\\n.\\n\\nˆyi = ˆα + ˆβxi,\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2\\n\\n(3.10)\\n\\nwe use (3.10) to determine ˆβ and then (3.7) to solve for\\n\\nˆα = ¯y − ˆβ ¯x.\\n\\nWe call ˆyi the ﬁt for the value xi.\\n\\nNotice that the units of α (and ˆα) are the same as the units of the response variable, y. For the slope,\\n\\nunits of ˆβ = units of β =\\n\\nunits of y\\nunits of x\\n\\n.\\n\\nYou can check that the formula for ˆβ in (3.10) has this property.\\nExample 3.9. Let’s begin with 6 points and derive by hand the equation for regression line.\\n\\nAdd the x and y values and divide by n = 6 to see that ¯x = 0.5 and ¯y = 0.\\n\\nx\\ny\\n\\n-2\\n-3\\n\\n-1\\n-1\\n\\n0\\n-2\\n\\n1\\n0\\n\\n2\\n4\\n\\n3\\n2\\n\\nyi\\nxi\\n-3\\n-2\\n-1\\n-1\\n-2\\n0\\n0\\n1\\n2\\n4\\n2\\n3\\nsum\\n\\nxi − ¯x\\n-2.5\\n-1.5\\n-0.5\\n0.5\\n1.5\\n2.5\\n0\\n\\nyi − ¯y\\n\\n-3\\n-1\\n-2\\n0\\n4\\n2\\n0\\n\\n(xi − ¯x)(yi − ˆy)\\n\\n7.5\\n1.5\\n1.0\\n0.0\\n6.0\\n5.0\\n\\n(xi − ¯x)2\\n\\n6.25\\n2.25\\n0.25\\n0.25\\n2.25\\n6.25\\n\\ncov(x, y) = 21/5\\n\\nvar(x) = 17.50/5\\n\\nˆβ =\\n\\n21/5\\n17.5/5\\n\\n= 1.2\\n\\nand ˆα = 0 − 1.2 × 0.5 = −0.6\\n\\nThus,\\n\\nAs seen in this example, ﬁts are rarely perfect. The difference between the ﬁt and the data is an estimate ˆ\\x01i for the\\n\\nerror \\x01i. This difference is called the residual. So,\\n\\nor, by rearranging terms,\\n\\nˆ\\x01i = RESIDUALi = DATAi − FITi = yi − ˆyi\\n\\nDATAi = FITi + RESIDUALi,\\n\\nor\\n\\nyi = ˆyi + ˆ\\x01i.\\n\\n38\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nFigure 3.3: Scatterplot and the regression line for the six point data set below. The regression line is the choice that minimizes the square of the\\nvertical distances from the observation values to the line, indicated here in green. Notice that the total length of the positive residuals (the lengths of\\nthe green line segments above the regression line) is equal to the total length of the negative residuals. This property is derived in equation (3.11).\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\nFIT\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\nˆyi\\n\\ns\\nc yi DATA\\n\\n\\x1b\\n\\nRESIDUAL = yi − ˆyi\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\n\\x11\\n\\nWe can rewrite equation (3.6) with ˆ\\x01i estimating the error in (3.5).\\n\\n0 =\\n\\n(yi − ˆα − ˆβxi) =\\n\\n(yi − ˆyi) =\\n\\nˆ\\x01i\\n\\n(3.11)\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\nto see that the sum of the residuals is 0. Thus, we started with a criterion for a line of best ﬁt, namely, least squares,\\nand discover that a consequence of this criterion the regression line has the property that the sum of the residual values\\nis 0. This is illustrated in Figure 3.3.\\n\\nLet’s check this property for the example above.\\n\\nDATA FIT RESIDUAL\\n\\nxi\\n-2\\n-1\\n0\\n1\\n2\\n3\\n\\nyi\\n-3\\n-1\\n-2\\n0\\n4\\n2\\ntotal\\n\\nˆyi\\n-3.0\\n-1.8\\n-0.6\\n0.6\\n1.8\\n3.0\\n\\n39\\n\\nˆyi − yi\\n0\\n0.8\\n-1.4\\n-0.6\\n2.2\\n-1.0\\n0\\n\\n−2−10123−4−3−2−101234xy\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nGenerally speaking, we will look at a residual plot, the plot of the residuals versus the explanatory variable, to\\nassess the appropriateness of a regression line. Speciﬁcally, we will look for circumstances in which the explanatory\\nvariable and the residuals have no systematic pattern.\\n\\nExercise 3.10. Use R to perform the following operations on the data set in Example 3.9.\\n\\n1. Enter the data and make a scatterplot.\\n\\n2. Use the lm command to ﬁnd the equation of the regression line.\\n\\n3. Use the abline command to draw the regression line on the scatterplot.\\n\\n4. Use the resid and the predict command command to ﬁnd the residuals and place them in a data.frame\\n\\nwith x and y\\n\\n5. Draw the residual plot and use abline to add the horizontal line at 0.\\n\\nWe next show three examples of the residuals plotting against the value of the explanatory variable.\\n\\n6\\n\\n6\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\nRegression ﬁts the data well - homoscedasticity\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\n-\\n\\na\\n\\n-\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\nPrediction is less accurate for large x, an example of heteroscedasticity\\n\\n40\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\n6\\n\\na\\n\\na\\n\\na\\n\\na\\n\\nCorrelation and Regression\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\na\\n\\n-\\n\\na\\n\\nData has a curve. A straight line ﬁts the data poorly.\\n\\nFor any value of x, we can use the regression line to estimate or predict a value for y. We must be careful in using\\nthis prediction outside the range of x. This extrapolation will not be valid if the relationship between x and y is not\\nknown to be linear in this extended region.\\n\\nExample 3.11. For the 1974 bank data set, the regression line\\n\\n(cid:92)income = 7.680 + 4.975 · assets.\\n\\nSo, each dollar in assets brings in about $5 income.\\n\\nFor a bank having 10 billion dollars in assets, the predicted income is 56.430 billion dollars. However, if we\\nextrapolate this down to very small banks, we would predict nonsensically that a bank with no assets would have\\nan income of 7.68 billion dollars. This illustrates the caution necessary to perform a reliable prediction through an\\nextrapolation.\\n\\nIn addition for this data set, we see that three banks have assets much greater than the others. Thus, we should con-\\nsider examining the regression lines omitting the information from these three banks. If a small number of observations\\nhas a large impact on our results, we call these points inﬂuential.\\n\\nObtaining the regression line in R is straightforward:\\n\\n> lm(income˜assets)\\n\\nCall:\\nlm(formula = income ˜ assets)\\n\\nCoefficients:\\n(Intercept)\\n7.680\\n\\nassets\\n4.975\\n\\nExample 3.12 (regression line in standardized coordinates). Sir Francis Galton was the ﬁrst to use the term regression\\nin his study Regression towards mediocrity in hereditary stature. The rationale for this term and the relationship\\nbetween regression and correlation can be best seen if we convert the observations into a standardized form.\\n\\nFirst, write the regression line to point-slope form.\\n\\nBecause the slope\\n\\nwe can rewrite the point-slope form as\\n\\nˆyi − ¯y = ˆβ(xi − ¯x).\\n\\nˆβ =\\n\\ncov(x, y)\\nvar(x)\\n\\n=\\n\\nrsxsy\\n\\ns2\\nx\\n\\n=\\n\\nrsy\\nsx\\n\\n,\\n\\n41\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nFigure 3.4: Scatterplots of standardized variables and their regression lines. The red lines show the case in which x is the explanatory variable and\\nthe blue lines show the case in which y is the explanatory variable.\\n\\nˆyi − ¯y =\\n\\nrsy\\nsx\\n\\n(xi − ¯x) or\\n\\nˆyi − ¯y\\nsy\\n\\n= r\\n\\nxi − ¯x\\nsx\\n\\n,\\n\\n∗\\n∗\\ni = rx\\nˆy\\ni .\\n\\n(3.12)\\n\\nwhere the asterisk is used to indicate that we are stating our observations in standardized form. In words, if we use\\nthis standardized form, then the slope of the regression line is the correlation.\\n\\nFor Galton’s example, let’s use the height of a male as the explanatory variable and the height of his adult son as\\nthe response. If we observe a correlation r = 0.6 and consider a man whose height is 1 standard deviation above the\\nmean, then we predict that the son’s height is 0.6 standard deviations above the mean. If a man whose height is 0.5\\nstandard deviation below the mean, then we predict that the son’s height is 0.3 standard deviations below the mean.\\nIn either case, our prediction for the son is a height that is closer to the mean then the father’s height. This is the\\n“regression” that Galton had in mind.\\nExercise 3.13. Compute the regression line for the 6 pairs of observations above assuming that y is the explanatory\\nvariable. Show that the two region lines differ by showing that the product of the slopes in not equal to one.\\nExercise 3.14. Create a scatterplot of the x and y variables with correlation r = 0.5 and place both the regression\\nlines on the scatter, Verify that they cross at (¯x, ¯y).\\n\\nFrom the discussion above, we can see that if we reverse the role of the explanatory and response variable, then\\nwe change the regression line. This should be intuitively obvious since in the ﬁrst case, we are minimizing the total\\nsquare vertical distance and in the second, we are minimizing the total square horizontal distance. In the most extreme\\ncircumstance, cov(x, y) = 0. In this case, the value xi of an observation is no help in predicting the response variable.\\nThus, as the formula states, when x is the explanatory variable the regression line has slope 0 - it is a horizontal line\\nthrough ¯y. Correspondingly, when y is the explanatory variable, the regression is a vertical line through ¯x. Intuitively,\\nif x and y are uncorrelated, then the best prediction we can make for yi given the value of xi is just the sample mean\\n¯y and the best prediction we can make for xi given the value of yi is the sample mean ¯x.\\n\\nMore formally, the two regression equations are\\n\\n∗\\n∗\\ni = rx\\nˆy\\ni\\n\\nand\\n\\n∗\\ni = ry\\n\\n∗\\ni .\\n\\nˆx\\n\\nThese equations have slopes r and 1/r. This is shown by example in Figure 3.2.\\nExercise 3.15. Continuing the previous example, let ˆβx be the slope of the regression line obtained from regressing y\\non x and ˆβy be the slope of the regression line obtained from regressing x on y. Show that the product of the slopes\\nˆβx ˆβy = r2, the square of the correlation.\\n\\n42\\n\\n-2-1012-2-1012xy-2-1012-2-1012r=0.8-2-1012-2-1012-2-1012-2-1012r=0.0xy-2-1012-2-1012xy-2-1012-2-1012r=-0.8x1y1\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nBecause the point (¯x, ¯y) is on the regression line, we see from the exercise above that two regression lines coincide\\nprecisely when the slopes are reciprocals, namely precisely when r2 = 1. This occurs for the values r = 1 and\\nr = −1.\\nExercise 3.16. Show that the FIT, ˆy, and the RESIDUALS, y − ˆy are uncorrelated.\\n\\nLet’s again write the regression line in point slope form\\n\\nFITi − ¯y = ˆyi − ¯y = r\\n\\nsy\\nsx\\n\\n(xi − ¯x).\\n\\nUsing the quadratic identity for variance we ﬁnd that\\n\\nF IT = r2 s2\\ny\\ns2\\ns2\\nx\\n\\ns2\\nx = r2s2\\n\\ny = r2s2\\n\\nDAT A.\\n\\nThus, the variance in the FIT is reduced from the variance in\\nthe DATA by a factor of r2 and\\n\\nr2 =\\n\\ns2\\nF IT\\ns2\\nDAT A\\n\\n.\\n\\nExercise 3.17. Use the equation above to show that\\n\\nr2 =\\n\\nSSF IT\\nSSDAT A\\n\\nFigure 3.5: The relationship of the standard deviations of the\\nDATA, the FIT, and the RESIDUALS. s2\\nF IT +\\nRESID. We call r2 the coefﬁcient of determination and say\\ns2\\nthat r2 of the variation in the response variable is due to the ﬁt and\\nthe rest 1 − r2 is due to the residuals.\\n\\nDAT A = s2\\n\\nwhere the sums of squares of the ﬁt or the variation of the ﬁt, SSF IT =(cid:80)n\\nof the data or the variation of the data, SSDAT A =(cid:80)n\\n\\ni=1(yi − ¯y)2.\\n\\nBecause the ﬁt and the residuals are uncorrelated, the Pythagorean identity (3.2) applies and we see that that\\n\\ni=1(ˆyi − ¯y)2 ,and the sums of squares\\n\\nThis leads to the expression\\n\\ns2\\nDAT A = s2\\n\\nF IT + s2\\n\\nRESIDU AL = r2s2\\ns2\\nRESIDU AL = (1 − r2)s2\\n\\nDAT A.\\n\\nDAT A + s2\\n\\nRESIDU AL\\n\\nr2 = 1 −\\n\\ns2\\nRESIDU AL\\n\\ns2\\nDAT A\\n\\n= 1 −\\n\\nSSRESIDU AL\\n\\nSSDAT A\\n\\nwhere the sums of squares of the residuals or the variation of the residuals, SSRESIDU AL =(cid:80)n\\nThus, r2 of the variance in the data can be explained by the ﬁt. As a consequence of this computation, many\\nstatistical software tools report r2 as a part of the linear regression analysis. For this reason, r2 is sometimes called\\nthe coefﬁcient of determination. The remaining 1 − r2 of the variance in the data is found in the residuals. We saw\\na similar partitioning of the variation in Topic 2: Describing Distributions with Numbers when we ﬁrst introduced the\\nconcept of variance. We shall see it again in Topic 22: Analysis of Variance.\\n\\ni=1(yi − ˆy)2.\\n\\nExercise 3.18. For some situations, the circumstances dictate that the line contain the origin (α = 0). Use a least\\nsquares criterion to show that the slope of the regression line\\n\\nR accommodates this circumstance with the commands lm(y∼x-1) or lm(y∼0+x). Note that in this case,\\nthe sum of the residuals is not necessarily equal to zero. For least squares regression, this property followed from\\n∂SS(α, β)/∂α = 0 where α is the y-intercept.\\n\\nˆβ = (cid:80)n\\n(cid:80)n\\n\\ni=1 xiyi\\ni=1 x2\\ni\\n\\n.\\n\\n43\\n\\n00.20.40.60.811.2−0.0500.050.10.150.20.250.3sDATAsFITsRESIDUAL\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nFigure 3.6: (left) scatterplot with regression line in blue (right) residual plot with horizontal axis in red\\n\\nExample 3.19. We continue to investigate the relationship of age of parents to the de novo mutations in the offspring\\nfor the 78 Icelandic trios. We use the average age of the parents to predict the number of mutations in the offspring.\\nThus, age is on the horizontal axis. We can quickly obtain the regression line using R.\\n\\n> mutations.lm<- lm(mutations˜age)\\n> summary(mutations.lm)\\nCall:\\nlm(formula = mutations ˜ age)\\nResiduals:\\n\\nMin\\n\\nMedian\\n-15.7849 -7.1364 -0.1244\\nCoefficients:\\n\\n1Q\\n\\n3Q\\n5.1745\\n\\nMax\\n24.3591\\n\\n2.8145\\n2.1255\\n\\nEstimate Std. Error t value Pr(>|t|)\\n0.611\\n<2e-16 ***\\n\\n(Intercept)\\nage\\n---\\nSignif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1\\nResidual standard error: 8.79 on 76 degrees of freedom\\nMultiple R-squared: 0.6212,Adjusted R-squared:\\nF-statistic: 124.6 on 1 and 76 DF,\\n\\np-value: < 2.2e-16\\n\\n5.5034\\n0.1904\\n\\n0.511\\n11.164\\n\\n1\\n\\n0.6162\\n\\nThus, the regression line has the equation.\\n\\n(cid:92)mutations = 2.815 + 2.125 age.\\n\\nThe value for the coefﬁcient of determination, r2, is 0.6212, the variation in the data explained by the ﬁt.\\n\\nNext, we plot the data and add the regression line to the plot.\\n\\n> mutations.lm<-lm(mutations˜age)\\n> plot(age,mutation)\\n> abline(mutations.lm,col=\"blue\")\\n\\nWe continue our analysis in calling for the residuals, making a residual plot, and creating a horizontal line at 0.\\n\\n> residuals<-resid(mutations.lm)\\n> plot(age,residuals)\\n> abline(h=0,col=\"red\")\\n\\n44\\n\\n2025303540406080100agemutations2025303540-1001020ageresiduals\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nThe command h=0 add a horizontal line at 0. A similar command v=0 adds a vertical line. Finally, we can use\\n\\nthe regression line to predict the number of mutations for parents whose average age is 20, 30, or 40.\\n\\n> agepredict<-c(20,30,40)\\n> predictions<-predict(mutations.lm,newdata=data.frame(age=agepredict))\\n> data.frame(agepredict,predictions)\\n\\nagepredict predictions\\n45.32367\\n66.57824\\n87.83281\\n\\n20\\n30\\n40\\n\\n1\\n2\\n3\\n\\n3.2.1 Transformed Variables\\nFor pairs of observations (x1, y1), . . . , (xn, yn), the linear relationship may exist not with these variables, but rather\\nwith transformation of the variables. In this case we have,\\n\\ng(yi) = α + βψ(xi) + \\x01i.\\n\\n(3.13)\\n\\nA common choice to to perform linear regression on the variables ˜y = g(y) and ˜x = ψ(x) using the least squares\\n\\ncriterion. In this case, g is called the link function\\n\\nFor example, if\\n\\nwe take logarithms,\\n\\nyi = Aekxi+\\x01i,\\n\\nln yi = ln A + kxi + \\x01i\\n\\nSo, in (3.13), the link function g(yi) = ln yi. The parameters are α = ln A and β = k.\\n\\nBefore we look at an example, let’s review a few basic properties of logarithms\\n\\nRemark 3.20 (logarithms). We will use both log, the base 10 common logarthm, and ln, the base e natural loga-\\nrithm. Common logarithms more readily help us see orders of magnitude. For example, if log y = 5, then we know\\nif log y = −1, then we know that y = 10−1 = 1/10. Typically, we will use natural\\nthat y = 105 = 100, 000.\\nlogarithms when we want to emphasize instantaneous rates of growth. To understand how this works, consider the\\ndifferential equation\\n\\ndy\\ndt\\n\\n= ky.\\n\\nWe are saying that the instantaneous rate of growth of y is proportional to y with constant of proportionality k. The\\nsolution to this equation is\\n\\ny = y0ekt\\n\\nor\\n\\nln y = ln y0 + kt\\n\\nwhere y0 is the initial value for y. This gives a linear relationship between ln y and t. The two values of logarithm\\nhave a simple relationship. If we write\\n\\nThus, by substituting for a, we ﬁnd that\\n\\nx = 10a. Then log x = a and ln x = a ln 10.\\n\\nln x = log x · ln 10 = 2.3026 log x.\\n\\nIn R, the command for the natural logarithm of x is log(x). For the common logarithm, it is log(x,10).\\n\\nExample 3.21. In the data on world oil production, the relationship between the explanatory variable and response\\nvariable is nonlinear but can be made to be linear with a simple transformation, the common logarithm. Call the new\\n\\n45\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nresponse variable logbarrel. The explanatory variable remains year. With these variables, we can use a regression\\nline to help describe the data. Here the model is\\n\\nlog yi = α + βxi + \\x01i.\\n\\n(3.14)\\n\\nRegression is the ﬁrst example of a class of statistical models called linear models. At this point we emphasize that\\nlinear refers to the appearance of the parameters α and β linearly in the function (3.14). This acknowledges that, in\\nthis circumstance, the values xi and yi are known. Indeed, they are the data. Using the principle of least squares, our\\ngoal is to give an estimate using the principle of least squares to ˆα and ˆβ for the values of α and β. R accomplishes\\nthis gaol with the command lm (for linear model. Here is the output.\\n\\n> summary(lm(logbarrel˜year))\\n\\nCall:\\nlm(formula = logbarrel ˜ year)\\n\\nResiduals:\\n\\nMin\\n\\n3Q\\n-0.25562 -0.03390 0.03149 0.07220\\n\\nMedian\\n\\n1Q\\n\\nMax\\n0.12922\\n\\nCoefficients:\\n\\nEstimate Std. Error t value Pr(>|t|)\\n\\n(Intercept) -5.159e+01 1.301e+00\\nyear\\n2.675e-02 6.678e-04\\n---\\nSignif. codes: 0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1\\n\\n<2e-16 ***\\n<2e-16 ***\\n\\n-39.64\\n40.05\\n\\nResidual standard error: 0.1115 on 27 degrees of freedom\\nMultiple R-Squared: 0.9834,Adjusted R-squared: 0.9828\\nF-statistic: 1604 on 1 and 27 DF,\\n\\np-value: < 2.2e-16\\n\\nNote that the output outputs the ﬁve number summary for the residuals and reports a coefﬁcient of determination\\nr2 = 0.9828. Thus, the correlation is r = 0.9914 is very nearly one and so the data lies very close to the regression\\nline.\\n\\nFor world oil production, we obtained the relationship\\n\\nbetween the common logarithm of the number of millions of barrels of oil and the year. If we rewrite the equation in\\nexponential form, we obtain, for some constant value, A,\\n\\n(cid:92)\\n\\nlog(barrel) = −51.59 + 0.02675 · year\\n\\n(cid:92)barrel = A100.02675·year = Ae\\n\\nˆk·year.\\n\\nThus, ˆk gives the instantaneous growth rate that best ﬁts the data. This is obtained by converting from a common\\nlogarithm to a natural logarithm.\\n\\nˆk = 0.02675 ln 10 = 0.0616\\n\\n.\\n\\nConsequently, the use of oil sustained a growth of 6% per year over a span of a hundred years.\\nNext, we will look for ﬁner scale structure in the scatterplot by examining the residual plot.\\n\\n> use<-lm(logbarrel˜year)\\n> plot(year,resid(use))\\n\\n46\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nExercise 3.22. Remove the data points after the oil crisis of the mid 1970s, ﬁnd the regression line and the instanta-\\nneous growth rate that best ﬁts the data. Look at the residual plot and use fact about American history to explain why\\nthe residuals increase until 1920’s, decrease until the early 1940’s and increase again until the early 1970’s.\\nExample 3.23 (Michaelis-Menten Kinetics). In this example, we will have to use a more sophisticated line of reason-\\ning to create a linear relationship between a explanatory and response variable. Consider the chemical reaction in\\nwhich an enzyme catalyzes the action on a substrate.\\n\\nE + S\\n\\nk1(cid:29)\\nk−1\\n\\nES\\n\\nk2→ E + P\\n\\n(3.15)\\n\\nHere\\n• E0 is the total amount of enzyme.\\n• E is the free enzyme.\\n• S is the substrate.\\n• ES is the substrate-bound enzyme.\\n• P is the product.\\n• V = d[P ]/dt is the production rate.\\n\\nThe numbers above or below the arrows gives the reaction rates. Using the symbol [ · ] to indicate concentration,\\nnotice that the enzyme, E0, is either free or bound to the substrate. Its total concentration is, therefore,\\n\\n(3.16)\\nOur goal is to relate the production rate V to the substrate concentration [S]. Because the total concentration [E0] is\\nset by the experimenter, we can assume that it is a known quantity.\\n\\n[E] = [E0] − [ES]\\n\\n[E0] = [E] + [ES],\\n\\nand, thus\\n\\nThe law of mass action turns the chemical reactions in (3.15) into differential equations. In particular, the reac-\\n\\ntions, focusing on the substrate-bound enzyme and the product, gives the equations\\n\\nd[ES]\\n\\ndt\\n\\n= k1[E][S] − [ES](k−1 + k2) and V =\\n\\nd[P ]\\ndt\\n\\n= k2[ES]\\n\\n(3.17)\\n\\n47\\n\\nlllllllllllllllllllllllllllll188019001920194019601980−0.2−0.10.00.1yearresid(use)\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nWe can meet our goal if we can ﬁnd an equation for V = k2[ES] that depends only on [S], the substrate concen-\\n\\ntration. Let’s look at data,\\n\\n[S] (mM)\\n\\nV (nmol/sec)\\n\\n1\\n1.0\\n\\n2\\n1.5\\n\\n5\\n2.2\\n\\n10\\n2.5\\n\\n20\\n2.9\\n\\nIf we wish to use linear regression, then we will have to transform the data. In this case, we will develop the\\nMichaelis-Menten transformation applied to situations in which the concentration of the substrate-bound enzyme (and\\nhence also the unbound enzyme) changes much more slowly than those of the product and substrate.\\n\\nd[ES]\\n\\ndt\\n\\n0 ≈\\n\\nIn words, the substrate-bound enzyme is nearly in steady state. Using the law of mass action equation (3.17) for\\nd[ES]/dt, we can rearrange terms to conclude that\\n\\n[ES] ≈\\n\\nk1[E][S]\\nk−1 + k2\\n\\n=\\n\\n[E][S]\\nKm\\n\\n.\\n\\n(3.18)\\n\\nThe ratio Km = (k−1 + k2)/k1 of the rate of loss of the substrate-bound enzyme to its production is called the\\nMichaelis constant. We have now met our goal part way, V is a function of [S], but it is also stated as a function of\\n[E].\\n\\nThus, we have shown in 3.16 that [E] as a function of [ES] . Now, we combine this with (3.18) and solve for [ES]\\n\\nto obtain\\n\\n[ES] ≈\\n\\n([E0] − [ES])[S]\\n\\nKm\\n\\n,\\n\\n[ES] ≈ [E0]\\n\\n[S]\\n\\nKm + [S]\\n\\nis:\\n\\nUnder this approximation, known as the Michaelis-Metens kinetic equation the production rate of the product\\n\\nV =\\n\\nd[P ]\\ndt\\n\\n= k2[ES] = k2[E0]\\n\\n[S]\\n\\nKm + [S]\\n\\n= Vmax\\n\\n[S]\\n\\nKm + [S]\\n\\n(3.19)\\n\\nHere, Vmax = k2[E0] is the maximum production rate. (To see this, let the substrate concentration [S] → ∞.) To\\nperform linear regression, we need to have a function of V be linearly related to a function of [S]. This is achieved via\\ntaking the reciprocal of both sides of this equation.\\n\\n1\\nV\\n\\n=\\n\\nKm + [S]\\nVmax[S]\\n\\n=\\n\\n1\\n\\nVmax\\n\\n+\\n\\nKm\\nVmax\\n\\n1\\n[S]\\n\\n48\\n\\n(3.20)\\n\\n●●●●●51015201.01.52.02.5concentration of substrateproduction rate\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nFigure 3.7: Lineweaver-Burke double reciprocal plot for the data presented above. The y-intercept gives the reciprocal of the maximum production.\\nThe dotted line indicates that negative concentrations are not physical. Nevertheless, the x-intercept give the negative reciprocal of the Michaelis\\nconstant.\\n\\nThus, we have a linear relationship between\\n\\n1\\nV\\n\\n, the response variable, and\\n\\n1\\n[S]\\n\\n, the explanatory variable\\n\\nsubject to experimental error. The Lineweaver-Burke double reciprocal plot provides a useful method for analysis\\nof the Michaelis-Menten equation. See Figure 3.6.\\n\\nFor the data,\\n\\nThe regression line is\\n\\n[S] (mM)\\n\\nV (nmol/sec)\\n\\n1\\n1.0\\n\\n2\\n1.5\\n\\n5\\n2.2\\n\\n10\\n2.5\\n\\n20\\n2.9\\n\\n1\\nV\\n\\n= 0.3211 +\\n\\n1\\n[S]\\n\\n0.6813.\\n\\nHere are the R commands. (Both 1 and the slash / have a speciﬁc meaning for the lm command and so we set\\n\\nvariables Vinv and Sinv for the inverses.)\\n\\n> S<-c(1,2,5,10,20)\\n> V<-c(1.0,1.5,2.2,2.5,2.9)\\n> Sinv<-1/S\\n> Vinv<-1/V\\n> lm(Vinv˜Sinv)\\n\\nCall:\\nlm(formula = Vinv ˜ Sinv)\\n\\nCoefficients:\\n(Intercept)\\n0.3211\\n\\nSinv\\n0.6813\\n\\n49\\n\\n−0.500.5100.20.40.60.811.21.41/[S]1/V1/Vmax−1/Km\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nUsing (3.20), we ﬁnd that Vmax = 3.1141 and Km = 2.122. With more access to computational software, this method\\nis not used as much as before. The measurements for small values of the concentration (and thus large value of 1/[S])\\nare more variable and consequently the residuals are likely to be heteroscedastic. We look in the next section for an\\nalternative approach, namely nonlinear regression.\\n\\nExample 3.24 (Frank Amscombe). Consider the three data sets:\\n\\n10\\n8.04\\n10\\n9.14\\n\\n8\\n\\n8\\n\\n6.95\\n\\n8\\n\\n8.14\\n\\n8\\n\\n13\\n7.58\\n13\\n8.47\\n\\n8\\n\\n9\\n\\n8.81\\n\\n9\\n\\n8.77\\n\\n8\\n\\n11\\n8.33\\n11\\n9.26\\n\\n8\\n\\n14\\n9.96\\n14\\n8.10\\n\\n8\\n\\nx\\ny\\nx\\ny\\nx\\ny\\n\\n6\\n\\n4\\n\\n12\\n\\n7\\n\\n7.24\\n\\n4.26\\n\\n10.84\\n\\n4.82\\n\\n6\\n\\n4\\n\\n6.13\\n\\n3.10\\n\\n8\\n\\n8\\n\\n12\\n9.13\\n\\n8\\n\\n7.91\\n\\n7\\n\\n7.26\\n\\n8\\n\\n6.89\\n\\n5\\n\\n5.68\\n\\n5\\n\\n4.74\\n19\\n\\n12.50\\n\\nt\\n\\n6.58\\n\\n5.76\\n\\n7.71\\n\\n8.84\\n\\n8.47\\n\\n7.04\\n\\n5.25\\n\\n5.56\\n\\nEach of these data sets has a regression line ˆy = 3 + 0.5x and correlations between 0.806 and 0.816. However,\\nonly the ﬁrst is a suitable data set for linear regression. This example is meant to emphasize the point that software\\nwill happily compute a regression line and an coefﬁcient of determination value, but further examination of the data\\nis required to see if this method is appropriate for any given data set.\\n\\n3.3 Extensions\\nWe will discuss brieﬂy two extensions - the ﬁrst is a least squares criterion between x and y that is nonlinear in the\\nparameters β = (β0, . . . , βk). Thus, the model is\\n\\nfor g, a nonlinear function of the parameters.\\n\\nThe second considers situations with more than one explanatory variable.\\n\\nyi = g(xi|β) + \\x01i\\n\\nyi = β0 + β1xi1 + β2xi2 + ··· + βkxik + \\x01i.\\n\\n(3.21)\\n\\nThis brief discussion does not have the detail necessary to begin to use these methods. It serves primarily as an\\n\\ninvitation to begin to consult resources that more fully develop these ideas.\\n\\n50\\n\\nlllllllllll5101520468101214xylllllllllll5101520468101214xylllllllllll5101520468101214xy\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\n3.3.1 Nonlinear Regression\\nHere, we continue using estimation of parameters using a least squares criterion.\\n\\nSS(β) =\\n\\n(yi − g(xi|β))2.\\n\\nn(cid:88)i=1\\n\\nFor most choices of g(x|β) the solution to the nonlinear least square criterion cannot be expressed in closed form.\\nThus, a numerical strategy for the solution ˆβ is necessary. This generally begins with some initial guess of parameter\\nvalues and an iteration scheme to minimize SS(β). Such a scheme is likely to use local information about the ﬁrst\\nand second partial derivatives of g with respect to the parameters βi. For example, gradient descent (also known\\nas steepest descent, or the method of steepest descent) is an iterative method in which produces a sequence of\\nparameter values. The increment of the parameter values for an iteration is proportional to the negative of the gradient\\nof SS(β) evaluated at the current point. The hope is that the sequence converges to give the desired minimum value\\nfor SS(β). The R command gnls for general nonlinear least squares is used to accomplish this. As above, you\\nshould examine the residual plot to see that it has no structure. For, example, if the Lineweaver-Burke method for\\nMichaelis-Mentens kinetics yields structure in the residuals, then linear regression is not considered a good method.\\nUnder these circumstances, one can next try to use the parameter estimates derived from Lineweaver-Burke as an\\ninitial guess in a nonlinear least squares regression using a least square criterion based on the sum of squares\\n\\nSS(Vmax, Km) =\\n\\nn(cid:88)j=1(cid:18)Vj − Vmax\\n\\n[S]j\\n\\nKm + [S]j(cid:19)2\\n\\nfor data (V1, [S]1), (V2, [S]2), . . . (Vn, [S]n).\\n\\nTo use the gnls command we need to install the R package nmle. The command requires a model equation,\\nhere, the Michaelis-Metens equation (3.19), written as response variable model equation, the data, and a starting\\npoint for the numerical method for ﬁnding the parameter values that minimize SS(Vmax, Km). Here, we begin with\\nthe values obtained from the regression equation for the Lineweaver-Burke double reciprocal plot.\\n\\n> gnls(V˜Vmax*S/(Km+S),data=data.frame(V,S),start=list(Vmax=3.1141,Km=2.1216))\\nGeneralized nonlinear least squares fit\\n\\nModel: V ˜ Vmax * S/(Km + S)\\nData: data.frame(V, S)\\nLog-likelihood: 8.212577\\n\\nCoefficients:\\n\\nVmax\\n\\nKm\\n3.154482 2.210205\\n\\nDegrees of freedom: 5 total; 3 residual\\nResidual standard error: 0.06044381\\n\\nWe see a small change in the estimates ˆVmax and ˆKm from the previous estimates.\\n\\n3.3.2 Multiple Linear Regression\\nBefore we start with multiple linear regression, we ﬁrst recall a couple of concepts and results from linear algebra.\\n\\n• Let Cij denote the entry in the i-th row and j-th column of a matrix C.\\n• A matrix A with rA rows and cA and a matrix B with rB rows and cB columns can be multiplied to form a\\n\\nmatrix AB provide that cA = rB, the number of columns in A equals the number of rows in B. In this case\\n\\nAikBkj.\\n\\ncA(cid:88)k=1\\n\\n(AB)ij =\\n\\n51\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\n• The d-dimensional identity matrix I is the matrix with the value 1 for all entries on the diagonal (Ijj = 1, j =\\n\\n1 . . . , d) and 0 for all other entries. Notice for and d-dimensional vector x,\\n\\nIx = x.\\n\\n• A d × d matrix C is called invertible with inverse C−1 provided that\\n−1C = I.\\n\\n−1 = C\\n\\nCC\\n\\nOnly one matrix can have this property.\\n\\n• Suppose we have a d-dimensional vector a of known values and a d × d matrix C and we want to determine the\\n\\nvectors x that satisfy\\n\\na = Cx.\\n\\nThis equation could have no solutions, a single solution, or an inﬁnite number of solutions. If the matrix C is\\ninvertible, then we have a single solution\\n\\nx = C\\n\\n−1a.\\n\\n• The transpose of a matrix is obtained by reversing the rows and columns of a matrix. We use a superscript T to\\n\\nindicate the transpose. Thus, the ij entry of a matrix C is the ji entry of its transpose, C T .\\nExample 3.25.\\n\\n• A square matrix C is invertible if and only if its determinant det(C) (cid:54)= 0. For a 2 × 2 matrix\\n\\ndet(C) = ad − bc and the matrix inverse\\n\\nC\\n\\n−1 =\\n\\nExercise 3.26. (Cx)T = xT C T\\nExercise 3.27. For\\n\\nﬁnd C T , det(C) and C−1 by hand and using R\\n\\nC =(cid:18) 1 3\\n2 4(cid:19) ,\\n\\n• y = (y1, y2, . . . , yn)T is a column vector of responses,\\n• X is a matrix of predictors,\\n\\nIn multiple linear regression, we have more than one predictor or explanatory random variable. Thus can write\\n\\n(3.21) in matrix form\\n\\n(3.22)\\n\\n(3.23)\\n\\n.\\n\\nThe column of ones on the left give the constant term in a multilinear equation. This matrix X is an an example\\nof what is know as a design matrix.\\n\\n(cid:18) 2 1 3\\n4 2 7(cid:19)T\\n\\n2 4\\n1 2\\n\\n3 7\\uf8f6\\uf8f8\\n=\\uf8eb\\uf8ed\\nC =(cid:18) a b\\nc d(cid:19)\\ndet(C)(cid:18) d −b\\n−c a (cid:19)\\n\\n1\\n\\ny = Xβ + \\x01\\n\\nX =\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n\\n1 x11 ··· x1k\\n1 x21 ··· x2k\\n...\\n...\\n1 xn1 ··· xnk\\n\\n...\\n\\n...\\n\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n\\n52\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\n• β = (β0, β1, . . . , βk)T is a column vector of parameters, and\\n• \\x01 = (\\x011, \\x012, . . . , \\x01n)T is a column vector of “errors”.\\n\\nExercise 3.28. Show that the least squares criterion\\n\\ncan be written in matrix form as\\n\\nSS(β) =\\n\\n(yi − β0 − xi1β1 − ··· − βkxik)2.\\n\\nn(cid:88)i=1\\nSS(β) = (y − Xβ)T (y − Xβ).\\n\\nTo minimize SS, we take the gradient and set it equal to 0.\\n\\nExercise 3.29. Check that the gradient is\\n\\n∇βSS(β) = −2(y − Xβ)T X.\\n\\nBased on the exercise above, the value ˆβ that minimizes SS is\\n\\n(y − X ˆβ)T X = 0, yTX = ˆβT X T X.\\nThe transpose of this last equation is sometimes known at the normal equations\\n\\n(3.24)\\n\\n(3.25)\\n\\n(3.26)\\nIf X T X is invertible, then we can multiply both sides of the equation above by (X T X)−1 to obtain an equation\\n\\nX T X ˆβ = X T y.\\n\\nfor the parameter values ˆβ = ( ˆβ0, ˆβ1, . . . , ˆβn) in the least squares regression.\\n\\nˆβ = (X T X)\\n\\n−1X T y.\\n\\n(3.27)\\n\\nThus the estimates ˆβ are a linear transformation of the repsonses y through the so-called hat matrix H =\\n\\n(X T X)−1X T , i.e. ˆβ = Hy.\\nExercise 3.30. Verify that the hat matrix H is a left inverse of the design matrix X.\\n\\nThis gives the regression equation\\n\\nExample 3.31 (ordinary least squares regression). In this case,\\n\\ny = ˆβ0 + ˆβ1x1 + ˆβ2x2 + ··· + ˆβkxk\\n\\nand\\n\\n1 x1\\n1 x2\\n...\\n...\\n1 xn\\n\\nx1 x2 ··· xn(cid:19)\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nX T X =(cid:18) 1 1 ··· 1\\nx1 x2 ··· xn(cid:19)\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nX T y =(cid:18) 1 1 ··· 1\\nxi(cid:33)2\\ni −(cid:32) n(cid:88)i=1\\nn(cid:88)i=1\\n\\nx2\\n\\nn\\n\\ny1\\ny2\\n...\\nyn\\n\\n53\\n\\n\\uf8f6\\uf8f8\\n\\ni=1 xi\\n\\ni=1 x2\\ni\\n\\nn\\n\\n=\\uf8eb\\uf8ed\\n(cid:80)n\\ni=1 xi (cid:80)n\\n(cid:80)n\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n\\uf8f6\\uf8f8 .\\n=\\uf8eb\\uf8ed (cid:80)n\\n(cid:80)n\\n\\ni=1 xiyi\\n\\ni=1 yi\\n\\n= n(n − 1)var(x).\\n\\nThe determinant of X T X is\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nand thus\\n\\nand\\n\\nˆβ = (X T X)\\n\\n(X T X)\\n\\n−1X T y =\\n\\nFor example, for the second row, we obtain\\n\\n1\\n\\nn(n − 1)var(x)(cid:32)(cid:32)−\\n\\nas seen in equation (3.10).\\n\\ni=1 x2\\ni\\n\\ni=1 xi\\n\\n1\\n\\n1\\n\\n−1 =\\n\\nn(n − 1)var(x)\\uf8eb\\uf8ed (cid:80)n\\n−(cid:80)n\\nn(n − 1)var(x)\\uf8eb\\uf8ed (cid:80)n\\n−(cid:80)n\\nxi(cid:33)(cid:32) n(cid:88)i=1\\nyi(cid:33) + n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\ni=1 x2\\ni\\n\\ni=1 xi\\n\\nxiyi(cid:33) =\\n\\nn\\n\\ni=1 xi\\n\\ni=1 xi\\n\\n−(cid:80)n\\n−(cid:80)n\\n\\n\\uf8f6\\uf8f8\\n\\uf8f6\\uf8f8\\uf8eb\\uf8ed (cid:80)n\\n(cid:80)n\\nn(n − 1)cov(x, y)\\nn(n − 1)var(x)\\n\\nn\\n\\n=\\n\\ni=1 yi\\n\\ni=1 xiyi\\n\\n\\uf8f6\\uf8f8 .\\n\\ncov(x, y)\\nvar(x)\\n\\nExample 3.32. We can estimate the number of mutations from each parent by using two explanatory variables, namely\\nboth the father’s and mother’s age at the time of the conception of the offspring to the de novo mutations in the offspring\\nfor the 78 Icelandic trios.\\n\\n> iceland.lm<-lm(mutations˜paternal+maternal)\\n> summary(iceland.lm)\\n\\nCall:\\nlm(formula = mutations ˜ paternal + maternal)\\n\\nResiduals:\\n\\nMin\\n\\n1Q\\n-18.6769 -6.5272\\n\\nMedian\\n0.7632\\n\\n3Q\\n4.8546\\n\\nMax\\n21.0775\\n\\nCoefficients:\\n\\nEstimate Std. Error t value Pr(>|t|)\\n0.625\\n\\n(Intercept)\\npaternal\\nmaternal\\n---\\nSignif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1\\n\\n0.490\\n6.165 3.25e-08 ***\\n0.693\\n\\n5.2829\\n0.2987\\n0.3203\\n\\n2.5901\\n1.8414\\n0.2220\\n\\n0.491\\n\\n1\\n\\nResidual standard error: 8.437 on 75 degrees of freedom\\nMultiple R-squared: 0.6556,Adjusted R-squared:\\n0.6465\\nF-statistic: 71.4 on 2 and 75 DF,\\n\\np-value: < 2.2e-16\\n\\nHere, we estimate that, on average, each year of the father’s age adds 1.84 mutations. The mother adds 0.22 mutations\\nper year of age. Thus, it is the father’s age that dominates the number of de novo mutations in the offspring. The\\ncoefﬁcient of determination is now 0.6556, increased from 0.6212 with a single explanatory variable. Because the\\nsource of mutations is unknown and the parents’ ages are highly correlated (∼ 0.8), estimation is difﬁcult.\\nExample 3.33. The choice of xij = xj\\n\\ni in (3.23) results in polynomial regression\\n\\nyi = β0 + β1xi + β2x2\\n\\ni + ··· + βkxk\\n\\ni + \\x01i.\\n\\nin equation (3.21).\\n\\nExample 3.34 (US population). Below are the census populations\\n\\n54\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\ncensus\\npopulation\\n3,929,214\\n5,236,631\\n7,239,881\\n9,638,453\\n12,866,020\\n17,069,453\\n\\nyear\\n1790\\n1800\\n1810\\n1820\\n1830\\n1840\\n\\ncensus\\npopulation\\n23,191,876\\n31,443,321\\n38,558,371\\n49,371,340\\n62,979,766\\n76,212,168\\n\\ncensus\\npopulation\\n92,228,496\\n106,021,537\\n123,202,624\\n132,164,569\\n151,325,798\\n179,323,175\\n\\nyear\\n1910\\n1920\\n1930\\n1940\\n1950\\n1960\\n\\nyear\\n1850\\n1860\\n1870\\n1880\\n1890\\n1900\\n\\ncensus\\npopulation\\n203,211,926\\n226,545,805\\n248,709,873\\n281,421,906\\n308,745,538\\n\\nyear\\n1970\\n1980\\n1990\\n2000\\n2010\\n\\nTo analyze this in R we enter the data:\\n\\n> uspop<-c(3929214,5236631,7239881,9638453,12866020,17069453,23191876,31443321,\\n+ 38558371,49371340,62979766,76212168,92228496,106021537,123202624,132164569,\\n+ 151325798,179323175,203211926,226545805,248709873,281421906,308745538)\\n> year<-c(0:22)*10+1790\\n> plot(year,uspop)\\n> loguspop<-log(uspop,10)\\n> plot(year,loguspop)\\n\\nFigure 3.8: (a) United States census population from 1790 to 2010 and (b) its base 10 logarithm.\\n\\nNote that the logarithm of the population still has a bend to it, so we will perform a quadratic regression on the\\nlogarithm of the population. In order to keep the numbers smaller, we shall give the year minus 1790, the year of the\\nﬁrst census for our explanatory variable.\\n\\nlog(uspopulation) = β0 + β1(year − 1790) + β2(year − 1790)2.\\n\\n> year1<-year-1790\\n> year2<-year1ˆ2\\n\\nThus, loguspop is the reponse variable. The + sign is used in the case of more than one explanatory variable\\n\\nand here is placed between the response variables year1 and year2.\\n\\n55\\n\\n180018501900195020000.0e+005.0e+071.0e+081.5e+082.0e+082.5e+083.0e+08yearuspop180018501900195020007.07.58.08.5yearloguspop\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\n> lm.uspop<-lm(loguspop˜year1+year2)\\n> summary(lm.uspop)\\n\\nCall:\\nlm(formula = loguspop ˜ year1 + year2)\\n\\nResiduals:\\n\\n3Q\\n-0.037387 -0.013453 -0.000912 0.015281\\n\\nMedian\\n\\nMin\\n\\n1Q\\n\\nMax\\n0.029782\\n\\nCoefficients:\\n\\nEstimate Std. Error t value Pr(>|t|)\\n\\n(Intercept) 6.582e+00 1.137e-02\\n1.471e-02 2.394e-04\\nyear1\\nyear2\\n-2.808e-05 1.051e-06\\n---\\nSignif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1\\n\\n578.99\\n61.46\\n-26.72\\n\\n<2e-16 ***\\n<2e-16 ***\\n<2e-16 ***\\n\\n1\\n\\nResidual standard error: 0.01978 on 20 degrees of freedom\\nMultiple R-squared: 0.999,Adjusted R-squared: 0.9989\\nF-statistic: 9781 on 2 and 20 DF,\\n\\np-value: < 2.2e-16\\n\\nThe R output shows us that\\n\\nˆβ0 = 6.587\\n\\nˆβ1 = 0.01471 quad ˆβ2 = −0.00002808.\\n\\nSo, taking the the regression line to the power 10, we have that\\n\\n(cid:92)uspopulation = 3863670 × 100.0147(year−1790)−0.00002808(year−1790)2\\n\\nIn Figure 3.8, we show the residual plot for the logarithm of the US population.\\n\\n> resid.uspop<-resid(lm.uspop)\\n> plot(year,resid.uspop)\\n\\n3.4 Answers to Selected Exercises\\n\\n3.1. Negative covariance means that the terms (xi − ¯x)(yi − ¯y) in the sum are more likely to be negative than positive.\\nThis occurs whenever one of the x and y variables is above the mean, then the other is likely to be below.\\n\\n3.2. We expand the product inside the sum.\\n\\ncov(x, y) =\\n\\n=\\n\\n1\\n\\n(xi − ¯x)(yi − ¯y) =\\n\\nn − 1(cid:32) n(cid:88)i=1\\nxiyi − n¯x¯y − n¯x¯y + n¯x¯y(cid:33) =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\nn − 1(cid:32) n(cid:88)i=1\\n\\n1\\n\\nxiyi − ¯y\\n\\nn(cid:88)i=1\\nn − 1(cid:32) n(cid:88)i=1\\n\\n1\\n\\nyi + n¯x¯y(cid:33)\\n\\nxi − ¯x\\n\\nn(cid:88)i=1\\nxiyi − n¯x¯y(cid:33)\\n\\nThe change in measurements from centimeters to meters would divide the covariance by 10,000.\\n\\n56\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nFigure 3.9: Residual plot for US population regression.\\n\\n3.3. We rearrange the terms and simplify.\\n\\ncov(ax + b, cy + d) =\\n\\n=\\n\\n1\\n\\nn − 1\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\n((axi + b) − (a¯x + b)((cyi + d) − (c¯y − d))\\n\\n(axi − a¯x)(cyi − c¯y) = ac ·\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)(yi − ¯y) = ac · cov(x, y)\\n\\n3.5. Assume that a (cid:54)= 0 and c (cid:54)= 0. If a = 0 or c = 0, then the covariance is 0 and so is the correlation.\\n\\nr(ax + b, cy + d) =\\n\\nac\\n|ac|\\nWe take the plus sign if the sign of a and c agree and the minus sign if they differ.\\n\\nac · cov(x, y)\\n|a|sx · |c|sy\\n\\ncov(ax + b, cy + d)\\n\\nsax+bscy+d\\n\\n=\\n\\n=\\n\\n·cov(x, y)\\nsx · sy\\n\\n= ±r(x, y)\\n\\n3.6. First we rearrange terms\\n\\n1\\n\\nn − 1\\n\\n1\\n\\ns2\\nx+y =\\n\\n=\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\n((xi + yi) − (¯x + ¯y))2 =\\n\\n((xi − ¯x) + (yi − ¯y))2\\n\\n(xi − ¯x)2 + 2\\n\\nn − 1\\nx + 2cov(x, y) + s2\\n\\n= s2\\n\\nn − 1\\nx + 2rsxsy + s2\\ny\\n\\ny = s2\\n\\n(xi − ¯x)(yi − ¯y) +\\n\\n1\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(yi − ¯y)2\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\n57\\n\\n18001850190019502000-0.04-0.03-0.02-0.010.000.010.020.03yearresid.uspop\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nFor a triangle with sides a, b and c, the law of cosines states that\\n\\nc2 = a2 + b2 − 2ab cos θ\\nwhere θ is the measure of the angle opposite side c. Thus the analogy is\\n\\nsx corresponds to a,\\n\\nsy corresponds to b,\\n\\nsx+y corresponds to c,\\n\\nand\\n\\nr corresponds to − cos θ\\n\\nNotice that both r and cos θ take values between −1 and 1.\\n3.7. Using the hint,\\n\\nFor a quadratic equation to always take on non-negaitive values, we must have a non-positive discriminant, i. e.,\\n\\n0 ≤\\n\\nn(cid:88)i=1\\n\\ni(cid:33) ζ 2 = A + Bζ + Cζ 2\\n\\n(vi + wiζ)2 =\\n\\nv2\\n\\nw2\\n\\nn(cid:88)i=1\\n\\ni + 2(cid:32) n(cid:88)i=1\\nviwi(cid:33) ζ +(cid:32) n(cid:88)i=1\\nviwi(cid:33)2\\n0 ≥ B2 − 4AC = 4(cid:32) n(cid:88)i=1\\ni(cid:33)(cid:32) n(cid:88)i=1\\n− 4(cid:32) n(cid:88)i=1\\ni(cid:33) ≥(cid:32) n(cid:88)i=1\\ni(cid:33)(cid:32) n(cid:88)i=1\\nviwi(cid:33)2\\n\\n(cid:32) n(cid:88)i=1\\n\\nw2\\n\\nv2\\n\\nv2\\n\\n.\\n\\ni(cid:33) .\\n\\nw2\\n\\nNow, divide by 4 and rearrange terms.\\n\\n3.8. The value of the correlation is the same for pairs of observations and for their standardized versions. Thus, we\\ntake x and y to be standardized observations. Then sx = sy = 1. Now, using equation (3.1), we have that\\n\\nx+y = 1 + 1 + 2r = 2 + 2r. Simplifying, we have − 2 ≤ 2r and r ≥ −1.\\nFor the second inequality, use the similar identity to (3.1) for the difference in the observations\\n\\n0 ≤ s2\\n\\ns2\\nx−y = s2\\n\\nx + s2\\n\\ny − 2rsxsy.\\n\\nThen,\\n\\n0 ≤ sx−y = 1 + 1 − 2r = 2 − 2r. Simplifying, we have 2r ≤ 2 and r ≤ 1.\\n\\nThus, correlation must always be between -1 and 1.\\n\\nIn the case that r = −1, we that that s2\\n\\nx+y = 0 and thus using the standardized coordinates\\n\\nxi − ¯x\\nsx\\n\\n+\\n\\nyi − ¯y\\nsy\\n\\n= 0.\\n\\nThus, ζ = sy/sx.\\n\\nIn the case that r = 1, we that that s2\\n\\nx−y = 0 and thus using the standardized coordinates\\n\\nxi − ¯x\\nsx −\\n\\nyi − ¯y\\nsy\\n\\n= 0.\\n\\nThus, ζ = −sy/sx.\\n3.10.\\n\\n1. First the data and the scatterplot, preparing by using mfrowto have side-by-side plots\\n\\n58\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\n> x<-c(-2:3)\\n> y<-c(-3,-1,-2,0,4,2)\\n> par(mfrow=c(1,2))\\n> plot(x,y)\\n\\n2. Then the regression line and its summary.\\n\\n> regress.lm<-lm(y˜x)\\n> summary(regress.lm)\\n\\nCall:\\nlm(formula = y ˜ x)\\n\\nResiduals:\\n1\\n\\n4\\n-2.776e-16 8.000e-01 -1.400e+00 -6.000e-01\\n\\n2\\n\\n3\\n\\n6\\n2.200e+00 -1.000e+00\\n\\n5\\n\\nCoefficients:\\n\\nEstimate Std. Error t value Pr(>|t|)\\n0.3955\\n0.0277 *\\n\\n(Intercept) -0.6000\\nx\\n1.2000\\n---\\nSignif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1\\n\\n0.6309\\n0.3546\\n\\n-0.951\\n3.384\\n\\n1\\n\\nResidual standard error: 1.483 on 4 degrees of freedom\\nMultiple R-squared: 0.7412,Adjusted R-squared:\\nF-statistic: 11.45 on 1 and 4 DF,\\n\\np-value: 0.02767\\n\\n0.6765\\n\\n3. Add the regression line to the scatterplot.\\n\\n> abline(regress.lm)\\n\\n4. Make a data frame to show the predictions and the residuals.\\n\\n> residuals<-resid(regress.lm)\\n> predictions<-predict(regress.lm,newdata=data.frame(x=c(-2:3)))\\n> data.frame(x,y,predictions,residuals)\\n\\nx y predictions\\n\\n1 -2 -3\\n2 -1 -1\\n0 -2\\n3\\n4\\n1 0\\n2 4\\n5\\n6\\n3 2\\n\\nresiduals\\n-3.0 -2.775558e-16\\n-1.8 8.000000e-01\\n-0.6 -1.400000e+00\\n0.6 -6.000000e-01\\n1.8 2.200000e+00\\n3.0 -1.000000e+00\\n\\n5. FInally, the residual plot and a horizontal line at 0.\\n\\n> plot(x,residuals)\\n> abline(h=0)\\n\\n59\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nFigure 3.10: (left) scatterplot and regression line (right) residual plot and horizontal line at 0\\n\\n3.13. Use the subscript y in ˆαy and ˆβy to emphasize that y is the explanatory variable. We still have ¯x = 0.5, ¯y = 0.\\n\\nxi\\nyi\\n-2\\n-3\\n-1\\n-1\\n0\\n-2\\n1\\n0\\n4\\n2\\n2\\n3\\ntotal\\n\\nyi − ¯y\\n\\n-3\\n-1\\n-2\\n0\\n4\\n2\\n0\\n\\nxi − ¯x\\n-2.5\\n-1.5\\n-0.5\\n0.5\\n1.5\\n2.5\\n0\\n\\n(xi − ¯x)(yi − ˆy)\\n\\n7.5\\n1.5\\n1.0\\n0.0\\n6.0\\n5.0\\n\\n(yi − ¯y)2\\n\\n9\\n1\\n4\\n0\\n16\\n4\\n\\ncov(x, y) = 21/5\\n\\nvar(y) = 34/5\\n\\nSo, the slope ˆβy = 21/34 and\\n\\n¯x = ˆαy + ˆβy ¯y,\\n\\n1/2 = ˆαy.\\n\\nThus, to predict x from y, the regression line is ˆxi = 1/2 + 21/34yi. Because the product of the slopes\\n\\n6\\n5 ×\\nthis line differs from the line used to predict y from x.\\n3.14. First we select point, plot them, and add the regression line with x as the explanatory variable.\\n> r<- 0.5;x<-rnorm(25);z<-rnorm(25);y<-r*x + sqrt(1-rˆ2)*z\\n> plot(x,y)\\n> abline(lm(y˜x),col=\"red\")\\n\\n63\\n85 (cid:54)= 1,\\n\\n21\\n34\\n\\n=\\n\\nNow, determine the reverse regression with y as the explanatory variable.\\n\\nx = ˆαy + ˆβyy.\\n\\n(3.28)\\n\\n> lm(x˜y)\\nCall:\\nlm(formula = x ˜ y)\\nCoefficients:\\n(Intercept)\\n0.1505\\n\\ny\\n0.4978\\n\\n60\\n\\n-2-10123-3-2-101234xy-2-10123-1.5-1.0-0.50.00.51.01.52.0xresiduals\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nNow, solve (3.28) for y.\\n\\ny = −\\n\\nˆαy\\nˆβy\\n\\n+\\n\\n1\\nˆβy\\n\\nx\\n\\nThus, in R, we ﬁne the slope and intercept and add a point (¯x, ¯y).\\n\\n> ahat<-0.1505\\n> bhat<-0.4978\\n> abline(a=-ahat/bhat,b=1/bhat,col=\"blue\")\\n> points(mean(x),mean(y),pch=19)\\n\\n3.15. Recall that the covariance of x and y is symmetric, i.e.,\\ncov(x, y) = cov(y, x). Thus,\\n\\ncov(x, y)\\n\\ncov(y, x)\\n\\ncov(x, y)2\\n\\nˆβx· ˆβy =\\nIn the example above, the coefﬁcient of determination,\\n\\ns2\\nxs2\\ny\\n\\ns2\\nx\\n\\ns2\\ny\\n\\n=\\n\\n·\\n\\n=(cid:18)cov(x, y)\\nsxsy (cid:19)2\\n\\n= r2.\\n\\nFigure 3.11: Plot of two regression lines. Notice that they\\ncross at (¯x, ¯y).\\n\\nr2 =\\n\\ncov(x, y)2\\n\\ns2\\nxx2\\ny\\n\\n=\\n\\n(21/5)2\\n\\n(17.5/5) · (34/5)\\n\\n=\\n\\n212\\n\\n17.5 · 34\\n\\n=\\n\\n21\\n35·\\n\\n21\\n17\\n\\n=\\n\\n3\\n5·\\n\\n21\\n17\\n\\n=\\n\\n63\\n85\\n\\n3.16. To show that the correlation is zero, we show that the numera-\\ntor in the deﬁnition, the covariance is zero. First,\\n\\nThe ﬁrst term in this difference,\\n\\ncov(ˆy, y − ˆy) = cov(ˆy, y) − cov(ˆy, ˆy).\\n\\ncov(ˆy, y) = cov(cid:18)cov(x, y)\\n\\ns2\\nx\\n\\nx, y(cid:19) =\\n\\ncov(x, y)2\\n\\ns2\\nx\\n\\n=\\n\\nxs2\\nr2s2\\ny\\ns2\\nx\\n\\n= r2s2\\ny.\\n\\nFor the second,\\n\\nSo, the difference is 0.\\n3.17. For the denominator\\n\\ncov(ˆy, ˆy) = s2\\n\\nˆy = r2s2\\ny.\\n\\ns2\\nDAT A =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(yi − ¯y)2\\n\\nFor the numerator, recall that (¯x, ¯y) is on the regression line. Consequently, ¯y = ˆα + ˆβ ¯x. Thus, the mean of the ﬁts\\n\\nˆy =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nˆyi =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n(ˆα + ˆβxi) = ˆα + ˆβ ¯x = ¯y.\\n\\nThis could also be seen by using the fact (3.11) that the sum of the residuals is 0. For the denominator,\\n\\ns2\\nF IT =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(ˆyi − ˆy)2 =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(ˆyi − ¯y)2.\\n\\nNow, take the ratio and notice that the fractions 1/(n − 1) in the numerator and denominator cancel.\\n\\n61\\n\\n-2.0-1.5-1.0-0.50.00.51.01.5-2-101xy\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\n3.18. The least squares criterion becomes\\n\\nSS(β) =\\n\\n(yi − βxi)2.\\n\\n(β) = −2\\n\\nxi(yi − βxi).\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\nˆβ = (cid:80)n\\n(cid:80)n\\nn(cid:88)j=1\\n\\ni=1 xiyi\\ni=1 x2\\ni\\n\\n.\\n\\nCijxj.\\n\\nji =\\n\\nxjCij.\\n\\nxjC T\\n\\nn(cid:88)j=1\\n\\nn(cid:88)j=1\\nC T =(cid:18) 1 2\\n3 4(cid:19) .\\n2(cid:18) 4 −3\\n−2 1 (cid:19) =(cid:18)−2 3/2\\n1 −1/2(cid:19) .\\n\\n1\\n\\nThe derivative with respect to β is\\n\\n(cid:48)\\n\\nSS\\n\\nSS(cid:48)(β) = 0 for the value\\n\\n3.23. The i-th component of (Cx)T is\\n\\nNow the i-th component of xT C T is\\n\\n3.26. The transpose\\n\\nthe determinant det(C) = 4 − 6 = −2 and\\n\\n−1 =\\n\\nC\\n\\nUsing R,\\n\\n> C<-matrix(c(1,2,3,4),nrow=2)\\n> C\\n\\n[,1] [,2]\\n3\\n4\\n\\n1\\n2\\n\\n[1,]\\n[2,]\\n> t(C)\\n\\n1\\n3\\n\\n[,1] [,2]\\n2\\n4\\n\\n[1,]\\n[2,]\\n> det(C)\\n[1] -2\\n> chol2inv(C)\\n\\n[,1]\\n\\n[,2]\\n[1,]\\n1.5625 -0.1875\\n[2,] -0.1875 0.0625\\n\\n3.27. Using equation (3.22), the i-th component of y − Xβ,\\n\\n(y − Xβ)i = yi −\\n\\nn(cid:88)j=0\\n\\nβjxjk = yi − β0 − xi1β1 − ··· − βkxin.\\n\\n62\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nCorrelation and Regression\\n\\nNow, (y − Xβ)T (y − Xβ) is the dot product of y − Xβ with itself. This gives (3.24).\\n3.28. Write xi0 = 1 for all i, then we can write (3.24) as\\n\\nThen,\\n\\nSS(β) =\\n\\nn(cid:88)i=1\\n\\n(yi − xi0β0 − xi1β1 − ··· − βkxik)2.\\n\\n∂\\n∂βj\\n\\nS(β) = −2\\n\\n= −2\\n\\n(yi − xi0β0 − xi1β1 − ··· − βkxik)xij\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n(yi − (Xβ)i)xij = −2((y − Xβ)T X))j.\\n\\nThis is the j-th coordinate of (3.25).\\n3.29. HX = (X T X)−1X T X = (X T X)−1(X T X) = I, the identity matrix.\\n\\n63\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\n64\\n\\n\\x0cTopic 4\\n\\nProducing Data\\n\\nStatistics has been the handmaid of science, and has poured a ﬂood of light upon the dark questions of\\nfamine and pestilence, ignorance and crime, disease and death. - James A. Garﬁeld, December 16, 1867\\nOur health care is too costly; our schools fail too many; and each day brings further evidence that the\\nways we use energy strengthen our adversaries and threaten our planet.\\n\\nThese are the indicators of crisis, subject to data and statistics. Less measurable but no less profound\\nis a sapping of conﬁdence across our land a nagging fear that America’s decline is inevitable, and that\\nthe next generation must lower its sights. - Barack Obama, January 20, 2009\\n\\n4.1 Preliminary Steps\\nMany questions begin with an anecdote or an unexplained occurrence in the lab or in the ﬁeld. This can lead to fact-\\nﬁnding interviews or easy to perform experimental assays. The next step will be to review the literature and begin\\nan exploratory data analysis often using publically available data. At this stage, we are looking, on the one hand,\\nfor patterns and associations, and, on the other hand, apparent inconsistencies occurring in the scientiﬁc literature.\\nNext we will examine the data using quantitative methods - summary statistics for quantitative variables, tables for\\ncategorical variables - and graphical methods - boxplots, histograms, scatterplots, time plots for quantitative data - bar\\ncharts for categorical data.\\n\\nThe strategy of these investigations is frequently the same - look at a sample in order to learn something about a\\n\\npopulation or to take a census or the total population.\\n\\nDesigns for producing data begin with some basic questions:\\n\\n• What can I measure?\\n• What shall I measure?\\n• How shall I measure it?\\n• How frequently shall I measure it?\\n• What obstacles do I face in obtaining a reliable measure?\\nThe frequent goal of a statistical study is to investigate the nature of causality. In this way we try to explain the\\nvalues of some response variables based on knowing the values of one or more explanatory variables. The major\\nissue is that the associated phenomena could be caused by a third, previously unconsidered factor, called a lurking\\nvariable or confounding variable.\\n\\nTwo approaches are generally used to mitigate the impact of confounding. The ﬁrst, primarily statistical, involves\\nsubdividing the population under study into smaller groups that are more similar. This subdivision is called cross\\n\\n65\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\ntabulation or stratiﬁcation. For human studies, this could mean subdivision by gender, by age, by economic class,\\nby geographic region, or by level of education. For laboratory, this could mean subdivision by temperature, by pH,\\nby length of incubation, or by concentration of certain compounds (e.g. ATP). For ﬁeld studies, this could mean\\nsubdivision by soil type, by average winter temperature or by total rainfall. Naturally, as the number of subgroups\\nincrease, the size of these groups can decrease to the point that chance effects dominate the data.\\n\\nThe second is mathematical or probabilistic modeling. These models often take the form of a mechanistic model\\n\\nthat takes into an account the variables in the cross tabulation and builds a parametric model.\\n\\nThe best methodologies, of course, make a comprehensive use of both of these types of approaches.\\n\\n4.2 Professional Ethics\\n\\nAs a citizen, we should participate in public discourse. Those with particular training have a special obligation to bring\\nto the public their special knowledge. Such public statements can take several forms. We can speak out as a member\\nof society with no particular basis in our area of expertise. We can speak out based on the wisdom that comes with this\\nspecialized knowledge. Finally, we can speak out based on a formal procedure of gathering information and reporting\\ncarefully the results of our analysis. In each case, it is our obligation to be clear about the nature of that communication\\nand that the our statements follow the highest ethical standards. In the same vein, as consumers of information, we\\nshould have a clear understanding of the perspective in any document that presents statistical information.\\n\\nProfessional statistical societies have provided documents that provide guidance on what can be sometimes be\\ndifﬁcult judgements and decisions. Two sources of guidance are the Ethical Guidelines for Statistical Practice from\\nthe American Statistical Society.\\n\\nhttp://www.amstat.org/about/ethicalguidelines.cfm\\n\\nand the International Statistical Institute Declaration on Professional Ethics\\n\\nhttp://www.isi-web.org/about-isi/professional-ethics\\n\\n4.3 Formal Statistical Procedures\\n\\nThe formal procedures that will be described in this section presume that we will have a sufﬁciently well understood\\nmathematical model to support the analysis of data obtained under a given procedure. Thus, this section anticipates\\nsome of the concepts in probability theory like independence, conditional probability, distributions under different\\nsampling protocols and expected values. It also will rely fundamentally on some of the consequences of this theory\\nas seen, for example, in the law of large numbers and the central limit theorem. These are topics that we shall soon\\nexplore in greater detail.\\n\\n4.3.1 Observational Studies\\nThe goal is to learn about a population by observing a sample with as little disturbance as possible to the sample.\\n\\nSometimes the selection of treatments is not under the control of the researcher. For example, if we suspect that a\\ncertain mutation would render a virus more or less virulent, we cannot ethically perform the genetic engineering and\\ninfect humans with the viral strains.\\n\\nFor an observational study, effects are often confounded and thus causation is difﬁcult to assert. The link between\\nsmoking and a variety of diseases is one very well known example. We have seen the data set relating student smoking\\nhabits in Tucson to their parents. We can see that children of smokers are more likely to smoke. This is more easily\\ndescribed if we look at conditional distributions.\\n\\n66\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\n0 parents smoke\\n\\nstudent smokes\\n\\n0.1386\\n\\nstudent does not smoke\\n\\n0.8614\\n\\n1 parent smoke\\n\\nstudent smokes\\n\\n0.1858\\n\\nstudent does not smoke\\n\\n0.8142\\n\\n2 parents smoke\\n\\nstudent smokes\\n\\n0.2247\\n\\nstudent does not smoke\\n\\n0.7753\\n\\nTo display these conditional distributions in R:\\n\\n> smoking<-matrix(c(400,1380,416,1823,188,1168),ncol=3)\\n> smoking\\n\\n[,1] [,2] [,3]\\n[1,]\\n400 416 188\\n[2,] 1380 1823 1168\\n> condsmoke<-matrix(rep(0,6),ncol=3)\\n> for (i in 1:3)\\n\\n{condsmoke[,i]=smoking[,i]/sum(smoking[,i])}\\n\\n> colnames(condsmoke)\\n\\n<-c(\"2 parents\",\"1 parent\", \"0 parents\")\\n\\n> rownames(condsmoke)\\n\\n<-c(\"smokes\",\"does not smoke\")\\n\\n> condsmoke\\n\\n2 parents 1 parent 0 parents\\nsmokes\\n0.2247191 0.1857972 0.1386431\\ndoes not smoke 0.7752809 0.8142028 0.8613569\\n> barplot(condsmoke,legend=rownames(condsmoke))\\n\\nEven though we see a trend - children are more likely to smoke in households with parents who smoke, we cannot\\nassert causation, i.e., children smoke because their parents smoke. An alternative explanation might be, for example,\\npeople may have a genetic predisposition to smoking.\\n\\n4.3.2 Randomized Controlled Experiments\\nIn a controlled experiment, the researcher imposes a treatment on the experimental units or subjects in order to\\nobserve a response. Great care and knowledge must be given to the design of an effective experiment. A University\\nof Arizona study on the impact of diet on cancers in women had as its goal speciﬁc recommendations on diet. Such\\nrecommendations were set to encourage lifestyle changes for millions of American women. Thus, enormous effort\\nwas taken in the design of the experiment so that the research team was conﬁdent in its results.\\n\\nA good experimental design is one that is based on a solid understanding of both the science behind the study and\\nthe probabilistic tools that will lead to the inferential techniques used for the study. This study is often set to assess\\nsome hypothesis - Do parents smoking habits inﬂuence their children? or estimate some value - What is the mean\\nlength of a given strain of bacteria?\\n\\nPrinciples of Experimental Design\\n\\n1. Control for the effects of lurking variables by comparing several treatments.\\n\\n67\\n\\n2 parents1 parent0 parentsdoes not smokesmokes0.00.20.40.60.81.01.2\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\n2. Randomize the assignment of subjects to treatments to eliminate bias due to systematic differences among\\n\\ncategories.\\n\\n3. Replicate the experiment on many subjects to reduce the impact of chance variation on the results.\\n\\nIssues with Control\\nThe desired control can sometimes be quite difﬁcult to achieve. For example;\\n\\n• In medical trials, some individuals may display a placebo effect, the favorable response to any treatment.\\n• Overlooking or introducing a lurking variable can introduce a hidden bias.\\n• The time and money invested can lead to a subconscious effect by the experimenter. Use an appropriate blind\\nor double blind procedure. In this case, neither the experimenter nor the subject are aware of which treatment\\nis being used.\\n\\n• Changes in the wording of questions can lead to different outcomes.\\n• Transferring discoveries from the laboratory to a genuine living situation can be difﬁcult to make.\\n• The data may suffer from undercoverage of difﬁcult to ﬁnd groups. For example, mobile phone users are less\\n\\naccessible to pollsters.\\n\\n• Some individuals leave the experimental group, especially in longitudinal studies.\\n• In some instances, a control is not possible. The outcomes of the absence of the enactment of an economic\\npolicy, for example, a tax cut or economic stimulus plan, cannot be directly measured. Thus, economists are\\nlikely to use a mathematical model of different policies and examine the outcomes of computer simulations as a\\nproxy for control.\\n\\n• Social desirability bias describes the tendency of survey respondents to answer questions in a manner that will\\nbe viewed favorably by others. Thus surveys on medical issues, religious practices, sexual practices, political\\npreferences, personal achievement typically use specialized techniques to obtain more truthful responses. The\\nBradley effect is a theory proposed to explain observed discrepancies between voter opinion polls and election\\noutcomes in some US government elections where a white candidate and a non-white candidate run against each\\nother. The theory proposes that some voters tend to tell pollsters that they are undecided or likely to vote for\\na black candidate, and yet, on election day, vote for his white opponent. It was named after Tom Bradley, an\\nAfrican-American who lost the 1982 California governor’s race despite being ahead in voter polls going into the\\nelections.\\n\\nSetting a Design\\nBefore data are collected, we must consider some basic questions:\\n• Decide on the number of explanatory variables or factors.\\n• Decide on the values or levels that will be used in the treatment.\\n\\nExample 4.1. For over a century, beekeepers have attempted to breed honey bees belonging to different races to take\\nadvantage of the effects of hybrid vigor to create a better honey producer. No less a ﬁgure than Gregor Mendel failed\\nin this endeavor because he could not control the matings of queens and drones.\\n\\nA more recent failure, a breeding experiment using African and European bees, occurred in 1956 in an apiary\\nin the southeast of Brazil. The hybrid Africanized honey bees escaped, and today, in the western hemisphere, all\\nAfricanized honey bees are descended from the 26 Tanzanian queen bees that resided in this apiary. By the mid-1990s,\\nAfricanized bees have spread to Texas, Arizona, New Mexico, Florida and southern California.\\n\\n68\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\nWhen the time arrives for replacing the mother queen in a colony (a process known as supercedure), the queen\\nwill lay about ten queen eggs. The ﬁrst queen that completes her development and emerges from her cell is likely to\\nbecome the next queen. Suppose we have chosen to investigate the question of whether a shorter time for development\\nfor Africanized bee queens than for the resident European bee queens is the mechanism behind the replacement by\\nAfricanized subspecies in South and Central American and in the southwestern United States. The development time\\nwill depend upon hive temperature, so we will determine a range of hive temperatures by looking through the literature\\nand making a few of our own measurements. From this, we will set a cool, medium, and warm hive temperature. We\\nwill use European honey bee (EHB) queens as a control. Thus, we have two factors.\\n\\n• Queen type - European or Africanized\\n• Hive temperature - cool, medium, or warm.\\nThus, this experiment has 6 treatment groups.\\n\\nFactor B: hive temperature\\n\\ncool\\n\\nmedium\\n\\nwarm\\n\\nFactor A:\\ngenotype\\n\\nAHB\\n\\nEHB\\n\\nThe response variable is the queen development time - the length of time from the depositing of the egg from the\\nmother queen to the time that the daughter queen emerges from the hive. The immature queen is kept in the hive to be\\nfed during the egg and larval stages. At that point the cell containing the larval queen is capped by the worker bees.\\nThe experimenter then transfers the cell to an incubator for the pupal stage. The hive where the egg is laid and the\\nincubator that houses the queen is checked using a remote camera so that we have an accurate measure of the queen\\ndevelopment time.\\n\\nFor our experimental design, we will rear 120 queens altogether and use 20 in each treatment group. A few queens\\nare chosen and their genotypes are determined to verify the genetic designations of the groups. To reduce hidden\\nbiases, the queens in the incubator are labeled in such a way that their genotype is unknown. The determination\\nhow the number of samples in the study is necessary to have the desired conﬁdence in our results is called a power\\nanalysis. We will investigate this aspect of experimental design when we study hypothesis testing.\\n\\nRandom Samples\\nA simple random sample (SRS) of size n consists of n individuals chosen in such a way that every set of n individuals\\nhas an equal chance to be in the sample actually selected. This is easy to accomplish in R. First, give labels to the\\nindividuals in the population and then use the command sample to make the random choice. For the experiment\\nabove, we rear 90 Africanized queens and choose a sample of 60. (Placing the command in parenthesis calls on R to\\nprint the output.)\\n\\n69\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\n> population<-1:90\\n> (subjects<-sample(population,60))\\n\\n[1] 61 16 65 73 13 25 10 82 24 62 28 66 55\\n\\n[25] 87 68 22 11 5 48 33 63 50 88 35 37 84 12\\n[49] 78 49 45 7 64 3 42 57 81 56 46 32\\n\\n4 59 90 86\\n\\n8 26 72 67 17 58 69\\n\\n6 27 41 20\\n2 60 19 18 74 23\\n\\nIf your experimental design call for grouping similar individuals, called strata, then a stratiﬁed random sample\\nfrom the full sample by choosing a separate random sample from each stratum. If one or more of the groups forms\\na small fraction of the population, then a stratiﬁed random sample ensures the desired number of sample from these\\ngroups is included in the sample.\\n\\nIf we mark the 180 queens 1 through 180 with 1 through 90 being Africanized bees and 91 through 180 being\\n\\nEuropean, then we can enter\\n\\n> population<-1:180\\n> subjectsAHB<-sample(population[1:90],60)\\n> subjectsEHB<-sample(population[91:180],60)\\n\\nto ensure that 60 come from each group.\\n\\nFor the example above, we divide the sampled Africanized queens into 3 treatment groups based on hive tempera-\\nture. Here dim=c(3,20) signiﬁes that the array has 3 rows and 20 columns. Let the ﬁrst row be the choice of queen\\nbees for the cool hive, the second row for the medium temperature hive, and row three for the warm hive.\\n\\n> groups<-array(subjectsAHB,dim=c(3,20))\\n> groups\\n\\n[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\\n84\\n12\\n4\\n\\n62\\n28\\n66\\n\\n55\\n8\\n26\\n\\n73\\n13\\n25\\n\\n72\\n67\\n17\\n\\n58\\n69\\n6\\n\\n27\\n41\\n20\\n\\n11\\n5\\n48\\n\\n33\\n63\\n50\\n\\n88\\n35\\n37\\n\\n10\\n82\\n24\\n\\n61\\n16\\n65\\n\\n87\\n68\\n22\\n\\n[,14] [,15] [,16] [,17] [,18] [,19] [,20]\\n56\\n46\\n32\\n\\n59\\n90\\n86\\n\\n2\\n60\\n19\\n\\n18\\n74\\n23\\n\\n78\\n49\\n45\\n\\n7\\n64\\n3\\n\\n42\\n57\\n81\\n\\n[1,]\\n[2,]\\n[3,]\\n\\n[1,]\\n[2,]\\n[3,]\\n\\nMost of the data sets that we shall encounter in this book have a modest size with hundreds and perhaps thousands\\nIn these situation, we can be careful in assuring that the\\nof observations based on a small number of variables.\\nexperimental design was followed. We can make the necessary visual and numerical summaries of the data set to\\nassess its quality and make appropriate corrections to ethically clean the data from issues of mislabeling and poorly\\ncollected observations. This will prepare us for the more formal procedures that are the central issues of the second\\nhalf of this book.\\n\\nWe are now in a world of massive datasets, collected, for example, from genomic, astronomical observations or\\nsocial media. Data collection, management and analysis require new and more sophisticated approaches that maintain\\ndata integrity and security. These considerations form a central issue in modern statistics.\\n\\n4.3.3 Natural experiments\\nIn this situation, a naturally occurring instance of the observable phenomena under study approximates the situation\\nfound in a controlled experiment. For example, during the oil crisis of the mid 1970s, President Nixon imposed a 55\\nmile per hour speed limit as a strategy to reduce gasoline consumption. This action had a variety of consequences\\nfrom reduced car accidents to the economic impact of longer times for the transportation of goods. In this case, the\\nstatus quo ante served as the control and the imposition of new highway laws became the natural experiment.\\n\\nHelena, Montana during the six-month period from June 2002 to December 2002 banned smoking ban in all public\\nspaces including bars and restaurants. This becomes the natural experiment with the control groups being Helena\\nbefore and after the ban or other Montana cities during the ban. More recently, neighboring states either decided for\\n\\n70\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\nor against Medicaid expansion under the Affordable Care act. This natural experiment allowed for comparison of a\\nvariety of health and quality of life measures.\\n\\n4.4 Case Studies\\n4.4.1 Observational Studies\\nGovernments and private consortia maintain databases to assist the public and researchers obtain data both for ex-\\nploratory data analysis and for formal statistical procedures. We present several examples below.\\n\\nUnited States Census\\nThe ofﬁcial United States Census is described in Article I, Section 2 of the Constitution of the United States.\\n\\nThe actual enumeration shall be made within three years after the ﬁrst meeting of the Congress of the\\nUnited States, and within every subsequent term of 10 years, in such manner as they shall by Law direct.\\n\\nIt calls for an actual enumeration to be used for apportionment of seats in the House of Representatives among the\\n\\nstates and is taken in years that are multiples of 10 years. See the plans for the 2020 census at\\n\\nhttps://www.census.gov/2020census\\n\\nU.S. Census ﬁgures are based on actual counts of persons dwelling in U.S. residential structures. They include\\ncitizens, non-citizen legal residents, non-citizen long-term visitors, and undocumented immigrants. In recent censuses,\\nestimates of uncounted housed, homeless, and migratory persons have been added to the directly reported ﬁgures.\\n\\nIn addition, the Censsu Bureau provides a variety of interactive internet data tools:\\n\\nhttps://www.census.gov/2010census/\\n\\nCurrent Population Survey\\nThe Current Population Survey (CPS) is a monthly survey of about 60,000 households conducted by the Bureau of the\\nCensus for the Bureau of Labor Statistics. The survey has been conducted for more than 50 years.\\n\\nhttps://www.census.gov/programs-surveys/cps.html\\n\\nSelecting a random sample requires a current database of every household. The random sample is mutistage.\\n\\n1. Take a sample from the 3000 counties (or contiguous counties inside a state) in the United States.\\n\\n2. Take a sample of unit frames consisting of housing units in census blocks that contain a very high proportion of\\n\\ncomplete addresses\\n\\n3. Take a sample of households (called primary sampling units) from each unit frame.\\n\\nHouseholds are interviewed for 4 consecutive months, leave the sample for 8 months, and then returns for 4 more\\n\\nconsecutive months. An adult member of each household provides information for all members of the household.\\n\\nWorld Health Organization Global Health Observatory (GHO)\\nThe Global Health Observatory is the World Heatlh Organization’s internet gateway to health-related statistics. The\\nGHO compiles and veriﬁes major sources of health data to provide easy access to scientiﬁcally sound information.\\nGHO covers global health priorities such as the health-related Millennium Development Goals, women and health,\\nmortality and burden of disease, disease outbreaks, and health equity and health systems.\\n\\nhttp://www.who.int/gho/en/\\n\\n71\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\nThe Women’s Health Initiative\\nThe Women’s Health Initiative (WHI) was a major 15-year research program to address the most common causes of\\ndeath, disability and poor quality of life in postmenopausal women.\\n\\nhttps://www.nhlbi.nih.gov/science/womens-health-initiative-whi\\n\\nThe WHI observational study had several goals. These goals included:\\n• To give reliable estimates of the extent to which known risk factors to predict heart disease, cancers and fractures.\\n• To identify ”new” risk factors for these and other diseases in women.\\n• To compare risk factors, presence of disease at the start of the study, and new occurrences of disease during the\\n\\nWHI across all study components.\\n\\n• To create a future resource to identify biological indicators of disease, especially substances and factors found\\n\\nin blood.\\n\\nThe observational study enlisted 93,676 postmenopausal women between the ages of 50 to 79. The health of\\nparticipants was tracked over an average of eight years. Women who joined this study ﬁlled out periodic health forms\\nand also visited the clinic three years after enrollment. Participants were not required to take any medication or change\\ntheir health habits.\\n\\nGenBank\\nThe GenBank sequence database is an open access of nucleotide sequences and their protein translations. This database\\nis produced at National Center for Biotechnology Information (NCBI) as part of the International Nucleotide Sequence\\nDatabase Collaboration, or INSDC. GenBank has a new release every two months. As of 15 August 2017, GenBank\\nrelease 221.0 has 203,180,606 loci, 240,343,378,258 bases, from 203,180,606 reported sequences. .\\n\\nhttp://www.ncbi.nlm.nih.gov/genbank/\\n\\n4.4.2 Experiments\\nThe history of science has many examples of experiments whose results strongly changed our view of the nature of\\nthings. Here we highlight two very important examples.\\n\\n72\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\nLight: Its Speed and Medium of Propagation\\n\\nFor many centuries before the seventeenth, a debate continued as to whether light travelled instantaneously or at\\na ﬁnite speed. In ancient Greece, Empedocles maintained that light was something in motion, and therefore must\\ntake some time to travel. Aristotle argued, to the contrary, that “light is due to the presence of something, but it is\\nnot a movement.” Euclid and Ptolemy advanced the emission theory of vision, where light is emitted from the eye.\\nConsequently, Heron of Alexandria argued, the speed of light must be inﬁnite because distant objects such as stars\\nappear immediately upon opening the eyes.\\n\\nIn 1021, Islamic physicist Alhazen (Ibn al-Haytham) published the Book of Optics, in which he used experiments\\nrelated to the camera obscura to support the now accepted intromission theory of vision, in which light moves from\\nan object into the eye. This led Alhazen to propose that light must therefore have a ﬁnite speed. In 1574, the Ottoman\\nastronomer and physicist Taqi al-Din also concluded that the speed of light is ﬁnite, correctly explained refraction as\\nthe result of light traveling more slowly in denser bodies, and suggested that it would take a long time for light from\\ndistant stars to reach the Earth. In the early 17th century, Johannes Kepler believed that the speed of light was inﬁnite\\nsince empty space presents no obstacle to it.\\n\\nIn 1638, Galileo Galilei ﬁnally proposed an experiment to mea-\\nsure the speed of light by observing the delay between uncovering\\na lantern and its perception some distance away. In 1667, Galileo’s\\nexperiment was carried out by the Accademia del Cimento of Flo-\\nrence with the lanterns separated by about one mile. No delay was\\nobserved. The experiment was not well designed and led to the con-\\nclusion that if light travel is not instantaneous, it is very fast. A\\nmore powerful experimental design to estimate of the speed of light\\nwas made in 1676 by Ole Christensen Romer, one of a group of\\nastronomers of the French Royal Academy of Sciences. From his\\nobservations, the periods of Jupiter’s innermost moon Io appeared\\nto be shorter when the earth was approaching Jupiter than when re-\\nceding from it, Romer concluded that light travels at a ﬁnite speed,\\nand was able to estimate that would it take light 22 minutes to cross\\nthe diameter of Earth’s orbit. Christiaan Huygens combined this es-\\ntimate with an estimate for the diameter of the Earth’s orbit to obtain\\nan estimate of speed of light of 220,000 km/s, 26% lower than the\\nactual value.\\n\\nWith the ﬁnite speed of light established, nineteenth century\\nphysicists, noting that both water and sound waves required a\\nmedium for propagation, postulated that the vacuum possessed a “lu-\\nminiferous aether”, the medium for light waves. Because the Earth\\nis in motion, the ﬂow of aether across the Earth should produce a\\ndetectable “aether wind”. In addition, because the Earth is in orbit\\nabout the Sun and the Sun is in motion relative to the center of the\\nMilky Way, the Earth cannot remain at rest with respect to the aether\\nat all times. Thus, by analysing the speed of light in different di-\\nrections at various times, scientists could measure the motion of the\\nEarth relative to the aether.\\n\\nIn order to detect aether ﬂow, Albert Michelson designed a light\\ninterferometer sending a single source of white light through a half-\\nsilvered mirror that split the light into two beams travelling at right\\nangles to one another. The split beams were recombined producing\\na pattern of constructive and destructive interference based on the travel time in transit.\\nIf the Earth is traveling\\nthrough aether, a beam reﬂecting back and forth parallel to the ﬂow of ether would take longer than a beam reﬂecting\\nperpendicular to the aether because the time gained from traveling with the aether is less than that lost traveling against\\n\\nFigure 4.1: Romer’s diagram of Jupiter (B) eclipsing its\\nmoon Io (DC) as viewed from different points in earth’s\\norbit around the sun\\n\\n73\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\nthe ether. The result would be a delay in one of the light beams that could be detected by their interference patterns\\nresulting for the recombined beams. Any slight change in the travel time would then be observed as a shift in the\\npositions of the interference fringes. While Michaelson’s prototype apparatus showed promise, it produced far too\\nlarge experimental errors.\\n\\nIn 1887, Edward Morley joined the effort to create a new device with enough accuracy to detect the aether wind.\\nThe new apparatus had a longer path length, it was built on a block of marble, ﬂoated in a pool of mercury, and located\\nin a closed room in the basement of a stone building to eliminate most thermal and vibrational effects. The mercury\\npool allowed the device to be turned, so that it could be rotated through the entire range of possible angles to the\\nhypothesized aether wind. Their results were the ﬁrst strong evidence against the aether theory and formed a basic\\ncontribution to the foundation of the theory of relativity. Thus, two natural questions - how fast does light travel and\\ndoes it need a medium - awaited elegant and powerful experiments to achieve the understanding we have today and\\nset the stage for the theory of relatively, one of the two great theories of modern physics.\\n\\nPrinciples of Inheritance and Genetic Material\\n\\nPatterns of inheritance have been noticed for millenia. Because of the needs for food, domesticated plants and animals\\nhave been bred according to deliberate patterns for at least 5000 years. Progress towards the discovery of the laws\\nfor inheritance began with a good set of model organisms. For example, annual ﬂowering plants had certainly been\\nused successfully in the 18th century by Josef Gottlieb K¨olreuter. His experimental protocols took the advantage of\\nthe fact that these plants are easy to grow, have short generation times, have individuals that possess both male and\\nfemale reproductive organs, and have easily controlled mating through artiﬁcial pollination. K¨olreuter established a\\nprinciple of equal parental contribution. The nature of inheritance remained unknown with a law of blending becoming\\na leading hypothesis. Indeed, Charles Darwin adopted this rationale, calling it pangenesis.\\n\\nIn the 1850s and 1860s, the Austrian monk Gregor Mendel used pea plants to work out the basic principles of\\ngenetics as we understand them today. Through careful inbreeding, Mendel found 7 true-breeding traits - traits that\\nremained present through many generations and persisted from parent to offspring. By this process, Mendel was sure\\nthat potential parent plants were from a true-breeding strain. Mendel’s explanatory variables were the traits of the\\nparental generation, G. His response variables were the traits of the individual plants in the ﬁrst ﬁlial generation,\\nF1 and second ﬁlial generation, F2.\\n\\nMendel noted that only one trait was ever ex-\\npressed in the F1 generation and called it domi-\\nnant. The alternative trait was called recessive.\\nThe most striking result is that in the F2 generation\\nthe fraction expressing the dominant trait was very\\nclose to 3/4 for each of the seven traits. (See the\\ntable below summarizing Mendel’s data.) These\\nresults in showing no intermediate traits disprove\\nthe blending hypothesis. Also, the blending theory\\ncould not explain the appearance of a pea plant ex-\\npressing the recessive trait that is the offspring of\\ntwo plants each expressing the dominant trait. This\\nlead to the hypothesis that each plant has two units\\nof inheritance and transmits one of them to each of\\nits offspring. Mendel could check this hypothesis by crossing, in modern terms, heterozygous plants with those that\\nare dominant homozygous. Mendel went on to examine the situation in which two traits are examined simultaneously\\nand showed that the two traits sort independently. We now use the squares devised in 1905 by Reginald Punnett to\\ncompute the probabilities of a particular cross or breeding experiment.\\n\\nFigure 4.2: Mendel’s traits and experiments.\\n\\n74\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\nparental phenotypes\\n\\ndominant\\n\\nrecessive\\n\\nspherical seeds × wrinkled seeds\\nyellow seeds × green seeds\\npurple ﬂowers × white ﬂowers\\ninﬂated pods × constricted pods\\ngreen pods × yellow pods\\naxial ﬂowers × terminal ﬂowers\\ntall stems × dwarf stems\\n\\nF2 generation phenotypes\\nrecessive\\ndominant\\n1850\\n5474\\n6022\\n2001\\n224\\n705\\n299\\n882\\n152\\n428\\n207\\n651\\n787\\n277\\n\\ntotal\\n7324\\n8023\\n929\\n1181\\n580\\n858\\n1064\\n\\nfraction dominant\\n\\n0.747\\n0.751\\n0.758\\n0.747\\n0.738\\n0.759\\n0.740\\n\\nWe now know that many traits whose expression depends on environment can vary continuously. We can also see\\nthat some genes are linked by their position and do not sort independently. (A pea plant has 7 pairs of chromosomes.)\\nThe effects can sometimes look like blending. But thanks to Mendel’s work, we can see how these expressions are\\nbuilt from the expression of several genes.\\n\\nNow we know that inheritance is given in “packets”. The next question is what material in the living cell is\\nthe source of inheritance. Theodor Boveri using sea urchins and Walter Sutton using grasshoppers independently\\ndeveloped the chromosome theory of inheritance in 1902. From their work, we know that all the chromosomes\\nhad to be present for proper embryonic development and that chromosomes occur in matched pairs of maternal and\\npaternal chromosomes which separate during meiosis. Soon thereafter, Thomas Hunt Morgan, working with the fruit\\nﬂy Drosophila melanogaster as a model system, noticed that a mutation resulting in white eyes was linked to sex - only\\nmales had white eyes. Microscopy revealed a dimorphism in the sex chromosome and with this information, Morgan\\ncould predict the inheritance of sex linked traits. Morgan continued to learn that genes must reside on a particular\\nchromosomes.\\n\\nWe now think of chromosomes as composed of\\nDNA, but it is in reality an organized structure of DNA\\nand protein. Thus, which of the two formed the in-\\nheritance material was in doubt. Phoebus Levene, who\\nidentiﬁed the components of DNA, declared that it could\\nnot store the genetic code because it was chemically far\\ntoo simple. At that time, DNA was wrongly thought to\\nbe made up of regularly repeated tetranucleotides and\\nso could not be the carrier of genetic information. In-\\ndeed, in 1944 when Oswald Avery, Colin MacLeod, and\\nMaclyn McCarty found that DNA to be the substance\\nthat causes bacterial transformation, the scientiﬁc com-\\nmunity was reluctant to accept the result despite the care\\ntaken in the experiments. These researchers considered\\nseveral organic molecules - proteins, nucleic acids, car-\\nbohydrates, and lipids.\\nIn each case, if the DNA was\\ndestroyed, the ability to continue heritability ended.\\n\\nAlfred Hershey and Martha Chase\\n\\ncontinued the\\nsearch for the genetic material with an experiment using\\nbacteriophage. This virus that infects bacteria is made\\nup of liitle more than DNA inside a protein shell. The\\nvirus introduces material into the bacterium that co-opts\\nthe host, producing dozens of viruses that emerge from the lysed bacterium. Their experiment begins with growing\\none culture of phage in a medium containing radioactive phosphorus (that appears in DNA but not in proteins) and\\nanother culture in a medium containing radioactive sulfur (that appears in proteins but not in DNA). Afterwards they\\nagitated the bacteria in a blender to strip away the parts of the virus that did not enter the cell in a way that does\\nminimal damage to the bacteria. They then isolated the bacteria ﬁnding that the sulfur separated from the bacteria and\\nthat the phosphorus had not. By 1952 when Hershey and Chase conﬁrmed that DNA was the genetic material with\\n\\n75\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\ntheir experiment using bacteriophage, scientists were more prepared to accept the result. This, of course, set the stage\\nfor the importance of the dramatic discovery by Watson, Crick, and Franklin of the double helix structure of DNA.\\n\\nAgain, for both of these fundamental discoveries, the principles of inheritance and DNA as the carrier of inheritance\\ninformation, the experimental design was key. In the second case, we learned that even though Avery, MacLeod, and\\nMcCarty had designed their experiment well, they did not, at that time, have a scientiﬁc community prepared to\\nacknowledge their ﬁndings.\\n\\nSalk Vaccine Field Trials\\nPoliomyelitis, often called polio or infantile paral-\\nysis, is an acute viral infectious disease spread\\nfrom person to person, primarily via the fecal-oral\\nroute. The overwhelming majority of polio infec-\\ntions have no symptoms. However, if the virus en-\\nters the central nervous system, it can infect motor\\nneurons, leading to symptoms ranging from mus-\\ncle weakness and paralysis. The effects of polio\\nhave been known since prehistory; Egyptian paint-\\nings and carvings depict otherwise healthy people\\nwith withered limbs, and children walking with\\ncanes at a young age. The ﬁrst US epidemic was\\nin 1916. By 1950, polio had claimed hundreds of\\nthousands of victims, mostly children.\\n\\nIn 1950, the Public Health Service (PHS) orga-\\nnized a ﬁeld trial of a vaccine developed by Jonas\\nSalk.\\n\\nPolio is an epidemic disease with\\n• 60,000 cases in 1952, and\\n• 30,000 cases in 1953.\\nSo, a low incidence without control could mean\\n• the vaccine works, or\\n• no epidemic in 1954.\\nSome basic facts were known before the trial started:\\n• Higher income parents are more likely to consent to allow children to take the vaccine.\\n• Children of lower income parents are thought to be less susceptible to polio. The reasoning is that these children\\nlive in less hygienic surroundings and so are more likely to contract very mild polio and consequently more\\nlikely to have polio antibodies.\\n\\nTo reduce the role of chance variation dominating the results, the United States Public Health Service (PHS)\\ndecided on a study group of two million people. At the same time, a parents advocacy group, the National Foundation\\nfor Infantile Paralysis (NFIP) set out its own design. Here are the essential features of the NFIP design:\\n\\n• Vaccinate all grade 2 children with parental consent.\\n• Use grades 1 and 3 as controls.\\nThis design fails to have some of essential features of the principles of experimental design. Here is a critique:\\n\\n76\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\n• Polio spreads through contact, so infection of one child in a class can spread to the classmates.\\n• The treatment group is biased towards higher income.\\nThus, the treatment group and the control group have several differences beyond the fact that the treatment group\\nreceives the vaccine and the control group does not. This leaves the design open to having lurking variables be the\\nprimary cause in the differences in outcomes between the treatment and control groups. The Public Health Service\\ndesign is intended to take into account these shortcomings. Their design has the following features:\\n\\n• Flip a coin for each child. (randomized control)\\n• Children in the control group were given an injection of salt water. (placebo)\\n• Diagnosticians were not told whether a child was in treatment or control group. (double blind)\\nThe results:\\n\\nPHS\\n\\nSize\\n\\nTreatment\\nControl\\nNo consent\\n\\n200,000\\n200,000\\n350,000\\n\\nNFIP\\n\\nSize\\n\\n225,000\\n725,000\\n125,000\\n\\nRate\\n25\\n54\\n44\\n\\nRate\\n28\\n71\\n46\\n\\nRates are per 100,000\\n\\nWe shall learn later that the evidence is overwhelming that the vaccine reduces the risk of contracting polio. As a\\nconsequence of the study, universal vaccination was undertaken in the United States in the early 1960s. A global effort\\nto eradicate polio began in 1988, led by the World Health Organization, UNICEF, and The Rotary Foundation. These\\nefforts have reduced the number of annual diagnosed from an estimated 350,000 cases in 1988 to 1,310 cases in 2007.\\nStill, polio persists. The world now has four polio endemic countries - Nigeria, Afghanistan, Pakistan, and India. One\\ngoal of the Gates Foundation is to eliminate polio.\\n\\nThe National Foundation for Infantile Paralysis was founded in 1938 by Franklin D. Roosevelt. Roosevelt was\\ndiagnosed with polio in 1921, and left him unable to walk. The Foundation is now known as the March of Dimes.\\nThe expanded mission of the March of Dimes is to improve the health of babies by preventing birth defects, premature\\nbirth and infant mortality. Its initiatives include rubella (German measles) and pertussis (whooping cough) vaccination,\\nmaternal and neonatal care, folic acid and spin biﬁda, fetal alcohol syndrome, newborn screening, birth defects and\\nprematurity.\\n\\nThe INCAP Study\\nThe World Health Organization cites malnutrition as the gravest single threat to the world’s\\npublic health. Improving nutrition is widely regarded as the most effective form of aid. Ac-\\ncording to Jean Ziegler (the United Nations Special Rapporteur on the Right to Food from\\n2000 to 2008) mortality due to malnutrition accounted for 58% of the total mortality in 2006.\\nIn that year, more than 36 million died of hunger or diseases due to deﬁciencies in micronu-\\ntrients.\\n\\nMalnutrition is by far the biggest contributor to child mortality, present in half of all cases.\\nUnderweight births and inter-uterine growth restrictions cause 2.2 million child deaths a year.\\nPoor or non-existent breastfeeding causes another 1.4 million. Other deﬁciencies, such as\\nlack of vitamins or minerals, for example, account for 1 million deaths. According to The\\nLancet, malnutrition in the ﬁrst two years is irreversible. Malnourished children grow up with\\nworse health and lower educational achievements.\\n\\nThus, understanding the root causes of malnutrition and designing remedies is a major\\nglobal health care imperative. As the next example shows, not every design sufﬁciently con-\\nsiders the necessary aspects of human behavior to allow for a solid conclusion.\\n\\n77\\n\\nFigure 4.3: The orange\\nribbon is often used as\\na\\npromote\\nawareness of malnutrition,\\n\\nsymbol\\n\\nto\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nProducing Data\\n\\nThe Instituto de Nutrici´on de Centro Americo y Panama (INCAP) conducted a study on the effects of malnutrition.\\nThis 1969 study took place in Guatemala and was administered by the World Health Organization, and supported by\\nthe United States National Institute of Health.\\n\\nGrowth deﬁciency is thought to be mainly due to protein deﬁciency. Here are some basic facts known in advance\\n\\nof the study:\\n\\n• Guatemalan children eat 2/3 as much as children in the United States.\\n• Age 7 Guatemalan children are, on average, 5 inches shorter and 11 pounds lighter than children in the United\\n\\nStates.\\n\\nWhat are the confounding factors that might explain these differences?\\n• Genetics\\n• Prevalence of disease\\n• Standards of hygiene.\\n• Standards of medical care.\\nThe experimental design: Measure the effects in four very similar Guatemalan villages. Here are the criterion used\\n\\nfor the Guatemalan villages chosen for the study..\\n\\n• The village size is 150 families, 700 inhabitants with 100 under 6 years of age.\\n• The village is culturally Latino and not Mayan\\n• Village life consists of raising corn and beans for food and tomatoes for cash.\\n• Income is approximately $200 for a family of ﬁve.\\n• The literacy rate is approximately 30% for individuals over age 7.\\nFor the experiment:\\n• Two villages received the treatment, a drink called atole, rich in calories and protein.\\n• Two villages received the control, a drink called fresca, low in calories and no protein.\\n• Both drinks contain missing vitamins and trace elements. The drinks were served at special cafeterias. The\\n\\namount consumed by each individual was recorded, but the use of the drinks was unrestricted.\\n\\n• Free medical care was provided to compensate for the burden on the villagers.\\nThe lack of control in the amount of the special drink consumed resulted in enormous variation in consumption.\\nIn particular, much more fresca was consumed. Consequently, the design fails in that differences beyond the speciﬁc\\ntreatment and control existed among the four villages.\\n\\nThe researchers were able to salvage some useful information from the data. They found a linear relationship\\n\\nbetween a child’s growth and the amount of protein consumed:\\n\\nchild’s growth rate = 0.04 inches/pound protein\\n\\nNorth American children consume an extra 100 pounds of protein by age 7. Thus, the protein accounts for 4 of the\\n\\n5 inches in the average difference in heights between Latino Guatemalans and Americans.\\n\\n78\\n\\n\\x0cPart II\\n\\nProbability\\n\\n79\\n\\n\\x0c\\x0cTopic 5\\n\\nThe Basics of Probability\\n\\nThe theory of probability as mathematical discipline can and should be developed from axioms in exactly\\nthe same way as Geometry and Algebra. - Andrey Kolmogorov, 1933, Foundations of the Theory of\\nProbability\\n\\n5.1 Introduction\\nMathematical structures like Euclidean geometry or algebraic ﬁelds are deﬁned by a set of axioms. “Mathematical\\nreality” is then developed through the introduction of concepts and the proofs of theorems. These axioms are inspired,\\nin the instances introduced above, by our intuitive understanding, for example, of the nature of parallel lines or the\\nreal numbers. Probability is a branch of mathematics based on three axioms inspired originally by calculating chances\\nfrom card and dice games.\\n\\nStatistics, in its role as a facilitator of science, begins with the collection of data. From this collection, we are\\nasked to make inference on the state of nature, that is to determine the conditions that are likely to produce these data.\\nProbability, in undertaking the task of investigating differing states of nature, takes the complementary perspective. It\\nbegins by examining random phenomena, i.e., those whose exact outcomes are uncertain. Consequently, in order to\\ndetermine the “scientiﬁc reality” behind the data, we must spend some time working with the concepts of the theory\\nof probability to investigate properties of the data arising from the possible states of nature to assess which are most\\nuseful in making inference.\\n\\nWe will motivate the axioms of probability through the case of equally likely outcomes for some simple games of\\nchance and look at some of the direct consequences of the axioms. In order to extend our ability to use the axioms, we\\nwill learn counting techniques, e.g, permutations and combinations, based on the fundamental principle of counting.\\n\\nA probability model has two essential pieces of its description.\\n• Ω, the sample space, the set of possible outcomes.\\n\\n– An event is a collection of outcomes. We can deﬁne an event by explicitly giving its outcomes,\\n\\nA = {ω1, ω2,··· , ωn}\\n\\nor with a description\\n\\nIn either case, A is subset of the sample space, A ⊂ Ω.\\n\\nA = {ω; ω has property P}.\\n\\n• P , the probability assigns a number to each event.\\n\\n81\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nThus, a probability is a function. We are familiar with functions in which both the domain and range are subsets of the\\nreal numbers. The domain of a probability function is the collection of all events. The range is still a number. We will\\nsee soon which numbers we will accept as probabilities of events.\\n\\nYou may recognize these concepts from a basic introduction to sets. In talking about sets, we use the term universal\\nset instead of sample space, element instead of outcome, and subset instead of event. At ﬁrst, having two words for\\nthe same concept seems unnecessarily redundant. However, we will later consider more complex situations which will\\ncombine ideas from sets and from probability. In these cases, having two expression for a concept will facilitate our\\nunderstanding. A Set Theory - Probability Theory Dictionary is included at the end of this topic to relate to the new\\nprobability terms with the more familiar set theory terms.\\n\\n5.2 Equally Likely Outcomes and the Axioms of Probability\\nThe essential relationship between events and the probability are described through the three axioms of probability.\\nThese axioms can be motivated through the ﬁrst uses of probability, namely the case of equal likely outcomes.\\n\\nIf Ω is a ﬁnite sample space, then if each outcome is equally likely, we deﬁne the probability of A as the fraction of\\noutcomes that are in A. Using #(A) to indicate the number of elements in an event A, this leads to a simple formula\\n\\nThus, computing P (A) means counting the number of outcomes in the event A and the number of outcomes in the\\n\\nsample space Ω and dividing.\\n\\nP (A) =\\n\\n#(A)\\n#(Ω)\\n\\n.\\n\\nExercise 5.1. Find the probabilities under equal likely outcomes.\\n(a) Toss a coin.\\n\\nP{heads} =\\n\\n#(A)\\n#(Ω)\\n\\n=\\n\\n.\\n\\nP{toss at least two heads in a row} =\\n\\n#(A)\\n#(Ω)\\n\\n=\\n\\n(b) Toss a coin three times.\\n\\n(c) Roll two dice.\\n\\n#(A)\\n#(Ω)\\nBecause we always have 0 ≤ #(A) ≤ #(Ω), we always have\\nP (A) ≥ 0\\n\\nP{sum is 7} =\\n\\n=\\n\\nand\\n\\nP (Ω) = 1\\n\\nThis gives us 2 of the three axioms. The third will require more development.\\nToss a coin 4 times.\\nA = {exactly 3 heads} = {HHHT, HHTH, HTHH, THHH}\\n4\\n16\\n\\nP (A) =\\n\\n1\\n4\\n\\n=\\n\\nB = {exactly 4 heads} = {HHHH}\\n\\n82\\n\\n(5.1)\\n\\n(5.2)\\n\\n#(Ω) = 16\\n#(A) = 4\\n\\n#(B) = 1\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nNow let’s deﬁne the set C = {at least three heads}. If you are asked the supply the probability of C, your intuition\\n\\nis likely to give you an immediate answer.\\n\\nP (B) =\\n\\n1\\n16\\n\\nP (C) =\\n\\n5\\n16\\n\\n.\\n\\nLet’s have a look at this intuition. The events A and B have no outcomes in common,. We say that the two events\\n\\nare disjoint or mutually exclusive and write A ∩ B = ∅. In this situation,\\n#(A ∪ B) = #(A) + #(B).\\n\\nIf we take this addition principle and divide by #(Ω), then we obtain the following identity:\\n\\nIf A ∩ B = ∅, then\\n\\nor\\n\\nUsing this property, we see that\\n\\n#(A ∪ B)\\n\\n#(Ω)\\n\\n=\\n\\n#(A)\\n#(Ω)\\n\\n+\\n\\n#(B)\\n#(Ω)\\n\\n.\\n\\nP (A ∪ B) = P (A) + P (B).\\n\\n(5.3)\\n\\nP{at least 3 heads} = P{exactly 3 heads} + P{exactly 4 heads} =\\n\\n4\\n16\\n\\n+\\n\\n1\\n16\\n\\n=\\n\\n5\\n16\\n\\n.\\n\\nWe are saying that any function P that accepts events as its domain and returns numbers as its range and satisﬁes\\n\\nAxioms 1, 2, and 3 as deﬁned in (5.1), (5.2), and (5.3) can be called a probability.\\n\\nthen\\n\\nIf we iterate the procedure in Axiom 3, we can also state that if the events, A1, A2,··· , An, are mutually exclusive,\\n(5.3(cid:48))\\nThis is a sufﬁcient deﬁnition for a probability if the sample space Ω is ﬁnite. However, we will want to examine\\ninﬁnite sample spaces and to use the idea of limits. This introduction of limits is the pathway that allows to bring in\\ncalculus with all of its powerful theory and techniques as a tool in the development of the theory of probability.\\n\\nP (A1 ∪ A2 ∪ ··· ∪ An) = P (A1) + P (A2) + ··· + P (An).\\n\\nExample 5.2. For the random experiment, consider a rare event - a lightning strike at a given location, winning the\\nlottery, ﬁnding a planet with life - and look for this event repeatedly until it occurs, we can write\\n\\nThen, each of the Aj are mutually exclusive and\\n\\nAj = {the ﬁrst occurrence appears on the j-th observation}.\\n\\n{event occurs eventually} = A1 ∪ A2 ∪ ··· ∪ An ∪ ··· =\\n\\n∞(cid:91)j=1\\n\\nAj = {ω; ω ∈ Aj for some j}.\\n\\nWe would like to say that\\n\\nP{event occurs ventually} = P (A1) + P (A2) + ··· + P (An) + ··· =\\n\\n∞(cid:88)j=1\\n\\nP (Aj) = lim\\nn→∞\\n\\nP (Aj).\\n\\nn(cid:88)j=1\\n\\n83\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nFigure 5.1: (left) Difference and Monotonicity Rule. If A ⊂ B, then P (B \\\\ A) = P (B) − P (A). (right) The Inclusion-Exclusion Rule.\\nP (A ∪ B) = P (A) + P (B) − P (A ∩ B). Using area as an analogy for probability, P (B \\\\ A) is the area between the circles and the area\\nP (A) + P (B) double counts the lens shaped area P (A ∩ B).\\nThis would call for an extension of Axiom 3 to an inﬁnite number of mutually exclusive events. This is the general\\nversion of Axiom 3 we use when we want to use calculus in the theory of probability:\\n\\nFor mutually exclusive events, {Aj; j ≥ 1}, then\\n\\nP\\uf8eb\\uf8ed\\n∞(cid:91)j=1\\n\\nAj\\uf8f6\\uf8f8 =\\n\\n∞(cid:88)j=1\\n\\nP (Aj)\\n\\n(5.3(cid:48)(cid:48))\\n\\nThus, statements (5.1), (5.2), and (5.3”) give us the complete axioms of probability.\\n\\n5.3 Consequences of the Axioms\\nOther properties that we associate with a probability can be derived from the axioms.\\n\\n1. The Complement Rule. Because A and its complement Ac = {ω; ω /∈ A} are mutually exclusive\\n\\nor\\n\\nP (A) + P (Ac) = P (A ∪ Ac) = P (Ω) = 1\\n\\nP (Ac) = 1 − P (A).\\n\\nFor example, if we toss a biased coin. We may want to say that P{heads} = p where p is not necessarily equal\\nto 1/2. By necessity,\\n\\nExample 5.3. Toss a coin 4 times.\\n\\nP{tails} = 1 − p.\\n\\nP{fewer than 3 heads} = 1 − P{at least 3 heads} = 1 −\\n\\n5\\n16\\n\\n=\\n\\n11\\n16\\n\\n.\\n\\n2. The Difference Rule. Write B \\\\ A to denote the outcomes that are in B but not in A. If A ⊂ B, then\\n\\n(The symbol ⊂ denotes “contains in”. A and B \\\\ A are mutually exclusive and their union is B. Thus P (B) =\\nP (A) + P (B \\\\ A).) See Figure 5.1 (left).\\n\\nP (B \\\\ A) = P (B) − P (A).\\n\\n84\\n\\nABAB\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nExercise 5.4. Give an example for which P (B \\\\ A) (cid:54)= P (B) − P (A)\\nBecause P (B \\\\ A) ≥ 0, we have the following:\\n\\n3. Monotonicity Rule. If A ⊂ B, then P (A) ≤ P (B)\\n\\nWe already know that for any event A, P (A) ≥ 0. The monotonicity rule adds to this the fact that\\n\\nThus, the range of a probability is a subset of the interval [0, 1].\\n\\n4. The Inclusion-Exclusion Rule. For any two events A and B,\\n\\nP (A) ≤ P (Ω) = 1.\\n\\nP (A ∪ B) = P (A) + P (B) − P (A ∩ B)\\n\\n(5.4).\\n\\n(P (A) + P (B) accounts for the outcomes in A ∩ B twice, so remove P (A ∩ B).) See Figure 5.1 (right).\\nExercise 5.5. Show that the inclusion-exclusion rule follows from the axioms. Hint: A ∪ B = (A ∩ Bc) ∪ B\\nand A = (A ∩ Bc) ∪ (A ∩ B).\\nExercise 5.6. Give a generalization of the inclusion-exclusion rule for three events.\\n\\nDeal two cards.\\nA = {ace on the second card},\\n\\nB = {ace on the ﬁrst card}\\n\\nP (A ∪ B) = P (A) + P (B) − P (A ∩ B)\\n1\\nP{at least one ace} =\\n13 − ?\\n\\n1\\n13\\n\\n+\\n\\nTo complete this computation, we will need to compute P (A ∩ B) = P{both cards are aces} = #(A∩B)\\n\\nWe will learn a strategy for this when we learn the fundamental principles of counting. We will also learn a\\nsimpler strategy in the next topic where we learn about conditional probabilities.\\n\\n#(Ω)\\n\\n5. The Bonferroni Inequality. For any two events A and B,\\n\\n6. Continuity Property. If events satisfy\\n\\nP (A ∪ B) ≤ P (A) + P (B).\\n\\nB1 ⊂ B2 ⊂ ··· and B =\\n\\n∞(cid:91)i=1\\n\\nBi\\n\\nThen, by the monotonicity rule, P (Bi) is an increasing sequence. In addition, they satisfy\\n\\ni→∞ P (Bi).\\nSimilarly, use the symbol ⊃ to denote “contains”. If events satisfy\\nC1 ⊃ C2 ⊃ ··· and C =\\n\\nP (B) = lim\\n\\n∞(cid:92)i=1\\n\\nCi\\n\\nAgain, by the monotonicity rule, P (Ci) is a decreasing sequence. In addition, they satisfying\\n\\nP (C) = lim\\n\\ni→∞ P (Ci).\\n\\n85\\n\\n(5.5)\\n\\n(5.6)\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nFigure 5.2: Continuity Property. (left) Bi increasing to an event B. Here, equation (5.5) is satisﬁed. (right) Ci decreasing to an event C. Here,\\nequation (5.6) is satisﬁed.\\n\\nExercise 5.7. Establish the continuity property. Hint: For the ﬁrst, let A1 = B1 and Ai = Bi \\\\ Bi−1, i > 1 in axiom\\n(5.3”). For the second, use the complement rule and de Morgan’s law\\n\\nExercise 5.8 (odds). The statement of a : b odds for an event A indicates that\\n\\nC c =\\n\\n∞(cid:91)i=1\\n\\nC c\\ni\\n\\nShow that\\n\\nP (A)\\nP (Ac)\\n\\n=\\n\\na\\nb\\n\\nP (A) =\\n\\na\\n\\na + b\\n\\n.\\n\\nSo, for example, 1 : 2 odds means P (A) = 1/3 and 5 : 3 odds means P (A) = 5/8.\\n\\n5.4 Counting\\nIn the case of equally likely outcomes, ﬁnding the probability of an event A is the result of two counting problems -\\nnamely ﬁnding #(A), the number of outcomes in the event A and ﬁnding #(Ω), the number of outcomes in the sample\\nspace, Ω. These counting problems can become quite challenging and many advanced mathematical techniques have\\nbeen developed to address these issues. However, having some facility in counting is necessary to have a sufﬁciently\\nrich number of examples to give meaning to the axioms of probability. Consequently, we shall develop a few counting\\ntechniques leading to the concepts of permutations and combinations.\\n\\n5.4.1 Fundamental Principle of Counting\\nWe start with the fundamental principle of counting.\\nSuppose that two experiments are to be performed.\\n• Experiment 1 can have n1 possible outcomes and\\n• for each outcome of experiment 1, experiment 2 has n2 possible outcomes.\\n\\n86\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nThen together there are n1 × n2 possible outcomes.\\nExample 5.9. For a group of n individuals, one is chosen to become the president and a second is chosen to become\\nthe treasurer. By the multiplication principle, if these position are held by different individuals, then this task can be\\naccomplished in\\n\\npossible ways\\n\\nn × (n − 1)\\n\\nExercise 5.10. Find the number of ways to draw two cards and the number of ways to draw two aces.\\nExercise 5.11. Generalize the fundamental principle of counting to k experiments.\\n\\nAssume that we have a collection of n objects and we wish to make an ordered arrangement of k of these\\nobjects. We call this a permutation of n objects of size k. Using the generalized multiplication principle, the number\\nof possible outcomes is\\n\\nWe will write this as (n)k and say n falling k. Correspondingly, we hve the notion of the rising factorial, also referred\\nto as the Pochhammer symbol.\\n\\nn × (n − 1) × ··· × (n − k + 1).\\n\\nn(k) = n × (n + 1) × ··· × (n + k − 1).\\n\\n5.4.2 Permutations\\nExample 5.12 (birthday problem). In a list the birthday of k people, there are 365k possible lists (ignoring leap year\\nday births) and\\n\\n(365)k\\n\\npossible lists with no date written twice. Thus, the probability, under equally likely outcomes, that no two people on\\nthe list have the same birthday is\\n\\nand, by the complement rule,\\n\\n(365)k\\n365k =\\n\\n365 · 364··· (365 − k + 1)\\n\\n365k\\n\\nP{at least one pair of individuals share a birthday} = 1 −\\n\\n(365)k\\n365k\\n\\n(5.1)\\n\\nHere is a short table of these probabilities. A graph is given in Figure 5.3.\\n\\nk\\n\\n5\\n\\n10\\n\\n15\\n\\n18\\n\\n20\\n\\n22\\n\\n23\\n\\n25\\n\\n30\\n\\n40\\n\\n50\\n\\nprobability\\n\\n0.027\\n\\n0.117\\n\\n0.253\\n\\n0.347\\n\\n0.411\\n\\n0.476\\n\\n0.507\\n\\n0.569\\n\\n0.706\\n\\n0.891\\n\\n0.970\\n\\n100\\n0.994\\n\\nThe R code and output follows. We can create an iterative process by noting that\\n\\n(365)k\\n365k =\\n\\n(365)k−1\\n365k−1\\n\\n(365 − k + 1)\\n\\n365\\n\\nThus, we can ﬁnd the probability that no pair in a group of k individuals has the same birthday by taking the probability\\nthat no pair in a group of k − 1 individuals has the same birthday and multiplying by (365 − k + 1)/365. Here is the\\noutput for k = 1 to 45.\\n\\n> prob=rep(1,45)\\n> for (k in 2:45){prob[k]=prob[k-1]*(365-k+1)/365}\\n> data.frame(c(1:15),1-prob[1:15],c(16:30),1-prob[16:30],c(31:45),1-prob[31:45])\\n\\nand the output\\n\\n87\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nc.1.15. X1...prob.1.15. c.16.30. X1...prob.16.30. c.31.45. X1...prob.31.45.\\n0.7304546\\n0.7533475\\n0.7749719\\n0.7953169\\n0.8143832\\n0.8321821\\n0.8487340\\n0.8640678\\n0.8782197\\n0.8912318\\n0.9031516\\n0.9140305\\n0.9239229\\n0.9328854\\n0.9409759\\n\\n0.000000000\\n0.002739726\\n0.008204166\\n0.016355912\\n0.027135574\\n0.040462484\\n0.056235703\\n0.074335292\\n0.094623834\\n0.116948178\\n0.141141378\\n0.167024789\\n0.194410275\\n0.223102512\\n0.252901320\\n\\n0.2836040\\n0.3150077\\n0.3469114\\n0.3791185\\n0.4114384\\n0.4436883\\n0.4756953\\n0.5072972\\n0.5383443\\n0.5686997\\n0.5982408\\n0.6268593\\n0.6544615\\n0.6809685\\n0.7063162\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n\\nDeﬁnition 5.13. The number of ordered arrangements of all n objects (also called permutations) is\\n\\n(n)n = n × (n − 1) × ··· × 1 = n!,\\n\\nn factorial. We take 0! = 1\\nExercise 5.14.\\n\\n(n)k =\\n\\nn!\\n\\n(n − k)!\\n\\n.\\n\\n5.4.3 Combinations\\nIn the case that the order does not matter, a combination is\\na subset from a ﬁnite set. Write\\n\\n(cid:18)n\\nk(cid:19)\\n\\nfor the number of different combinations of k objects that\\ncan be chosen from a set of size n.\\n\\nWe will next ﬁnd a formula for this number by counting\\nthe number of possible outcomes in two different ways. To\\nintroduce this with a concrete example, suppose 3 cities will\\nbe chosen out of 8 under consideration for a vacation. If we\\nthink of the vacation as visiting three cities in a particular\\norder, for example,\\n\\nFigure 5.3: The Birthday Problem.\\ntaining k individuals.\\nPk{at least one pair of individuals share a birthday}.\\n\\nUsing (5.1),\\n\\nFor a room of con-\\na plot of k versus\\n\\nThen we are counting the number of ordered arrangements. This results in\\n\\nNew York\\n\\nthen Boston\\n\\nthen Montreal.\\n\\n(8)3 = 8 · 7 · 6\\n\\nchoices.\\n\\nIf we are just considering the 3 cities we visit, irrespective of order, then these unordered choices are combina-\\n\\ntions. The number of ways of doing this is written\\n\\n3(cid:19),\\n(cid:18)8\\n\\n88\\n\\n01020304050600.00.20.40.60.81.0peopleprobability\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\na number that we do not yet know how to determine. After we have chosen the three cities, we will also have to pick\\nan order to see the cities and so using the fundamental principle of counting, we have\\n\\n3(cid:19) × 3 · 2 · 1 =(cid:18)8\\n(cid:18)8\\n3(cid:19)3!\\n\\npossible vacations if the order of the cities is included in the choice.\\n\\nThese two strategies are counting the same possible outcomes and so the number of them must be equal.\\n\\n(8)3 = 8 · 7 · 6 =(cid:18)8\\n\\n3(cid:19) × 3 · 2 · 1 =(cid:18)8\\n3(cid:19)3!\\n3(cid:1). Let’s do this more generally.\\n\\nThus, we have a formula for(cid:0)8\\n\\nTheorem 5.15.\\n\\n(n)k\\nk!\\nThe second equality follows from the previous exercise.\\n\\n(cid:18)n\\nk(cid:19) =\\n\\nn!\\n\\n=\\n\\n.\\n\\nk!(n − k)!\\n\\nor (cid:18)8\\n3(cid:19) =\\n\\n8 · 7 · 6\\n3 · 2 · 1\\n\\n=\\n\\n(8)3\\n3!\\n\\n.\\n\\nThe number of ordered arrangements of k objects out of n is\\n\\nAlternatively, we can form an ordered arrangement of k objects from a collection of n by:\\n\\n(n)k = n × (n − 1) × ··· × (n − k + 1).\\n\\n1. First choosing a group of k objects.\\n\\nThe number of possible outcomes for this experiment is(cid:0)n\\nk(cid:1).\\n\\n2. Then, arranging this k objects in order.\\n\\nThe number of possible outcomes for this experiment is k!.\\n\\nSo, by the fundamental principle of counting,\\n\\n(n)k =(cid:18)n\\n\\nk(cid:19) × k!.\\n\\nNow complete the argument by dividing both sides by k!.\\n\\nExercise 5.16 (binomial theorem).\\n\\nExercise 5.17. Verify the identities\\n\\nThus, we set\\n\\n(cid:18)n\\n1(cid:19) =(cid:18) n\\n\\nk(cid:19)xkyn−k.\\n\\n(x + y)n =\\n\\nn(cid:88)k=0(cid:18)n\\nn − 1(cid:19) = n and (cid:18)n\\n0(cid:19) = 1.\\n\\n(cid:18)n\\nn(cid:19) =(cid:18)n\\n\\nk(cid:19) =(cid:18) n\\nn − k(cid:19).\\n\\nThe number of combinations is computed in R using choose. In the vacation example above,(cid:0)8\\n\\n3(cid:1) is determined\\n\\nby entering\\n\\n> choose(8,3)\\n[1] 56\\n\\n89\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nTheorem 5.18 (Pascal’s triangle).\\n\\nTo see this using the example on vacations,\\n\\n(cid:18)n\\nk(cid:19) =(cid:18)n − 1\\nk − 1(cid:19) +(cid:18)n − 1\\nk (cid:19).\\n2(cid:19) +(cid:18)7\\n(cid:18)8\\n3(cid:19).\\n3(cid:19) =(cid:18)7\\n3(cid:1) possible vacations, Then of the(cid:0)8\\nAssume that New York is one of 8 vacation cities. Then of the(cid:0)8\\n\\nNew York is on the list, then we must choose the remaining 2 cities from the remaining 7. If New York in not on the\\nlist, then all 3 choices must be from the remaining 7. Because New York is either on the list or off the list, but never\\nboth, the two types of choices have no overlap.\\n\\n3(cid:1) vacations, if\\n\\nTo establish this identity in general, distinguish one of the n objects in the collection. Say that we are looking at a\\n\\ncollection of n marbles, n − 1 are blue and 1 is red.\\n\\nmarbles.\\n\\nk−1(cid:1) different outcomes have the red marble.\\n\\n1. For outcomes in which the red marble is chosen, we must choose k − 1 marbles from the n − 1 blue marbles.\\n(The red marble is the remaining choice.) Thus,(cid:0)n−1\\n2. If the red marble is not chosen, then we must choose k blue marbles. Thus,(cid:0)n−1\\nk (cid:1) outcomes do not have the red\\n3. These choices of groups of k marbles have no overlap. And so(cid:0)n\\nThis gives us an iterative way to compute the values of(cid:0)n\\nk(cid:1). Let’s build a table of values for n (vertically) and\\nk ≤ n (horizontally). Then, by the Pascal’s triangle formula, a given table entry is the sum of the number directly\\nabove it and the number above and one column to the left. We can get started by noting that(cid:0)n\\n\\nk(cid:1) is the sum of the values in 1 and 2.\\nn(cid:1) = 1.\\n\\n0(cid:1) =(cid:0)n\\n\\nPascal’s triangle\\n\\nk\\n\\nk − 1\\n\\nn − 1(cid:0)n−1\\n\\nn\\n\\nk (cid:1) ← the sum of these two numbers\\nk−1(cid:1)(cid:0)n−1\\n(cid:0)n\\nk(cid:1) ← equals this number\\n\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n\\n0\\n1\\n2\\n3\\nn 4\\n5\\n6\\n7\\n8\\n\\n1\\n\\n2\\n\\n3\\n\\nk\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n1\\n3\\n6\\n\\n1\\n2\\n1\\n3\\n1\\n4\\n4\\n1\\n5 10 10\\n5\\n1\\n6 15 20 15\\n6\\n7 21 35 35 21\\n7\\n8 28 56 70 56 28\\n\\n1\\n8\\n\\n1\\n\\nExample 5.19. For the experiment on honey bee queen - if we rear 60 of the 90 queen eggs, the we have\\n> choose(90,60)\\n[1] 6.73133e+23\\n\\nmore than 1023 different possible simple random samples.\\nExample 5.20. Deal out three cards. There are\\n\\npossible outcomes. Let x be the number of hearts. Then we have chosen x hearts out of 13 and 3 − x cards that are\\nnot hearts out of the remaining 39. Thus, by the multiplication principle there are\\n\\n(cid:18)52\\n3(cid:19)\\n\\nx(cid:19) ·(cid:18) 39\\n3 − x(cid:19)\\n(cid:18)13\\n\\n90\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\npossible outcomes.\\n\\nIf we assume equally likely outcomes, the probability of x hearts is the ratio of these two numbers. To compute\\n\\nthese numbers in R for x = 0, 1, 2, 3, the possible values for x, we enter\\n\\n> x<-0:3\\n> prob<-choose(13,x)*choose(39,3-x)/choose(52,3)\\n> data.frame(x,prob)\\n\\nx\\n\\nprob\\n1 0 0.41352941\\n2 1 0.43588235\\n3 2 0.13764706\\n4 3 0.01294118\\n\\nNotice that\\n\\n> sum(prob)\\n[1] 1\\n\\nWe can simulate this activity repeatedly and see how it matches the result of the computation. First, we create a\\nvector of length 52 - c(rep(1,13),rep(0,39)). The 13 ones for the hearts and 39 zeros for the other cards. We\\ncan sample 3 with the sample command and use the sum command to add the ones (and hence the hearts). The ﬁrst\\nline in R below creates a space for our simulation and the table allows for a quick display of the results.\\n\\n> hearts<-numeric(10000)\\n> for (i in 1:10000){hearts[i]<-sum(sample(c(rep(1,13),rep(0,39)),3))}\\n> table(hearts)\\nhearts\\n\\n0\\n\\n3\\n4133 4382 1335 150\\n\\n2\\n\\n1\\n\\nNow divide by 10,000 to see that these simulated probabilities match closely the computed values above. Because it is\\na random simulation, we will have different results with a second simulation\\n\\nAlternatively, we can use the replicate command.\\n\\n> hearts<-replicate(10000,sum(sample(c(rep(1,13),rep(0,39)),3)))\\n> table(hearts)\\nhearts\\n\\n0\\n\\n3\\n4164 4284 1414 138\\n\\n2\\n\\n1\\n\\nExercise 5.21. Deal out 5 cards. Let x be the number of fours. What values can x take? Find the probability of x\\nfours for each possible value. Repeat this with 6 cards. Compare your answers to a simulation.\\n\\n5.5 Answers to Selected Exercises\\n5.1. (a) 1/2,\\n5.3. Toss a coin 6 times. Let A = {at least 3 heads} and Let B = {at least 3 tails}. Then\\n\\n(c) 6/36 = 1/6\\n\\n(b) 3/8,\\n\\nThus, P (B) − P (A) = 0. However, the event\\n\\nP (A) = P (B) =\\n\\n42\\n64\\n\\n=\\n\\n21\\n32\\n\\n.\\n\\nB \\\\ A = {exactly 3 tails} = {exactly 3 heads}\\n\\n91\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nand P (B \\\\ A) = 20/64 = 5/16 (cid:54)= 0.\\n5.5. Using the hint, we have that\\n\\nP (A ∪ B) = P (A ∩ Bc) + P (B)\\n\\nP (A) = P (A ∩ Bc) + P (A ∪ B)\\n\\nSubtract these two equations\\n\\nP (A ∪ B) − P (A) = P (B) − P (A ∪ B).\\n\\nNow add P (A) to both sides of the equation to obtain (5.4).\\n5.6. Use the associativity property of unions to write A∪ B ∪ C = (A∪ B)∪ C and use (5.4), the inclusion-exclusion\\nproperty for the 2 events A ∪ B and C and then to the 2 events A and B,\\n\\nP ((A ∪ B) ∪ C) = P (A ∪ B) + P (C) − P ((A ∪ B) ∩ C)\\n\\n= (P (A) + P (B) − P (A ∩ B)) + P (C) − P ((A ∩ C) ∪ (B ∩ C))\\n\\nFor the ﬁnal expression, we use one of De Morgan’s Laws. Now rearrange the other terms and apply inclusion-\\nexlcusion to the ﬁnal expression.\\n\\nP (A ∪ B ∪ C) = P (A) + P (B) − P (A ∩ B) + P (C) − P ((A ∩ C) ∪ (B ∩ C))\\n\\n= P (A) + P (B) + P (C) − P (A ∩ B) − (P (A ∩ C) + P (B ∩ C) − P ((A ∩ C) ∩ (B ∩ C)))\\n= P (A) + P (B) + P (C) − P (A ∩ B) − P (A ∩ C) − P (B ∩ C) + P (A ∩ B ∩ C)\\n\\nThe last expression uses the identity (A ∩ C) ∩ (B ∩ C)) = A ∩ B ∩ C.\\n5.7. Using the hint and writing B0 = ∅, we have that P (Ai) = P (Bi) − P (Bi−1) and that\\n\\nBecause the Ai are disjoint, we have by (5.3’)\\n\\nBi =\\n\\nn(cid:91)i=1\\n\\nAi\\n\\nn(cid:91)i=1\\n\\nP(cid:32) n(cid:91)i=1\\n\\nBi(cid:33) = P(cid:32) n(cid:91)i=1\\n\\nAi(cid:33)\\n\\nP (An−1)\\n\\n+\\n\\nP (An)\\n\\n=\\n= (P (Bn) − P (Bn−1)) + (P (Bn−1) − P (Bn−2)) + ··· + (P (B2) − P (B1)) + (P (B1) − P (B0))\\n= P (Bn) − (P (Bn−1) − (P (Bn−1)) − P (Bn−2)) + ··· + P (B2) − (P (B1) − (P (B1)) − P (∅)\\n= P (Bn)\\n\\n+ ··· +\\n\\nP (A2)\\n\\nP (A1)\\n\\n+\\n\\nbecause all of the other terms cancel. This is an example of a telescoping sum. Now use (5.3”) to obtain\\n\\nP(cid:32) ∞(cid:91)i=1\\n\\nBi(cid:33) = lim\\n\\nn→∞ P (Bn).\\n\\nFor the second part. Write Bi = C c\\n\\ni . Then, the Bi satisfy the required conditions and that B = C c. Thus,\\n\\n1 − P (C) = P (C c) = lim\\n\\ni→∞ P (C c\\n\\ni ) = lim\\n\\ni→∞(1 − P (Ci)) = 1 − lim\\n\\ni→∞ P (Ci)\\n\\nand\\n\\nP (C) = lim\\n\\ni→∞ P (Ci)\\n\\n92\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\n5.8. If\\n\\nThen,\\n\\na\\nb\\n\\n=\\n\\nP (A)\\nP (Ac)\\n\\n=\\n\\nP (A)\\n1 − P (A)\\n\\n.\\n\\na − aP (A) = bP (A),\\n\\na = (a + b)P (A), P (A) =\\n\\na\\n\\na + b\\n\\n.\\n\\n5.10. The number of ways to obtain two cards is 52 · 51. The number of ways to obtain two aces is 4 · 3.\\n5.11. Suppose that k experiments are to be performed and experiment i can have ni possible outcomes irrespective of\\nthe outcomes on the other k − 1 experiments. Then together there are n1 × n2 × ··· × nk possible outcomes.\\n5.14.\\n\\n(n)k = n × (n − 1) × ··· × (n − k + 1) ×\\n\\nn × (n − 1) × ··· × (n − k + 1)(n − k)!\\n\\n(n − k)!\\n\\n(n − k)!\\n(n − k)!\\n\\n=\\n\\n=\\n\\nn!\\n\\n(n − k)!\\n\\n.\\n\\n5.15. Expansion of (x + y)n = (x + y)× (x + y)×···× (x + y) will result in 2n terms. Each of the terms is achieved\\nby one choice of x or y from each of the factors in the product (x + y)n. Each one of these terms will thus be a result\\nin n factors - some of them x and the rest of them y. For a given k from 0, 1, . . . , n, we will see choices that will result\\nin k factors of x and n − k factors of y, i. e., xkyn−k. The number of such choices is the combination\\n\\nAdd these terms together to obtain\\n\\nNext adding these values over the possible choices for k results in\\n\\n(cid:18)n\\nk(cid:19)\\n(cid:18)n\\nk(cid:19)xkyn−k.\\nn(cid:88)k=0(cid:18)n\\n\\n(x + y)n =\\n\\nk(cid:19)xkyn−k.\\n\\n5.17. The formulas are easy to work out. One way to consider\\n\\nways to exclude 1 out of a possible n. A similar reasoning gives\\n\\n1(cid:1) is the number of ways to choose 1 out of a possible n. This is the same as(cid:0) n\\n\\nis to note that(cid:0)n\\n\\nn−1(cid:1), the number of\\n\\n1(cid:19) =(cid:18) n\\n(cid:18)n\\nn − 1(cid:19)\\n\\nk(cid:19) =(cid:18) n\\n(cid:18)n\\nn − k(cid:19).\\n\\n(Replace 1 by k in the argument above.)\\n\\n93\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\n5.21. The possible values for x are 0, 1, 2, 3, and 4. When we have chosen x fours out of 4, we also have 5 − x cards\\nthat are not fours out of the remaining 48. Thus, by the multiplication principle, the probability of x fours is\\n\\nSimilarly for 6 cards, the probability of x fours is\\n\\nx(cid:1) ·(cid:0) 48\\n(cid:0)4\\n5−x(cid:1)\\n(cid:0)52\\n5(cid:1)\\nx(cid:1) ·(cid:0) 48\\n(cid:0)4\\n6−x(cid:1)\\n(cid:0)52\\n6(cid:1)\\n\\n.\\n\\n.\\n\\nTo compute the numerical values for the probability of x fours:\\n\\n> x<-c(0:4)\\n> prob5<-choose(4,x)*choose(48,5-x)/choose(52,5)\\n> sum(prob5)\\n[1] 1\\n> prob6<-choose(4,x)*choose(48,6-x)/choose(52,6)\\n> sum(prob6)\\n[1] 1\\n> data.frame(x,prob5,prob6)\\n\\nx\\n\\nprob5\\n\\nprob6\\n1 0 6.588420e-01 6.027703e-01\\n2 1 2.994736e-01 3.364300e-01\\n3 2 3.992982e-02 5.734602e-02\\n4 3 1.736079e-03 3.398282e-03\\n5 4 1.846893e-05 5.540678e-05\\n\\nFor the simulation of 5 cards,\\n\\n> fours<-replicate(10000,sum(sample(c(rep(1,4),rep(0,48)),5)))\\n> table(fours)\\nfours\\n\\n0\\n\\n2\\n6596 3024 365\\n\\n1\\n\\n3\\n14\\n\\n4\\n1\\n\\nFigure 5.4: Empirical cumulative distribution functions for the number of fourss in a draw of 6 cards.\\n\\n94\\n\\n012340.00.20.40.60.81.0number of foursprobability\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\nThe simulated values are a bit lower than computed values for 0 fours (0.6522 vs 0.6558), and a bit higher for 1, 2\\n\\nand 3 fours. Notice that 4 fours appeared only once. For 6 cards,\\n\\n> fours<-replicate(10000,sum(sample(c(rep(1,4),rep(0,48)),6)))\\n> table(fours)\\nfours\\n\\n0\\n\\n2\\n6035 3382 543\\n\\n1\\n\\n3\\n40\\n\\nThe simulated values are a bit higher than computed values for 0 fours (0.6086 vs 0.6027), a bit lower for 1, 2, and\\n\\n3 fours. In this simulation, 4 fours never appeared.\\n\\nWe can add an empirical cumulative distribution functions for the number of fours in a draw of 6 cards.\\n\\n> plot(sort(fours),1:length(fours)/length(fours),xlim=c(0,4),ylim=c(0,1),\\n\\nxlab=c(\"number of fours\"),ylab=c(\"probability\"),type=\"s\")\\n\\n95\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Basics of Probability\\n\\n5.6 Set Theory - Probability Theory Dictionary\\n\\nEvent Language\\n\\nSet Language\\n\\nSet Notation\\n\\nsample space\\n\\nuniversal set\\n\\nΩ\\n\\nevent\\n\\nsubset\\n\\nA, B, C,···\\n\\noutcome\\n\\nelement\\n\\nimpossible event\\n\\nempty set\\n\\nω\\n\\n∅\\n\\nnot A\\n\\nA complement\\n\\nAc\\n\\nA or B\\n\\nA union B\\n\\nA and B\\n\\nA intersect B\\n\\nA ∪ B\\n\\nA ∩ B\\n\\nA and B are\\n\\nmutually exclusive\\n\\nA and B are\\n\\ndisjoint\\n\\nA ∩ B = ∅\\n\\nif A then B\\n\\nA is a subset of B\\n\\nA ⊂ B\\n\\n96\\n\\n\\x0cTopic 6\\n\\nConditional Probability and Independence\\n\\nUnder Bayes’ theorem, no theory is perfect. Rather, it is a work in progress, always subject to further\\n\\nreﬁnement and testing. - Nate Silver\\n\\nOne of the most important concepts in the theory of probability is based on the question: How do we modify the\\nprobability of an event in light of the fact that something new is known? What is the chance that we will win the game\\nnow that we have taken the ﬁrst point? What is the chance that I am a carrier of a genetic disease now that my ﬁrst\\nchild does not have the genetic condition? What is the chance that a child smokes if the household has two parents\\nwho smoke? This question leads us to the concept of conditional probability.\\n\\n6.1 Restricting the Sample Space - Conditional Probability\\nToss a fair coin 3 times. Let winning be “at least two heads out of three”\\n\\nHHH HHT HTH HTT\\nTTH TTT\\nTHH THT\\n\\nFigure 6.1: Outcomes on three tosses of a coin, with the winning event indicated.\\n\\nIf we now know that the ﬁrst coin toss is heads, then only the top row is possible\\n\\nand we would like to say that the probability of winning is\\n\\n#(outcomes that result in a win and also have a heads on the ﬁrst coin toss)\\n\\n#(outcomes with heads on the ﬁrst coin toss)\\n\\n=\\n\\n#{HHH, HHT, HTH}\\n\\n#{HHH, HHT, HTH, HTT}\\n\\n=\\n\\n3\\n4\\n\\n.\\n\\nWe can take this idea to create a formula in the case of equally likely outcomes for\\n\\nthe statement the conditional probability of A given B.\\n\\nP (A|B) = the proportion of outcomes in A that are also in B\\n\\n=\\n\\n#(A ∩ B)\\n\\n#(B)\\n\\nWe can turn this into a more general statement using only the probability, P , by\\n\\ndividing both the numerator and the denominator in this fraction by #(Ω).\\n\\nP (A|B) =\\n\\n#(A ∩ B)/#(Ω)\\n\\n#(B)/#(Ω)\\n\\n=\\n\\nP (A ∩ B)\\n\\nP (B)\\n\\n97\\n\\nFigure 6.2: Two Venn diagrams\\nto illustrate conditional probability.\\nFor the top diagram P (A) is large\\nbut P (A|B) is small. For the bot-\\ntom diagram P (A) is small but\\nP (A|B) is large.\\n\\n(6.1)\\n\\n00.20.40.60.81−1−0.8−0.6−0.4−0.200.20.40.60.81AABB\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nWe thus take this version (6.1) of the identity as the general deﬁnition of conditional probability for any pair of\\n\\nevents A and B as long as the denominator P (B) > 0.\\n\\nExercise 6.1. Pick an event B so that P (B) > 0. Deﬁne, for every event A,\\n\\nShow that Q satisﬁes the three axioms of a probability. In words, a conditional probability is a probability.\\nExercise 6.2. Roll two dice. Find P{sum is 8|ﬁrst die shows 3}, and P{sum is 8|ﬁrst die shows 1}\\n\\nQ(A) = P (A|B).\\n\\n(1,1)\\n(2,1)\\n(3,1)\\n(4,1)\\n(5,1)\\n(6,1)\\n\\n(1,2)\\n(2,2)\\n(3,2)\\n(4,2)\\n(5,2)\\n(6,2)\\n\\n(1,3)\\n(2,3)\\n(3,3)\\n(4,3)\\n(5,3)\\n(6,3)\\n\\n(1,4)\\n(2,4)\\n(3,4)\\n(4,4)\\n(5,4)\\n(6,4)\\n\\n(1,5)\\n(2,5)\\n(3,5)\\n(4,5)\\n(5,5)\\n(6,5)\\n\\n(1,6)\\n(2,6)\\n(3,6)\\n(4,6)\\n(5,6)\\n(6,6)\\n\\nFigure 6.3: Outcomes on the roll of two dice. The event {ﬁrst roll is 3} is indicated.\\n\\nExercise 6.3. Roll two four-sided dice. With the numbers 1 through 4 on each die, the value of the roll is the number\\non the side facing downward. Assuming all 16 outcomes are equally likely, ﬁnd P{sum is at least 5}, P{ﬁrst die is 2}\\nand P{sum is at least 5|ﬁrst die is 2}\\n\\n6.2 The Multiplication Principle\\nThe deﬁning formula (6.1) for conditional probability can be rewritten to obtain the multiplication principle,\\n\\nNow, we can complete an earlier problem:\\n\\nP (A ∩ B) = P (A|B)P (B).\\n\\n(6.2)\\n\\nP{ace on ﬁrst two cards} = P{ace on second card|ace on ﬁrst card}P{ace on ﬁrst card}\\n\\n=\\n\\n3\\n51 ×\\n\\n4\\n52\\n\\n=\\n\\n1\\n17 ×\\n\\n1\\n13\\n\\n.\\n\\nWe can continue this process to obtain a chain rule:\\n\\nP (A ∩ B ∩ C) = P (A|B ∩ C)P (B ∩ C) = P (A|B ∩ C)P (B|C)P (C).\\n\\nThus,\\n\\nP{ace on ﬁrst three cards}\\n= P{ace on third card|ace on ﬁrst and second card}P{ace on second card|ace on ﬁrst card}P{ace on ﬁrst card}\\n=\\n\\n=\\n\\n.\\n\\n2\\n50 ×\\n\\n3\\n51 ×\\n\\n4\\n52\\n\\n1\\n25 ×\\n\\n1\\n17 ×\\n\\n1\\n13\\n\\nExtending this to 4 events, we consider the following question:\\n\\nExample 6.4. In an urn with b blue balls and g green balls, the probability of green, blue, green, blue (in that order)\\nis\\n\\ng\\nb + g ·\\n\\nb\\n\\nb + g − 1 ·\\n\\ng − 1\\nb + g − 2 ·\\n\\nb − 1\\n\\nb + g − 3\\n\\n=\\n\\n(g)2(b)2\\n(b + g)4\\n\\n.\\n\\n98\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nNotice that any choice of 2 green and 2 blue would result in the same probability. There are(cid:0)4\\n\\nThus, with 4 balls chosen without replacement\\n\\n2(cid:1) = 6 such choices.\\n\\nExercise 6.5. Show that\\n\\n2(cid:19) (g)2(b)2\\nP{2 blue and 2 green} =(cid:18)4\\n2(cid:1)(cid:0)g\\n= (cid:0)b\\n2(cid:1)\\n(cid:0)b+g\\n4 (cid:1) .\\nExplain in words why P{2 blue and 2 green} is the expression on the right.\\n\\n(cid:18)4\\n2(cid:19) (g)2(b)2\\n\\n(b + g)4\\n\\n(b + g)4\\n\\n.\\n\\nWe will later extend this idea when we introduce sampling without replacement in the context of the hypergeomet-\\n\\nric random variable.\\n\\n6.3 The Law of Total Probability\\nIf we know the fraction of the population in a given state of the United States that has a given attribute - is diabetic,\\nover 65 years of age, has an income of $100,000, owns their own home, is married - then how do we determine what\\nfraction of the total population of the United States has this attribute? We address this question by introducing a\\nconcept - partitions - and an identity - the law of total probability.\\nDeﬁnition 6.6. A partition of the sample space Ω is a ﬁnite collection of pairwise mutually exclusive events\\n\\n{C1, C2, . . . , Cn}\\n\\nwhose union is Ω.\\n\\nThus, every outcome ω ∈ Ω belongs to ex-\\nactly one of the Ci. In particular, distinct mem-\\nbers of the partition are mutually exclusive. (Ci∩\\nCj = ∅, if i (cid:54)= j)\\nIf we know the fraction of the population from\\n18 to 25 that has been infected by the H1N1 in-\\nﬂuenza A virus in each of the 50 states, then we\\ncannot just average these 50 values to obtain the\\nfraction of this population infected in the whole\\ncountry. This method fails because it give equal\\nweight to California and Wyoming. The law of\\ntotal probability shows that we should weigh\\nthese conditional probabilities by the probability\\nof residence in a given state and then sum over all\\nof the states.\\n\\nTheorem 6.7 (law of total probability). Let P be\\na probability on Ω and let {C1, C2, . . . , Cn} be a\\npartition of Ω chosen so that P (Ci) > 0 for all i.\\nThen, for any event A ⊂ Ω,\\n\\nFigure 6.4: A partition {C1 . . . , C9} of the sample space Ω. The event A can be\\nwritten as the union (A ∩ C1) ∪ · · · ∪ (A ∩ C9) of mutually exclusive events.\\n\\nP (A) =\\n\\nP (A|Ci)P (Ci).\\n\\n(6.3)\\n\\nBecause {C1, C2, . . . , Cn} is a partition, {A ∩ C1, A ∩ C2, . . . , A ∩ Cn} are pairwise mutually exclusive events.\\n\\nBy the distributive property of sets, their union is the event A. (See Figure 6.4.)\\n\\nn(cid:88)i=1\\n\\n99\\n\\n−0.200.20.40.60.811.2−0.200.20.40.60.811.2C1C2C3C5C4C6C7C8C9A\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nTo refer the example above the Ci are the residents of state i, A ∩ Ci are those residents who are from 18 to 25\\nyears old and have been been infected by the H1N1 inﬂuenza A virus. Thus, distinct A ∩ Ci are mutually exclusive -\\nindividuals cannot reside in 2 different states. Their union is A, all individuals in the United States between the ages\\nof 18 and 25 years old who have been been infected by the H1N1 virus.\\n\\nThus,\\n\\nP (A) =\\n\\nP (A ∩ Ci).\\n\\n(6.4)\\n\\nn(cid:88)i=1\\n\\nFinish by using the multiplication identity (6.2)\\n\\nP (A ∩ Ci) = P (A|Ci)P (Ci),\\n\\ni = 1, 2, . . . , n\\n\\nand substituting into (6.4) to obtain the identity in (6.3).\\n\\nThe most frequent use of the law of total probability\\ncomes in the case of a partition of the sample space into two\\nevents, {C, C c}. In this case, the law of total probability\\nbecomes the identity\\n\\nP (A) = P (A|C)P (C) + P (A|C c)P (C c).\\n\\n(6.5)\\n\\nExercise 6.8. The problem of points is a classical prob-\\nlem in probability theory. The problem concerns a series\\nof games with two sides who have equal chances of winning each game. The winning side is one that ﬁrst reaches a\\ngiven number n of wins. Let n = 4 for a best of seven playoff. Determine\\n\\nFigure 6.5: A partition into two events C and Cc.\\n\\npij = P{winning the playoff after i wins vs j opponent wins}\\n\\n(Hint: pii = 1\\n\\n2 for i = 0, 1, 2, 3.)\\n\\n6.4 Bayes formula\\nLet A be the event that an individual tests positive for some disease and C be the event that the person actually has\\nthe disease. We can perform clinical trials to estimate the probability that a randomly chosen individual tests positive\\ngiven that they have the disease,\\n\\nP{tests positive|has the disease} = P (A|C),\\n\\nby taking individuals with the disease and applying the test. However, we would like to use the test as a method of\\ndiagnosis of the disease. Thus, we would like to be able to give the test and assert the chance that the person has the\\ndisease. That is, we want to know the probability with the reverse conditioning\\n\\nP{has the disease|tests positive} = P (C|A).\\nExample 6.9. The Public Health Department gives us the following information.\\n\\n• A test for the disease yields a positive result 90% of the time when the disease is present.\\n• A test for the disease yields a positive result 1% of the time when the disease is not present.\\n• One person in 1,000 has the disease.\\nLet’s ﬁrst think about this intuitively and then look to a more formal way using Bayes formula to ﬁnd the probability\\n\\nof\\n\\nP (C|A).\\n\\n100\\n\\nACcC\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\n\\x01\\n\\x01\\n1,000,000 people\\nA\\nA\\n\\n\\x01\\x15\\n\\x01\\n\\nA\\nAU\\n\\nP (C) = 0.001\\n\\n1,000 have the disease\\n\\nP (A|C)P (C) = 0.0009\\n900 test positive\\n\\nP (Ac|C)P (C) = 0.0001\\n100 test negative\\n\\n\\x01\\x15\\n\\x01\\n\\nA\\nAU\\n\\nP (A|Cc )P (Cc ) = 0.00999\\n\\x01\\x15\\n\\x01\\n999,000 do not have the disease\\nA\\nP (Ac|Cc )P (Cc ) = 0.98901\\nAU\\n989,010 test negative\\n\\n9,990 test positive\\n\\nP (Cc ) = 0.999\\n\\nFigure 6.6: Tree diagram. We can use a tree diagram to indicate the number of individuals, on average, in each group (in black) or the probablity\\n(in blue). Notice that in each column the number of individuals adds to give 1,000,000 and the probabilities add to give 1. In addition, each pair of\\narrows divides an events into two mutually exclusive subevents. Thus, both the numbers and the probabilities at the tip of the arrows add to give the\\nrespective values at the head of the arrow.\\n\\n• In a city with a population of 1 million people, on average,\\n\\n1,000 have the disease and 999,000 do not\\n\\n• Of the 1,000 that have the disease, on average,\\n\\n900 test positive and 100 test negative\\n\\n• Of the 999,000 that do not have the disease, on average,\\n\\n999,000 × 0.01 = 9990 test positive and 989,010 test negative.\\n\\nWe can record this information in a table. First we place the total number of those with (1,000) and without\\n(999,000) the disease along the bottom row. We then use the information on positive and negative results for the test\\nto ﬁll in the columns to show 9:1 odds for positive test for those who have the disease and a 1:99 odds for those who\\ndo not.\\n\\ntest positive\\ntest negative\\n\\ntotal\\n\\nhas the disease\\n900\\n100\\n1,000\\n\\ndoes not have the disease\\n9,990\\n989,010\\n990,000\\n\\ntotal\\n10,890\\n989,110\\n1,000,000\\n\\nHaving ﬁlled in the columns, Bayes formula has us look at odds along the rows. For example, from the top row of\\n\\nthe table, we see that among those that test positive, the odds of having the disease is\\n\\n#(have the disease):#(does not have the disease)\\n\\n900:9990\\n\\n101\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nresearcher\\n\\nhas\\n\\ndisease\\n\\nC\\n\\ntests positive\\n\\nP (A|C)\\n0.90\\ntests negative P (Ac|C)\\n\\nA\\n\\nAc\\nsum\\n\\n0.10\\n\\n1\\n\\npublic health\\n\\nworker\\n\\n−→\\n\\nP (C) = 0.001\\nP (C c) = 0.999\\n\\n(cid:45)\\n\\ndoes not\\n\\nhave disease\\n\\nC c\\n\\n0.01\\n\\nP (A|C c)\\nP (Ac|C c)\\n\\n0.99\\n\\n1\\n\\nclinician\\n\\nhas\\n\\ndisease\\n\\nC\\n\\ndoes not\\n\\nhave disease\\n\\nC c\\n\\nA\\n\\ntests positive\\n\\nP (C|A)\\n0.0826\\ntests negative P (C|Ac)\\n0.0001\\n\\nAc\\n\\nP (C c|A)\\n0.9174\\nP (C c|Ac)\\n0.9999\\n\\nsum\\n\\n1\\n\\n1\\n\\nTable I: Using Bayes formula to evaluate a test for a disease. Successful analysis of the results of a clinical test requires researchers to provide\\nresults on the quality of the test and public health workers to provide information on the prevalence of a disease. The conditional probabilities\\nprovided by the researchers tell us about the chances of having the disease given the outcome of the test. The probability of a person having the\\ndisease might be provided by the public health service (shown by the east arrow). Both are necessary for the clinician to be able to use Bayes\\nformula (6.7), to give the conditional probability of having the disease given the test result. Notice, in particular, that the order of the conditioning\\nneeded by the clinician is the reverse of that provided by the researcher. If the clinicians provide reliable data to the public health service, then this\\ninformation can be used to update the probabilities for the prevalence of the disease (indicated by the northwest arrow). The numbers in gray can\\nbe computed from the numbers in black by using the complement rule. In particular, the column sums for the researchers and the row sums for the\\nclinicians much be 1.\\n\\nand converting odds to probability we see that\\n\\nP{have the disease|test is positive} =\\n\\n900\\n\\n900 + 9990\\n\\n=\\n\\n900\\n10890\\n\\n= 0.0826.\\n\\nWe now derive Bayes formula. First notice that we can ﬂip the order of conditioning by using the multiplication\\n\\nformula (6.2) twice\\n\\nP (A|C)P (C)\\nP (C|A)P (A)\\nNow we can create a formula for P (C|A) as desired in terms of P (A|C).\\nP (C|A)P (A) = P (A|C)P (C) or P (C|A) =\\n\\nP (A ∩ C) =\\uf8f1\\uf8f2\\uf8f3\\n\\nP (A|C)\\nP (A)\\n\\nP (C)\\n\\n(6.6)\\n\\nGenerally, we call P (C) the prior probability of C. With A given, we call P (C|A) the posterior probability of\\n\\nA. The Bayes factor\\n\\nP (A|C)\\nP (A)\\n\\n.\\n\\nis their ratio as given by the second equality in (6.6).\\n\\nExample 6.10. Both autism A and epilepsy C exists at approximately 1% in human populations. In this case, from\\nthe ﬁrst identity in (6.6),\\n\\nClinical evidence shows that this common value is about 30%. The Bayes factor is\\n\\nP (A|C) ≈ P (C|A)\\n\\nThus, the knowledge of one disease increases the chance of the other by a factor of 30.\\n\\nP (A|C)\\nP (A) ≈\\n\\n0.3\\n0.01\\n\\n= 30.\\n\\n102\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nFrom this formula we see that in order to determine P (C|A) from P (A|C), we also need to know P (C), the\\nfraction of the population with the disease and P (A). We can ﬁnd P (A) using the law of total probability in (6.5) and\\nwrite Bayes formula as\\n\\nP (C|A) =\\n\\nP (A|C)P (C)\\n\\nP (A|C)P (C) + P (A|C c)P (C c)\\n\\n.\\n\\n(6.7)\\n\\nThis shows us that we can determine P (A) if, in addition, we collect information from our clinical trials on P (A|C c),\\nthe fraction that test positive who do not have the disease.\\n\\nLet’s now compute P (C|A) for the example above using Bayes formula di-\\nrectly and use this opportunity to introduce some terminology. We have that\\nP (A|C) = 0.90. We use the expression true positive probability or the sensi-\\ntivity for the chance that we have a correct positive diagnosis to those who have\\nthe disease. If one tests negative for the disease (the outcome is in Ac) given that\\none has the disease, (the outcome is in C), then we call this a false negative. In\\nthis case, the false negative probability is P (Ac|C) = 0.10\\nIf one tests positive for the disease (the outcome is in A) given that one does\\nnot have the disease, (the outcome is in C c), then we call this a false positive. In\\nthis case, the false positive probability is P (A|C c) = 0.01. The true negative\\nprobability P (Ac|C c) = 0.99 is also called the speciﬁcity.\\nThe probability of having the disease is P (C) = 0.001 and so the probability\\nof being disease free is P (C c) = 0.999. Now, we apply the law of total probability\\n(6.5) as the ﬁrst step in Bayes formula (6.7),\\n\\ntrue\\n\\nfalse\\n\\npositive\\npositive\\nP (A|C c)\\nP (A|C)\\nfalse\\nnegative\\nnegative\\nP (Ac|C) P (Ac|C c)\\n\\ntrue\\n\\nTable II: Terminology for conditional\\nprobabilities. A is the event “tests posi-\\ntive” and C is the event ”has the disease”.\\nNotice that the columns sum to one.\\n\\nP (A) = P (A|C)P (C) + P (A|C c)P (C c)\\n\\n= 0.90 · 0.001 + 0.01 · 0.999 = 0.0009 + 0.009999 = 0.01089.\\n\\nThus, the probability of having the disease given that the test was positive is\\n\\nP (C|A) =\\n\\nP (A|C)P (C)\\n\\nP (A)\\n\\n=\\n\\n0.0009\\n0.01089\\n\\n= 0.0826.\\n\\nNotice that the numerator is one of the terms that was summed to compute the denominator.\\n\\nWe can take the terminology above a give formulas to the elements in the table above.\\n\\ntest positive\\ntest negative\\n\\ntotal\\n\\n900\\n100\\n1,000\\n\\nhas the disease\\n\\n= true positive rate × #disease\\n= false negative rate × #disease\\n\\n9,990\\n989,010\\n990,000\\n\\ndoes not have the disease\\n= false positive rate × #disease free\\n= true negative rate × #disease free\\n\\ntotal\\n10,890\\n989,110\\n1,000,000\\n\\nThe answer in the previous example may be surprising. Only 8% of those who test positive actually have the\\ndisease. This example underscores the fact that good predictions based on intuition are hard to make in this case. To\\ndetermine the probability, we must weigh the odds of two terms, each of them itself a product.\\n\\n• P (A|C)P (C), a big number (the true positive probability) times a small number (the probability of having the\\n\\ndisease) versus\\n\\n• P (A|C c)P (C c), a small number (the false positive probability) times a large number (the probability of being\\n\\ndisease free).\\n\\nWe do not need to restrict Bayes formula to the case of C, has the disease, and C c, does not have the disease, as\\nseen in (6.5), but rather to any partition of the sample space. Indeed, Bayes formula can be generalized to the case of\\na partition {C1, C2, . . . , Cn} of Ω chosen so that P (Ci) > 0 for all i. Then, for any event A ⊂ Ω and any j\\n\\nP (Cj|A) =\\n\\n(cid:80)n\\n\\nP (A|Cj)P (Cj)\\ni=1 P (A|Ci)P (Ci)\\n103\\n\\n.\\n\\n(6.8)\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nTo understand why this is true, use the law of total probability to see that the denominator is equal to P (A). By\\nthe multiplication identity for conditional probability, the numerator is equal to P (Cj ∩ A). Now, make these two\\nsubstitutions into (6.8) and use one more time the deﬁnition of conditional probability.\\nExample 6.11. We begin with a simple and seemingly silly example involving fair and two sided coins. However, we\\nshall soon see that this leads us to a question in the vertical transmission of a genetic disease.\\n\\nA box has a two-headed coin and a fair coin. It is ﬂipped n times, yielding heads each time. What is the probability\\n\\nthat the two-headed coin is chosen?\\n\\nTo solve this, note that\\n\\nP{two-headed coin} =\\n\\n1\\n2\\n\\n,\\n\\nP{fair coin} =\\n\\n1\\n2\\n\\n.\\n\\nand\\n\\nBy the law of total probability,\\n\\nP{n heads|two-headed coin} = 1,\\n\\nP{n heads|fair coin} = 2\\n\\n−n.\\n\\nP{n heads} = P{n heads|two-headed coin}P{two-headed coin} + P{n heads|fair coin}P{fair coin}\\n\\n1\\n2\\nNext, we use Bayes formula.\\n\\n= 1 ·\\n\\n+ 2\\n\\n−n ·\\n\\n1\\n2\\n\\n=\\n\\n2n + 1\\n2n+1 .\\n\\nP{two-headed coin|n heads} =\\n\\nP{n heads|two-headed coin}P{two-headed coin}\\n\\nP{n heads}\\n\\n=\\n\\n1 · (1/2)\\n\\n(2n + 1)/2n+1 =\\n\\n2n\\n\\n2n + 1\\n\\n< 1.\\n\\nNotice that as n increases, the probability of a two headed coin approaches 1 - with a longer and longer sequence\\nof heads we become increasingly suspicious (but, because the probability remains less than one, are never completely\\ncertain) that we have chosen the two headed coin.\\n\\nThis is the related genetics question: Based on the pedigree of her past, a female knows that she has in her history\\na allele on her X chromosome that indicates a genetic condition. The allele for the condition is recessive. Because\\nshe does not have the condition, she knows that she cannot be homozygous for the recessive allele. Consequently, she\\nwants to know her chance of being a carrier (heteorzygous) or not a carrier (homozygous for the common genetic\\ntype) of the condition. The female is a mother with n male offspring, none of which show the recessive allele on their\\nsingle X chromosome and so do not have the condition. What is the probability that the female is not a carrier?\\n\\nLet’s look at the computation above again, based on her pedigree, the female estimates that\\n\\nP{mother is not a carrier} = p,\\n\\nP{mother is a carrier} = 1 − p.\\n\\nThen, from the law of total probability\\n\\nP{n male offspring condition free}\\n\\n= P{n male offspring condition free|mother is not a carrier}P{mother is not a carrier}\\n\\n+P{n male offspring condition free|mother is a carrier}P{mother is a carrier}\\n\\n= 1 · p + 2\\n\\n−n · (1 − p).\\n\\nand Bayes formula\\n\\n=\\n\\n=\\n\\nP{mother is not a carrier|n male offspring condition free}\\nP{n male offspring condition free|mother is not a carrier}P{mother is not a carrier}\\nP{n male offspring condition free}\\n.\\np + 2−n(1 − p)\\n2np + (1 − p)\\n104\\n\\n1 · p + 2−n · (1 − p)\\n\\n1 · p\\n\\n2np\\n\\n=\\n\\n=\\n\\np\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nFigure 6.7: Probability of mother being carrier free, given n sons are disease free for n = 1 (black), 2 (orange), 3 (red), 4 ( (magenta), and 5\\n(blue), The vertical dashed line at p = 1/2 is the caae for the the boxes, one with a fair coin and one with a two-headed coin. Note how the posterior\\n\\nAgain, with more sons who do not have the condition, we become increasingly more certain that the mother is not\\na carrier. One way to introduce Bayesian statistics is to consider the situation in which we do not know the value of\\np and replace it with a probability distribution. Even though we will concentrate on classical approaches to statistics,\\nwe will take the time in later sections to explore the Bayesian approach\\n\\n6.5 Independence\\nAn event A is independent of B if its Bayes factor is 1, i.e.,\\n\\n1 =\\n\\nP (A|B)\\nP (A)\\n\\n, P (A) = P (A|B).\\n\\nIn words, the occurrence of the event B does not alter the prob-\\nability of the event A. Multiply this equation by P (B) and use the\\nmultiplication rule to obtain\\n\\nP (A)P (B) = P (A|B)P (B) = P (A ∩ B).\\n\\nThe formula\\n\\nP (A)P (B) = P (A ∩ B)\\n\\n(6.9)\\n\\nis the usual deﬁnition of independence and is symmetric in the events\\nA and B. If A is independent of B, then B is independent of A.\\nConsequently, when equation (6.9) is satisﬁed, we say that A and B\\nare independent.\\n\\nFigure 6.8: The Venn diagram for independent events is\\nrepresented by the horizontal strip A and the vertical strip\\nB is shown above. The identity P (A∩ B) = P (A)P (B)\\nis now represented as the area of the rectangle. Other as-\\npects of Exercise 6.12 are indicated in this Figure.\\n\\n105\\n\\n0.00.20.40.60.81.00.00.20.40.60.81.0pP{mother is not a carrier|n male offspring condition free}0.00.20.40.60.81.00.00.20.40.60.81.0pP{mother is not a carrier|n male offspring condition free}0.00.20.40.60.81.00.00.20.40.60.81.0pP{mother is not a carrier|n male offspring condition free}0.00.20.40.60.81.00.00.20.40.60.81.0pP{mother is not a carrier|n male offspring condition free}0.00.20.40.60.81.00.00.20.40.60.81.0pP{mother is not a carrier|n male offspring condition free}−0.4−0.325−0.25−0.175−0.07500.050.1250.20.2750.350.4250.50.5750.650.7250.80.8750.951.0251.11.1751.251.3251.41.4−0.2−0.100.10.20.30.40.50.60.70.80.911.0751.21.2P(A)P(Ac)P(B)P(Bc)P(A and B) = P(A)P(B)P(Ac and B) = P(Ac)P(B)P(A and Bc) = P(A)P(Bc)P(A and Bc)= P*(Ac)P(Bc)\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nExample 6.12. Roll two dice.\\n\\n1\\n36\\n\\n= P{a on the ﬁrst die, b on the second die}\\n=\\n= P{a on the ﬁrst die}P{b on the second die}\\n\\n1\\n6 ×\\n\\n1\\n6\\n\\nand, thus, the outcomes on two rolls of the dice are independent.\\n\\nExercise 6.13. If A and B are independent events, then show that Ac and B, A and Bc, Ac and Bc are also indepen-\\ndent.\\n\\nWe can also use this to extend the deﬁnition to n independent events:\\n\\nDeﬁnition 6.14. The events A1,··· , An are called independent if for any choice Ai1 , Ai2 ,··· , Aik taken from this\\ncollection of n events, then\\n\\nP (Ai1 ∩ Ai2 ∩ ··· ∩ Aik ) = P (Ai1 )P (Ai2)··· P (Aik ).\\nA similar product formula holds if some of the events are replaced by their complement.\\n\\n(6.10)\\n\\nExercise 6.15. Flip 10 biased coins. Their outcomes are independent with the i-th coin turning up heads with proba-\\nbility pi. Find\\n\\nP{ﬁrst coin heads, third coin tails, seventh & ninth coin heads}.\\n\\nExample 6.16. Mendel studied inheritance by conducting experiments using a garden peas. Mendel’s First Law, the\\nlaw of segregation states that every diploid individual possesses a pair of alleles for any particular trait and that each\\nparent passes one randomly selected allele to its offspring.\\n\\nIn Mendel’s experiment, each of the 7 traits under study express themselves independently. This is an example of\\nMendel’s Second Law, also known as the law of independent assortment. If the dominant allele was present in the\\npopulation with probability p, then the recessive allele is expressed in an individual when it receive this allele from\\nboth of its parents. If we assume that the presence of the allele is independent for the two parents, then\\n\\nP{recessive allele expressed} = P{recessive allele paternally inherited} × P{recessive allele maternally inherited}\\n\\n= (1 − p) × (1 − p) = (1 − p)2.\\n\\nIn Mendel’s experimental design, p was set to be 1/2. Consequently,\\n\\nUsing the complement rule,\\n\\nP{recessive allele expressed} = (1 − 1/2)2 = 1/4.\\n\\nP{dominant allele expressed} = 1 − (1 − p)2 = 1 − (1 − 2p + p2) = 2p − p2.\\n\\nThis number can also be computed by added the three alternatives shown in the Punnett square in Table 6.1.\\n\\np2 + 2p(1 − p) = p2 + 2p − 2p2 = 2p − p2.\\n\\nNext, we look at two traits - 1 and 2 - with the dominant alleles present in the population with probabilities p1 and\\n\\np2. If these traits are expressed independently, then, we have, for example, that\\n\\nP{dominant allele expressed in trait 1, recessive trait expressed in trait 2}\\n= P{dominant allele expressed in trait 1} × P{recessive trait expressed in trait 2}\\n= (1 − (1 − p1)2)(1 − p2)2.\\n\\n106\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nExercise 6.17. Show that if two traits are genetically linked, then the appearance of one increases the probability of\\nthe other. Thus,\\n\\nP{individual has allele for trait 1|individual has allele for trait 2} > P{individual has allele for trait 1}.\\n\\nimplies\\n\\nP{individual has allele for trait 2|individual has allele for trait 1} > P{individual has allele for trait 2}.\\n\\nMore generally, for events A and B,\\n\\nP (A|B) > P (A)\\nthen we way that A and B are positively associated.\\nExercise 6.18. A genetic marker B for a disease A is one in which P (A|B) ≈ 1. In this case, approximate P (B|A).\\n\\nimplies P (B|A) > P (B)\\n\\n(6.11)\\n\\nMore precisely, if P (A|B) = 1 if and only if P (B|A) = P (A)/P (B).\\n\\nDeﬁnition 6.19. Linkage disequilibrium is the non-independent association of alleles at two loci on single chromo-\\nsome. To deﬁne linkage disequilibrium, let\\n\\n• A be the event that a given allele is present at the ﬁrst locus, and\\n• B be the event that a given allele is present at a second locus.\\n\\nThen the linkage disequilibrium,\\n\\nDA,B = P (A)P (B) − P (A ∩ B).\\n\\nThus if DA,B = 0, the the two events are independent.\\nExercise 6.20. Show that DA,Bc = −DA,B\\n\\n6.6 Answers to Selected Exercises\\n6.1. Let’s check the three axioms;\\n\\n1. For any event A,\\n\\nQ(A) = P (A|B) =\\n\\nP (A ∩ B)\\n\\nP (B) ≥ 0.\\n\\n2. For the sample space Ω,\\n\\nQ(Ω) = P (Ω|B) =\\n\\nP (Ω ∩ B)\\n\\nP (B)\\n\\n=\\n\\nP (B)\\nP (B)\\n\\n= 1.\\n\\nS\\n\\nS SS\\n\\ns\\n\\nsS\\n\\ns\\n\\nSs\\n\\nss\\n\\np2\\n\\np(1 − p)\\n\\n(1 − p)p\\n\\n(1 − p)2\\n\\nTable III: Punnett square for a monohybrid cross using a dominant trait\\nS (say spherical seeds) that occurs in the population with probability p\\nand a recessive trait s (wrinkled seeds) that occurs with probability 1 − p.\\nMaternal genotypes are listed on top, paternal genotypes on the left. See\\nExample 6.14. The probabilities of a given genotype are given in the lower\\nright hand corner of the box.\\n\\n3. For mutually exclusive events, {Aj; j ≥ 1}, we\\n\\nhave that {Aj ∩ B; j ≥ 1} are also mutually exclusive and\\n\\nQ\\uf8eb\\uf8ed\\n\\n∞(cid:91)j=1\\n\\nAj\\uf8f6\\uf8f8 = P\\uf8eb\\uf8ed\\n∞(cid:91)j=1\\n= (cid:80)∞\\n\\nAj(cid:12)(cid:12)(cid:12)B\\uf8f6\\uf8f8 =\\n\\nj=1 P (Aj ∩ B)\\n\\nP (B)\\n\\n=\\n\\nP (B)\\n\\nP(cid:16)(cid:16)(cid:83)∞\\n∞(cid:88)j=1\\n\\nj=1 Aj(cid:17) ∩ B(cid:17)\\n∞(cid:88)j=1\\n\\nP (Aj ∩ B)\\n\\nP (B)\\n\\n=\\n\\n=\\n\\nP ((cid:83)∞\\n\\nj=1(Aj ∩ B))\\nP (B)\\n\\nP (Aj|B) =\\n\\n∞(cid:88)j=1\\n\\nQ(Aj)\\n\\n6.2. P{sum is 8|ﬁrst die shows 3} = 1/6, and P{sum is 8|ﬁrst die shows 1} = 0.\\n\\n107\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\n6.3. Here is a table of outcomes. The symbol × indicates an outcome in the event\\n{sum is at least 5}. The rectangle indicates the event {ﬁrst die is 2}. Because\\nthere are 10 ×’s,\\n\\nP{sum is at least 5} = 10/16 = 5/8.\\n\\nThe rectangle contains 4 outcomes, so\\n\\n3\\n\\n2\\n\\n1\\n\\n4\\n1\\n×\\n2\\n× ×\\n3\\n× × ×\\n4 × × × ×\\n\\nP{ﬁrst die is 2} = 4/16 = 1/4.\\n\\nInside the event {ﬁrst die is 2}, 2 of the outcomes are also in the event {sum is at least 5}. Thus,\\n\\nUsing the deﬁnition of conditional probability, we also have\\n\\nP{sum is at least 5|ﬁrst die is 2} = 2/4 = 1/2\\n\\nP{sum is at least 5|ﬁrst die is 2} =\\n\\nP{sum is at least 5 and ﬁrst die is 2}\\n\\nP{ﬁrst die is 2}\\n\\n=\\n\\n2/16\\n4/16\\n\\n=\\n\\n2\\n4\\n\\n=\\n\\n1\\n2\\n\\n.\\n\\n6.5. We modify both sides of the equation.\\n\\n(b + g)4\\n\\n2(cid:19) (g)2(b)2\\n(cid:18)4\\n(cid:0)b\\n2(cid:1)(cid:0)g\\n2(cid:1)\\n(cid:0)b+g\\n4 (cid:1) =\\n\\n=\\n\\n4!\\n2!2!\\n\\n(g)2(b)2\\n(b + g)4\\n\\n(b)2/2! · (g)2/2!\\n\\n(b + g)4/4!\\n\\n=\\n\\n4!\\n2!2!\\n\\n(g)2(b)2\\n(b + g)4\\n\\n.\\n\\n4 (cid:1) outcomes. The number of choices of 2\\nThe sample space Ω is set of collections of 4 balls out of b + g. This has(cid:0)b+g\\n2(cid:1). Thus, by the fundamental principle of counting,\\nblue out of b is(cid:0)b\\n2(cid:1). The number of choices of 2 green out of g is(cid:0)g\\n2(cid:1). For equally likely outcomes, the probability\\n2(cid:1)(cid:0)g\\nthe total number of ways to obtain the event 2 blue and 2 green is(cid:0)b\\n4 (cid:1), the number of outcomes in the sample space.\\n2(cid:1)(cid:0)g\\nis the ratio of(cid:0)b\\n2(cid:1), the number of outcomes in the event, and(cid:0)b+g\\n\\n6.8. Let Aij be the event of winning the series that has i wins versus j wins for the opponent. Then pij = P (Aij). We\\nknow that\\n\\nbecause the series is lost when the opponent has won 4 games. Also,\\n\\np0,4 = p1,4 = p2,4 = p3,4 = 0\\n\\np4,0 = p4,1 = p4,2 = p4,3 = 1\\n\\nbecause the series is won with 4 wins in games. For a tied series, the probability of winning the series is 1/2 for both\\nsides.\\n\\np0,0 = p1,1 = p2,2 = p3,3 =\\n\\n1\\n2\\n\\n.\\n\\nThese values are ﬁlled in blue in the table below. We can determine the remaining values of pij iteratively by looking\\nforward one game and using the law of total probability to condition of the outcome of the (i + j + 1-st) game. Note\\nthat P{win game i + j + 1} = P{lose game i + j + 1} = 1\\n2 .\\npij = P (Aij|win game i + j + 1}P{win game i + j + 1} + P (Aij|lose game i + j − 1}P{lose game i + j + 1}\\n\\n=\\n\\n(pi+1,j + pi,j+1)\\n\\n1\\n2\\n\\nThis can be used to ﬁll in the table above the diagonal. For example,\\n\\np23 =\\n\\n1\\n2\\n\\n(p33 + p42) =\\n\\n108\\n\\n1\\n\\n2(cid:18) 1\\n\\n2\\n\\n+ 1(cid:19) =\\n\\n3\\n4\\n\\n.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\nFor below the diagonal, note that\\n\\nFor example,\\n\\nFilling in the table, we have:\\n\\npij = 1 − pji.\\n\\np23 = 1 − p32 = 1 −\\n\\n3\\n4\\n\\n=\\n\\n1\\n4\\n\\n.\\n\\n0\\n1/2\\n11/32\\n3/16\\n1/16\\n\\n0\\n\\n1\\n\\n21/32\\n1/2\\n5/16\\n1/8\\n0\\n\\n0\\n1\\n2\\n3\\n4\\n\\nj\\n\\ni\\n2\\n\\n13/16\\n11/16\\n1/2\\n1/4\\n0\\n\\n3\\n\\n15/16\\n7/8\\n3/4\\n1/2\\n0\\n\\n4\\n1\\n1\\n1\\n1\\n-\\n\\n6.13. We take the questions one at a time. Because A and B are independent P (A ∩ B) = P (A)P (B).\\n(a) B is the disjoint union of A ∩ B and Ac ∩ B. Thus,\\n\\nP (B) = P (A ∩ B) + P (Ac ∩ B)\\n\\nSubtract P (A ∩ B) to obtain\\n\\nP (Ac ∩ B) = P (B) − P (A ∩ B) = P (B) − P (A)P (B) = (1 − P (A))P (B) = P (Ac)P (B)\\n\\nand Ac and B are independent.\\n\\n(b) Just switch the roles of A and B in part (a) to see that A and Bc are independent.\\n\\n(c) Use the complement rule and inclusion-exclusion\\n\\nP (Ac ∩ Bc) = P ((A ∪ B)c) = 1 − P (A ∪ B) = 1 − P (A) − P (B) − P (A ∩ B)\\n\\n= 1 − P (A) − P (B) − P (A)P (B) = (1 − P (A))(1 − P (B))\\n= P (Ac)P (Bc)\\n\\nand Ac and Bc are independent.\\n\\n6.15. Let Ai be the event {i-th coin turns up heads}. Then the event can be written A1 ∩ Ac\\n\\n3 ∩ A7 ∩ A9. Thus,\\n\\nP (A1 ∩ Ac\\n\\n3 ∩ A7 ∩ A9) = P (A1)P (Ac\\n\\n3)P (A7)P (A9)\\n\\n= p1(1 − p3)p7p9.\\n\\n6.17. Multiply both of the expressions in (6.11) by the appropriate probability to see that they are equivalent to\\n\\nP (A ∩ B) > P (A)P (B).\\n\\n109\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nConditional Probability and Independence\\n\\n6.18. By using Bayes formula, if P (A|B) = 1, then\\nP (B)\\nP (A)\\nOn the other hand, if P (B|A) = P (B)/P (A), then\\n\\nP (A|B)P (B)\\n\\nP (B|A) =\\n\\nP (A)\\n\\n=\\n\\n.\\n\\nP (A|B) =\\n\\nP (B|A)P (A)\\n\\nP (B)\\n\\n= 1.\\n\\n6.20 Because A is the disjoint union of A ∩ B and A ∩ Bc, we have\\nP (A) = P (A ∩ B) + P (A ∩ Bc) or P (A ∩ Bc) = P (A) − P (A ∩ B).\\nThus,\\n\\nDA,Bc = P (A)P (Bc) − P (A ∩ Bc)\\n\\n= P (A)(1 − P (B)) − (P (A) − P (A ∩ B))\\n= −P (A)P (B) + P (A ∩ B) = −DA,B.\\n\\nFigure 6.9: If P (A|B) ≈ 1,\\nthen nearly all of\\nB is inside A and the probability of P (B|A) ≈\\nP (B)/P (A) as shown in the ﬁgure.\\n\\n110\\n\\nAB\\x0cTopic 7\\n\\nRandom Variables and Distribution\\nFunctions\\n\\nWhile writing my book I had an argument with Feller. He asserted that everyone said “random variable”\\nand I asserted that everyone said “chance variable.” We obviously had to use the same name in our books,\\nso we decided the issue by a stochastic procedure. That is, we tossed for it and he won. – Joseph Doob,\\nStatistical Science\\n\\n7.1 Introduction\\nFrom the universe of possible information, we ask\\na question. To address this question, we might col-\\nlect quantitative data and organize it, for example,\\nusing the empirical cumulative distribution func-\\ntion. With this information, we are able to com-\\npute sample means, standard deviations, medians\\nand so on.\\n\\nSimilarly, even a fairly simple probability\\nmodel can have an enormous number of outcomes.\\nFor example, ﬂip a coin 333 times. Then the num-\\nber of outcomes is more than a google (10100) –\\na number at least 100 quintillion times the num-\\nber of elementary particles in the known universe.\\nWe may not be interested in an analysis that con-\\nsiders separately every possible outcome but rather\\nsome simpler concept like the number of heads or\\nthe longest run of tails. To focus our attention on\\nthe issues of interest, we take a given outcome and\\ncompute a number. This function is called a ran-\\ndom variable.\\nDeﬁnition 7.1. A random variable is a real val-\\nued function from the probability space.\\n\\nstatistics\\n\\nuniverse of\\ninformation\\n\\nprobability\\n\\nsample space - Ω\\nand probability - P\\n\\n⇓\\n\\n⇓\\n\\n⇓\\n\\nask a question and\\n\\ncollect data\\n\\ndeﬁne a random\\n\\nvariable X\\n\\norganize into the\\n\\nempirical cumulative\\ndistribution function\\n\\norganize into the\\n\\ncumulative\\n\\ndistribution function\\n\\ncompute sample\\n\\ncompute distributional\\nmeans and variances means and variances\\n\\n⇓\\n\\n⇓\\n\\n⇓\\n\\nTable I: Corresponding notions between statistics and probability. Examining\\nprobabilities models and random variables will lead to strategies for the collection\\nof data and inference from these data.\\n\\nGenerally speaking, we shall use capital letters near the end of the alphabet, e.g., X, Y, Z for random variables.\\n\\nThe range S of a random variable is sometimes called the state space.\\n\\nX : Ω → R.\\n\\n111\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nExercise 7.2. Roll a die twice and consider the sample space Ω = {(i, j); i, j = 1, 2, 3, 4, 5, 6} and give some random\\nvariables on Ω.\\nExercise 7.3. Flip a coin 10 times and consider the sample space Ω, the set of 10-tuples of heads and tails, and give\\nsome random variables on Ω.\\n\\nWe often create new random variables via composition of functions:\\n\\nThus, if X is a random variable, then so are\\n\\nω (cid:55)→ X(ω) (cid:55)→ f (X(ω))\\n\\nX 2,\\n\\nexp αX, (cid:112)X 2 + 1,\\n\\ntan2 X,\\n\\n(cid:98)X(cid:99)\\n\\nand so on. The last of these, rounding down X to the nearest integer, is called the ﬂoor function.\\nExercise 7.4. How would we use the ﬂoor function to round down a number x to n decimal places.\\n\\n7.2 Distribution Functions\\nHaving deﬁned a random variable of interest, X, the question typically becomes, “What are the chances that X lands\\nin some subset of values B?” For example,\\n\\nB = {odd numbers}, B = {greater than 1},\\n\\nor B = {between 2 and 7}.\\n\\nWe write\\n\\n(7.1)\\nto indicate those outcomes ω which have X(ω), the value of the random variable, in the subset B. We shall often\\nabbreviate (7.1) to the shorter {X ∈ B}. Thus, for the example above, we may write the events\\n\\n{ω ∈ Ω; X(ω) ∈ B}\\n\\n{X is an odd number},\\n\\n{X is greater than 1} = {X > 1},\\n\\n{X is between 2 and 7} = {2 < X < 7}\\n\\nto correspond to the three choices above for the subset B.\\n\\nMany of the properties of random variables are not concerned with the speciﬁc random variable X given above,\\nbut rather depends on the way X distributes its values. This leads to a deﬁnition in the context of random variables\\nthat we saw previously with quantitive data.\\nDeﬁnition 7.5. A (cumulative) distribution function of a random variable X is deﬁned by\\n\\nRecall that with quantitative observations, we called the analogous notion the empirical cumulative distribution\\n\\nfunction. Using the abbreviated notation above, we shall typically write the less explicit expression\\n\\nFX (x) = P{ω ∈ Ω; X(ω) ≤ x}.\\n\\nFX (x) = P{X ≤ x}\\n\\nfor the distribution function.\\nExercise 7.6. Establish the following identities that relate a random variable the complement of an event and the\\nunion and intersection of events\\n1. {X ∈ B}c = {X ∈ Bc}\\n2. For sets B1, B2, . . .,\\n(cid:91)i\\n\\n{X ∈ Bi} = {X ∈(cid:92)i\\n\\n{X ∈ Bi} = {X ∈(cid:91)i\\n\\nand (cid:92)i\\n\\nB}.\\n\\nB}\\n\\n112\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n3. If B1, . . . Bn form a partition of the sample space S, then Ci = {X ∈ Bi}, i = 1, . . . , n form a partition of the\\n\\nprobability space Ω.\\n\\nExercise 7.7. For a random variable X and subset B of the sample space S, deﬁne\\n\\nShow that PX is a probability.\\n\\nPX (B) = P{X ∈ B}.\\n\\nFor {X > x}, the complement of {X ≤ x}, we have the survival function\\n\\n¯FX (x) = P{X > x} = 1 − P{X ≤ x} = 1 − FX (x).\\nChoose a < b, then the event {X ≤ a} ⊂ {X ≤ b}. Their set theoretic difference\\n\\n{X ≤ b} \\\\ {X ≤ a} = {a < X ≤ b}.\\n\\nIn words, the event that “X is less than or equal to b but not less than or equal to a” is the same event as “X is greater\\nthan a and less than or equal to b.” Consequently, by the difference rule for probabilities,\\n\\nP{a < X ≤ b} = P ({X ≤ b} \\\\ {X ≤ a}) = P{X ≤ b} − P{X ≤ a} = FX (b) − FX (a).\\n\\n(7.2)\\n\\nThus, we can compute the probability that a random variable takes values in an interval by subtracting the distri-\\nbution function evaluated at the endpoints of the intervals. Care is needed on the issue of the inclusion or exclusion of\\nthe endpoints of the interval.\\n\\nExample 7.8. To give the cumulative distribution function for X, the sum of the values for two rolls of a die, we start\\nwith the table\\n\\nx\\n\\nP{X = x}\\nand create the graph.\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n1/36\\n\\n2/36\\n\\n3/36\\n\\n4/36\\n\\n5/36\\n\\n6/36\\n\\n5/36\\n\\n4/36\\n\\n10\\n3/36\\n\\n11\\n2/36\\n\\n12\\n1/36\\n\\n6\\n\\n1\\n\\n3/4\\n\\n1/2\\n\\n1/4\\n\\nr\\n\\n2\\n\\nr\\n\\n3\\n\\n1\\n\\nr\\n\\nr\\n\\nr\\n\\nr\\n\\nr\\n\\nr\\n\\nr\\n\\nr\\n\\nr\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n12\\n\\n-\\n\\nFigure 7.1: Graph of FX, the cumulative distribution function for the sum of the values for two rolls of a die.\\n\\n113\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nIf we look at the graph of this cumulative distribution function, we see that it is constant in between the possible\\nvalues for X and that the jump size at x is equal to P{X = x}. In this example, P{X = 5} = 4/36, the size of the\\njump at x = 5. In addition,\\n\\nFX (5) − FX (2) = P{2 < X ≤ 5} = P{X = 3} + P{X = 4} + P{X = 5} = (cid:88)2<x≤5\\n\\n=\\n\\n+\\n\\n+\\n\\n=\\n\\n3\\n36\\n\\n4\\n36\\n\\n2\\n36\\n\\n9\\n36\\n\\n.\\n\\nP{X = x}\\n\\nWe shall call a random variable discrete if it has a ﬁnite or countably inﬁnite state space. Thus, we have in general\\n\\nthat:\\n\\nP{a < X ≤ b} = (cid:88)a<x≤b\\n\\nP{X = x}.\\n\\nExercise 7.9. Let X be the number of heads on three independent ﬂips of a biased coin that turns ups heads with\\nprobability p. Give the cumulative distribution function FX for X.\\nExercise 7.10. Let X be the number of spades in a collection of three cards. Give the cumulative distribution function\\nfor X. Use R to plot this function.\\nExercise 7.11. Find the cumulative distribution function of Y = X 3 in terms of FX, the distribution function for X.\\n\\n7.3 Properties of the Distribution Function\\nA distribution function FX has the property that it starts at 0, ends at 1 and does not decrease with increasing values\\nof x. This is the content of the next exercise.\\nExercise 7.12. The disribution function FX has the properties:\\n\\n1. limx→−∞ FX (x) = 0.\\n2. limx→∞ FX (x) = 1.\\n3. FX is nondecreasing.\\n\\n7.3.1 Discrete Random Variables\\nThe cumulative distribution function FX of a discrete random variable X is constant except for jumps. At the jump,\\nFX is right continuous,\\n\\nlim\\n\\nx→x0+\\n\\nFX (x) = FX (x0).\\n\\n(7.3)\\n\\nThe next exercise ask that this be shown more generally.\\nExercise 7.13. Prove the statement (7.3) concerning the right continuity of the distribution function from the continuity\\nproperty of a probability.\\nExercise 7.14. Show that for any x0,\\n\\nP{X < x0} = lim\\n\\nx→x− FX (x) = FX (x0−),\\n\\nthe left limit of FX at x0.\\n\\nPutting the previous two exercises together, we ﬁnd that\\n\\nP{X = x0} = P ({X ≤ x0} \\\\ {X < x0}) = P{X ≤ x0} − P{X < x0} = FX (x0) − FX (x0−),\\n\\nThe size of the jump in FX (x) at the value x0.\\n\\n114\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n7.3.2 Continuous Random Variables\\nDeﬁnition 7.15. A continuous random variable has a cumulative distribution function FX that is differentiable.\\n\\nSo, distribution functions for continuous random variables increase\\nsmoothly. To show how this can occur, we will develop an example of a\\ncontinuous random variable.\\n\\nExample 7.16. Consider a dartboard having unit radius. Assume that\\nthe dart lands randomly uniformly on the dartboard.\\n\\nLet X be the distance from the center. For x ∈ [0, 1],\\narea inside circle of radius x\\n\\nFX (x) = P{X ≤ x} =\\nThus, we have the distribution function\\n\\narea of circle\\n\\n=\\n\\nπx2\\nπ12 = x2.\\n\\nFX (x) =\\uf8f1\\uf8f2\\uf8f3\\n\\n0\\nx2\\n1\\n\\nif x ≤ 0,\\nif 0 < x ≤ 1,\\nif x > 1.\\n\\nThe ﬁrst line states that X cannot be negative. The third states that X\\nis at most 1, and the middle lines describes how X distributes is values\\nbetween 0 and 1. For example,\\n\\nFX(cid:18) 1\\n\\n2(cid:19) =\\n\\n1\\n4\\n\\nindicates that with probability 1/4, the dart will land within 1/2 unit of\\nthe center of the dartboard.\\n\\nExercise 7.17. Find the probability that the dart lands between 1/3 unit\\nand 2/3 unit from the center. Find the median, the ﬁrst quartile, and the\\nthird quartiles.\\n\\nFigure 7.2: (top) Dartboard.\\n(bottom) Cumulative\\ndistribution function for the dartboard random vari-\\nable.\\n\\nExercise 7.18. Let the reward Y for throwing the dart be the inverse 1/X\\nof the distance from the center. Find the cumulative distribution function for Y .\\n\\nExercise 7.19. An exponential random variable X has cumulative distribution function\\n\\n1 − exp(−λx)\\nfor some λ > 0. Show that FX has the properties of a distribution function.\\n\\nFX (x) = P{X ≤ x} =(cid:26) 0\\n\\nif x ≤ 0,\\nif x > 0.\\n\\nWe can create an expression and perform an evaluation using R.\\n\\n> F<-expression(1-exp(-lambda*x))\\n\\nWe can then evaluate FX (3) and FX (1) with λ = 2 as follows.\\n\\n> x<-c(10,30);lambda<-1/10\\n> (Feval<-eval(F))\\n[1] 0.6321206 0.9502129\\n> Feval[2]-Feval[1]\\n[1] 0.3180924\\n\\n115\\n\\n(7.4)\\n\\n!1!0.8!0.6!0.4!0.200.20.40.60.81!1!0.8!0.6!0.4!0.200.20.40.60.81x0.00.51.00.00.20.40.60.81.0xprobability0.00.51.00.00.20.40.60.81.00.00.51.00.00.20.40.60.81.00.00.51.00.00.20.40.60.81.00.00.51.00.00.20.40.60.81.0\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nThe last expression gives the value for FX (30) − FX (10) = P{10 < X ≤ 30}.\\nfor λ = 1/10. Thus, we make the computation above by\\n\\nThis function is also stored in R and so its value at x can be computed in R using the command pexp(x,0.1)\\n\\n> pexp(30,0.1)-pexp(10,0.1)\\n[1] 0.3180924\\n\\nWe can draw the distribution function using the curve command.\\n\\n> curve(pexp(x,0.1),0,80)\\n\\nFigure 7.3: Cumulative distribution function for an exponential random variable with λ = 1/10.\\n\\nExercise 7.20. The time until the next bus arrives is an exponential random variable with λ = 1/10 minutes. A\\nperson waits at the bus stop until the bus arrives, giving up when the wait reaches 20 minutes. Give the cumulative\\ndistribution function for T , the time that the person remains at the bus station and sketch a graph.\\n\\nEven though the cumulative distribution function is deﬁned for every random variable, we will often use other\\ncharacterizations, namely, the mass function for discrete random variable and the density function for continuous\\nrandom variables. Indeed, we typically will introduce a random variable via one of these two functions. In the next\\ntwo sections we introduce these two concepts and develop some of their properties.\\n\\n7.4 Mass Functions\\nDeﬁnition 7.21. The (probability) mass function of a discrete random variable X is\\n\\nThe mass function has a value at x equal to the size of the jump in the distribution function. In symbols,\\n\\nfX (x) = P{X = x}.\\n\\nfX (x) = FX (x) − FX (x−)\\n\\nwhere FX (x−) is the left limit of FX at x.\\n\\nThe mass function has two basic properties:\\n• fX (x) ≥ 0 for all x in the state space.\\n• (cid:80)x fX (x) = 1.\\n\\n116\\n\\n0204060800.00.20.40.60.81.0xpexp(x, 0.1)\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nThe ﬁrst property is based on the fact that probabilities are non-negative. The second follows from the observation\\nthat the collection Cx = {ω; X(ω) = x} for all x ∈ S, the state space for X, forms a partition of the probability\\nspace Ω. In Example 7.8, we saw the mass function for the random variable X that is the sum of the values on two\\nindependent rolls of a fair dice.\\nExample 7.22. Let’s make tosses of a biased coin whose outcomes are independent. We shall continue tossing until\\nwe obtain a toss of heads. Let X denote the random variable that gives the number of tails before the ﬁrst head and p\\ndenote the probability of heads in any given toss. Then\\n\\nfX (0) = P{X = 0} = P{H} = p\\nfX (1) = P{X = 1} = P{T H} = (1 − p)p\\nfX (2) = P{X = 2} = P{T T H} = (1 − p)2p\\n\\n...\\n\\n...\\n\\n...\\n\\nfX (x) = P{X = x} = P{T ··· T H} = (1 − p)xp\\n\\nSo, the probability mass function fX (x) = (1 − p)xp. Because the terms in this mass function form a geometric\\nsequence, X is called a geometric random variable. Recall that a geometric sequence c, cr, cr2, . . . , crn has sum\\n\\nsn = c + cr + cr2 + ··· + crn =\\n\\nc(1 − rn+1)\\n\\n1 − r\\n\\nfor r (cid:54)= 1. If |r| < 1, then limn→∞ rn = 0 and thus sn has a limit as n → ∞. In this case, the inﬁnite sum is the limit\\n(7.5)\\n\\nc + cr + cr2 + ··· + crn + ··· = lim\\n\\nn→∞ sn =\\n\\n.\\n\\nc\\n1 − r\\n\\nExercise 7.23. Establish the formula (7.5) above for sn.\\n\\nThe mass function above forms a geometric sequence with the ratio r = 1 − p. Consequently, for positive integers\\n\\na and b,\\n\\nb(cid:88)x=a+1\\n\\nP{a < X ≤ b} =\\n\\n=\\n\\n(1 − p)xp = (1 − p)a+1p + ··· + (1 − p)bp\\n\\n(1 − p)a+1p − (1 − p)b+1p\\n\\n1 − (1 − p)\\n\\n= (1 − p)a+1 − (1 − p)b+1\\n\\nWe can take a = −1 to ﬁnd the distribution function for a geometric random variable.\\n\\nFX (b) = P{X ≤ b} = 1 − (1 − p)b+1.\\n\\n(7.6)\\nTo obtain (7.6) in another way, note that the event {X ≥ b + 1} = {X > b} is the same as having the ﬁrst\\nb + 1 coin tosses turn up tails. This event consists of b + 1 independent events each with probability 1 − p. Thus,\\nP{X ≥ b + 1} = P{X > b} = (1 − p)b+1. By noting that the distribution function, FX (b) = 1 − P{X > b}, we\\nagain obtain (7.6).\\nExercise 7.24. Show that for a geometric random variable X,\\n\\nP{X ≥ a + b|X ≥ b} = P{X ≥ a}.\\n\\n(7.7)\\n\\nThis property is called memorylessness. In words, if the ﬁrst b trials results in failures, then the probability of at least\\na additional failures is the same as the probability of at least a failures from the beginning. The fact that we begin with\\nb failures does not impact the number of trials afterwards until a success.\\n\\nConversely, if the memoryless property holds for an N-valued random variable X, then X is a geometric random\\n\\nvariable.\\n\\n117\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nThe mass function and the cumulative distribution function for the geometric random variable with parameter\\n\\np = 1/3 can be found in R by writing\\n\\n> x<-0:10\\n> f<-dgeom(x,1/3)\\n> F<-pgeom(x,1/3)\\n\\nThe initial d indicates density and p indicates the probability from the distribution function.\\n> data.frame(x,f,F)\\n\\nf\\n\\nx\\nF\\n0 0.333333333 0.3333333\\n1\\n1 0.222222222 0.5555556\\n2\\n2 0.148148148 0.7037037\\n3\\n3 0.098765432 0.8024691\\n4\\n4 0.065843621 0.8683128\\n5\\n5 0.043895748 0.9122085\\n6\\n6 0.029263832 0.9414723\\n7\\n7 0.019509221 0.9609816\\n8\\n8 0.013006147 0.9739877\\n9\\n10\\n9 0.008670765 0.9826585\\n11 10 0.005780510 0.9884390\\n\\nNote that the difference in values in the distribution function FX (x) − FX (x − 1), giving the height of the jump\\n\\nin FX at x, is equal to the value of the mass function. For example,\\n\\nFX (3) − FX (2) = 0.8024691 − 0.7037037 = 0.0987654 = fX (3).\\n\\nExercise 7.25. Check that the jumps in the cumulative distribution function for the geometric random variable above\\nis equal to the values of the mass function.\\nExercise 7.26. For the geometric random variable above, ﬁnd P{X ≤ 3}, P{2 < X ≤ 5}. P{X > 4}.\\n\\nWe can simulate 100 geometric random variables with parameter p = 1/3 using the R command rgeom(100,1/3).\\n\\n(See Figure 7.4.)\\n\\nFigure 7.4: Histogram of 100 and 10,000 simulated geometric random variables with p = 1/3. Note that the histogram looks much more like a\\ngeometric series for 10,000 simulations. We shall see later how this relates to the law of large numbers.\\n\\n118\\n\\nxFrequency051015202530010203040xFrequency051015202530050015002500\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n7.5 Density Functions\\nDeﬁnition 7.27. For X a random variable whose distribution function FX has a derivative. The function fX satisfying\\n\\nFX (x) =(cid:90) x\\n\\n−∞\\n\\nfX (t) dt\\n\\nis called the probability density function and X is called a continuous random variable.\\n\\nBy the fundamental theorem of calculus, the density function is the derivative of the distribution function.\\n\\nfX (x) = lim\\n∆x→0\\n\\nFX (x + ∆x) − FX (x)\\n\\n∆x\\n\\n(cid:48)\\nX (x).\\n\\n= F\\n\\nIn other words,\\n\\nWe can compute probabilities by evaluating deﬁnite integrals\\n\\nFX (x + ∆x) − FX (x) ≈ fX (x)∆x.\\n\\nP{a < X ≤ b} = FX (b) − FX (a) =(cid:90) b\\n\\na\\n\\nfX (t) dt.\\n\\nThe density function has two basic properties that mirror\\n\\nthe properties of the mass function:\\n\\n• fX (x) ≥ 0 for all x in the state space.\\n\\n−∞ fX (x) dx = 1.\\n\\n• (cid:82) ∞\\n\\nReturn to the dart board example, letting X be the dis-\\ntance from the center of a dartboard having unit radius.\\nThen,\\n\\nP{x < X ≤ x + ∆x} = FX (x + ∆x) − FX (x)\\n\\n≈ fX (x)∆x = 2x∆x\\n\\nand X has density\\n\\nFigure 7.5: The probability P{a < X ≤ b} is the area under the\\ndensity function, above the x axis between y = a and y = b.\\n\\nfX (x) =\\uf8f1\\uf8f2\\uf8f3\\n\\n0\\n2x\\n0\\n\\nif x < 0,\\nif 0 ≤ x ≤ 1,\\nif x > 1.\\n\\nExercise 7.28. Let fX be the density for a random variable X and pick a number x0. Explain why P{X = x0} = 0.\\nExercise 7.29. Plot, on both the distribution function and the density function, the probability that the dart lands\\nbetween 1/3 unit and 2/3 unit from the center.\\n\\nExample 7.30. For the exponential distribution function (7.4), we have the density function\\n\\nfX (x) =(cid:26) 0\\n\\nλe−λx\\n\\nif x ≤ 0,\\nif x > 0.\\n\\nR performs differentiation. We must ﬁrst create an expression\\n\\n> F<-expression(1-exp(-lambda*x))\\n\\nWe then differentiate using the D command, placing x, the variable of differentiation in quotes.\\n\\n119\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n> f<-D(F,\"x\")\\n> f\\nexp(-lambda * x) * lambda\\n\\nExample 7.31. Density functions do not need to be bounded, for example, if we take\\n\\nif x ≤ 0,\\nif 0 < x < 1,\\nif 1 ≤ x.\\nThen, to ﬁnd the value of the constant c, we compute the integral\\n\\n0\\nc√\\nx\\n\\n0\\n\\nfX (x) =\\uf8f1\\uf8f2\\uf8f3\\n1 =(cid:90) 1\\n\\nc\\n√t\\n\\n0\\n\\nSo c = 1/2. For 0 ≤ a < b ≤ 1,\\n\\nP{a < X ≤ b} =(cid:90) b\\n\\na\\n\\n= 2c.\\n\\n0\\n\\n1\\n\\ndt = 2c√t(cid:12)(cid:12)(cid:12)\\ndt = √t(cid:12)(cid:12)(cid:12)\\n\\n1\\n2√t\\n\\nb\\n\\na\\n\\n= √b − √a.\\n\\nExercise 7.32. Give the cumulative distribution function for the random variable in the previous example.\\n\\nExercise 7.33. Let X be a continuous random variable with density fX, then the random variable Y = aX + b has\\ndensity\\n\\nfY (y) =\\n\\na (cid:19)\\nfX(cid:18) y − b\\n\\n1\\n|a|\\n\\n(Hint: Begin with the deﬁnition of the cumulative distribution function FY for Y . Consider the cases a > 0 and a < 0\\nseparately.)\\n\\n7.6 Mixtures\\nExercise 7.34. Let F1 and F2 be two cumulative distribution functions and let π ∈ (0, 1), then\\n\\nis a cumulative distribution function.\\n\\nF (x) = πF1(x) + (1 − π)F2(x)\\n\\nWe call the distribution F a mixture of F1 and F2. Mixture distributions occur routinely. To see this, ﬁrst ﬂip a\\n\\ncoin, heads occurring with probability π. In this case the random variable\\n\\nX =(cid:26) X1\\n\\nX2\\n\\nif the coin lands heads,\\nif the coin lands tails.\\n\\nIf Xi has distribution function Fi, i = 1, 2, then, by the law of total probability,\\n\\nFX (x) = P{X ≤ x} = P{X ≤ x|coin lands heads}P{coin lands heads}\\n+P{X ≤ x|coin lands tails}P{coin lands tails}\\n\\n= P{X1 ≤ x}π + P{X2 ≤ x}(1 − π) = πF1(x) + (1 − π)F2(x)\\n\\nMore generally, let X1, . . . , Xn be random variables with distribution functions F1, . . . , Fn and π1, . . . , πn be\\ni=1 πi = 1. In this case, roll an n sided die, i showing with probability πi. If the die shows\\ni, then we use the random variable Xi. To be concrete, individuals arriving to take an airline ﬂight are assigned to\\n\\npositive numbers with(cid:80)n\\n\\n120\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\ngroup i with probability πi. Let Xi be the (random) time until individuals in group i are seated. Then the distribution\\nfunction for the time to be seated\\n\\nFX (x) = P{X ≤ x} =\\n\\n=\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\nP{X ≤ x|assigned group i}P{assigned group i}\\n\\nP{Xi ≤ x}πi = π1F1(x) + ··· + πnFn(x).\\n\\nF is call the mixture of F1, . . . , Fn with weights π1, . . . , πn.\\n\\nIf the Xi are discrete random variables, then so is X. The mass function for X is\\n\\nfX (x) = FX (x) − FX (x−) = π1(F1(x) − F1(x−)) + ··· + πn(Fn(x) − Fn(x−))\\n\\n= π1f1(x) + ··· + πnfn(x).\\n\\nExercise 7.35. Check that fX is a mass function.\\n\\nExercise 7.36. Find the mass function for the mixture of the three mass functions\\n\\nx\\n1\\n2\\n3\\n4\\n5\\n\\nf1(x)\\n0.2\\n0.3\\n0.1\\n0.4\\n0\\n\\nf2(x)\\n0.5\\n0.5\\n0\\n0\\n0\\n\\nf3(x)\\n0.1\\n0.1\\n0.2\\n0.2\\n0.4\\n\\nand weights π = (1/4, 1/4, 1/2),\\n\\nIf the Xi are continuous random variables, then so is X. The density function for X is\\n\\nfX (x) = F\\n\\n(cid:48)\\nX (x) = π1F\\n\\n(cid:48)\\n1(x) + ··· + πnF\\n\\n(cid:48)\\nn(x)\\n= π1f1(x) + ··· + πnfn(x)\\n=\\n\\nfi(x)πi.\\n\\nn(cid:88)i=1\\n\\nChecking that fX is a density function is similar to the exercise above. Just replace the sum on x with an integral.\\n\\n7.7 Joint and Conditional Distributions\\nBecause we will collect data on several observations, we must, as well, consider more than one random variable at a\\ntime in order to model our experimental procedures. Consequently, we will expand on the concepts above to the case\\nof multiple random variables and their joint distribution. For the case of two random variables, X1 and X2, this means\\nlooking at the probability of events,\\n\\nP{X1 ∈ B1, X2 ∈ B2}.\\n\\nFor discrete random variables, take B1 = {x1} and B2 = {x2}. Then, we have\\n7.7.1 Discrete Random Variables\\nDeﬁnition 7.37. The joint probability mass function\\n\\nfX1,X2 (x1, x2) = P{X1 = x1, X2 = x2}.\\n\\n121\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nThe mass functions for X1 and X2 can be obtained from the joint mass function by summing over the values for\\n\\nthe other random variable. Thus, for example,\\n\\nfX1 (x1) = P{X1 = x1} =(cid:88)x2\\n\\nP{X1 = x1, X2 = x2} =(cid:88)x2\\n\\nfX1,X2(x1, x2).\\n\\n(7.8)\\n\\nIn this case, we use the expression marginal probability mass function to distinguish it from the joint probability\\n\\nmass function.\\nExercise 7.38. Let X1 and X2 have the joint mass function displayed in the table below\\n\\nx2\\\\x1\\n-1\\n0\\n1\\n2\\n\\n1\\n\\n0.09\\n0.07\\n0.10\\n0.01\\n\\nfX1,X2(x1, x2)\\n2\\n4\\n\\n3\\n\\n0.04\\n\\n0\\n\\n0.06\\n0.08\\n\\n0.03\\n0.07\\n0.05\\n0.09\\n\\n0.01\\n0.02\\n0.08\\n0.05\\n\\n5\\n\\n0.02\\n0.03\\n0.06\\n0.04\\n\\nShow that the sum of the entries is 1 and determine the marginal mass functions.\\nThe conditional mass functions looks at the probabilities that one random variable takes on a given value, given\\na value for the second random variable. The conditional mass function of X2 given X1 is denoted fX2|X1 (x2|x1) =\\nP{X2 = x2|X1 = x1}. To compute this function,\\n\\nfX2|X1(x2|x1) = P{X2 = x2|X1 = x1} =\\n\\nP{X1 = x1, X2 = x2}\\n\\nP{X1 = x1}\\n\\n=\\n\\nfX1,X2 (x1, x2)\\n\\nfX1(x1)\\n\\n(7.9)\\n\\nprovided fX1 (x1) > 0.\\nExercise 7.39. Show that, for each value of x1, fX2|X1(x2|x1) is a mass function, that is, the values are non-negative\\nand the sum over all values for x2 equals 1.\\nExercise 7.40. For each value of x1, ﬁnd the conditional mass function. fX2|X1(x2|x1) for the values in the table\\nabove.\\n\\n7.7.2 Continuous Random Variables\\nFor continuous random variables, we consider B1 = (x1, x1 + ∆x1] and B2 = (x2, x2 + ∆x2] and ask that for some\\nfunction fX1,X2, the joint probability density function to satisfy\\n\\nP{x1 < X1 ≤ x1 + ∆x1, x2 < X2 ≤ x2 + ∆x2} ≈ fX1,X2(x1, x2)∆x1∆x2.\\n\\nSimilar to mass functions, the density functions for X1 and X2 can be obtained from the joint density function\\nby integrating over the values for the other random variable. Also, we sometimes say marginal probability density\\nfunction to distinguish it from the joint probability density function. Thus, for example, in analogy with (7.8).\\n\\nfX1 (x1) =(cid:90) ∞\\n\\n−∞\\n\\nfX1,X2 (x1, x2)dx2.\\n\\n(7.10)\\n\\nWe can obtain this identity starting with (7.8) and using Riemann sums in a manner similar to the argument that led to\\nthe formula for expectation for a continuous random variable.\\n\\nFor the conditional density, we start with\\n\\nP{x2 < X2 ≤ x2 + ∆x2|x1 < X1 ≤ x1 + ∆x1} =\\n\\n≈\\n\\nP{x1 < X1 ≤ x1 + ∆x1, x2 < X2 ≤ x2 + ∆x2}\\n\\nP{x1 < X1 ≤ x1 + ∆x1}\\n\\nfX1,X2(x1, x2)∆x1∆x2\\n\\nfX1,X2(x1, x2)\\n\\n=\\n\\n∆x2\\n\\nfX1(x1)∆x1\\n\\nfX1 (x1)\\n\\n122\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nNext, divide by ∆x2 and let ∆x2 → 0. In keeping with the analogies between discrete and continuous densities, we\\nhave the following deﬁnition.\\nDeﬁnition 7.41. The conditional density function\\n\\nfX2|X1 (x2|x1) =\\n\\nfX1,X2 (x1, x2)\\n\\nfX1(x1)\\n\\nprovided fX1(x1) > 0.\\nExercise 7.42. Show that, for each value of x1, fX2|X1(x2|x1) is a density function, that is, the values are non-negative\\nand the integral over all values for x2 equals 1.\\nExercise 7.43. Verify that\\n\\nfX1,X2(x1, x2)(cid:26) x1 + 3\\n\\n0\\n\\n2 x2\\n\\n2\\n\\n0 < x1 ≤ 1, 0 < x2 ≤ 1.\\notherwise\\n\\nis a joint density function. Find the marginal densities.\\n\\nIndependent Random Variables\\n\\n7.7.3\\nMany of our experimental protocols will be designed so that observations are independent. More precisely, we will\\nsay that two random variables X1 and X2 are independent if any two events associated to them are independent, i.e.,\\n\\nP{X1 ∈ B1, X2 ∈ B2} = P{X1 ∈ B1}P{X2 ∈ B2}.\\n\\nIn words, the probability that the two events {X1 ∈ B1} and {X2 ∈ B2} happen simultaneously is equal to the\\nproduct of the probabilities that each of them happen individually.\\n\\nFor independent discrete random variables, we have that\\n\\nfX1,X2 (x1, x2) = P{X1 = x1, X2 = x2} = P{X1 = x1}P{X2 = x2} = fX1(x1)fX2 (x2).\\nIn this case, we say that the joint probability mass function is the product of the marginal mass functions.\\n\\nFor continuous random variables,\\n\\nfX1,X2(x1, x2)∆x1∆x2 ≈ P{x1 < X1 ≤ x1 + ∆x1, x2 < X2 ≤ x2 + ∆x2}\\n\\n= P{x1 < X1 ≤ x1 + ∆x1}P{x2 < X2 ≤ x2 + ∆x2} ≈ fX1(x1)∆x1fX2(x2)∆x2\\n= fX1 (x1)fX2(x2)∆x1∆x2.\\n\\nThus, for independent continuous random variables, the joint probability density function\\n\\nfX1,X2(x1, x2) = fX1(x1)fX2 (x2)\\n\\nis the product of the marginal density functions.\\nExercise 7.44. Generalize the notion of independent mass and density functions to more than two random variables.\\nSoon, we will be looking at n independent observations x1, x2, . . . , xn arising from an unknown density or mass\\n\\nfunction f. Thus, the joint density is\\n\\nf (x1)f (x2)··· f (xn).\\n\\nGenerally speaking, the density function f will depend on the choice of a parameter value θ. (For example, the\\nunknown parameter in the density function for an exponential random variable that describes the waiting time for a\\nbus.) Given the data from the n observations, the likelihood function arises by considering this joint density not as\\na function of x1, . . . , xn, but rather as a function of the parameter θ. We shall learn how the study of the likelihood\\nplays a major role in parameter estimation and in the testing of hypotheses.\\n\\n123\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n7.8 Simulating Random Variables\\nOne goal for these notes is to provide the tools needed to design inferential procedures based on sound principles of\\nstatistical science. Thus, one of the very important uses of statistical software is the ability to generate pseudo-data\\nto simulate the actual data. This provides the opportunity to explore the properties of the data through simulation\\nand to test and reﬁne methods of analysis in advance of the need to use these methods on genuine data. For many of\\nthe frequently used families of random variables, R provides commands for their simulation. We shall examine these\\nfamilies and their properties in Topic 9, Examples of Mass Functions and Densities. For other circumstances, we will\\nneed to have methods for simulating sequence of independent random variables that possess a common distribution.\\nWe ﬁrst consider the case of discrete random variables.\\n\\n7.8.1 Discrete Random Variables and the sample Command\\nThe sample command is used to create simple and stratiﬁed random samples. Thus, if we enter a sequence x,\\nsample(x,40) chooses 40 entries from x in such a way that all choices of size 40 have the same probability.\\n\\nThis uses the default R command of sampling without replacement. We can use this command to simulate\\ndiscrete random variables. To do this, we need to give the state space in a vector x and a mass function f. The call for\\nreplace=TRUE indicates that we are sampling with replacement. Then to give a sample of n independent random\\nvariables having common mass function f, we use sample(x,n,replace=TRUE,prob=f).\\n\\nExample 7.45. Let X be described by the mass function\\n\\nx\\n\\nfX (x)\\n\\n1\\n0.1\\n\\n2\\n0.2\\n\\n3\\n0.3\\n\\n4\\n0.4\\n\\nThen to simulate 50 independent observations from this mass function:\\n\\n> x<-c(1,2,3,4); f<-c(0.1,0.2,0.3,0.4)\\n> sum(f)\\n[1] 1\\n> data<-sample(x,50,replace=TRUE,prob=f)\\n> data\\n\\n[1] 1 4 4 4 4 4 3 3 4 3 3 2 3 3 3 4 4 3 3 2 4 1 3 3 4 2 3 3 3 1 2 4 3 2 3 4 4 4 4 2 4 1\\n\\n[43] 2 3 4 4 1 4 3 4\\n\\nNotice that 1 is the least represented value and 4 is the most represented. If the command prob=f is omitted, then\\nsample will choose uniformly from the values in the vector x. Let’s check our simulation against the mass function\\nthat generated the data. First, recount the observations that take on each possible value for x. We can make a table.\\n\\n> table(data)\\ndata\\n\\n1\\n5\\n\\n2\\n3 4\\n7 18 20\\n\\nor use the counts to determine the simulated proportions.\\n\\n> counts<-numeric(4)\\n> for (i in 1:4){counts[i]<-sum(data==i)}\\n> simprob<-counts/(sum(counts))\\n> data.frame(x,f,simprob)\\n\\nx\\n\\nf simprob\\n\\n1 1 0.1 0.10\\n2 2 0.2 0.14\\n3 3 0.3 0.36\\n4 4 0.4 0.40\\n\\n124\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nThe expression data==i returns a sequence FALSE and TRUE. the sum command adds up the number of times\\nTRUE appears.\\n\\nExercise 7.46. Simulate the sums on each of 20 rolls of a pair of dice. Repeat this for 1000 rolls and compare the\\nsimulation with the appropriate mass function.\\n\\nExercise 7.47. Simulate the mixture in Exercise 7.36 and comment on how it matches the mixture mass function.\\n\\n7.8.2 Continuous Random Variables and the Probability Transform\\nIf X a continuous random variable with a density fX that is positive everywhere in its domain, then the distribution\\n−1\\nfunction FX (x) = P{X ≤ x} is strictly increasing. In this case FX has a inverse function F\\nX , known as the\\nquantile function.\\nExercise 7.48. FX (x) ≤ u if and only if x ≤ F\\n\\n−1\\nX (u).\\n\\nThe probability transform follows from an analysis of the random variable\\n\\nU = FX (X)\\n\\nNote that FX has range from 0 to 1. It cannot take values below 0 or above 1. Thus, U takes on values between 0 and\\n1 and, therefore,\\n\\nFor values of u between 0 and 1, note that\\n\\nFU (u) = 0 for u < 0\\n\\nand FU (u) = 1 for u ≥ 1.\\n\\n−1\\nX (u)} = FX (F\\nTaken together, we have the distribution function for the random variable U,\\n\\nP{FX (X) ≤ u} = P{X ≤ F\\n\\n−1\\nX (u)) = u.\\n\\nFU (u) =\\uf8f1\\uf8f2\\uf8f3\\n\\n0\\nu\\n1\\n\\nu < 0,\\n0 ≤ u < 1,\\n1 ≤ u.\\n\\nFigure 7.6: Illustrating the Probability Transform. First simulate uniform random variables u1, u2, . . . , un on the interval [0, 1]. About 10%\\nof the random numbers should be in the interval [0.3, 0.4]. This corresponds to the 10% of the simulations on the interval [0.28, 0.38] for a random\\nvariable with distribution function FX shown. Similarly, about 10% of the random numbers should be in the interval [0.7, 0.8] which corresponds\\nto the 10% of the simulations on the interval [0.96, 1.51] for a random variable with distribution function FX, These values on the x-axis can be\\nobtained from taking the inverse function of FX, i.e., xi = F\\n\\n−1\\nX (ui).\\n\\n125\\n\\n00.20.40.60.811.21.41.61.800.10.20.30.40.50.60.70.80.91x\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nIf we can simulate U, we can simulate a random variable with distribution FX via the quantile function\\n\\nX = F\\n\\n−1\\nX (U ).\\n\\n(7.11)\\n\\nTake a derivative of FU (u) to see that its density\\n\\nfU (u) =\\uf8f1\\uf8f2\\uf8f3\\n\\n0\\n1\\n0\\n\\nu < 0,\\n0 ≤ u < 1,\\n1 ≤ u.\\n\\nBecause the random variable U has a constant density over\\nthe interval of its possible values, it is called uniform on the\\ninterval [0, 1]. It is simulated in R using the runif command.\\nThe identity (7.11) is called the probability transform. This\\ntransform is illustrated in Figure 7.6. We can see how the prob-\\nability transform works in the following example.\\n\\nExample 7.49. For the dart board, for x between 0 and 1, the\\ndistribution function u = FX (x) = x2 and thus the quantile\\nfunction\\n\\nx = F\\n\\nX (u) = √u.\\n−1\\n\\nWe can simulate independent observations of the distance from\\nthe center X1, X2, . . . , Xn of the dart board by simulating in-\\ndependent uniform random variables U1, U2, . . . Un and tak-\\ning the quantile function\\n\\nXi =(cid:112)Ui.\\n\\nFigure 7.7: The distribution function (red) and the empirical cu-\\nmulative distribution function (black) based on 100 simulations of\\nthe dart board distribution. R commands given below.\\n\\n> u<-runif(100)\\n> xu<-sqrt(u)\\n> plot(sort(xu),1:length(xu)/length(xu),\\n+\\n> x<-seq(0,1,0.01)\\n> lines(x,xˆ2,col=\"red\") #add the distribution function to the graph\\n\\ntype=\"s\",xlim=c(0,1),ylim=c(0,1), xlab=\"x\",ylab=\"probability\")\\n\\nWe have used the lines command to ad the distribution function FX (x) = x2. Notice how it follows the empirical\\n\\ncumulative distribution function.\\nExercise 7.50. If U is uniform on [0, 1], then so is V = 1 − U.\\n\\nSometimes, it is easier to simulate X using F\\n\\n−1\\nX (V ).\\nExample 7.51. For an exponential random variable, set\\n\\nu = FX (x) = 1 − exp(−λx), and thus x = −\\n\\n1\\nλ\\n\\nln(1 − u)\\n\\nConsequently, we can simulate independent exponential random variables X1, X2, . . . , Xn by simulating independent\\nuniform random variables V1, V2, . . . Vn and taking the transform\\n\\nXi = −\\nR accomplishes this directly through the rexp command.\\n\\n1\\nλ\\n\\nln Vi.\\n\\n126\\n\\n0.00.20.40.60.81.00.00.20.40.60.81.0xprobability0.00.20.40.60.81.00.00.20.40.60.81.0\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n7.9 Answers to Selected Exercises\\n7.2. The sum, the maximum, the minimum, the difference, the value on the ﬁrst die, the product.\\n\\n7.3. The roll with the ﬁrst H, the number of T , the longest run of H, the number of T s after the ﬁrst H.\\n\\n7.4. (cid:98)10nx(cid:99)/10n\\n7.6. A common way to show that two events A1 and A2 are equal is to pick an element ω ∈ A1 and show that it is in\\nA2. This proves A1 ⊂ A2. Then pick an element ω ∈ A2 and show that it is in A1, proving that A2 ⊂ A1. Taken\\ntogether, we have that the events are equal, A1 = A2. Sometimes the logic needed in showing A1 ⊂ A2 consists not\\nsolely of implications, but rather of equivalent statements. (We can indicate this with the symbol ⇐⇒.) In this case\\nwe can combine the two parts of the argument. For this exercise, as the lines below show, this is a successful strategy.\\n\\nWe follow an arbitrary outcome ω ∈ Ω.\\n1. ω ∈ {X ∈ B}c ⇐⇒ ω /∈ {X ∈ B} ⇐⇒ X(ω) /∈ B ⇐⇒ X(ω) ∈ Bc ⇐⇒ ω ∈ {X ∈ Bc}. Thus,\\n{X ∈ B}c = {X ∈ Bc}.\\n2. ω ∈ (cid:83)i{X ∈ Bi} ⇐⇒ ω ∈ {X ∈ Bi} for some i ⇐⇒ X(ω) ∈ Bi for some i ⇐⇒ X(ω) ∈ (cid:83)i Bi ⇐⇒\\nω ∈ {X ∈(cid:83)i B}. Thus,(cid:83)i{X ∈ Bi} = {X ∈(cid:83)i B}. The identity with intersection is similar with for all\\n\\ninstead of for some.\\n\\n3. We must show that the union of the Ci is equal to the state space S and that each pair are mutually exclusive.\\n\\nFor this\\n\\n(a) Because Bi are a partition of Ω,(cid:83)i Bi = Ω, and\\n\\n(cid:91)i\\n\\nCi =(cid:91)i\\n\\n{X ∈ Bi} = {X ∈(cid:91)i\\n\\nBi} = {X ∈ Ω} = S,\\n\\nthe state space.\\n\\n(b) For i (cid:54)= j, Bi ∩ Bj = ∅, and\\n\\nCi ∩ Cj = {X ∈ Bi} ∩ {X ∈ Bj} = {X ∈ Bi ∩ Bj} = {X ∈ ∅} = ∅.\\n\\n7.7. Let’s check the three axioms. Each veriﬁcation is based on the corresponding axiom for the probability P .\\n\\n1. For any subset B, PX (B) = P{X ∈ B} ≥ 0.\\n2. For the sample space S, PX (S) = P{X ∈ S} = P (Ω) = 1.\\n3. For mutually exclusive subsets Bi, i = 1, 2,··· , we have by the exercise above the mutually exclusive events\\n\\n{X ∈ Bi}, i = 1, 2,··· . Thus,\\nBi(cid:33) = P(cid:40)X ∈\\n\\nPX(cid:32) ∞(cid:91)i=1\\n\\n∞(cid:91)i=1\\n\\nBi(cid:41) = P(cid:32) ∞(cid:91)i=1\\n\\n{X ∈ Bi}(cid:33) =\\n\\n∞(cid:88)i=1\\n\\nP{X ∈ Bi} =\\n\\n∞(cid:88)i=1\\n\\nPX (Bi).\\n\\n7.9. For three tosses of a biased coin, we have\\n\\nx\\n\\n0\\n\\nP{X = x}\\n\\n(1 − p)3\\n\\n1\\n\\n3p(1 − p)2\\n127\\n\\n2\\n\\n3p2(1 − p)\\n\\n3\\np3\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nThus, the cumulative distribution function,\\n\\nFX (x) =\\n\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n\\n0\\n(1 − p)3\\n(1 − p)3 + 3p(1 − p)2 = (1 − p)2(1 + 2p)\\n(1 − p)2(1 + 2p) + 3p2(1 − p) = 1 − p3\\n1\\n\\nfor x < 0,\\nfor 0 ≤ x < 1,\\nfor 1 ≤ x < 2,\\nfor 2 ≤ x < 3,\\nfor 3 ≤ x\\n\\n7.10. From the example in the section Basics of Probability, we know that\\n\\nP{X = x}\\nTo plot the distribution function, we use,\\n\\nx\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n0.41353\\n\\n0.43588\\n\\n0.13765\\n\\n0.01294\\n\\n> hearts<-c(0:3)\\n> f<-choose(13,hearts)*choose(39,3-hearts)/choose(52,3)\\n> (F<-cumsum(f))\\n[1] 0.4135294 0.8494118 0.9870588 1.0000000\\n> plot(hearts,F,ylim=c(0,1),type=\"s\")\\n\\nThus, the cumulative distribution function,\\n\\nFX (x) =\\n\\n0\\n0.41353\\n0.84941\\n0.98706\\n1\\n\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n\\nfor x < 0,\\nfor 0 ≤ x < 1,\\nfor 1 ≤ x < 2,\\nfor 2 ≤ x < 3,\\nfor 3 ≤ x\\n\\n7.11. The cumulative distribution function for Y ,\\n\\nFY (y) = P{Y ≤ y} = P{X 3 ≤ y}\\n\\n= P{X ≤ 3√y} = FX ( 3√y).\\n\\n7.12. To verify the three properties for the distri-\\nbution function:\\n\\n1. Let xn → −∞ be a decreasing sequence. Then x1 > x2 > ···\\n\\nThus,\\n\\n{X ≤ x1} ⊃ {X ≤ x2} ⊃ ···\\n\\nP{X ≤ x1} ≥ P{X ≤ x2} ≥ ···\\n\\nFor each outcome ω, eventually, for some n, X(ω) > xn, and ω /∈ {X ≤ xn} and consequently no outcome ω\\nis in all of the events {X ≤ xn} and\\n\\nNow, use the second continuity property of probabilities.\\n\\n∞(cid:92)n=1\\n\\n{X ≤ xn} = ∅.\\n\\n128\\n\\n0.00.51.01.52.02.53.00.00.20.40.60.81.0heartsF\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n2. Let xn → ∞ be an increasing sequence. Then x1 < x2 < ···\\n\\n{X ≤ x1} ⊂ {X ≤ x2} ⊂ ··· .\\n\\nThus,\\n\\nFor each outcome ω, eventually, for some n, X(ω) ≤ xn, and\\n\\nP{X ≤ x1} ≤ P{X ≤ x2} ≤ ··· .\\n\\n∞(cid:91)n=1\\n\\n{X ≤ xn} = Ω.\\n\\nNow, use the ﬁrst continuity property of probabilities.\\n\\n3. Let x1 < x2, then {X ≤ x1} ⊂ {X ≤ x2} and by the monotonicity rule for probabilities\\n\\nP{X ≤ x1} ≤ P{X ≤ x2},\\n\\nor written in terms of the distribution function, FX (x1) ≤ FX (x2)\\n\\n∞(cid:92)n=1\\n\\n∞(cid:91)n=1\\n\\n7.13. Let xn → x0 be a strictly decreasing sequence. Then x1 > x2 > ···\\n\\n{X ≤ x1} ⊃ {X ≤ x2} ⊃ ··· ,\\n\\n{X ≤ xn} = {X ≤ x0}.\\n\\n(Check this last equality.) Then P{X ≤ x1} ≥ P{X ≤ x2} ≥ ··· . Now, use the second continuity property of\\nprobabilities to obtain limn→∞ FX (xn) = limn→∞ P{X ≤ xn} = P{X ≤ x0} = FX (x0). Because this holds for\\nevery strictly decreasing sequencing sequence with limit x0, we have that\\n\\nlim\\n\\nx→x0+\\n\\nFX (x) = FX (x0).\\n\\n7.14. Correspondingly from the previous exercise, let xn → x0 be a strictly increasing sequence. Then x1 < x2 < ···\\n\\n{X ≤ x1} ⊂ {X ≤ x2} ⊂ ··· ,\\n\\n{X ≤ xn} = {X < x0}.\\n\\n(Again, check this last equality.) Then P{X ≤ x1} ≤ P{X ≤ x2} ≤ ··· . Now, use the second continuity property\\nof probabilities to obtain = FX (x0−) = limn→∞ FX (xn) = limn→∞ P{X ≤ xn} = P{X < x0}. Because this\\nholds for every strictly increasing sequencing sequence with limit x0, we have that\\n\\nFX (c0) = lim\\n\\nx→x0− FX (x) = P{X < x0}.\\n\\n7.17. Using the identity in (7.2), we have\\n\\nP(cid:26) 1\\n\\n3\\n\\n< X ≤\\n\\n2\\n\\n3(cid:27) = Fx(cid:18) 2\\n\\n3(cid:19) − Fx(cid:18) 1\\n\\n3(cid:19) =\\n\\n4\\n9 −\\n\\n1\\n9\\n\\n=\\n\\n3\\n9\\n\\n=\\n\\n1\\n3\\n\\n.\\n\\nCheck Exercise 7.22 to see that the answer does not depend on whether or not the endpoints of the interval are included.\\n\\nFor the median and the quartiles, set q = FX (xq) = x2\\n\\nq, q = 1/2, 1/4 and 3/4. Then\\n\\nSo the median x1/2 = 1/√2, the ﬁrst and third quartiles are x1/4 = 1/2 and x3/4 = √3/4, respectively.\\n\\nxq = √q.\\n\\n129\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n7.18. Using the relation Y = 1/X, we ﬁnd that the distribution function for Y , Clearly FY (y) = 0 for y ≤ 1. For\\ny > 1,\\n\\nFY (y) = P{Y ≤ y} = P{1/X ≤ y} = P{X ≥ 1/y} = 1 − P{X < 1/y} = 1 −\\n\\n1\\ny2 .\\n\\nThus uses the fact that P{X = 1/y} = 0.\\n7.19. We use the fact that the exponential function is increasing, and that limu→∞ exp(−u) = 0. Using the numbering\\nof the properties above\\n\\n1. Because FX (x) = 0 for all x < 0, limx→−∞ FX (x) = 0.\\n2. limx→∞ exp(−λx) = 0. Thus, limx→∞ FX (x) = limx→∞ 1 − exp(−λx) = 1.\\n3. For x < 0, FX is constant, FX (0) = 0. For x ≥ 0, note that exp(−λx) is decreasing. Thus, FX (x) =\\n\\n1 − exp(−λx) is increasing. Consequenlty, the distribution function FX is non-decreasing.\\n\\n7.20. The distribution function has the graph shown in Figure 7.8.\\n\\nFigure 7.8: Cumulative distribution function for an exponential random variable with λ = 1/10 and a jump at x = 20.\\n\\nThe formula\\n\\nFT (x) = P{X ≤ x} =\\uf8f1\\uf8f2\\uf8f3\\n\\n0\\n1 − exp(−x/10)\\n1\\n\\nif x < 0,\\nif 0 ≤ x < 20,\\nif 20 ≤ x.\\n\\n7.23. For r (cid:54)= 1, write the expressions for sn and rsn and subtract. Notice that most of the terms cancel.\\n\\nsn = c+ cr +cr2+ ··· + crn\\nrsn =\\n(1 − r)sn = c\\nNow divide by 1 − r to obtain the formula.\\n7.24. First, {X ≥ a + b} ⊂ {X ≥ b} (If X ≥ a + b, then automatically X ≥ b. Thus, {X ≥ b + a, X ≥ b} = {X ≥\\nb + a}. By the deﬁnition of conditional probability\\n\\ncr +cr2+ ··· + crn +crn+1\\n\\n−crn+1 = c(1 − rn+1)\\n\\nP{X ≥ b + a|X ≥ b} =\\n\\nP{X ≥ b + a, X ≥ b}\\n\\nP{X ≥ b}\\n\\n=\\n\\nP{X ≥ b + a}\\n\\nP{X ≥ b}\\n\\n=\\n\\n130\\n\\npb+a\\npb = pa = P{X ≥ a}.\\n\\n0204060800.00.20.40.60.81.00204060800.00.20.40.60.81.00204060800.00.20.40.60.81.0xF\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nConversely, taking a = 1, then by the memorylessness property, the conditional probabilities\\n\\ndo not depend on b. Call their common value p. Then\\n\\nP{X ≥ b + 1|X ≥ b} =\\n\\nP{X ≥ b + 1}\\nP{X ≥ b}\\n\\nP{X > b} = P{X ≥ b + 1} = pP{X ≥ b} = p2P{X ≥ b − 1} = ··· = pb+1P{X ≥ 0} = pb+1,\\n\\nThe cumulative distribution\\n\\nFX (b) = 1 − P{X > b} = 1 − pb+1,\\n\\nand X is a geometric random variable.\\n7.26. P{X ≤ 3} = FX (3) = .8024691, P{2 < X ≤ 5} = FX (5)−FX (2) = 0.9122085−0.7037037 = 0.2085048,\\nand P{X > 4} = 1 − FX (4) = 1 − 0.8683128 = 0.1316872.\\n7.28. Let fX be the density. Then\\n\\n0 ≤ P{X = x0} ≤ P{x0 − ∆x < X ≤ x + ∆x} =(cid:90) x0+∆x\\n\\nx0−∆x\\n\\nfX (x) dx.\\n\\nNow the integral goes to 0 as ∆x → 0. So, we must have P{X = x0} = 0.\\n7.29. The R code is below.\\n\\n> x<-seq(0,1,0.01)\\n> par(mfrow=c(2,1))\\n> plot(x,xˆ2,type=\"l\",xlim=c(0,1),ylim=c(0,1),\\n\\nylab=\"distribution function\")\\n\\n> par(new=TRUE)\\n> plot(c(0,1/3),c(1/3,1/3)ˆ2,type=\"l\",xlim=c(0,1),\\n\\nylim=c(0,1),xlab=\"\",ylab=\"\",col=\"red\")\\n\\n> par(new=TRUE)\\n> plot(c(0,2/3),c(2/3,2/3)ˆ2,type=\"l\",xlim=c(0,1),\\n\\nylim=c(0,1),xlab=\"\",ylab=\"\",col=\"red\")\\n\\n> plot(x,2*x,type=\"l\",xlim=c(0,1),ylim=c(0,2),\\n\\nylab=\"density function\")\\n\\n> xl<-seq(1/3,2/3,length=100)\\n> lines(xl,2*xl,type=\"h\",col=\"pink\")\\n\\nThe upper plot displays P{1/3 < X ≤ 2/3} = FX (2/3) − FX (1/3) = 4/9 − 1/9 = 1/3 by the difference\\n\\nbetween the two horizontal lines. The lower plot show the same probability from the integral\\n\\n(cid:90) 2/3\\n\\n1/3\\n\\nfX (x)dx =(cid:90) 2/3\\n\\n1/3\\n\\n2x dx\\n\\nas the shaded trapezoid under the density function fX (x).\\n7.32. Because the density is non-negative on the interval [0, 1], FX (x) = 0 if x < 0 and FX (x) = 1 if x ≥ 1. For x\\nbetween 0 and 1,\\n\\nThus,\\n\\n0\\n\\n1\\n2√t\\n\\n(cid:90) x\\nFX (x) =\\uf8f1\\uf8f2\\uf8f3\\n\\n= √x.\\n\\nx\\n\\n0\\n\\ndt = √t(cid:12)(cid:12)(cid:12)\\n\\n0\\n√x\\n1\\n\\nif x ≤ 0,\\nif 0 < x < 1,\\nif 1 ≤ x.\\n\\n131\\n\\n0.00.20.40.60.81.00.00.40.8xdistribution function0.00.20.40.60.81.00.00.40.80.00.20.40.60.81.00.00.40.80.00.20.40.60.81.00.01.02.0xdensity function\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n7.33. The random variable Y has distribution function\\n\\nFY (y) = P{Y ≤ y} = P{aX + b ≤ y} = P{aX ≤ y − b}.\\n\\nFor a > 0\\n\\nNow take a derivative and use the chain rule to ﬁnd the density\\n\\nfY (y) = F\\n\\nFY (y) = P(cid:26)X ≤\\n\\n(cid:48)\\n\\ny − b\\n\\na (cid:27) = FX(cid:18) y − b\\na (cid:19) .\\nY (y) = fX(cid:18) y − b\\na (cid:19) 1\\nfX(cid:18) y − b\\na (cid:19) .\\na (cid:27) = 1 − FX(cid:18) y − b\\na (cid:19) .\\nfX(cid:18) y − b\\nY (y) = −fX(cid:18) y − b\\na (cid:19) 1\\na (cid:19) .\\n\\ny − b\\n\\n1\\n|a|\\n\\n1\\n|a|\\n\\n=\\n\\n=\\n\\na\\n\\na\\n\\n(cid:48)\\n\\nFY (y) = P(cid:26)X ≥\\n\\nfY (y) = F\\n\\nFor a < 0\\n\\nNow the derivative\\n\\n7.34. First, notice that the sum of right continuous functions is right continuous. Then, check the properties in Exercise\\n7.12 using the basic properties of limits and of right continuity.\\n7.35. Because the fi are mass functions, fi(x) ≥ 0 for all x. Using the fact that the πi ≥ 0 for all i, we have that\\nπifi(x) ≥ 0 and thus their sum, fX (x) ≥ 0. Also, for each i,\\n\\nfi(x) = 1 and\\n\\nπi = 1.\\n\\n(cid:88)x\\nπifi(x)(cid:33) =\\nf (x) =(cid:88)x (cid:32) n(cid:88)i=1\\n\\nn(cid:88)i=1\\nπi(cid:32)(cid:88)x\\n\\nn(cid:88)i=1\\n\\nfi(x)(cid:33) =\\n\\nn(cid:88)i=1\\n\\nπi(1) = 1.\\n\\nTherefore,\\n\\n(cid:88)x\\n\\n7.36. We enter π and f1, f2, f3 into R and use matrix multiplication.\\n\\n> pi<-c(1/4,1/4,1/2)\\n> f<-matrix(c(0.2,0.3,0.1,0.4,0,0.5,0.5,0,0,0,0.1,0.1,0.2,0.2,0.4),ncol=3)\\n> f\\n\\n[,1] [,2] [,3]\\n0.2 0.5 0.1\\n0.3 0.5 0.1\\n0.1 0.0 0.2\\n0.4 0.0 0.2\\n0.0 0.0 0.4\\n\\n[1,]\\n[2,]\\n[3,]\\n[4,]\\n[5,]\\n> f%*%pi\\n\\n[,1]\\n[1,] 0.225\\n[2,] 0.250\\n[3,] 0.125\\n[4,] 0.200\\n[5,] 0.200\\n\\n132\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nSo the mixture distribution is\\n\\nx\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\nfX (x)\\n\\n0.225\\n\\n0.250\\n\\n0.125\\n\\n0.200\\n\\n0.200\\n\\n7.38. The marginal mass function for X1 are the column sums.\\n\\nx1\\n\\nfX1(x1)\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n0.27\\n\\n0.18\\n\\n0.24\\n\\n0.16\\n\\n0.15\\n\\nThe marginal mass function for X2 are the row sums.\\n\\nx2\\n\\nfX2(x2)\\n\\n-1\\n0.19\\n\\n0\\n\\n1\\n\\n2\\n\\n0.19\\n\\n0.35\\n\\n0.27\\n\\nNotice that both fX1 (x1) and fX2(x2) satisfy the properties of a mass function.\\n7.39. Because fX2|X1 (x2|x1) is a conditional probability, it is non-negative. For fX1(x1) > 0,\\n\\n(cid:88)x2\\n\\nfX2|X1(x2|x1) =(cid:88)x2\\n\\nfX1,X2(x1, x2)\\n\\nfX1 (x1)\\n\\n=\\n\\n1\\n\\nfX1(x1)(cid:88)x2\\n\\nfX1,X2(x1, x2) =\\n\\n1\\n\\nfX1 (x1)\\n\\nfX1 (x1) = 1.\\n\\n7.40. We start with a table of the joint mass function fX1,X2 (x1, x2) and the marginal mass function fX1(x1).\\n\\nx2\\\\x1\\n-1\\n0\\n1\\n2\\n\\nfX1 (x1)\\n\\n1\\n\\n0.09\\n0.07\\n0.10\\n0.01\\n0.27\\n\\nfX1,X2 (x1, x2)\\n2\\n4\\n\\n3\\n\\n0.04\\n\\n0\\n\\n0.06\\n0.08\\n0.18\\n\\n0.03\\n0.07\\n0.05\\n0.09\\n0.24\\n\\n0.01\\n0.02\\n0.08\\n0.05\\n0.16\\n\\n5\\n\\n0.02\\n0.03\\n0.06\\n0.04\\n0.15\\n\\nThe marginal mass function, fX2|X1 (x12|x1) is simply the table entry fX1,X2(x1, x2) divided by the corresponding\\nrow sum fX1(x1).\\n\\nx2\\\\x1\\n-1\\n0\\n1\\n2\\n\\n1\\n1/3\\n7/27\\n10/27\\n1/27\\n\\nfX2|X1(x2|x1)\\n2\\n4\\n1/16\\n2/9\\n1/8\\n0\\n1/3\\n1/2\\n5/16\\n4/9\\n\\n3\\n1/8\\n7/24\\n5/24\\n3/8\\n\\n5\\n\\n2/15\\n1/5\\n2/5\\n4/15\\n\\nNotice each row sum is 1, as expected.\\n7.42. Let A = {x1 : fX1(x1) > 0}. On this set the conditional density function\\n\\nfX2|X1 (x2|x1) =\\n\\nfX1,X2(x1, x2)\\n\\nfX1(x1)\\n\\nis ratio of density functions and this is non-negative. The integral\\n\\n(cid:90)A\\n\\nfX2|X1(x2|x1)dx2 =(cid:90)A\\n\\nfX1,X2 (x1, x2)\\n\\nfX1(x1)\\n\\ndx2 = fX1(x1)(cid:90)A\\n\\nfX1,X2(x1, x2)dx2 =\\n\\nfX1 (x1)\\nfX1 (x1)\\n\\n= 1\\n\\nusing (7.10)\\n\\n133\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\nFigure 7.9: Sum on two fair dice. The empirical cumulative distribution function from the simulation (in black) and the cumulative distribution\\nfunction (in red) are shown for Exercise 7.46.\\n\\n0\\n\\nx2\\n\\n(cid:90) 1\\n0 (cid:90) 1\\n\\n7.43. fX1,X2(x1, x2) is nonnegative. Its integral over [0, 1] × [0, 1] is\\n0 (cid:18)x1 +\\nfX1,X2(x1, x2)dx2dx1 =(cid:90) 1\\n0 (cid:90) 1\\n3\\n2\\n0 (cid:18)x1x2 +\\n=(cid:90) 1\\nx1(cid:12)(cid:12)(cid:12)\\n2(cid:19) dx2 =(cid:18)x1x2 +\\n2(cid:19) dx1 =(cid:18) 1\\n\\nfX1,X2 (x1, x2)dx2 =(cid:90) 1\\nfX1,X2 (x1, x2)dx1 =(cid:90) 1\\n\\nfX1(x1) =(cid:90) 1\\nfX2(x2) =(cid:90) 1\\n\\n2(cid:19) dx2dx1\\n2(cid:19)(cid:12)(cid:12)(cid:12)\\ndx1 =(cid:90) 1\\n\\n0 (cid:18)x1 +\\n0 (cid:18)x1 +\\n\\nas was needed. For the marginal densities,\\n\\n0\\n1\\n2\\n\\nx2\\n1 +\\n\\nx2\\n1 +\\n\\n= 1,\\n\\n3\\n2\\n\\n1\\n2\\n\\n1\\n2\\n\\n3\\n2\\n\\n1\\n2\\n\\n1\\n2\\n\\n3\\n2\\n\\nx2\\n\\nx3\\n\\nx2\\n\\n=\\n\\n=\\n\\n+\\n\\n2\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\nIt is easy to check that fX1(x1) and fX2 (x2) are probability density functions.\\n7.44. The joint density (mass function) for X1, X2, . . . , Xn\\n\\n0 (cid:18)x1 +\\n\\n1\\n\\n2(cid:19) dx1\\n\\n= x1 +\\n\\n1\\n2\\n\\n1\\n\\n0\\n\\n=\\n\\n1\\n2\\n\\n+\\n\\n3\\n2\\n\\nx2\\n2.\\n\\n1\\n2\\n\\nx3\\n\\n0\\n\\n1\\n\\n2(cid:19)(cid:12)(cid:12)(cid:12)\\n2x1(cid:19)(cid:12)(cid:12)(cid:12)\\n\\nx2\\n\\nfX1,X2,...,Xn (x1, x2, . . . , xn) = fX1 (x1)fX2(x2)··· fXn (xn)\\n\\nis the product of the marginal densities (mass functions).\\n7.46. Here is the R code.\\n\\n> x<-2:12\\n> f<-c(1,2,3,4,5,6,5,4,3,2,1)/36\\n> sum(f)\\n[1] 1\\n> (twodice<-sample(x,20,replace=TRUE,prob=f))\\n\\n[1]\\n\\n9 7 3 9 3 6 9 5 5\\n\\n5\\n\\n5 10 10 12\\n\\n9\\n\\n8\\n\\n6\\n\\n8 11\\n\\n8\\n\\n134\\n\\n246810120.00.20.40.60.81.0246810120.00.20.40.60.81.0xF\\x0cIntroduction to the Science of Statistics\\n\\nRandom Variables and Distribution Functions\\n\\n> twodice<-sample(x,1000,replace=TRUE,prob=f)\\n> data.frame(table(twodice)/1000,f)\\n\\ntwodice Freq\\n\\nf\\n2 0.029 0.02777778\\n3 0.061 0.05555556\\n4 0.098 0.08333333\\n5 0.129 0.11111111\\n6 0.136 0.13888889\\n7 0.151 0.16666667\\n8 0.130 0.13888889\\n9 0.107 0.11111111\\n10 0.092 0.08333333\\n11 0.039 0.05555556\\n12 0.028 0.02777778\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n\\nWe also have a plot to compare the empirical cumulative distribution function from the simulation with the cumulative\\ndistribution function.\\n\\n> plot(sort(twodice),1:length(twodice)/length(twodice),type=\"s\",xlim=c(2,12),\\nylim=c(0,1),xlab=\"\",ylab=\"\")\\n> par(new=TRUE)\\n> F<-cumsum(f)\\n> plot(x,F,type=\"s\",xlim=c(2,12),ylim=c(0,1),col=\"red\")\\n\\n7.47. Using the information from Exercise 7.36, we have\\n\\n> data<-rep(0,10000)\\n> for (i in 1:10000){toss<-sample(1:3,1,prob=pi);\\n\\ndata[i]<-sample(1:5,1,prob=f[,toss])}\\n\\n> table(data)\\ndata\\n1\\n\\n2\\n\\n5\\n2260 2522 1249 2000 1969\\n\\n3\\n\\n4\\n\\nAs can be seen from the table below, all of the simulated probabilities are within 0.3% of the distributional values.\\n\\nx\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\nfX (x)\\n\\nsimulated\\n\\n0.225\\n0.2260\\n\\n0.250\\n0.2522\\n\\n0.125\\n0.1249\\n\\n0.200\\n0.2000\\n\\n0.200\\n0.1969\\n\\n7.48. FX is increasing and continuous, so the set {x; FX (x) ≤ u} is the interval (−∞, F\\nthis invterval precisely when x ≤ F\\n7.50 . Let’s ﬁnd FV . If v < 0, then\\n\\n−1\\nX (u).\\n\\n−1\\nX (u)]. In addition, x is in\\n\\nP{V ≤ v} = P{1 − U ≤ v} = P{1 − v ≤ U} = P{U ≥ 1 − v = 0\\n\\nbecause U is never greater than 1 − v > 1. Thus, FV (v) = 0 Similarly, if v ≥ 1,\\n\\nP{V ≤ v} = P{1 − U ≤ v} = P{1 − v ≤ U} = 1\\nbecause U is always greater than 1 − v < 0. Thus, FV (v) = 1. Finally, for 0 ≤ v < 1,\\n\\nFV (v) = P{V ≤ v} = P{1 − U ≤ v} = P{1 − v ≤ U} = 1 − P{U < 1 − v} = 1 − (1 − v) = v.\\n\\nThis matches the distribution function of a uniform random variable on [0, 1].\\n\\n135\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\n136\\n\\n\\x0cTopic 8\\n\\nThe Expected Value\\n\\nMultiply each gain and loss by the probability of the event on which it depends; compare the total of\\nthe result of the gains with that of the losses; the balance is the average required, and is known by the\\nname of the mathematical expectation.- August de Morgan, An Essay on Probabilities, 1838\\n\\nAmong the simplest summaries of quantitative data is the sample mean. Given a random variable, the correspond-\\ning concept is given a variety of names, including the distributional mean, the expectation or the expected value.\\nWe begin with the case of discrete random variables where this analogy is more apparent. The formula for contin-\\nuous random variables is obtained by approximating with a discrete random variable and noticing that the formula\\nfor the expected value is a Riemann sum. Thus, expected values for continuous random variables are determined by\\ncomputing an integral.\\n\\n8.1 Deﬁnition and Properties\\n\\nRecall for a data set taking numerical values x1, x2, . . . , xn, one of the methods for computing the sample mean of a\\nreal-valued function h of the data is accomplished by evaluating the sum,\\n\\nh(x) =(cid:88)x\\n\\nh(x)p(x),\\n\\nwhere p(x) is the proportion of observations taking the value x.\\n\\nFor a ﬁnite sample space Ω = {ω1, ω2, . . . , ωN} and a probability P on Ω, we can deﬁne the expectation or the\\n\\nexpected value of a random variable X by an analogous average,\\n\\nMore generally for a real-valued function g of the random vector X = (X1, X2, . . . , Xn), we have the formula\\n\\nEX =\\n\\nN(cid:88)j=1\\n\\nX(ωj)P{ωj}.\\n\\nEg(X) =\\n\\nN(cid:88)j=1\\n\\ng(X(ωj))P{ωj}.\\n\\n(8.1)\\n\\n(8.2)\\n\\nNotice that even though we have this analogy, the two formulas come from very different starting points. The value\\nof h(x) is derived from data whereas no data are involved in computing Eg(X). The starting point for the expected\\nvalue is a probability model.\\n\\n137\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nExample 8.1. Roll one die. Then Ω = {1, 2, 3, 4, 5, 6}. Let X be the value on the die. So, X(ω) = ω. If the die is\\nfair, then the probability model has P{ω} = 1/6 for each outcome ω. Using the formula (8.1), the expected value\\n\\nEX = 1 · P{1} + 2 · P{2} + 3 · P{3} + 4 · P{4} + 5 · P{5} + 6 · P{6}\\n\\n= 1 ·\\n\\n+ 2 ·\\n\\n+ 3 ·\\n\\n+ 4 ·\\n\\n+ 5 ·\\n\\n+ 6 ·\\n\\n1\\n6\\n\\n1\\n6\\n\\n1\\n6\\n\\n1\\n6\\n\\n1\\n6\\n\\n1\\n6\\n\\n=\\n\\n=\\n\\n21\\n6\\n\\n7\\n2\\n\\n.\\n\\nAn example of an unfair dice would be the probability with P{1} = P{2} = P{3} = 1/4 and P{4} = P{5} =\\n\\nP{6} = 1/12. In this case, the expected value\\n1\\n4\\n\\nEX = 1 ·\\n\\n+ 2 ·\\n\\n1\\n4\\n\\n+ 3 ·\\n\\n1\\n4\\n\\n+ 4 ·\\n\\n1\\n12\\n\\n+ 5 ·\\n\\n1\\n12\\n\\n+ 6 ·\\n\\n1\\n12\\n\\n=\\n\\n11\\n4\\n\\n.\\n\\nExercise 8.2. Use the formula (8.2) with g(x) = x2 to ﬁnd EX 2 for these two examples.\\n\\nTwo properties of expectation are immediate from the formula for EX in (8.1):\\n\\n1. If X(ω) ≥ 0 for every outcome ω ∈ Ω, then every term in the sum in (8.1) is nonnegative and consequently\\n\\ntheir sum EX ≥ 0.\\n\\n2. Let X1 and X2 be two random variables and c1, c2 be two real numbers, then by using g(x1, x2) = c1x1 + c2x2\\n\\nand the distributive property to the sum in (8.2), we ﬁnd out that\\n\\nE[c1X1 + c2X2] = c1EX1 + c2EX2.\\n\\nThe ﬁrst of these properties states that nonnegative random variables have nonnegative expected value. The second\\nstates that expectation is a linear operation. Taking these two properties together, we say that the operation of taking\\nan expectation\\n\\nX (cid:55)→ EX\\n\\nis a positive linear functional. We have studied extensively another example of a positive linear functional, namely,\\nthe deﬁnite integral\\n\\ng (cid:55)→(cid:90) b\\n\\na\\n\\ng(x) dx\\n\\nthat takes a continuous positive function and gives the area between the graph of g and the x-axis between the vertical\\nlines x = a and x = b. For this example, these two properties become:\\n\\n1. If g(x) ≥ 0 for every x ∈ [a, b], then(cid:82) b\\n\\na g(x) dx ≥ 0.\\n\\n2. Let g1 and g2 be two continuous functions and c1, c2 be two real numbers, then\\n\\n(cid:90) b\\n\\na\\n\\n(c1g1(x) + c2g2(x)) dx = c1(cid:90) b\\n\\na\\n\\ng1(x) dx + c2(cid:90) b\\n\\na\\n\\ng2(x) dx.\\n\\nThis analogy will be useful to keep in mind when considering the properties of expectation.\\n\\nExample 8.3. If X1 and X2 are the values on two rolls of a fair die, then the expected value of the sum\\n\\nE[X1 + X2] = EX1 + EX2 =\\n\\n7\\n2\\n\\n+\\n\\n7\\n2\\n\\n= 7.\\n\\n138\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nA\\nω\\n\\nHHH\\nHHT\\nHT H\\nT HH\\nHT T\\nT T H\\nT HT\\nT T T\\n\\nB\\n\\nX(ω)\\n\\n3\\n2\\n2\\n2\\n1\\n1\\n1\\n0\\n\\nF\\n\\nE\\n\\nP{X = x}\\n\\nD\\nP{ω}\\n\\nX(ω)P{ω}\\n\\nC\\nx\\n3 P{HHH} P{X = 3} X(HHH)P{HHH}\\nX(HHT )P{HHT}\\n2\\nP{X = 2} X(HT H)P{HT H}\\nX(T HH)P{T HH}\\nX(HHT )P{HHT}\\nP{X = 1} X(HT H)P{HT H}\\nX(T HH)P{T HH}\\nP{X = 0}\\nX(T T T )P{T T T}\\nTable I: Developing the formula for EX for the case of the coin tosses.\\n\\nP{HHT}\\nP{HT H}\\nP{T HH}\\nP{HT T}\\nP{T HT}\\nP{T T H}\\nP{T T T}\\n\\n1\\n\\n0\\n\\nG\\n\\nxP{X = x}\\n3P{X = 3}\\n2P{X = 2}\\n\\n1P{X = 1}\\n0P{X = 0}\\n\\n8.2 Discrete Random Variables\\nBecause sample spaces can be extraordinarily large even in routine situations, we rarely use the probability space Ω\\nas the basis to compute the expected value. We illustrate this with the example of tossing a coin three times. Let X\\ndenote the number of heads. To compute the expected value EX, we can proceed as described in (8.1). For the table\\nabove, we have grouped the outcomes ω that have a common value x = 3, 2, 1 or 0 for X(ω).\\n\\nFrom the deﬁnition of expectation in (8.1), EX, the expected value of X, is the sum of the values in column F. We\\n\\nwant to now show that EX is also the sum of the values in column G.\\n\\nNote, for example, that, three outcomes HHT, HT H and T HH each have two heads and thus give a value of 2\\n\\nfor X. Because these outcomes are disjoint, we can add probabilities\\n\\nP{HHT} + P{HT H} + P{T HH} = P{HHT, HT H, T HH}\\n\\n(8.3)\\n\\nBut, the event\\n\\n{HHT, HT H, T HH} can also be written as the event {X = 2}.\\n\\n(8.4)\\nThis is shown for each value of x in column C, P{X = x}, the probabilities in column E are obtained as a sum of\\nprobabilities in column D.\\nThus, by combining outcomes that result in the same value for the random variable, the sums in the boxes in\\ncolumn F are equal to the value in the corresponding box in column G. and thus their total sums are the same. In other\\nwords,\\n\\nEX = 0 · P{X = 0} + 1 · P{X = 1} + 2 · P{X = 2} + 3 · P{X = 3}.\\n\\nAs in the discussion above, we can, in general, ﬁnd for any function g the expectation Eg(X). First, to build a\\ntable, denote the outcomes in the probability space Ω as ω1, . . . , ωk, ωk+1, . . . , ωN and the state space for the random\\nvariable X as x1, . . . , xi, . . . , xn.\\n\\nNote that we have partitioned the sample space Ω into the outcomes ω that result in the same value x for the random\\nvariable X(ω). This is shown by the horizontal lines in the table above showing that X(ωk) = X(ωk+1) = ··· = xi.\\nThe equality of sum of the probabilities in a box in columns D and the probability in column E can be written, in\\nanalogy with (8.3) and (8.4),\\n\\n(cid:88){ω;X(ω)=xi}\\n\\nP{ω} = P{X = xi}.\\n\\nFor these particular outcomes, g(X(ω)) = g(xi) and the sum of the values in a boxes in column F,\\n\\n(cid:88)ω;X(ω)=xi\\n\\ng(X(ω))P{ω} = (cid:88)ω;X(ω)=xi\\n\\ng(xi)P{ω} = g(xi) (cid:88)ω;X(ω)=xi\\n\\nP{ω} = g(xi)P{X = xi},\\n\\n(8.5)\\n\\n139\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nA\\nω\\n\\nB\\n\\nX(ω)\\n\\n...\\n\\n...\\nωk\\nX(ωk)\\nωk+1 X(ωk+1)\\n...\\n...\\n\\n...\\n...\\n\\nC\\nx\\n\\n...\\n\\nD\\nP{ω}\\n\\n...\\n\\nE\\n\\nP{X = x}\\n\\n...\\n\\nF\\n\\ng(X(ω))P{ω}\\n\\n...\\n\\nP{ωk}\\n\\nxi P{ωk+1} P{X = xi}\\n\\n...\\n...\\n\\n...\\n\\n...\\n\\ng(X(ωk))P{ωk}\\n\\ng(X(ωk+1))P{ωk+1}\\n\\n...\\n...\\n\\nG\\n\\ng(x)P{X = x}\\n\\n...\\n\\ng(xi)P{X = xi}\\n\\n...\\n\\nTable II: Establishing the identity (8.6) from (8.2). Arrange the rows of the table so that common values of X(ωk), X(ωk+1), . . . in the box in\\ncolumn B have the value xi in column C. Thus, the probabilities in a box in column D sum to give the probability in the corresponding box in\\ncolumn E. Because the values for g(X(ωk)), g(X(ωk+1)), . . . equal g(xi), the sum in a box in column F sums to the value in the corresponding\\nbox in column G. Thus, the sums in columns F and G are equal. The sum in column F is the deﬁnition in (8.2). The sum in column G is the identity\\n(8.6).\\n\\nthe value in the corresponding box in column G. Now, sum over all possible value for X for each side of equation\\n(8.5).\\n\\nEg(X) =(cid:88)ω\\n\\ng(X(ω))P{ω} =\\n\\nn(cid:88)i=1\\n\\ng(xi)P{X = xi} =\\n\\ng(xi)fX (xi)\\n\\nn(cid:88)i=1\\n\\nThe identity\\n\\nwhere fX (xi) = P{X = xi} is the probability mass function for X.\\ng(xi)fX (xi) =(cid:88)x\\n\\nn(cid:88)i=1\\n\\nEg(X) =\\n\\ng(x)fX (x)\\n\\n(8.6)\\n\\nis the most frequently used method for computing the expectation of discrete random variables. We will soon see how\\nthis identity can be used to ﬁnd the expectation in the case of continuous random variables\\n\\nExample 8.4. Flip a biased coin twice and let X be the number of heads. Then, to compute the expected value of X\\nand X 2 we construct a table to prepare to use (8.6).\\n\\nxfX (x) x2fX (x)\\n\\nfX (x)\\n(1 − p)2\\n\\nx\\n0\\n1 2p(1 − p) 2p(1 − p) 2p(1 − p)\\n2\\n2p + 2p2\\nsum\\n\\n2p2\\n2p\\n\\np2\\n1\\n\\n4p2\\n\\n0\\n\\n0\\n\\nThus, EX = 2p and EX 2 = 2p + 2p2.\\n\\nExercise 8.5. Draw 5 cards from a standard deck. Let X be the number of hearts. Use R to ﬁnd the mass function for\\nX and use this to ﬁnd EX and EX 2.\\n\\nA similar formula to (8.6) holds if we have a vector of random variables X = (X1, X2, . . . , Xn), fX, the joint\\nprobability mass function and g a real-valued function of x = (x1, x2, . . . , xn). In the two dimensional case, this takes\\nthe form\\n\\ng(x1, x2)fX1,X2 (x1, x2).\\n\\n(8.7)\\n\\nEg(X1, X2) =(cid:88)x1 (cid:88)x2\\n\\nWe will return to (8.7) in computing the covariance of two random variables.\\n\\n140\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\n8.3 Bernoulli Trials\\nBernoulli trials are the simplest and among the most common models for an experimental procedure. Each trial has\\ntwo possible outcomes, variously called,\\nheads-tails, yes-no, up-down, left-right, win-lose, female-male, green-blue, dominant-recessive, or success-failure\\ndepending on the circumstances. We will use the principles of counting and the properties of expectation to analyze\\nBernoulli trials. From the point of view of statistics, the data have an unknown success parameter p. Thus, the goal of\\nstatistical inference is to make as precise a statement as possible for the value of p behind the production of the data.\\nConsequently, any experimenter that uses Bernoulli trials as a model ought to mirror its properties closely.\\nExample 8.6 (Bernoulli trials). Random variables X1, X2, . . . , Xn are called a sequence of Bernoulli trials provided\\nthat:\\n\\n1. Each Xi takes on two values, namely, 0 and 1. We call the value 1 a success and the value 0 a failure.\\n2. Each trial has the same probability for success, i.e., P{Xi = 1} = p for each i.\\n3. The outcomes on each of the trials is independent.\\n\\nFor each trial i, the expected value\\n\\nEXi = 0 · P{Xi = 0} + 1 · P{Xi = 1} = 0 · (1 − p) + 1 · p = p\\n\\nis the same as the success probability. Let Sn = X1 + X2 + ··· + Xn be the total number of successes in n Bernoulli\\ntrials. Using the linearity of expectation, we see that\\n\\nthe expected number of successes in n Bernoulli trials is np.\\n\\nESn = E[X1 + X2 ··· + Xn] = p + p + ··· + p = np,\\n\\nIn addition, we can use our ability to count to determine the probability mass function for Sn. Beginning with a\\n\\nconcrete example, let n = 8, and the outcome\\n\\nUsing the independence of the trials, we can compute the probability of this outcome:\\n\\nsuccess, fail, fail, success, fail, fail, success, fail.\\n\\np × (1 − p) × (1 − p) × p × (1 − p) × (1 − p) × p × (1 − p) = p3(1 − p)5.\\n\\n3(cid:1) particular sequences of 8 Bernoulli trials having 3 successes also has probability\\nMoreover, any of the possible(cid:0)8\\np3(1 − p)5. Each of the outcomes are mutually exclusive, and, taken together, their union is the event {S8 = 3}.\\nConsequently, by the axioms of probability, we ﬁnd that\\n\\nP{S8 = 3} =(cid:18)8\\n\\n3(cid:19)p3(1 − p)5.\\n\\nReturning to the general case, we replace 8 by n and 3 by x to see that any particular sequence of n Bernoulli\\n\\ntrials having x successes has probability\\n\\nIn addition, we know that we have\\n\\nmutually exclusive sequences of n Bernoulli trials that have x successes. Thus, we have the mass function\\n\\nfSn (x) = P{Sn = x} =(cid:18)n\\n\\nx = 0, 1, . . . , n.\\n\\npx(1 − p)n−x.\\n\\n(cid:18)n\\nx(cid:19)\\nx(cid:19)px(1 − p)n−x,\\n\\n141\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nThe fact that the sum\\n\\nfSn (x) =\\n\\nn(cid:88)x=0\\n\\nn(cid:88)x=0(cid:18)n\\n\\nx(cid:19)px(1 − p)n−x = (p + (1 − p))n = 1n = 1\\n\\nfollows from the binomial theorem. Consequently, Sn is called a binomial random variable.\\n\\nIn the exercise above where X is the number of hearts in 5 cards, let Xi = 1 if the i-th card is a heart and 0 if it\\nis not a heart. Then, the Xi are not Bernoulli trials because the chance of obtaining a heart on one card depends on\\nwhether or not a heart was obtained on other cards. Still,\\n\\nis the number of hearts and\\n\\nX = X1 + X2 + X3 + X4 + X5\\n\\nEX = EX1 + EX2 + EX3 + EX4 + EX5 = 1/4 + 1/4 + 1/4 + 1/4 + 1/4 = 5/4.\\n\\n8.4 Continuous Random Variables\\nFor X a continuous random variable with density\\nfX, consider the discrete random variable ˜X ob-\\ntained from X by rounding down. Say, for exam-\\nple, we give lengths by rounding down to the near-\\nest millimeter. Thus, ˜X = 2.134 meters for any\\nlengths X satisfying 2.134 meters < X ≤ 2.135\\nmeters.\\nThe random variable ˜X is discrete. To be pre-\\ncise about the rounding down procedure, let ∆x be\\nthe spacing between values for ˜X. Then, ˜x, an inte-\\nger multiple of ∆x, represents a possible value for\\n˜X, then this rounding becomes\\n\\n˜X = ˜x if and only if\\n\\n˜x < X ≤ ˜x + ∆x.\\n\\nWith this, we can give the mass function\\n\\nFigure 8.1: The discrete random variable ˜X is obtained by rounding down the\\ncontinuous random variable X to the nearest multiple of ∆x. The mass function\\nf ˜X (˜x) is the integral of the density function from ˜x to ˜x + ∆x indicated at the\\narea under the density function between two consecutive vertical lines.\\n\\nNow, by the property of the density function,\\n\\nf ˜X (˜x) = P{ ˜X = ˜x} = P{˜x < X ≤ ˜x + ∆x}.\\n\\nP{˜x ≤ X < ˜x + ∆x} ≈ fX (x)∆x.\\n\\n(8.8)\\n\\nIn this case, we need to be aware of a possible source of confusion due to the similarity in the notation that we have for\\nboth the mass function f ˜X for the discrete random variable ˜X and a density function fX for the continuous random\\nvariable X.\\n\\nFor this discrete random variable ˜X, we can use identity (8.6) and the approximation in (8.8) to approximate the\\n\\nexpected value.\\n\\nEg( ˜X) =(cid:88)˜x\\n≈(cid:88)˜x\\n\\ng(˜x)f ˜X (˜x) =(cid:88)˜x\\n\\ng(˜x)fx(˜x)∆x.\\n\\n142\\n\\ng(˜x)P{˜x ≤ X < ˜x + ∆x}\\n\\n!0.5!0.2500.250.50.7511.251.51.75200.511.522.53density fXround down! x\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nThis last sum is a Riemann sum and so taking limits as ∆x → 0, we have that the distribution of ˜X converges to\\n\\nthat for X and the Riemann sum converges to the deﬁnite integral. Thus,\\n\\nEg(X) =(cid:90) ∞\\n\\n−∞\\n\\ng(x)fX (x) dx.\\n\\n(8.9)\\n\\nprovided this possibly improper Riemann integral converges.\\n\\nAs in the case of discrete random variables, a similar formula to (8.9) holds if we have a vector of random variables\\nX = (X1, X2, . . . , Xn), fX, the joint probability density function and g a real-valued function of the vector x =\\n(x1, x2, . . . , xn). The expectation in this case is an n-dimensional Riemann integral. For example, if X1 and X2 has\\njoint density fX1,X2(x1, x2), then\\n\\nEg(X1, X2) =(cid:90) ∞\\n\\n−∞(cid:90) ∞\\n\\n−∞\\n\\ng(x1, x2)fX1,X2(x1, x2) dx2dx1,\\n\\nagain, provided that the improper Riemann integral converges.\\nExample 8.7. For the dart example, the density fX (x) = 2x on the interval [0, 1] and 0 otherwise. Thus,\\n\\n1\\n\\n0\\n\\n2x2 dx =\\n\\nOn the other hand, if we award the dart thrower for an amount equal to the inverse of the square of the distance from\\nthe center,\\n\\nEX =(cid:90) 1\\nx · 2x dx =(cid:90) 1\\nE(cid:20) 1\\nX 2(cid:21) =(cid:90) 1\\nThe antiderivative of the integrand is 2 ln x and does not converge as x → 0. In this case, because the integrand is\\npositive, we may say that E[1/X 2] = ∞.\\nExercise 8.8. If X is a nonnegative random variable, then FX (0) = 0.\\n\\nx2 · 2x dx =(cid:90) 1\\n\\nx3(cid:12)(cid:12)(cid:12)\\n\\ndx.\\n\\n2\\nx\\n\\n2\\n3\\n\\n2\\n3\\n\\n=\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n.\\n\\nIf we were to compute the mean of T , an exponential random variable,\\n\\nET =(cid:90) ∞\\n\\n0\\n\\ntfT (t) dt =(cid:90) ∞\\n\\n0\\n\\n−λt dt,\\n\\ntλe\\n\\nthen our ﬁrst step is to integrate by parts. This situation occurs with enough regularity that we will beneﬁt in making\\nthe effort to see how integration by parts gives an alternative formula for computing expectation. In the end, we will\\nsee an analogy between the mean with the survival function P{X > x} = 1 − FX (x) = ¯FX (x), and the sample\\nmean with the empirical survival function.\\n\\nLet X be a positive random variable, then the expectation is the improper integral\\n\\nEX =(cid:90) ∞\\n\\n0\\n\\nxfX (x) dx\\n\\n(The unusual choice for v is made to simplify some computations and to anticipate the appearance of the survival\\nfunction.)\\n\\nFirst integrate from 0 to b and take the limit as b → ∞. Then, because FX (0) = 0, ¯FX (0) = 1 and\\n\\nu(x) = x\\nu(cid:48)(x) = 1\\n\\nX (x).\\n\\nv(x) = −(1 − FX (x)) = − ¯FX (x)\\nv(cid:48)(x) = fX (x) = − ¯F (cid:48)\\n+(cid:90) b\\nxfX (x) dx = −x ¯FX (x)(cid:12)(cid:12)(cid:12)\\n= −b ¯FX (b) +(cid:90) b\\n\\n¯FX (x) dx\\n\\n¯FX (x) dx\\n\\n0\\n\\n0\\n\\n0\\n\\nb\\n\\n(cid:90) b\\n\\n0\\n\\n143\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nFigure 8.2: The cumulative distribution function FX (x) and the survival function ¯FX (x) = 1 − FX (x) for the dart board example. Using the\\nexpression (8.10), we see that the expected value EX = 2/3 is the area under the survival function.\\n\\nThe product term in the integration by parts formula converges to 0 as b → ∞. Thus, we can take a limit to obtain\\n\\nthe identity,\\n\\nEX =(cid:90) ∞\\n\\n0\\n\\nP{X > x} dx.\\n\\n(8.10)\\n\\nExercise 8.9. Show that the product term in the integration by parts formula does indeed converge to 0 as b → ∞.\\n\\nIn words, the expected value is the area between the cumulative distribution function and the line y = 1 or the area\\nunder the survival function. For the case of the dart board, we see that the area under the distribution function between\\n\\n0 x2dx = 1/3, so the area below the survival function EX = 2/3. (See Figure 8.2.)\\n\\ny = 0 and y = 1 is(cid:82) 1\\n\\nExample 8.10. Let T be an exponential random variable, then for some λ, the survival function\\n\\n¯FT (t) = P{T > t} = exp(−λt).\\n\\nThus,\\n\\nET =(cid:90) ∞\\n\\n0\\n\\nP{T > t} dt =(cid:90) ∞\\n\\n0\\n\\nexp(−λt) dt = −\\n\\n1\\nλ\\n\\n∞\\n\\n0\\n\\n= 0 − (−\\n\\n1\\nλ\\n\\n) =\\n\\n1\\nλ\\n\\n.\\n\\nexp(−λt)(cid:12)(cid:12)(cid:12)\\n\\nExercise 8.11. Generalize the identity (8.10) above to X be a positive random variable and g a non-decreasing\\nfunction to show that the expectation\\n\\nEg(X) =(cid:90) ∞\\n\\n0\\n\\ng(x)fX (x) dx = g(0) +(cid:90) ∞\\n\\n0\\n\\n(cid:48)\\n\\ng\\n\\n(x)P{X > x} dx.\\n\\n144\\n\\n0.00.51.01.50.00.20.40.60.81.0Cumulative Distribution Functionx0.00.51.01.50.00.20.40.60.81.00.00.51.01.50.00.20.40.60.81.0Survival Functionx0.00.51.01.50.00.20.40.60.81.0\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nThe most important density function we shall en-\\n\\ncounter is\\n\\nφ(z) =\\n\\n1\\n√2π\\n\\nexp(−\\n\\nz2\\n2\\n\\n),\\n\\nz ∈ R.\\n\\nfor Z, the standard normal random variable. Because\\nthe function φ has no simple antiderivative, we must use\\na numerical approximation to compute the cumulative\\ndistribution function, denoted\\n\\nΦ(z) = P{Z ≤ z}\\n\\nfor a standard normal random variable. This value can\\nbe computed in R with the command pnorm(z).\\n\\nExercise 8.12. Show that φ is increasing for z < 0 and\\ndecreasing for z > 0. In addition, show that φ is con-\\ncave down for z between −1 and 1 and concave up oth-\\nerwise.\\n\\nFigure 8.3: The density of a standard normal density, drawn in R using\\nthe command curve(dnorm(x),-3,3).\\n\\nExample 8.13. The expectation of a standard normal random variable,\\n\\nEZ =\\n\\n1\\n\\n√2π(cid:90) ∞\\n\\n−∞\\n\\nz exp(−\\n\\nz2\\n2\\n\\n) dz = 0\\n\\nbecause the integrand is an odd function. Next to evaluate\\n\\nEZ 2 =\\n\\n1\\n\\n√2π(cid:90) ∞\\n\\n−∞\\n\\nz2 exp(−\\n\\nz2\\n2\\n\\n) dz,\\n\\nwe integrate by parts. (Note the choices of u and v(cid:48).)\\n\\nu(z) = z\\nu(cid:48)(z) = 1\\n\\nv(z) = − exp(− z2\\n2 )\\nv(cid:48)(z) = z exp(− z2\\n2 )\\n\\nThus,\\n\\nEZ 2 =\\n\\n1\\n\\n√2π(cid:18)−z exp(−\\n\\nz2\\n2\\n\\n∞\\n\\n−∞ +(cid:90) ∞\\n\\n−∞\\n\\nexp(−\\n\\nz2\\n2\\n\\n) dz(cid:19) = 1.\\n\\n)(cid:12)(cid:12)(cid:12)\\n\\nUse l’Hˆopital’s rule to see that the ﬁrst term is 0. The fact that the integral of a probability density function is 1 shows\\nthat the second term equals 1.\\n\\nExercise 8.14. For Z a standard normal random variable, show that EZ 3 = 0 and EZ 4 = 3.\\n\\n145\\n\\n-3-2-101230.00.10.20.30.4xdnorm(x)\\x0cIntroduction to the Science of Statistics\\n\\n8.5 Summary\\n\\ndiscrete\\n\\nmass function\\n\\nfX(x) = P{X = x}\\n\\nfX(x) ≥ 0\\n\\n(cid:80)all x fX(x) = 1\\n\\nP{X ∈ A} =(cid:80)x∈A fX(x)\\nEg(X) =(cid:80)all x g(x)fX(x)\\n\\ndistribution function\\nFX(x) = P{X ≤ x}\\n\\nrandom variable\\n\\nproperties\\n\\nprobability\\n\\nexpectation\\n\\nThe Expected Value\\n\\ncontinuous\\n\\ndensity function\\n\\nfX(x)∆x ≈ P{x ≤ X < x + ∆x}\\n\\n−∞\\n\\nfX(x) ≥ 0\\nfX(x) dx = 1\\n\\n(cid:82) ∞\\nP{X ∈ A} =(cid:82)A fX(x) dx\\nEg(X) =(cid:82) ∞\\n\\ng(x)fX(x) dx\\n\\n−∞\\n\\n8.6 Names for Eg(X).\\nSeveral choice for g have special names. We shall later have need for several of these expectations. Others are included\\nto create a comprehensive reference list.\\n\\n1. If g(x) = x, then µ = EX is called variously the (distributional) mean, and the ﬁrst moment.\\n2. If g(x) = xk, then EX k is called the k-th moment. These names were made in analogy to a similar concept in\\n\\nphysics. The second moment in physics is associated to the moment of inertia.\\n\\n3. For integer valued random variables, if g(x) = (x)k, where (x)k = x(x − 1)··· (x − k + 1), then E(X)k is\\ncalled the k-th factorial moment. For random variable taking values in the natural numbers x = 0, 1, 2, . . .,\\nfactorial moments are typically easier to compute than moments for these random variables.\\n\\n4. If g(x) = (x − µ)k, then E(X − µ)k is called the k-th central moment.\\n5. The most frequently used central moment is the second central moment σ2 = E(X − µ)2 commonly called the\\n\\n(distributional) variance. Using the linearity properties of expectation, we see that\\n\\nσ2 = Var(X) = E(X − µ)2 = EX 2 − 2µEX + µ2 = EX 2 − 2µ2 + µ2 = EX 2 − µ2.\\n\\nThis gives a frequently used alternative to computing the variance. In analogy with the corresponding concept\\nwith quantitative data, we call σ the standard deviation for the square root of the variance.\\nExercise 8.15. Find the variance of a single Bernoulli trial.\\nExercise 8.16. Compute the variance for the two types of dice in Exercise 8.2.\\nExercise 8.17. Compute the variance for the dart example.\\n\\nIf we subtract the mean and divide by the standard deviation, the resulting random variable\\n\\nhas mean 0 and variance 1. Z is called the standardized version of X.\\n\\nZ =\\n\\nX − µ\\n\\nσ\\n\\n146\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\n6. The third moment of the standardized random variable\\n\\nγ1 = E(cid:34)(cid:18) X − µ\\nσ (cid:19)3(cid:35)\\n\\nis called the skewness. Random variables with positive skewness have a more pronounced tail to the density\\non the right. Random variables with negative skewness have a more pronounced tail to the density on the left.\\nExercise 8.18. Show that the skewness of X a Bernoulli random variable Ber(p) is\\n\\nThus, X is positively skewed if p < 1/2 and is negatively skewed if p > 1/2.\\n\\n1 − 2p\\n\\n(cid:112)p(1 − p)\\n\\n7. The fourth moment of the standard normal random variable is 3. The kurtosis compares the fourth moment of\\n\\nthe standardized random variable to this value\\n\\nE(cid:34)(cid:18) X − µ\\n\\nσ (cid:19)4(cid:35) − 3.\\n\\nRandom variables with a negative kurtosis are called leptokurtic. Lepto means slender. Random variables with\\na positive kurtosis are called platykurtic. Platy means broad.\\n\\n8. For d-dimensional vectors x = (x1, x2, . . . , xd) and y = (y1, y2, . . . , yd) deﬁne the standard inner product,\\n\\nIf X is Rd-valued and g(x) = ei(cid:104)θ,x(cid:105), then χX (θ) = Eei(cid:104)θ,X(cid:105) is called the Fourier transform or the charac-\\nteristic function. The characteristic function receives its name from the fact that the mapping\\n\\n(cid:104)x, y(cid:105) =\\n\\nxiyi.\\n\\nd(cid:88)i=1\\n\\nfrom the distribution function to the characteristic function is one-to-one. Consequently, if we have a function\\nthat we know to be a characteristic function, then it can only have arisen from one distribution. In this way, χX\\ncharacterizes that distribution.\\n\\nFX (cid:55)→ χX\\n\\n9. Similarly, if X is Rd-valued and g(x) = e(cid:104)θ,x(cid:105), then MX (θ) = Ee(cid:104)θ,X(cid:105) is called the Laplace transform or the\\nmoment generating function. The moment generating function also gives a one-to-one mapping. However,\\nnot every distribution has a moment generating function. To justify the name, consider the one-dimensional case\\nMX (θ) = EeθX. Then, by noting that\\n\\ndk\\ndθk eθx = xkeθx,\\n\\nwe substitute the random variable X for x, take expectation and evaluate at θ = 0.\\n\\nM(cid:48)\\nX (θ) = EXeθX\\nM(cid:48)(cid:48)\\nX (θ) = EX 2eθX\\n\\n...\\n\\nM(cid:48)\\nX (0) = EX\\nM(cid:48)(cid:48)\\nX (0) = EX 2\\n\\n...\\n\\nM (k)\\n\\nX (θ) = EX keθX\\n\\nM (k)\\n\\nX (0) = EX k.\\n\\n10. Let X have the natural numbers for its state space and g(x) = zx, then ρX (z) = EzX =(cid:80)∞\\n\\nx=0 P{X = x}zx\\nis called the (probability) generating function. For these random variables, the probability generating function\\nallows us to use ideas from the analysis of the complex variable power series.\\n\\n147\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nExercise 8.19. Show that the moment generating function for an exponential random variable is\\n\\nUse this to ﬁnd Var(X).\\nExercise 8.20. For the probability generating function, show that ρ(k)\\nshows that falling factorial moments are easier to compute for natural number valued random variables.\\n\\nX (1) = E(X)k. This gives an instance that\\n\\nMX (t) =\\n\\nλ\\nλ − t\\n\\n.\\n\\nParticular attention should be paid to the next exercise.\\n\\nExercise 8.21. Quadratic identity for variance Var(a + bX) = b2Var(X).\\n\\nThe variance is meant to give a sense of the spread of the values of a random variable. Thus, the addition of a\\n\\nconstant a should not change the variance. If we write this in terms of standard deviation, we have that\\n\\nThus, multiplication by a factor b spreads the data, as measured by the standard deviation, by a factor of |b|.\\nparticular,\\n\\nIn\\n\\nThese identities are identical to those for a sample variance s2 and sample standard deviation s.\\n\\nσa+bX = |b|σX .\\n\\nVar(X) = Var(−X).\\n\\n8.7 Independence\\nExpected values in the case of more than one random variable is based on the same concepts as for a single random\\nvariable. For example, for two discrete random variables X1 and X2, the expected value is based on the joint mass\\nfunction fX1,X2 (x1, x2). In this case the expected value is computed using a double sum seen in the identity (8.7).\\n\\nWe will not investigate this in general, but rather focus on the case in which the random variables are independent.\\nHere, we have the factorization identity fX1,X2 (x1, x2) = fX1(x1)fX2 (x2) for the joint mass function. Now, apply\\nidentity (8.7) to the product of functions g(x1, x2) = g1(x1)g2(x2) to ﬁnd that\\n\\nE[g1(X1)g2(X2)] =(cid:88)x1 (cid:88)x2\\n=(cid:32)(cid:88)x1\\n\\ng1(x1)g2(x2)fX1,X2 (x1, x2) =(cid:88)x1 (cid:88)x2\\ng1(x1)fX1 (x1)(cid:33)(cid:32)(cid:88)x2\\n\\ng2(x2)fX2(x2)(cid:33) = E[g1(X1)] · E[g2(X2)]\\n\\ng1(x1)g2(x2)fX1(x1)fX2(x2)\\n\\nA similar identity holds for continuous random variables - the expectation\\nof the product of two independent random variables equals to the product of\\nthe expectation.\\n\\n8.8 Covariance and Correlation\\nA very important example begins by taking X1 and X2 random variables with\\nrespective means µ1 and µ2. Then by the deﬁnition of variance\\n\\nVar(X1 + X2) = E[((X1 + X2) − (µ1 + µ2))2]\\n= E[((X1 − µ1) + (X2 − µ2))2]\\n= E[(X1 − µ1)2] + 2E[(X1 − µ1)(X2 − µ2)]\\n\\n+E[(X2 − µ2)2]\\n\\n= Var(X1) + 2Cov(X1, X2) + Var(X2).\\nwhere the covariance Cov(X1, X2) = E[(X1 − µ1)(X2 − µ2)].\\n\\n148\\n\\nindependent\\n\\nthe standard deviations σX1\\n\\nFigure 8.4: For\\nrandom vari-\\nables,\\nand\\nσX2 satisfy the Pythagorean theorem identity\\nσ2\\nX1+X2\\n\\n= σ2\\n\\n+ σ2\\n\\nX1\\n\\nX2\\n\\n.\\n\\n!0.200.20.40.60.811.2!0.200.20.40.60.811.2!X1+X2!X1!X2\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nExercise 8.22. Cov(X1, X2) = E[X1X2] − µ1µ2.\\n\\nAs you can see, the deﬁnition of covariance is analogous to that for a sample covariance. The analogy continues\\n\\nto hold for the correlation ρ, deﬁned by\\n\\nρ(X1, X2) =\\n\\nCov(X1, X2)\\n\\n(cid:112)Var(X1)(cid:112)Var(X2)\\n\\n.\\n\\nWe can also use the computation for sample covariance to see that distributional covariance is also between −1 and 1.\\nCorrelation 1 occurs only when X and Y have a perfect positive linear association. Correlation −1 occurs only when\\nX and Y have a perfect negative linear association.\\n\\nIf X1 and X2 are independent, then Cov(X1, X2) = E[X1 − µ1] · E[X2 − µ2] = 0 and the variance of the sum is\\n\\nthe sum of the variances. This identity and its analogy to the Pythagorean theorem is shown in Figure 8.4.\\n\\nThe following exercise is the basis in Topic 3 for the simulation of scatterplots having correlation ρ.\\n\\nExercise 8.23. Let X and Z be independent random variables mean 0, variance 1. Deﬁne Y = ρ0X +(cid:112)1 − ρ2\\n\\nThen Y has mean 0, variance 1. Moreover, X and Y have correlation ρ0.\\n\\n0Z.\\n\\nWe can extend this to a generalized Pythagorean identity for n independent random variable X1, X2, . . . , Xn each\\n\\nhaving a ﬁnite variance. Then, for constants c1, c2, . . . , cn, we have the identity\\n\\nVar(c1X1 + c2X2 + ··· cnXn) = c2\\n\\n1Var(X1) + c2\\n\\n2Var(X2) + ··· + c2\\n\\nnVar(Xn).\\n\\nWe will see several opportunities to apply this identity. For example, if we take c1 = c2 ··· = cn = 1, then we\\n\\nhave that for independent random variables\\n\\nVar(X1 + X2 + ··· Xn) = Var(X1) + Var(X2) + ··· + Var(Xn),\\n\\nthe variance of the sum is the sum of the variances.\\n\\nExercise 8.24. Find the variance of a binomial random variable based on n trials with success parameter p.\\n\\nExercise 8.25. For random variables X1, X2, . . . , Xn with ﬁnite variance and constants c1, c2, . . . , cn\\n\\nVar(c1X1 + c2X2 + ··· cnXn) =\\n\\nn(cid:88)i=1\\n\\nn(cid:88)j=1\\n\\ncicjCov(Xi, Xj).\\n\\nRecall that Cov(Xi, Xi) = Var(Xi). If the random variables are independent, then Cov(Xi, Xj) = 0 and the\\n\\nidentity above give the generalized Pythagorean identity.\\n\\nWe can write this identity more compactly in matrix form. Let c be the vector c1, c2, . . . , cn, X = (X1, X2, . . . Xn,\\n\\nand deﬁne the covariance matrix Cov(X) with i, j entry Cov(Xi, Xj). Then\\n\\nVar(cT X) = cT Cov(X)c.\\n\\n8.8.1 Equivalent Conditions for Independence\\nWe can summarize the discussions of independence to present the following 4 equivalent conditions for independent\\nrandom variables X1, X2, . . . , Xn.\\n\\n1. For events A1, A2, . . . An,\\n\\nP{X1 ∈ A1, X2 ∈ A2, . . . Xn ∈ An} = P{X1 ∈ A1}P{X2 ∈ A2}··· P{Xn ∈ An}.\\n\\n149\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\n2. The joint distribution function equals to the product of marginal distribution function.\\n\\nFX1,X2,...,Xn (x1, x2, . . . , xn) = FX1(x1)FX2 (x2)··· FXn(xn).\\n\\n3. The joint density (mass) function equals to the product of marginal density (mass) functions.\\n\\nfX1,X2,...,Xn (x1, x2, . . . , xn) = fX1(x1)fX2 (x2)··· fXn(xn).\\n\\n4. For bounded functions g1, g2, . . . , gn, the expectation of the product of the random variables equals to the\\n\\nproduct of the expectations.\\n\\nE[g1(X1)g2(X2)··· gn(Xn)] = Eg1(X1) · Eg2(X2)··· Egn(Xn).\\n\\nWe will have many opportunities to use each of these conditions.\\n\\n8.9 Quantile Plots and Probability Plots\\nWe have seen the quantile-quantile or Q-Q plot provides a visual method way to compare two quantitative data sets. A\\nmore common comparison is between quantitative data and the quantiles of the probability distribution of a continuous\\nrandom variable. We will demonstrate the properties of these plots with an example.\\nExample 8.26. As anticipated by Galileo, errors in independent accurate measurements of a quantity follow approx-\\nimately a sample from a normal distribution with mean equal to the true value of the quantity. The standard deviation\\ngives information on the precision of the measuring devise. We will learn more about this aspect of measurements\\nwhen we study the central limit theorem. Our example is Morley’s measurements of the speed of light, found in the\\nthird column of the data set morley. The values are the measurements of the speed of light minus 299,000 kilometers\\nper second.\\n\\n> length(morley[,3])\\n[1] 100\\n> mean(morley[,3])\\n[1] 852.4\\n> sd(morley[,3])\\n[1] 79.01055\\n> par(mfrow=c(1,2))\\n> hist(morley[,3])\\n> qqnorm(morley[,3])\\n\\nThe histogram has the characteristic bell shape of the normal density. We can obtain a clearer picture of the\\ncloseness of the data to a normal distribution by drawing a Q-Q plot. (In the case of the normal distribution, the Q-Q\\nplot is often called the normal probability plot.) One method of making this plot begins by ordering the measurements\\nfrom smallest to largest:\\n\\nx(1), x(2), . . . , x(n)\\n\\nNow. give the standardized versions of these values. Let ¯x be the sample mean and sx be the sample standard deviation\\nfor these data. Then the standardized versions of the ordered measurements are\\n\\nIf these are independent measurements from a standard normal distribution, then these values should be close to\\n\\nthe quantiles of the evenly space values\\n\\nz(i) =\\n\\nx(i) − ¯x\\n\\nsx\\n\\n.\\n\\n(8.11)\\n\\n1\\n\\nn + 1\\n\\n,\\n\\n2\\n\\nn + 1\\n\\n,··· ,\\n\\nn\\n\\nn + 1\\n\\n150\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nFigure 8.5: Histogram and normal probability plot of Morley’s measurements of the speed of light.\\n\\n(For the Morley data, n = 100). Thus, the next step is to ﬁnd the values in the standard normal distribution that have\\nthese quantiles. We can ﬁnd these values by applying Φ−1, the inverse distribution function for the standard normal\\n(qnorm in R), applied to the n values listed in (8.11).\\n\\nor\\n\\nx(i) − ¯x\\n\\nsx\\n\\nThe Q-Q plot is the scatterplot of the pairs\\n\\n(cid:18)x(1), Φ\\n\\n−1(cid:18) 1\\n\\nn + 1(cid:19)(cid:19) ,(cid:18)x(2), Φ\\n\\nx(i) ≈ sxΦ\\n\\n= z(i) ≈ Φ\\n\\nn + 1(cid:19) .\\n−1(cid:18) i\\n−1(cid:18) i\\nn + 1(cid:19) + ¯x.\\nn + 1(cid:19)(cid:19) , . . . ,(cid:18)x(n), Φ\\n−1(cid:18) 2\\n\\n−1(cid:18) n\\n\\nn + 1(cid:19)(cid:19)\\n\\nThen a good ﬁt of the data and a normal distribution can be seen in how well the plot follows a straight line with\\n\\nslope sx and vertical intercept ¯x. Such a plot can be seen in Figure 8.4.\\nExercise 8.27. Describe the normal probability plot in the case in which the data X are skewed right.\\n\\n8.10 Answers to Selected Exercises\\n8.2. For the fair die\\nEX 2 = 12 ·\\nFor the unfair dice\\nEX 2 = 12 ·\\n\\n+ 52 ·\\n\\n+ 42 ·\\n\\n+ 22 ·\\n\\n+ 22 ·\\n\\n+ 32 ·\\n\\n+ 32 ·\\n\\n+ 42 ·\\n\\n+ 52 ·\\n\\n1\\n12\\n\\n1\\n12\\n\\n1\\n6\\n\\n1\\n6\\n\\n1\\n6\\n\\n1\\n6\\n\\n1\\n4\\n\\n1\\n4\\n\\n1\\n4\\n\\n1\\n6\\n\\n+ 62 ·\\n\\n1\\n6\\n\\n= (1 + 4 + 9 + 16 + 25 + 36) ·\\n\\n1\\n6\\n\\n=\\n\\n91\\n6\\n\\n.\\n\\n1\\n12\\n\\n= (1 + 4 + 9) ·\\n\\n1\\n4\\n\\n+ (16 + 25 + 36) ·\\n\\n1\\n12\\n\\n=\\n\\n119\\n12\\n\\n.\\n\\n+ 62 ·\\n151\\n\\nHistogram of morley[, 3]morley[, 3]Frequency60070080090010001100051015202530-2-10127008009001000Normal Q-Q PlotTheoretical QuantilesSample Quantiles\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\n8.5. The random variable X can take on the values 0, 1, 2, 3, 4, and 5. Thus,\\n\\nEX =\\n\\n5(cid:88)x=0\\n\\nxfX (x) and EX 2 =\\n\\nx2fX (x).\\n\\n5(cid:88)x=0\\n\\nThe R commands and output follow.\\n\\n> hearts<-c(0:5)\\n> f<-choose(13,hearts)*choose(39,5-hearts)/choose(52,5)\\n> sum(f)\\n[1] 1\\n> prod<-hearts*f\\n> prod2<-heartsˆ2*f\\n> data.frame(hearts,f,prod,prod2)\\n\\nf\\n\\nprod\\n\\nhearts\\n\\nprod2\\n0 0.2215336134 0.00000000 0.00000000\\n1 0.4114195678 0.41141957 0.41141957\\n2 0.2742797119 0.54855942 1.09711885\\n3 0.0815426170 0.24462785 0.73388355\\n4 0.0107292917 0.04291717 0.17166867\\n5 0.0004951981 0.00247599 0.01237995\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n> sum(prod);sum(prod2)\\n[1] 1.25\\n[1] 2.426471\\n\\nLook in the text for an alternative method to ﬁnd EX.\\n8.8. If X is a non-negative random variable, then P{X > 0} = 1. Taking complements, we ﬁnd that\\n\\nFX (0) = P{X ≤ 0} = 1 − P{X > 0} = 1 − 1 = 0.\\n\\n8.9. The convergence can be seen by the following argument.\\n\\n0 ≤ b(1 − FX (b)) = b(cid:90) ∞\\n\\nfX (x) dx =(cid:90) ∞\\nUse the fact that x ≥ b in the range of integration to obtain the inequality in the line above.. Because,(cid:82) ∞\\n∞ (The improper Riemann integral converges.) we have that(cid:82) ∞\\n\\nbfX (x) dx ≤(cid:90) ∞\\n\\nb(1 − FX (b)) → 0 as b → ∞ by the squeeze theorem.\\n8.11. The expectation is the integral\\n\\n0 xfX (x) dx <\\nb xfX (x) dx → 0 as b → ∞. Consequently, 0 ≤\\n\\nxfX (x) dx\\n\\nb\\n\\nb\\n\\nb\\n\\nEg(X) =(cid:90) ∞\\n\\n0\\n\\ng(x)fX (x) dx.\\n\\nIt will be a little easier to look at h(x) = g(x) − g(0). Then, by the linearity of expectation,\\n\\nFor integration by parts, we have\\n\\nEg(X) = g(0) + Eh(X).\\n\\nu(x) = h(x)\\nu(cid:48)(x) = h(cid:48)(x) = g(cid:48)(x)\\n\\nv(x) = −(1 − FX (x)) = − ¯FX (x)\\nv(cid:48)(x) = fX (x) = − ¯F (cid:48)\\n152\\n\\nX (x).\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nAgain, because FX (0) = 0, ¯FX (0) = 1 and\\n\\nEh(X) =(cid:90) b\\n\\n0\\n\\nb\\n\\n+(cid:90) b\\nh(x)fX (x) dx = −h(x) ¯FX (x)(cid:12)(cid:12)(cid:12)\\n= −h(b) ¯FX (b) +(cid:90) b\\n\\ng\\n\\n(cid:48)\\n\\n0\\n\\n0\\n\\n0\\n\\n(cid:48)\\nh\\n\\n(x)(1 − FX (x)) dx\\n\\n(x) ¯FX (x) dx\\n\\nTo see that the product term in the integration by parts formula converges to 0 as b → ∞, note that, similar to\\n\\nExercise 8.9,\\n\\n0 ≤ h(b)(1 − FX (b)) = h(b)(cid:90) ∞\\nh(x) ≥ h(b) if x ≥ b. Now, because(cid:82) ∞\\n\\nfX (x) dx =(cid:90) ∞\\n\\nh(b)fX (x) dx ≤(cid:90) ∞\\n0 h(x)fX (x) dx < ∞, we have that(cid:82) ∞\\n\\nThe ﬁrst inequality uses the assumption that h(b) ≥ 0. The second uses the fact that h is non-decreaasing. Thus,\\nb h(x)fX (x) dx → 0 as b → ∞.\\nConsequently, h(b)(1 − FX (b)) → 0 as b → ∞ by the squeeze theorem.\\n8.12. For the density function φ, the derivative\\n\\nh(x)fX (x) dx\\n\\nb\\n\\nb\\n\\nb\\n\\n1\\n√2π\\nThus, the sign of φ(cid:48)(z) is opposite to the sign of z, i.e.,\\n\\n(z) =\\n\\nφ\\n\\n(cid:48)\\n\\n(−z) exp(−\\n\\nz2\\n2\\n\\n).\\n\\n(cid:48)\\n\\nφ\\n\\n(z) > 0 when z < 0\\n\\nand φ\\n\\n(cid:48)\\n\\n(z) < 0 when z > 0.\\n\\nConsequently, φ is increasing when z is negative and φ is decreasing when z is positive. For the second derivative,\\n\\n(cid:48)(cid:48)\\n\\nφ\\n\\n(z) =\\n\\n1\\n\\n√2π(cid:18)(−z)2 exp(−\\n\\nz2\\n2\\n\\n) − 1 exp(−\\n\\nz2\\n2\\n\\n)(cid:19) =\\n\\n1\\n√2π\\n\\n(z2 − 1) exp(−\\n\\nz2\\n2\\n\\n).\\n\\nThus,\\n\\nφ is concave down if and only if φ\\n\\n(cid:48)(cid:48)\\n\\n(z) < 0\\n\\nif and only if\\n\\nz2 − 1 < 0.\\n\\nThis occurs if and only if z is between −1 and 1.\\n8.14. As argued above,\\n\\nEZ 3 =\\n\\n1\\n\\n√2π(cid:90) ∞\\n\\n−∞\\n\\nz3 exp(−\\n\\nz2\\n2\\n\\n) dz = 0\\n\\nbecause the integrand is an odd function. For EZ 4, we again use integration by parts,\\n\\nu(z) = z3\\nu(cid:48)(z) = 3z2\\n\\nv(z) = − exp(− z2\\n2 )\\nv(cid:48)(z) = z exp(− z2\\n2 )\\n\\nThus,\\n\\nEZ 4 =\\n\\n1\\n\\n√2π(cid:18)−z3 exp(−\\n\\nz2\\n2\\n\\n∞\\n\\n−∞ + 3(cid:90) ∞\\n\\n−∞\\n\\nz2 exp(−\\n\\nz2\\n2\\n\\n) dz(cid:19) = 3EZ 2 = 3.\\n\\nUse l’Hˆopital’s rule several times to see that the ﬁrst term is 0. The integral is EZ 2 which we have previously found\\nto be equal to 1.\\n8.15 For a single Bernoulli trial with success probability p, EX = EX 2 = p. Thus, Var(X) = p − p2 = p(1 − p).\\n8.16. For the fair die, the mean µ = EX = 7/2 and the second moment EX 2 = 91/6. Thus,\\n\\n)(cid:12)(cid:12)(cid:12)\\n\\nVar(X) = EX 2 − µ2 =\\n\\n91\\n\\n6 −(cid:18) 7\\n2(cid:19)2\\n\\n153\\n\\n=\\n\\n182 − 147\\n\\n12\\n\\n=\\n\\n35\\n12\\n\\n.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\nFor the unfair die, the mean µ = EX = 11/4 and the second moment EX 2 = 119/12. Thus,\\n\\nVar(X) = EX 2 − µ2 =\\n\\n119\\n\\n12 −(cid:18) 11\\n4(cid:19)2\\n\\n=\\n\\n476 − 363\\n\\n48\\n\\n=\\n\\n113\\n48\\n\\n.\\n\\n8.17. For the dart, we have that the mean µ = EX = 2/3.\\n\\nEX 2 =(cid:90) 1\\n\\n0\\n\\nx2 · 2x dx =(cid:90) 1\\n\\n0\\n\\n2x3 dx =\\n\\n2\\n4\\n\\nThus,\\n\\n8.18. The third central moment\\n\\nVar(X) = EX 2 − µ2 =\\n\\n1\\n\\n2 −(cid:18) 2\\n3(cid:19)2\\n\\n=\\n\\n1\\n2\\n\\n.\\n\\n1\\n\\n0\\n\\nx4(cid:12)(cid:12)(cid:12)\\n\\n=\\n\\n1\\n18\\n\\n.\\n\\nE[(X − p)3] = (−p)3P{X = 0} + (1 − p)3P{X = 1} = −p3(1 − p) + (1 − p)3p\\n\\n= p(1 − p)(−p2 + (1 − p)2) = p(1 − p)(−p2 + 1 − 2p + p2) = p(1 − p)(1 − 2p).\\n\\nNow, σ2 = Var(X) = p(1 − p). Thus, the skewness,\\n\\nE(cid:34)(cid:18) X − µ\\n\\nσ (cid:19)3(cid:35) = E\\uf8ee\\uf8f0(cid:32) X − p\\n\\n(cid:112)p(1 − p)(cid:33)3\\uf8f9\\uf8fb =\\n8.19. If t < λ, we have that e(t−λ)x → 0 as x → ∞ and so\\n−λx dx = λ(cid:90) ∞\\n\\nMX (t) = EetX = λ(cid:90) ∞\\n\\netxe\\n\\n0\\n\\n0\\n\\np(1 − p)(1 − 2p)\\n(p(1 − p))3/2 =\\n\\n.\\n\\n1 − 2p\\n\\n(cid:112)p(1 − p)\\n\\ne(t−λ)x dx =\\n\\nλ\\nt − λ\\n\\n∞\\n\\n0\\n\\ne(t−λ)x(cid:12)(cid:12)(cid:12)\\n\\n=\\n\\nλ\\nλ − t\\n\\n(cid:48)\\n\\nM\\n\\n(t) =\\n\\n(cid:48)(cid:48)\\n\\nM\\n\\n(t) =\\n\\nλ\\n\\n(λ − t)2 , EX = M\\n(cid:48)(cid:48)\\n\\n(λ − t)3 , EX = M\\n\\n2λ\\n\\n(cid:48)\\n\\n(0) =\\n\\n(0) =\\n\\n1\\nλ\\n\\n,\\n\\n2\\nλ2 .\\n\\nVar(X) = EX 2 − (EX)2 =\\n\\n2\\nλ2 −\\n\\n1\\nλ2 =\\n\\n1\\nλ2 .\\n\\nx=0 P{X = x}zx The k-th derivative of zx with respect to z is\\n\\nThus,\\n\\nand\\n\\nThus,\\n\\n8.20. ρX (z) = EzX =(cid:80)∞\\n\\nEvaluating at z = 1, we ﬁnd that\\n\\ndk\\n\\ndzk zx = (x)kzx−k.\\n\\ndk\\n\\ndzk zx(cid:12)(cid:12)(cid:12)z=1\\n\\n154\\n\\n= (x)k.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThus the k-th derivative of ρ,\\n\\nThe Expected Value\\n\\nρ(k)\\nX (z) =\\n\\nρ(k)\\nX (1) =\\n\\n∞(cid:88)x=0\\n(x)kP{X = x}zx−k and, thus,\\n∞(cid:88)x=0\\n\\n(x)kP{X = x} = E(X)k.\\n\\n8.21. Let EX = µ. Then the expected value E[a + bX] = a + bµ and the variance\\n\\nVar(a + bX) = E[((a + bX) − (a + bµ))2] = E[(b(X − µ))2] = b2E[(X − µ)2] = b2Var(X).\\n\\n8.22. Use the notation µ1 = EX1, µ2 = EX2 and the linearity of expectation to see that\\n\\nCov(X1, X2) = E[(X1 − µ1)(X2 − µ2)] = EX1X2 − µ2EX1 − µ1EX2 + µ1µ2\\n\\n= EX1X2 − µ2µ1 − µ1µ2 + µ1µ2 = E[X1X2] − µ1µ2\\n\\n8.23. By the linearity property of the mean\\n\\nBy the Pythorean identity and then the quadratic identity for the variance,\\n\\nEY = ρ0EX +(cid:113)1 − ρ2\\n\\n0EZ = 0.\\n\\nVar(Y ) = Var(ρ0X) + Var((cid:113)1 − ρ2\\n\\n0 Z) = ρ2\\n\\n0Var(X) + (1 − ρ2\\n\\n0)Var(Z) = ρ2\\n\\n0 + (1 − ρ2\\n\\n0) = 1.\\n\\nBecause X and Y both have variance 1, their correlation is equal to their covariance. Now use the linearity property\\n\\nof covariance\\n\\nρ(X, Y ) = Cov(X, Y ) = Cov(cid:18)X, ρ0X +(cid:113)1 − ρ2\\n\\n0 Z(cid:19) = ρ0Cov(X, X) +(cid:113)1 − ρ2\\n\\n0 Cov(X, Z)\\n\\n= ρ0 · 1 +(cid:113)1 − ρ2\\n\\n0 · 0 = ρ0\\n\\n8.24. This binomial random variable is the sum of n independent Bernoulli random variable. Each of these random\\nvariables has variance p(1 − p). Thus, the binomial random variable has variance np(1 − p).\\n8.27. For the larger order statistics, z(k) for the standardized version of the observations, the values are larger than\\nwhat one would expect when compared to observations of a standard normal random variable. Thus, the probability\\nplot will have a concave upward shape. As an example, we let X have the density shown below. Beside this is the\\nprobability plot for X based on 100 samples. (X is a gamma Γ(2, 3) random variable. We will encounter these random\\nvariables soon.)\\n\\n155\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Expected Value\\n\\n156\\n\\n0.00.51.01.52.02.53.00.00.20.40.60.81.0xdgamma(x, 2, 3)-2-10120.00.51.01.52.02.5Normal Q-Q PlotTheoretical QuantilesSample Quantiles\\x0cTopic 9\\n\\nExamples of Mass Functions and Densities\\n\\nFor a given state space, S, we will describe several of the most frequently encountered parameterized families of both\\ndiscrete and continuous random variables\\n\\nX : Ω → S.\\n\\nindexed by some parameter θ. We will add the subscript θ to the notation Pθ and Eθ to indicate the parameter value\\nused to compute, respectively, probabilities and expectations. This section is meant to serve as an introduction to\\nthese families of random variables and not as a comprehensive development. The section should be considered as a\\nreference to future topics that rely on this information.\\n\\nWe shall use the notation\\n\\nboth for a family of mass functions for discrete random variables and the density functions for continuous random\\nvariables that depend on the parameter θ. After naming the family of random variables, we will use the expression\\nF amily(θ) as shorthand for this family followed by the R command and state space S. A table of R commands,\\nparameters, means, and variances is given at the end of this section.\\n\\nfX (x|θ)\\n\\n9.1 Examples of Discrete Random Variables\\nIncorporating the notation introduced above, we write\\n\\nfX (x|θ) = Pθ{X = x}\\nfor the mass function of the given family of discrete random variables.\\n\\n1. (Bernoulli) Ber(p), S = {0, 1}\\n\\nfX (x|p) =(cid:26) 0 with probability (1 − p),\\n\\n1 with probability p,\\n\\n(cid:27) = px(1 − p)1−x.\\n\\nThis is the simplest random variable, taking on only two values, namely, 0 and 1. Think of it as the outcome of\\na Bernoulli trial, i.e., a single toss of an unfair coin that turns up heads with probability p.\\n\\n2. (binomial) Bin(n, p) (R command binom) S = {0, 1, . . . , n}\\n\\nfX (x|p) =(cid:18)n\\n\\nx(cid:19)px(1 − p)n−x.\\n\\n157\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nFigure 9.1: Binomial mass function f (x|p) for n = 12 and p = 1/4, 1/2, 3/4.\\n\\nWe gave a more extensive introduction to Bernoulli trials and the binomial distribution in the discussion on\\nThe Expected Value. Here we found that the binomial distribution arises from computing the probability of x\\nsuccesses in n Bernoulli trials. Considered in this way, the family Ber(p) is also Bin(1, p).\\nNotice that by its deﬁnition if Xi is Bin(ni, p), i = 1, 2 and are independent, then X1 + X2 is Bin(n1 + n2, p)\\n\\n3. (geometric) Geo(p) (R command geom) S = N = {0, 1, 2, . . .}.\\nfX (x|p) = p(1 − p)x.\\n\\nWe previously described this random variable as the number of failed Bernoulli trials before the ﬁrst success.\\nThe name geometric random variable is also applied to the number of Bernoulli trials Y until the ﬁrst success.\\nThus, Y = X + 1. As a consequence of these two choices for a geometric random variable, care should be taken\\nto be certain which deﬁnition is under considertion.\\nExercise 9.1. Give the mass function for Y .\\n\\n4. (negative binomial) N egbin(n, p) (R command nbinom) S = N\\n\\nfX (x|p) =(cid:18)n + x − 1\\n\\nx\\n\\n(cid:19)pn(1 − p)x.\\n\\nThis random variable is the number of failed Bernoulli trials before the n-th success. Thus, the family of\\ngeometric random variable Geo(p) can also be denoted N egbin(1, p). As we observe in our consideration\\n\\nFigure 9.2: The relationship between the binomial and geometric random variable in Bernoulli trials.\\n\\n158\\n\\n0246810120.000.050.100.150.200.25xdbinom(x, 12, 1/4)0246810120.000.050.100.150.20xdbinom(x, 12, 1/2)0246810120.000.050.100.150.200.25xdbinom(x, 12, 3/4)0.10.20.30.40.50.60.70.80.9Number of trials is n.Distance between successes is a geometric random variable, parameter p.Number of successes is a binomial random varible, parameters  n and p.\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nof Bernoulli trials, we see that the number of failures between consecutive successes is a geometric random\\nvariable. In addition, the number of failures between any two pairs of successes (say, for example, the 2nd and\\n3rd success and the 6th and 7th success) are independent. In this way, we see that N egbin(n, p) is the sum of n\\nindependent Geo(p) random variables.\\nTo determine the mass function, note that in order for X to take on a given value x, then the n-th success must\\noccur on the n + x-th trial. In other words, we must have n − 1 successes and x failures in ﬁrst n + x − 1\\nBernoulli trials followed by success on the last trial. The ﬁrst n + x − 1 trials and the last trial are independent\\nand so their probabilities multiply.\\n\\nPp{X = x} = Pp{n − 1 successes in n + x − 1 trials, success in the n − x-th trial}\\n\\n= Pp{n − 1 successes in n + x − 1 trials}Pp{success in the n − x-th trial}\\n=(cid:18)n + x − 1\\n\\nn − 1 (cid:19)pn−1(1 − p)x · p =(cid:18)n + x − 1\\n\\n(cid:19)pn(1 − p)x\\n\\nx\\n\\nThe ﬁrst factor is computed from the binomial distribution, the second from the Bernoulli distribution. Note the\\nuse of the identity\\n\\nin giving the ﬁnal formula.\\n\\nk(cid:19) =(cid:18) m\\n(cid:18)m\\nm − k(cid:19)\\n\\nFigure 9.3: Probability mass function for negative binomial random variables for n = 1, 2, 3, 4 and p = 2/5.\\n\\nExercise 9.2. Use the fact that a negative binomial random variable N egbin(r, p) is the sum of independent\\ngeometric random variable Geo(p) to ﬁnd its mean and variance. Use the fact that a geometric random variable\\nhas mean (1 − p)/p and variance (1 − p)/p2.\\n5. (Poisson) P ois(λ) (R command pois) S = N,\\n\\nfX (x|λ) =\\n\\n−λ.\\ne\\n\\nλx\\nx!\\n\\nThe Poisson distribution approximates of the binomial distribution when n is large, p is small, but the product\\nλ = np is moderate in size. One example for this can be seen in bacterial colonies. Here, n is the number of\\nbacteria and p is the probability of a mutation and λ, the mean number of mutations is moderate. A second is the\\n\\n159\\n\\n051015200.00.10.20.30.4xdnbinom(x, 1, 0.4)051015200.000.050.100.15xdnbinom(x, 2, 0.4)051015200.000.020.040.060.080.100.120.14xdnbinom(x, 3, 0.4)051015200.000.020.040.060.080.100.12xdnbinom(x, 4, 0.4)\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nnumber of recombination events occurring during meiosis. In this circumstance, n is the number of nucleotides\\non a chromosome and p is the probability of a recombination event occurring at a particular nucleotide.\\nThe approximation is based on the limit\\n\\nn→∞(cid:18)1 −\\n\\nlim\\n\\nλ\\n\\nn(cid:19)n\\n\\n−λ\\n\\n= e\\n\\n(9.1)\\n\\nWe now compute binomial probabilities, replace p by λ/n and take a limit as n → ∞. In this computation, we\\nuse the fact that for a ﬁxed value of x,\\n\\nλ\\n\\nλ\\n\\n−λ\\n\\n≈ e\\n\\n→ 1\\n\\nn(cid:19)−x\\n\\n(n)x\\nnx → 1\\n\\nand (cid:18)1 −\\n0(cid:1)p0(1 − p)n =(cid:18)1 −\\nn(cid:19)n\\nn(cid:19)n−1\\nn(cid:18)1 −\\n1(cid:1)p1(1 − p)n−1 = n\\nn(cid:19)n−2\\n(cid:18) λ\\nn(cid:19)2(cid:18)1 −\\n2(cid:1)p2(1 − p)n−2 =\\n...\\nn(cid:19)n−x\\nn(cid:19)x(cid:18)1 −\\nx(cid:1)px(1 − p)n−x =\\n\\nx! (cid:18) λ\\n\\nn(n − 1)\\n\\n≈ λe\\n\\n(n)x\\n\\n−λ\\n\\n=\\n\\nλ\\n\\nλ\\n\\nλ\\n\\nλ\\n\\n...\\n\\n2\\n\\nas n → ∞\\n\\n=\\n\\nn(n − 1)\\n\\nn2\\n\\n(n)x\\nnx\\n\\nλx\\n\\nx! (cid:18)1 −\\n\\nP{X = 0} =(cid:0)n\\nP{X = 1} =(cid:0)n\\nP{X = 2} =(cid:0)n\\nP{X = x} =(cid:0)n\\n\\n...\\n\\nλ2\\n\\n2 (cid:18)1 −\\nn(cid:19)n−x\\n\\nλ\\n\\nλ\\n\\nn(cid:19)n−2\\n\\n−λ\\n\\ne\\n\\nλ2\\n2\\n\\n≈\\n\\nλx\\nx!\\n\\n≈\\n\\n−λ.\\n\\ne\\n\\nThe Taylor series for the exponential function\\n\\nexp λ =\\n\\n∞(cid:88)x=0\\n\\nλx\\nx!\\n\\n.\\n\\nshows that\\n\\n∞(cid:88)x=0\\n\\n∞(cid:88)x=0\\n\\nfX (x) =\\n\\n−λ = e\\n\\n−λe\\n\\n−λ = 1.\\n\\ne\\n\\nλx\\nx!\\n\\nExercise 9.3. Take logarithms and use l’Hˆopital’s rule to establish the limit (9.1) above.\\n\\nExercise 9.4. We saw that the sum of independent binomial random variables with a common value for p, the\\nsuccess probability, is itself a binomial random variable. Show that the sum of independent Poisson random\\nvariables is itself a Poisson random variable. In particular, if Xi are P ois(λi), i = 1, 2, then X1 + X2 is\\nP ois(λ1 + λ2).\\n\\n6. (uniform) U (a, b) (R command sample) S = {a, a + 1, . . . , b},\\n\\nfX (x|a, b) =\\n\\n1\\n\\nb − a + 1\\n\\n.\\n\\nThus each value in the designated range has the same probability.. To produce a sample of n U (a, b) random\\nvariables, use the command sample(a:b,n,replace=TRUE).\\n\\n160\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nFigure 9.4: Probability mass function for binomial random variables for (a) n = 10, p = 0.3, (b) n = 100, p = 0.03, (c) n = 1000, p = 0.003\\nand for (d) the Poisson random varialble with λ = np = 3. This displays how the Poisson random variable approximates the binomial random\\nvariable with n large, p small, and their product λ = np moderate.\\n\\n7. (hypergeometric) Hyper(m, n, k) (R command hyper). The hypergeometric distribution will be used in com-\\nputing probabilities under circumstances that are associated with sampling without replacement. We will use\\nthe analogy of an urn containing balls having one of two possible colors.\\nBegin with an urn holding m white balls and n black balls. Remove k and let the random variable X denote the\\nnumber of white balls. The value of X has several restrictions. X cannot be greater than either the number of\\nwhite balls, m, or the number chosen k. In addition, if k > n, then we must consider the possibility that all of\\nthe black balls were chosen. If X = x, then the number of black balls, k − x, cannot be greater than the number\\nof black balls, n, and thus, k − x ≤ n or x ≥ k − n.\\nIf we are considering equally likely outcomes, then we ﬁrst compute the total number of possible outcomes,\\n#(Ω), namely, the number of ways to choose k balls out of an urn containing m + n balls. This is the number\\nof combinations\\n\\n(cid:18)m + n\\nk (cid:19).\\n\\nThis will be the denominator for the probability. For the numerator of P{X = x}, we consider the outcomes\\nthat result in x white balls from the total number m in the urn. We must also choose k − x black balls from\\nthe total number n in the urn. By the multiplication property, the number of ways #(Ax) to accomplish this is\\nproduct of the number of outcomes for these two combinations,\\n\\nx(cid:19)(cid:18) n\\n(cid:18)m\\nk − x(cid:19).\\n\\nThe mass function for X is the ratio of these two numbers.\\n\\nfX (x|m, n, k) =\\n\\n#(Ax)\\n#(Ω)\\n\\nx(cid:1)(cid:0) n\\n= (cid:0)m\\nk−x(cid:1)\\n(cid:0)m+n\\nk (cid:1) ,\\n\\n161\\n\\nx = max{0, k − n}, . . . , min{m, k}.\\n\\n02468100.000.050.100.150.200.25xdbinom(x, 10, 0.3)02468100.000.050.100.150.20xdbinom(x, 100, 0.03)02468100.000.050.100.150.20xdbinom(x, 1000, 0.003)02468100.000.050.100.150.20xdpois(x, 3)\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nExercise 9.5. Show that we can rewrite this probability as\\n\\nfX (x|m, n, k) =\\n\\nk!\\n\\nx!(k − x)!\\n\\n(m)x(n)k−x\\n(m + n)k\\n\\n=(cid:18)k\\nx(cid:19) (m)x(n)k−x\\n\\n(m + n)k\\n\\n.\\n\\n(9.2)\\n\\nThis gives probabilities using sampling without replacement. If we were to choose the balls one-by-one re-\\nturning the balls to the urn after each choice, then we would be sampling with replacement. This returns us to\\nthe case of k Bernoulli trials with success parameter p = m/(m + n), the probability for choosing a white ball.\\nIn the case the mass function for Y , the number of white balls, is\\n\\nfY (x|m, n, k) =(cid:18)k\\n\\nx(cid:19)px(1 − p)k−x =(cid:18)k\\n\\nx(cid:19)(cid:18) m\\n\\nm + n(cid:19)x(cid:18) n\\n\\nm + n(cid:19)k−x\\n\\n=(cid:18)k\\nx(cid:19) mxnk−x\\n\\n(m + n)k .\\n\\n(9.3)\\n\\nNote that the difference in the formulas between sampling with replacement in (9.3) and without replacement in\\n(9.2) is that the powers are replaced by the falling function, e.g., mx is replaced by (m)x.\\nLet Xi be a Bernoulli random variable indicating whether or not the color of the i-th ball is white. Thus, its\\nmean\\n\\nEXi =\\n\\nm\\n\\n.\\n\\nm + n\\n\\nThe random variable for the total number of white balls X = X1 + X2 + ··· + Xk and thus its mean\\n\\nEX = EX1 + EX2 + ··· + EXk = k\\n\\nm\\n\\nm + n\\n\\n.\\n\\nBecause the selection of white for one of the marbles decreases the chance for black for another selection,\\nthe trials are not independent. One way to see this is by noting the variance (not derived here) of the sum\\nX = X1 + X2 + ··· + Xk\\n\\nVar(X) = k\\n\\nm\\n\\nm + n\\n\\nn\\n\\nm + n ·\\n\\nm + n − k\\nm + n − 1\\n\\nis not the sum of the variances.\\nIf we write N = m + n for the total number of balls in the urn and p = m/(m + n) as above, then\\n\\nVar(X) = kp(1 − p)\\n\\nN − k\\nN − 1\\n\\nThus the variance of the hypergeometric random variable is reduced by a factor of (N − k)/(N − 1) from the\\ncase of the corresponding binomial random variable. In the cases for which k is much smaller than N, then\\nsampling with and without replacement are nearly the same process - any given ball is unlikely to be chosen\\nmore than once under sampling with replacement. We see this situation, for example, in a opinion poll with k at\\n1 or 2 thousand and N, the population of a country, typically many millions.\\nOn the the other hand, if k is a signiﬁcant fraction of N, then the variance is signiﬁcantly reduced under sampling\\nwithout replacement. We are much less uncertain about the fraction of white and black balls. In the extreme\\ncase of k = N, we have chosen every ball and know that X = m with probability 1. In the case, the variance\\nformula gives Var(X) = 0, as expected.\\n\\nExercise 9.6. Draw two balls without replacement from the urn described above. Let X1, X2 be the Bernoulli random\\nindicating whether or not the ball is white. Find Cov(X1, X2).\\n\\nExercise 9.7. Check that(cid:80)x∈S fX (x|θ) = 1 in the examples above.\\n\\n162\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\n9.2 Examples of Continuous Random Variables\\nFor continuous random variables, we have for the density\\n\\n1. (uniform) U (a, b) (R command unif) on S = [a, b],\\n\\nfX (x|θ) ≈\\n\\nPθ{x < X ≤ x + ∆x}\\n\\n∆x\\n\\n.\\n\\nfX (x|a, b) =\\n\\n1\\nb − a\\n\\n.\\n\\nFigure 9.5: Uniform density\\n\\nIndependent U (0, 1) are the most common choice for generating random numbers. Use the R command\\nrunif(n) to simulate n independent random numbers.\\nExercise 9.8. Find the mean and the variance of a U (a, b) random variable.\\n\\nFigure 9.6: The relationship between the Poission and exponential random variable in Bernoulli trials with large n, small p and moderate size\\nproduct λ = np. Notice the analogies from Figure 9.2. Imagine a bacterial colony with individual bacterium produced at a constant rate n per\\nunit time. Then, the times between mutations can be approximated by independent exponential random variables and the number of mutations is\\napproximately a Poisson random variable.\\n\\n2. (exponential) Exp(λ) (R command exp) on S = [0,∞),\\n\\nfX (x|λ) = λe\\n\\n−λx.\\n\\nTo see how an exponential random variable arises, consider Bernoulli trials arriving at a rate of n trials per time\\nunit and take the approximation seen in the Poisson random variable. Again, the probability of success p is\\nsmall, ns the number of trials up to a given time s is large, and λ = np. Let T be the time of the ﬁrst success.\\nThis random time exceeds a given time s if we begin with ns consecutive failures. Thus, the survival function\\n\\n¯FT (s) = P{T > s} = (1 − p)ns =(cid:18)1 −\\n\\nλ\\n\\nn(cid:19)ns\\n\\n−λs.\\n\\n≈ e\\n\\n163\\n\\n−0.500.511.522.533.54−0.200.20.40.60.8xYab1/(b−a)0.10.20.30.40.50.60.70.80.9Number of successes is a Poisson random varible, parameter       λ t.Number of trials is npt = λ t.Distance between successes is an exponential random variable, parameter         λ.\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nFigure 9.7: Density for a gamma random variable. Here, β = 1, α = 1 (black), 2 (red), 3 (blue) and 4 (green)\\n\\nThe cumulative distribution function\\n\\nFT (s) = P{T ≤ s} = 1 − P{T > s} ≈ 1 − e\\n\\n−λs.\\n\\nThe density above can be found by taking a derivative of FT (s).\\nExercise 9.9. Shoe that the exponential distribution also has the memorylessness property, namely\\n\\nIn words, given that the wait for an event has taken t time units, then the probability of waiting an additional s\\ntime units is the same as the probability of waiting s time units from the beginning.\\n\\nP{T > t + s|T > t} = P{T > s}.\\n\\n3. (gamma) Γ(α, β) (R command gamma) on S = [0,∞),\\nβα\\nΓ(α)\\n\\nf (x|α, β) =\\n\\nxα−1e\\n\\n−βx.\\n\\nObserve that Exp(λ) is Γ(1, λ). A Γ(n, λ) can be seen as an approximation to the negative binomial random\\nvariable using the ideas that leads from the geometric random variable to the exponential. Alternatively, for\\na natural number n, Γ(n, λ) is the sum of n independent Exp(λ) random variables. This special case of the\\ngamma distribution is sometimes called the Erlang distribution and was originally used in models for telephone\\ntrafﬁc.\\nThe gamma function Γ appears in the deﬁnition of the gamma density\\n\\nΓ(s) =(cid:90) ∞\\n\\n0\\n\\nxse\\n\\n−x dx\\nx\\n\\nThis is computed in R using gamma(s).\\nFor the graphs of the densities in Figure 9.7,\\n\\n> curve(dgamma(x,1,1),0,8)\\n> curve(dgamma(x,2,1),0,8,add=TRUE,col=\"red\")\\n> curve(dgamma(x,3,1),0,8,add=TRUE,col=\"blue\")\\n> curve(dgamma(x,4,1),0,8,add=TRUE,col=\"green\")\\n\\n164\\n\\n024680.00.20.40.60.81.0xdgamma(x, 1, 1)\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nExercise 9.10. Use integration by parts to show that\\n\\nIf n is a non-negative integer, show that\\n\\n.\\n\\nΓ(t + 1) = tΓ(t).\\n\\nΓ(n) = (n − 1)!\\n\\n4. (beta) Beta(α, β) (R command beta) on S = [0, 1],\\n\\n(9.4)\\n\\n(9.5)\\n\\nfX (x|α, β) =\\n\\nΓ(α + β)\\nΓ(α)Γ(β)\\n\\nxα−1(1 − x)β−1.\\n\\nBeta random variables appear in a variety of circumstances. One common example is the order statistics.\\nBeginning with n observations, X1, X2,··· , Xn, of independent uniform random variables on the interval\\n[0, 1] and rank them\\n\\nfrom smallest to largest. Then, the k-th order statistic X(k) is Beta(k, n − k + 1).\\nIn the deﬁnition of the density, we can also use the beta function\\n\\nX(1), X(2), . . . , X(n)\\n\\nB(α, β) =(cid:90) 1\\n\\n0\\n\\nxα−1(1 − x)β−1 dx.\\n\\nAs we can see ,using the fact that the integral of a density function equals to 1, that\\n\\nB(α, β) =\\n\\nΓ(α)Γ(β)\\nΓ(α + β)\\n\\n.\\n\\nThe R command for the beta function is beta.\\nBeta(1/2, 1/2) is also called the arcsine distribution. The distribution function is\\n\\nF (x|1/2, 1/2) =\\n\\narcsin(√x).\\n\\n2\\nπ\\n\\nExercise 9.11. Differential F (x|1/2, 1/2) to show that it is the Beta(1/2, 1/2) density.\\nExercise 9.12. Use the identity (9.4) for the gamma function to ﬁnd the mean and variance of the beta distribu-\\ntion.\\n\\n5. (normal) N (µ, σ) (R command norm) on S = R,\\n\\nfX (x|µ, σ) =\\n\\n1\\n\\nσ√2π\\n\\nexp(cid:18)−\\n\\n(x − µ)2\\n\\n2σ2 (cid:19) .\\n\\nThus, a standard normal random variable is N (0, 1). Other normal random variables are linear transformations\\nof Z, the standard normal. In particular, X = σZ + µ has a N (µ, σ) distribution. To simulate 200 normal\\nrandom variables with mean 1 and standard deviation 1/2, use the R command x<-rnorm(200,1,0.5).\\nHistograms of three simulations are given in the Figure 9.8.\\nWe often move from a random variable X to a function g of the random variable. Thus, Y = g(X). The\\nfollowing exercise give the density fY in terms of the density fX for X in the case that g is a monotone\\nfunction.\\n\\n165\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nFigure 9.8: Histrogram of three simulations of 200 normal random variables, mean 1, standard deviation 1/2\\n\\nExercise 9.13. Show that for g differentiable and monotone then g has a differentiable inverse g−1 and the\\ndensity\\n\\nfY (y) = fX (g\\n\\n−1(y))(cid:12)(cid:12)(cid:12)(cid:12)\\n\\nd\\ndy\\n\\ng\\n\\n−1(y)(cid:12)(cid:12)(cid:12)(cid:12) .\\n\\n(9.6)\\n\\nWe can ﬁnd this density geometrically by noting that for g increasing,\\n\\nY is between y and y + ∆y if and only if X is between g\\n\\n−1(y) and g\\n\\n−1(y + ∆y) ≈ g\\n\\n−1(y) +\\n\\nd\\ndy\\n\\n−1(y)∆y.\\n\\ng\\n\\nThus,\\n\\nfY (y)∆y = P{y < Y ≤ y + ∆y} ≈ P{g\\n\\n−1(y) < X ≤ g\\n\\n−1(y) +\\n\\nd\\ndy\\n\\ng\\n\\n−1(y)∆y}\\n\\n≈ fX (g\\n\\n−1(y))\\n\\nd\\ndy\\n\\n−1(y)∆y.\\n\\ng\\n\\nDropping the factors ∆y gives (9.6). (See Figure 9.9.)\\n\\n166\\n\\nHistogram of xxFrequency!0.50.00.51.01.52.02.53.00102030Histogram of xxFrequency!0.50.00.51.01.52.02.505101520253035Histogram of xxFrequency!0.50.00.51.01.52.02.505101520253035\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nFigure 9.9: Finding the density of Y = g(X) from the density of X. The areas of the two rectangles should be the same. Consequently, fY (y)∆y\\n≈ fX (g−1(y)) d\\n\\ndy g−1(y)∆y.\\n\\n6. (log-normal) ln N (µ, σ) (R command lnorm) on S = (0,∞). A log-normal random variable is the exponen-\\ntial of a normal random variable. Thus, the logarithm of a log-normal random variable is normal. The density\\nof this family is\\n\\nfX (x|µ, σ) =\\n\\n1\\n\\nxσ√2π\\n\\nexp(cid:18)−\\n\\n(ln x − µ)2\\n\\n2σ2\\n\\n(cid:19) .\\n\\nExercise 9.14. Use the exercise above to ﬁnd the density of a log-normal random variable.\\n\\n7. (Pareto) P areto(α, β) (R command pareto) on S = (α,∞). The Pareto distribution is used as a power law\\n\\ndistribution used by a variety of disciplines. The density of this family is\\n\\nfX (x|α, β) =\\n\\nβαβ\\nxβ+1 .\\n\\nThe pareto command is not a part of the main R package, but can be used after downloading, for example,\\nthe actuar package.\\nExercise 9.15. Let X be Exp(λ). Use the exercise above to show that exp(X) has a P areto(1, λ) distribution.\\n\\nAs we stated previously, the normal family of random variables will be the most important collection of distri-\\nbutions we will use. Indeed, the ﬁnal three examples of families of random variables are functions of normal\\nrandom variables. They are seen as the densities of statistics used in hypothesis testing. Even though their\\ndensities are given explicitly, in practice, these formulas are rarely explicitly used directly. Rather, probabilities\\nare generally computed using statistical software.\\n\\n8. (chi-square) χ2\\n\\nν (R command chisq) on S = [0,∞)\\n\\nxν/2−1\\n\\n−x/2.\\ne\\n\\nfX (x|ν) =\\n\\n2ν/2Γ(ν/2)\\n\\nThe value ν is called the number of degrees of freedom. For ν a positive integer, let Z1, Z2, . . . , Zν be inde-\\npendent standard normal random variables. Then,\\n\\nZ 2\\n\\n1 + Z 2\\n\\n2 + ··· + Z 2\\n\\nν\\n\\nhas a χ2\\n\\nν distribution.\\n\\n167\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nExercise 9.16. Modify the solution to the exercise above to ﬁnd the density of a χ2\\nY = X 2, P{Y ≤ y} = P{−√y ≤ X ≤ √y}.)\\nExercise 9.17. Maxwell-Boltzmann distribution models the distribution of particle speeds in an ideal gas in\\nthermal equilibrium. The density function is\\n\\n1 random variable. (Hint: For\\n\\nfS(s) =(cid:114)(cid:16) m\\n2πkT(cid:17)3\\n\\n4πs2e\\n\\n−ms2/(2kT ),\\n\\nwhere m is the particle mass and k is Boltzmann’s constant and T the absolute temperature in kelvins.\\n\\n(a) Let Y = S(cid:112)m/kT , Then, Y has density\\n\\nfY (y) =\\n\\n−y2/2,\\n\\ny2e\\n\\n1\\n√2π\\n\\n(b) Show that X = Y 2 is χ2\\n\\n3, a chi-square random variable with three degrees of freedom.\\n\\nTherefore, the particle speed\\n\\nwhere X is χ2\\n3.\\n\\nS =(cid:114) kT\\n\\nm\\n\\nY =(cid:114) kT X\\n\\nm\\n\\n9. (Student’s t) tν(µ, σ) (R command t) on S = R,\\n\\nfX (x|ν, µ.σ) =\\n\\nΓ((ν + 1)/2)\\n\\n√νπΓ(ν/2)σ(cid:18)1 +\\n\\n(x − µ)2\\n\\nνσ2 (cid:19)−(ν+1)/2\\n\\n.\\n\\nThe value ν is also called the number of degrees of freedom. If ¯Z is the sample mean of n standard normal\\nrandom variables and\\n\\nis the sample variance, then\\n\\nS2 =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n(Zi − ¯Z)2\\n\\n√n ¯Z + a\\n\\nS\\n\\n.\\n\\nT =\\n\\nhas a tn−1(a, 1) distribution. In this case, a is called the noncentrality parameter. We shall see this distribution\\nwhen we consider alternatives to hypotheses whose tests are based on the t distribution. The case a = 0\\n\\n√n ¯Z\\nS\\n\\n=\\n\\n¯Z\\n\\nS/√n\\n\\n.\\n\\nT =\\n\\nis the classical t distribution.\\n\\n10. (Fisher’s F ) Fν1,ν2 (R command f) on S = [0,∞),\\n\\nfX (x|ν1, ν2) =\\n\\nΓ((ν1 + ν2)/2)νν1/2\\nΓ(ν1/2)Γ(ν2/2)\\n\\n1\\n\\nνν2/2\\n2\\n\\nxν1/2−1(ν2 + ν1x)\\n\\n−(ν1+ν2)/2.\\n\\nThe F distribution will make an appearance when we see the analysis of variance test. It arises as the ratio of\\nindependent chi-square random variables. The chi-square random variable in the numerator has ν1 degrees of\\nfreedom; the chi-square random variable in the denominator has ν2 degrees of freedom\\n\\n168\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\n9.3 More on Mixtures\\nPreviously we introduced mixtures. The ingredient are a probability, i.e., non-negative numbers π1, . . . , πn that sum\\nto 1 and n probability density functions f1(x), . . . , fn(x) from either discrete or continuous random variables. The\\nmixture density\\n\\nf (x) = π1f1(x) + ··· + πnfn(x) =\\n\\nfi(x)πi..\\n\\nn(cid:88)i=1\\n\\nOne common version of mixtures is the case that where the density functions are taken from a particular family of\\n\\ndensities fX (x|θ). In other words, for the choice of n parameter values, θ1, . . . , θn, take densities\\n\\nNext, if Θ is a random variable with mass function fΘ(θi) = πi then, we can write the mixture as\\n\\nfi(x) = fX (x|θi)\\n\\nThis last sum is an expectation,\\n\\nf (x) =\\n\\nn(cid:88)i=1\\n\\nfi(x|θi)fΘ(θi).\\n\\nMore frequently, we see the case of a continuous mixture, Here the random variable Θ is continuous with density\\n\\nfunction fΘ(θ) and\\n\\nf (x) = Ef (x|Θ).\\n\\nf (x) = Ef (x|Θ) =(cid:90) f (x|θ)fΘ(θ)dθ.\\n\\nRemark 9.18. We have noted that several families of random variables, for example, the gamma random variables,\\nwere motivated by having one of its parameters taking on integral values. However, the density function makes sense\\nfor a range of real numbers. One additional case where this holds is the negative binomial family. Like the gamma\\nfamily, we use the fact that the gamma function is a generalization of the factorial function to all non-negative numbers.\\n(See identity (9.5).) Recall that N egbin(n, p) has mass function\\n\\nfX (x|n, p) =(cid:18)n + x − 1\\n\\nx\\n\\n(cid:19)pn(1 − p)x =\\n\\npn(1 − p)x.\\nNow, replace n with α and write the density with two parameters, α and p. Then, N egbin(α, p) has density\\n\\npn(1 − p)x =\\n\\n(n + x − 1)!\\n(n − 1)!x!\\n\\nΓ(n + x)\\nΓ(n)x!\\n\\nfX (x|α, p) =\\n\\nΓ(α + x)\\nΓ(α)x!\\n\\npα(1 − p)x,\\n\\nx = 0, 1, 2, . . . .\\n\\nExercise 9.19. A Γ(α, β) mixture of Poisson random variables is a negative binomial random variable with parame-\\nters\\n\\nα\\n\\nand\\n\\nβ\\n\\n.\\n\\n1 + β\\n\\nWe will see how continuous mixtures play a central role when we introduces the Bayesian approach to estimation.\\n\\n9.4 R Commands\\nR has built in commands so that computation a variety of values for many families of distributions is straightforwrd.\\n• dfamily(x, parameters) is the mass function (for discrete random variables) or probability density (for\\n\\ncontinuous random variables) of family evaluated at x.\\n\\n169\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\n• qfamily(p, parameters) returns x satisfying P{X ≤ x} = p, the p-th quantile where X has the given\\n\\ndistribution,\\n\\n• pfamily(x, parameters) returns P{X ≤ x} where X has the given distribution.\\n• rfamily(n, parameters) generates n random variables having the given distribution.\\n\\n9.5 Summary of Properties of Random Variables\\nFor the tables below, the parameters are presented in the order required by R.\\n\\n9.5.1 Discrete Random Variables\\n\\nparameters mean\\n\\nrandom variable\\nBernoulli\\nbinomial\\ngeometric\\nhypergeometric\\nnegative binomial\\nPoisson\\nuniform\\n\\nR\\n*\\n\\nbinom\\ngeom\\nhyper\\n\\nnbinom\\n\\npois\\n\\nsample\\n\\np\\n\\nn, p\\n\\np\\n\\nm, n, k\\n\\nα, p\\n\\nλ\\n\\na, b\\n\\nvariance\\np(1 − p)\\nnp(1 − p)\\n1−p\\np2\\nm+n · m+n−k\\nn\\nm+n−1\\nα 1−p\\np2\\nλ\\n\\n(b−a+1)2−1\\n\\nk m\\nm+n ·\\n\\np\\nnp\\n1−p\\np\\n\\nkm\\nm+n\\n\\nα 1−p\\np\\nλ\\n\\nb−a+1\\n\\n2\\n\\n12\\n\\ngenerating function\\n\\np\\n\\n(1 − p) + pz\\n((1 − p) + pz)n\\n1−(1−p)z\\n(cid:16)\\n1−(1−p)z(cid:17)α\\n\\nexp(−λ(1 − z))\\n1−zb−a+1\\nb−a+1\\n\\n1−z\\n\\nza\\n\\np\\n\\n∗For a Bernoulli random variable, use the binomial commands with n=1 trial.\\n\\nExample 9.20. We give several short examples that use the R commands for discrete random variables.\\n\\n• To ﬁnd the values of the mass function fX (x|4, 0.7) for a binomial random variable 4 trials with probability of\\n\\nsuccess p = 0.7.\\n\\n> x<-0:4\\n> binomprob<-dbinom(x,4,0.7)\\n> data.frame(x,binomprob)\\n\\nx binomprob\\n0.0081\\n0.0756\\n0.2646\\n0.4116\\n0.2401\\n\\n1 0\\n2 1\\n3 2\\n4 3\\n5 4\\n\\n• We can compare this to simulations of 10,000 independent Binom(4, 0.7) random variables.\\n\\n> x<-rbinom(10000,4,0.7)\\n> table(x)/10000\\nx\\n\\n2\\n\\n1\\n\\n0\\n\\n4\\n0.0092 0.0809 0.2562 0.4115 0.2422\\n> x<-rbinom(10000,4,0.7)\\n> table(x)/10000\\nx\\n\\n3\\n\\n0\\n\\n4\\n0.0072 0.0704 0.2720 0.4116 0.2388\\n\\n1\\n\\n2\\n\\n3\\n\\n170\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\n• To ﬁnd the probability P{X ≤ 3} for X, a geometric random variable with probability of success p = 0.3 enter\\n\\npgeom(3,0.3). R returns the answer 0.7599.\\n\\n• To give independent observations uniformly on a set S, use the sample command using replace=TRUE.\\n\\nHere is an example using 50 repeated rolls of a die\\n\\n> S<-1:6\\n> (x<-sample(S,50,replace=TRUE))\\n\\n[1] 4 2 2 5 4 5 2 3 2 6 3 6 4 4 6 5 6 4 6 4 4 6 1 1 1 5 3 5 3 1 3 4 6 3 5 6 2 4 4 4\\n\\n[41] 4 4 3 2 4 5 1 3 2 1\\n> table(x)\\nx\\n\\n1\\n6\\n\\n2 3\\n4\\n7 8 14\\n\\n5\\n7\\n\\n6\\n8\\n\\n> (x<-sample(S,50,replace=TRUE))\\n\\n[1] 2 1 5 6 2 5 1 4 1 6 3 3 2 1 5 5 3 1 2 5 4 2 5 6 4 6 6 5 6 5 1 3 5 1 1 6 3 5 3 6\\n\\n[41] 3 1 1 1 5 4 5 3 6 3\\n> table(x)\\nx\\n\\n1\\n11\\n\\n2 3\\n5\\n9\\n\\n4\\n5\\n4 12\\n\\n6\\n9\\n\\nR\\n\\nα, β\\n\\nexp\\n\\nparameters\\n\\nbeta\\nchisq\\n\\n9.5.2 Continuous Random Variables\\nrandom variable\\nbeta\\nchi-squared\\nexponential\\nlog-normal\\nF\\ngamma\\nnormal\\nPareto\\nt\\nuniform\\n\\nα, β\\nµ, σ2\\nα, β\\nν, a, σ\\n\\nν\\nλ\\nµ, σ\\nν1, ν2\\n\\ngamma\\nnorm\\n\\npareto\\n\\nlnorm\\n\\nunif\\n\\na, b\\n\\nf\\n\\nt\\n\\nmean\\n\\nα\\n\\nα+β\\n\\nν\\n1\\nλ\\n\\nexp(µ + σ2/2)\\nν2\\nν2−2 , ν2 > 2\\n\\nα\\nβ\\nµ\\n\\nαβ\\n\\nβ−1, (β > 1)\\na, (ν > 1)\\n\\na+b\\n\\n2\\n\\nvariance\\n\\nαβ\\n\\n(α+β)2(α+β+1)\\n\\n2ν\\n1\\nλ2\\n\\n(eσ2\\n\\n− 1) exp(2µ + σ2)\\nν1(ν2−4)(ν2−2)2\\n\\nν1+ν2−2\\n\\n2ν2\\n2\\n\\nα\\nβ2\\nσ2\\n\\nα2β\\n\\n(β−1)2(β−2), (β > 2)\\n\\nσ2 a\\n\\na−2 , (ν > 2)\\n\\n(b−a)2\\n\\n12\\n\\ncharacteristic function\\n\\nF1,1(a, b; iθ\\n2π )\\n(1−2iθ)ν/2\\n\\n1\\n\\niλ\\n\\nθ+iλ\\n\\nθ+iβ(cid:17)α\\n(cid:16) iβ\\nexp(iµθ − 1\\n\\n2 σ2θ2)\\n\\n−i exp(iθb)−exp(iθa)\\n\\nθ(b−a)\\n\\nExample 9.21. We continue with examples that use the R commands for continuous random variables.\\n\\n• The standard normal random variable has mean 0 and standard deviation. The value of the distribution function\\n\\nfor three standard deviations below and blow the mean.\\n\\n> z<- -3:3\\n> data.frame(z,pnorm(z))\\n\\nz\\n\\npnorm.z.\\n1 -3 0.001349898\\n2 -2 0.022750132\\n3 -1 0.158655254\\n0 0.500000000\\n4\\n1 0.841344746\\n5\\n6\\n2 0.977249868\\n3 0.998650102\\n7\\n\\n171\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\n• To ﬁnd the deciles of a gamma random variable with α = 4 and β = 5\\n\\n> decile<-seq(0,0.9,0.1)\\n> value<-qgamma(decile,4,5)\\n> data.frame(decile,value)\\n\\ndecile\\n\\nvalue\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n\\n0.0 0.0000000\\n0.1 0.3489539\\n0.2 0.4593574\\n0.3 0.5527422\\n0.4 0.6422646\\n0.5 0.7344121\\n0.6 0.8350525\\n0.7 0.9524458\\n0.8 1.1030091\\n0.9 1.3361566\\n\\n• The command rnorm(200,1,0.5) was used to create the histograms in Figure 9.8.\\n• Use the curve command to plot density and distribution functions. Thus was accomplished in Figure 9.7 using\\ndgamma for the density of a gamma random variable. For cumulative distribution functions use pdist and\\nsubstitute for dist the appropriate command from the table above.\\nTo add points for the deciles on the plot of the Γ(4, 5) density, we use the following R commands.\\n\\n> curve(dgamma(x,4,5),0,2.0)\\n> points(value,dgamma(value,4,5),\\n\\npch=19,col=\"blue\")\\n\\nExercise 9.22. Add point on the graph of the distribution function for\\nthe standard normal corresponding to the standard deviations in the\\nexample above.\\n\\n9.6 Answers to Selected Exercises\\n9.1. For y = 1, 2, . . .,\\n\\nfY (y) = P{Y = y} = P{X + 1 = y} = P{X = y− 1} = p(1− p)y−1.\\n\\n9.2. Write X a N egbin(n, p) random variable as X = Y1 + ··· + Yn\\nwhere the Yi are independent random variable. Then,\\n\\nFigure 9.10: Density for a Γ(4, 5) random\\nvariable. The plots contain the values Indi-\\ncated in blue) on the density plot matching the\\ndeciles.\\n\\nEX = EY1 + ··· + EYn =\\n\\n1 − p\\np\\n\\n+ ··· +\\n\\n1 − p\\np\\n\\n=\\n\\nn(1 − p)\\n\\np\\n\\nand because the Yi are independent\\n\\nVar(X) = Var(Y1) + ··· + Var(Yn) =\\n\\n1 − p\\np2 + ··· +\\n\\n1 − p\\np2 =\\n\\nn(1 − p)\\n\\np2\\n\\n172\\n\\n0.00.51.01.52.00.00.20.40.60.81.0xdgamma(x, 4, 5)\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\n9.3. By taking the logarithm, the limit above is equivalent to\\n\\nn→∞ n ln(cid:18)1 −\\n\\nlim\\n\\nλ\\n\\nn(cid:19) = −λ.\\n\\nNow change variables letting \\x01 = 1/n, then the limit becomes\\n\\nlim\\n\\x01→0\\n\\nln(1 − \\x01λ)\\n\\n\\x01\\n\\n= −λ.\\n\\nThe limit has the indeterminant form 0/0. Thus, by l’Hˆopital’s rule, we can take the derivative of the numerator and\\ndenominator to obtain the equivalent problem\\n\\nlim\\n\\x01→0\\n\\n−λ\\n1 − \\x01λ\\n\\n= −λ.\\n\\n9.4. We have mass functions\\n\\nThus,\\n\\nfX1(x1) =\\n\\n−λ1\\n\\ne\\n\\nλx1\\n1\\nx1!\\n\\nfX2 (x2) =\\n\\n−λ2\\n\\ne\\n\\nλx2\\n2\\nx2!\\n\\nP{X1 = x1, X2 = x − x1} =\\n\\nP{X1 = x1}P{X2 = x − x1}\\n\\nx(cid:88)x1=0\\nfX1+X2(x) = P{X1 + X2 = x} =\\n−λ2 =(cid:32) x(cid:88)x1=0\\n−λ1 λx−x1\\n(x − x1)!\\n2 (cid:33) 1\\n1 λx−x1\\nx!\\nλx1\\n\\nx(cid:88)x1=0\\n=(cid:32) x(cid:88)x1=0\\n\\nx1!(x − x1)!\\n\\nλx1\\n1\\nx1!\\n\\nx!\\n\\n=\\n\\ne\\n\\ne\\n\\ne\\n\\n2\\n\\nx(cid:88)x1=0\\n2 (cid:33) e\\n1 λx−x1\\nλx1\\n\\n−(λ1+λ2)\\n\\n1\\n\\nx1!(x − x1)!\\n\\n−(λ1+λ2) =\\n\\n(λ1 + λ2)x\\n\\nx!\\n\\n−(λ1+λ2).\\n\\ne\\n\\nThis is the probability mass function for a P ois(λ1+λ2) random variable. The last equality uses the binomial theorem.\\n\\n9.5. Using the deﬁnition of the choose function\\n\\nfX (x|m, n, k) = (cid:0)b\\n\\nx(cid:1)(cid:0) n\\nk−x(cid:1)\\n(cid:0)m+n\\nk (cid:1) =\\n\\n(m)x\\n\\n(n)k−x\\n(k−x)!\\n\\nx!\\n(m+n)k\\n\\nk!\\n\\n=\\n\\nk!\\n\\nx!(k − x)!\\n\\n(m)x(n)k−x\\n(m + n)k\\n\\n=(cid:18)k\\nx(cid:19) (m)x(n)k−x\\n\\n(m + n)k\\n\\n.\\n\\n9.6. Cov(X1, X2) = EX1X2 − EX1EX2. Now,\\n\\nEX1 = EX2 =\\n\\nm\\n\\nm + n\\n\\n= p\\n\\nand\\n\\nEX1X2 = P{X1X2 = 1} = P{X2 = 1, X1 = 1} = P{X2 = 1|X1 = 1}P{X1 = 1} =\\n\\nm − 1\\nm + n − 1 ·\\n\\nm\\n\\nm + n\\n\\n.\\n\\nThus,\\n\\nCov(X1, X2) =\\n\\n=\\n\\nm\\n\\nm + n −(cid:18) m\\n\\nm − 1\\nm + n − 1 ·\\nm + n(cid:18) (m + n)(m − 1) − m(m + n − 1)\\n\\n(m + n)(m + n − 1)\\n\\nm + n(cid:19)2\\n\\nm + n(cid:18) m − 1\\nm + n − 1 −\\nm + n(cid:18)\\n(cid:19) =\\n\\nm + n(cid:19)\\n(m + n)(m + n − 1)(cid:19) = −\\n\\n−n\\n\\nm\\n\\nm\\n\\nm\\n\\nm\\n\\n=\\n\\nnp\\n\\nN (N − 1)\\n\\n173\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nwhere, as above, N = m + n.\\n9.8. For the mean\\n\\nE(a,b)X =(cid:90) b\\n\\nb2 − a2\\n2(b − a)\\nthe average of endpoints a and b. For the variance, we ﬁrst ﬁnd the second moment\\n\\nxfX (x|a, b) dx =\\n\\n2(b − a)\\n\\nx dx =\\n\\n=\\n\\n1\\n\\na\\n\\na\\n\\nb\\n\\n=\\n\\n(b − a)(b + a)\\n\\n2(b − a)\\n\\n=\\n\\nb + a\\n\\n2\\n\\n,\\n\\nE(a,b)X 2 =(cid:90) b\\n\\na\\n\\nThus,\\n\\nx2fX (x|a, b) dx =\\n\\nx2 dx =\\n\\n1\\n\\n3(b − a)\\n\\n=\\n\\nb3 − a3\\n3(b − a)\\n\\n(b − a)(a2 + ab + b2)\\n\\n3(b − a)\\n\\n=\\n\\na2 + ab + b2\\n\\n3\\n\\n.\\n\\nx2(cid:12)(cid:12)(cid:12)\\nx3(cid:12)(cid:12)(cid:12)\\n\\na\\n\\nb\\n\\nVar(a,b)(X) =\\n\\na2 + ab + b2\\n\\n3\\n\\n4b2 + 4ab + 4b2\\n\\n12\\n\\n3a2 + 6ab + 3a2\\n\\n12\\n\\n=\\n\\na2 − 2ab + b2\\n\\n12\\n\\n=\\n\\n(a − b)2\\n\\n12\\n\\n.\\n\\n−\\n\\na\\n\\n1\\n\\n1\\n\\nb − a(cid:90) b\\nb − a(cid:90) b\\n−(cid:18) b + a\\n2 (cid:19)2\\n\\n=\\n\\na\\n\\n9.9. Because {T > t + s} ⊂ {T > t}, we have for T ∼ Exp(λ),\\nP{T > t + s}\\nP{T > t}\\n\\nP{T > t + s, T > t}\\n\\nP{T > t + s|T > t} =\\n9.10. Using integration by parts\\n\\nP{T > t}\\n\\n=\\n\\nu(x) = xt\\nu(cid:48)(z) = txt−1\\n\\nv(x) = −e−x\\nv(cid:48)(x) = e−x\\n\\n=\\n\\nexp(−λ(t + s))\\n\\nexp(−λt)\\n\\n= exp(−λs) = P{T > s}.\\n\\nTo obtain the gamma function recursion formula\\n\\nΓ(t + 1) =(cid:90) ∞\\nThe ﬁrst term is 0 because xte−x → 0 as x → ∞.\\nFor the case n = 1, Γ(1) = (cid:82) ∞\\nn = 1. Next, using (9.7),\\n\\nxte\\n\\n0\\n\\n−x dx = −xte\\n\\n∞\\n\\n0\\n\\n+ t(cid:90) ∞\\n\\n0\\n\\nxt−1e\\n\\n−x dx = tΓ(t).\\n\\n(9.7)\\n\\n0 e−s ds = 1 = (1 − 1)!. the veriﬁes the identity Γ(n) = (n − 1)! for the case\\n\\n−x(cid:12)(cid:12)(cid:12)\\n\\nΓ(n + 1) = nΓ(n) = n · (n − 1)! = n!.\\n\\nThus, by induction we have the formula for all integer values.\\n9.11. Recall that the derivative of arcsin(t)\\n\\nd\\ndt\\n\\narcsin(t) =\\n\\n1\\n\\n√1 − t2\\n\\n.\\n\\nThus, by the chain rule,\\n\\n(cid:48)\\n\\nF\\n\\n(x|1/2, 1/2 ) =\\n\\n=\\n\\n2\\nπ\\n1\\nπ\\n\\narcsin(√x) =\\n\\nd\\ndx\\n\\n1\\n\\n√1 − x\\n\\n1\\n√x\\n\\n=\\n\\n1\\nπ\\n\\nNow, use the fact that Γ(1/2)2 = π and Γ(1) = 1.\\n9.12. In order to be a probability density, we have that\\n\\nd\\ndx\\n\\n=\\n\\n√x\\nx1/2−1(1 − x)1/2−1.\\n\\n1\\nπ\\n\\n1\\n\\n2\\nπ\\n\\n1\\n\\n√1 − x\\n(cid:112)x(1 − x)\\n\\nΓ(a)Γ(b)\\nΓ(a + b)\\n\\n=(cid:90) 1\\n\\n0\\n\\nxa−1(1 − x)b−1 dx.\\n\\n174\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nWe use this identity and (9.4) to compute the ﬁrst two moments\\n\\nE(α,β)X =(cid:90) 1\\n\\nxfX (x|α, β) dx =\\n\\n0\\nΓ(α + β)Γ(α + 1)\\nΓ(α + β + 1)Γ(α)\\n\\n=\\n\\n=\\n\\nand\\n\\nΓ(α + β)\\n\\nΓ(α)Γ(β)(cid:90) 1\\n\\n0\\n\\nxα(1 − x)β−1 dx =\\n\\nΓ(α + β)\\nΓ(α)Γ(β) ·\\n\\nΓ(α + 1)Γ(β)\\nΓ(α + β + 1)\\n\\nΓ(α + β)αΓ(α)\\n\\n(α + β)Γ(α + β)Γ(α)\\n\\n=\\n\\nα\\n\\nα + β\\n\\n.\\n\\nE(α,β)X 2 =(cid:90) 1\\n\\nx2fX (x|α, β) dx =\\n\\n0\\nΓ(α + β)Γ(α + 2)\\nΓ(α + β + 2)Γ(α)\\n\\n=\\n\\nΓ(α + β)\\n\\nΓ(α)Γ(β)(cid:90) 1\\n\\n0\\n\\nxα+1(1 − x)β−1 dx =\\n\\nΓ(α + β)(α + 1)αΓ(α)\\n\\nΓ(α + β)\\nΓ(α)Γ(β) ·\\n(α + 1)α\\n\\nΓ(α + 2)Γ(β)\\nΓ(α + β + 2)\\n\\n.\\n\\n=\\n\\n(α + β + 1)(α + β)Γ(α + β)Γ(α)\\n\\n(α + β + 1)(α + β)\\n\\n=\\n\\nThus,\\n\\nVar(α,β)(X) = E(α,β)X 2 − (E(α,β)X)2 =\\n\\n(α + 1)α\\n\\n(α + β + 1)(α + β) −(cid:18) α\\n\\nα + β(cid:19)2\\n\\n=\\n\\n(α + 1)α(α + β) − α2(α + β + 1)\\n\\n(α + β + 1)(α + β)2\\n\\n=\\n\\nαβ\\n\\n(α + β + 1)(α + β)2\\n\\n9.13. We ﬁrst consider the case of g increasing on the range of the random variable X. In this case, g−1 is also an\\nincreasing function.\\n\\nTo compute the cumulative distribution of Y = g(X) in terms of the cumulative distribution of X, note that\\n\\nFY (y) = P{Y ≤ y} = P{g(X) ≤ y} = P{X ≤ g\\n\\n−1(y)} = FX (g\\n\\n−1(y)).\\n\\nNow use the chain rule to compute the density of Y\\n\\nfY (y) = F\\n\\n(cid:48)\\nY (y) =\\n\\nd\\ndy\\n\\nFX (g\\n\\n−1(y)) = fX (g\\n\\n−1(y))\\n\\n−1(y).\\n\\ng\\n\\nd\\ndy\\n\\nFor g decreasing on the range of X,\\n\\nFY (y) = P{Y ≤ y} = P{g(X) ≤ y} = P{X ≥ g\\n\\n−1(y)} = 1 − FX (g\\n\\n−1(y)),\\n\\nand the density\\n\\nfY (y) = F\\n\\n(cid:48)\\nY (y) = −\\n\\nd\\ndy\\n\\nFX (g\\n\\n−1(y)) = −fX (g\\n\\n−1(y))\\n\\nd\\ndy\\n\\n−1(y).\\n\\ng\\n\\nFor g decreasing, we also have g−1 decreasing and consequently the density of Y is indeed positive,\\n\\nWe can combine these two cases to obtain\\n\\n9.14. Let X be a normal random variable, then Y = exp X is log-normal. Thus y = g(x) = ex, g−1(y) = ln y, and\\ndy g−1(y) = 1\\n\\ny . Note that y must be positive. Thus,\\n\\nd\\n\\nfY (y) = fX (g\\n\\nd\\ndy\\n\\ng\\n\\n−1(y)(cid:12)(cid:12)(cid:12)(cid:12) .\\nexp(cid:18)−\\n\\n1\\n\\nσ√2π\\n\\n−1(y))(cid:12)(cid:12)(cid:12)(cid:12)\\n−1(y)(cid:12)(cid:12)(cid:12)(cid:12) =\\n\\n175\\n\\nfY (y|µ, σ) = fX (g\\n\\nd\\ndy\\n\\ng\\n\\n−1(y))(cid:12)(cid:12)(cid:12)(cid:12)\\n\\n(ln y − µ)2\\n\\n2σ2\\n\\n(cid:19) 1\\n\\ny\\n\\n.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\n9.15. Let X be Exp(λ) and y = g(x) = ex. As in the previous exercise,\\n\\nfY (y|λ) = fX (g\\n\\n−1(y))(cid:12)(cid:12)(cid:12)(cid:12)\\n\\nd\\ndy\\n\\ng\\n\\n−1(y)(cid:12)(cid:12)(cid:12)(cid:12) = exp(−λ ln(y))\\n\\nwhich is the density of a P areto(1, λ) random variable.\\n9.16. Let X be a standard normal random variable, then Y = X 2 is χ2\\n\\n1\\ny\\n\\n= λ exp(ln(y\\n\\n−λ))\\n\\n1\\ny\\n\\n= λy\\n\\n−λ 1\\ny\\n\\n=\\n\\nλ\\n\\nyλ+1\\n\\n1. From the hint, the distribution function of Y ,\\n\\nNow take a derivative with respect to y.\\n\\nFY (y) = P{Y ≤ y} = P{−√y ≤ X ≤ √y} = FX (√y) − FX (−√y)\\nfY (y) = P{Y ≤ y} = fX (√y)(cid:18) 1\\n\\n2√y(cid:19) − fX (−√y)(cid:18)−\\n\\n1\\n\\n2√y(cid:19)\\n2(cid:17)(cid:19) 1\\n\\ny\\n\\n2√y\\n\\n1\\n2√y\\n1\\n√2π\\n\\n= (fX (√y) + fX (−√y))\\n=(cid:18) 1\\n2(cid:17) +\\nexp(cid:16)−\\n2(cid:17) 1\\nexp(cid:16)−\\n\\n√2π\\n1\\n√2π\\n\\n√y\\n\\n=\\n\\ny\\n\\ny\\n\\nexp(cid:16)−\\n\\nFinally, Γ(1/2) = √π.\\n9.17. For both parts, we use the identity in Exercise 9.11.\\n\\n(a) Let y = g(s) = s(cid:112)m/kT , then g−1(y) = y(cid:112)kT /m and\\nfY (y) = fS(y(cid:112)kT /m)(cid:112)kT /m =\\n\\n(b) Let x = g(y) = y2, then g−1(x) = √x and\\n\\n1\\n\\n(cid:112)(2π)3\\n\\n−x/2 1\\n2√x\\n3 density function. Notice that Γ(3/2) = √π/2.\\n\\nfX (x) =(cid:114) 2\\n\\nxe\\n\\nπ\\n\\nThis is the χ2\\n\\n4πy2e\\n\\n−y2/2 =(cid:114) 2\\n\\nπ\\n\\n−y2/2.\\n\\ny2e\\n\\n=\\n\\n1\\n√2π\\n\\nx1/2e\\n\\n−x/2.\\n\\n9.19. The two families of densities are\\n\\nλx\\nP ois(λ) fX (x|λ) =\\nx!\\nΓ(α, β) fΛ(λ|α, β) =\\n\\ne\\n\\n−λ, x = 0, 1, ,···\\n−βλ λ > 0\\n\\nλα−1e\\n\\nβα\\nΓ(α)\\n\\nThe mixture has mass function\\n\\nf (x) =(cid:90) ∞\\n\\n0\\n\\n0\\n\\nβα\\n\\nλxe\\n\\nfX (x|λ)fΛ(λ|α, β)dλ\\n−λλα−1e\\n\\nx!Γ(α)(cid:90) ∞\\nx!Γ(α)(cid:90) ∞\\nx!Γ(α)(1 + β)x+α(cid:90) ∞\\n\\n0 (cid:18) u\\n\\n1 + β(cid:19)x+α−1\\n\\nβα\\n\\nβα\\n\\n0\\n\\nΓ(x + α)\\nx!Γ(α)\\n\\nβα\\n\\n(1 + β)x+α =\\n\\n=\\n\\n=\\n\\n=\\n\\n=\\n\\n−βλ dλ =\\n\\nλx+α−1e\\n\\n−λ(1+β) dλ\\n\\nβα\\n\\nx!Γ(α)(cid:90) ∞\\n\\n0\\n\\nu = λ(1 + β)\\n\\n−u du\\ne\\n1 + β\\n−u du\\n\\nux+α−1e\\n\\nΓ(x + α)\\n\\nx!Γ(α) (cid:18) β\\n\\n1 + β(cid:19)α(cid:18) 1\\n\\n1 + β(cid:19)x\\n\\n,\\n\\n176\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExamples of Mass Functions and Densities\\n\\nFigure 9.11: Distiribution function to the standard normal Z. Values for P{Z ≤ z} for z = −3, −2, −1, 0, 1, 2, 3 indicated in red.\\n\\nthe mass function of a N egbin(α, β/(1 + β)) random variable.\\n9.22. First we plot the distribution function for the normal, then we add the points.\\n\\n> curve(pnorm(x),-3.5,3.5,xlab=c(\"z\"),ylab=c(\"probability\"))\\n> z<- -3:3\\n> points(z,pnorm(z),pch=19,col=\"red\")\\n\\n.\\n\\n177\\n\\n-3-2-101230.00.20.40.60.81.0zprobability\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\n178\\n\\n\\x0cTopic 10\\n\\nThe Law of Large Numbers\\n\\nIndividuals vary, but percentages remain constant. So says the statistician – Sir Arthur Conan Doyle\\n\\n10.1 Introduction\\nA public health ofﬁcial want to ascertain the mean weight of healthy newborn babies in a given region under study. If\\nwe randomly choose babies and weigh them, keeping a running average, then, because individual weights vary, at the\\nbeginning we might see some larger ﬂuctuations in our average. However, as we continue to make measurements, we\\nexpect to see this running average settle and converge to the true mean weight of newborn babies. This phenomena is\\ninformally known as the law of averages. In probability theory, we call this the law of large numbers.\\nExample 10.1. We can simulate babies’ weights with independent normal random variables, mean 3 kg and standard\\ndeviation 0.5 kg. The following R commands perform this simulation and computes a running average of the heights.\\nThe results are displayed in Figure 10.1.\\n\\n> n<-1:100\\n> x<-rnorm(100,3,0.5)\\n> s<-cumsum(x)\\n> plot(s/n,xlab=\"n\",ylim=c(2,4),type=\"l\")\\n\\nHere, we begin with a sequence X1, X2, . . . of random variables having a common distribution. Their average, the\\n\\nsample mean,\\n\\n¯X =\\n\\n1\\nn\\n\\nSn =\\n\\n1\\nn\\n\\n(X1 + X2 + ··· + Xn),\\n\\nis itself a random variable.\\n\\nIf the common mean for the Xi’s is µ, then by the linearity property of expectation, the mean of the average,\\n\\nE(cid:20) 1\\n\\nn\\n\\nSn(cid:21) =\\n\\n1\\nn\\n\\nESn =\\n\\n1\\nn\\n\\n(EX1 + EX2 + ··· + EXn) =\\n\\n1\\nn\\n\\n(µ + µ + ··· + µ) =\\n\\n1\\nn\\n\\nnµ = µ.\\n\\n(10.1)\\n\\nis also µ.\\n\\nIf, in addition, the Xi’s are independent with common variance σ2, then ﬁrst by the quadratic identity and then the\\n\\nPythagorean identity for the variance of independent random variables, we ﬁnd that the variance of ¯X,\\n\\nσ2\\n¯X = Var(\\n\\n1\\nn\\n\\nSn) =\\n\\n1\\nn2 (Var(X1) + Var(X2) +··· + Var(Xn)) =\\n\\n1\\nn2 (σ2 + σ2 +··· + σ2) =\\n\\n1\\nn2 nσ2 =\\n\\n1\\nn\\n\\nσ2. (10.2)\\n\\nSo the mean of these running averages remains at µ but the variance is decreasing to 0 at a rate inversely propor-\\ntional to the number of terms in the sum. For example, the mean of the average weight of 100 newborn babies is 3\\n\\n179\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nFigure 10.1: Four simulations of the running average Sn/n, n = 1, 2, . . . , 100 for independent normal random variables, mean 3 kg and standard\\ndeviation 0.5 kg. Notice that the running averages have large ﬂuctuations for small values of n but settle down converging to the mean value µ =\\n3 kilograms for newborn birth weight. This behavior could have been predicted using the law of large numbers. The size of the ﬂuctuations, as\\nmeasured by the standard deviation of Sn/n, is σ/√n where σ is the standard deviation of newborn birthweight.\\n\\nkilograms, the standard deviation is σ ¯X = σ/√n = 0.5/√100 = 0.05 kilograms = 50 grams. For 10,000 males,\\nthe mean remains 3 kilograms, the standard deviation is σ ¯X = σ/√n = 0.5/√10000 = 0.005 kilograms = 5 grams.\\nNotice that\\n\\n• as we increase n by a factor of 100,\\n• we decrease σ ¯X by a factor of 10.\\nThe mathematical result, the law of large numbers, tells us that the results of these simulation could have been\\n\\nanticipated.\\n\\nTheorem 10.2. For a sequence of independent random variables X1, X2, . . . having a common distribution, their\\n\\n180\\n\\n0204060801002.02.53.03.54.0ns/n0204060801002.02.53.03.54.0ns/n0204060801002.02.53.03.54.0ns/n0204060801002.02.53.03.54.0ns/n\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nrunning average\\n\\n1\\nn\\n\\nSn =\\n\\n1\\nn\\n\\n(X1 + ··· + Xn)\\n\\nhas a limit as n → ∞ if and only if this sequence of random variables has a common mean µ. In this case the limit is\\nµ.\\n\\nThe theorem also states that if the random variables do not have a mean, then as the next example shows, the limit\\nwill fail to exist. We shall show with the following example. When we look at methods for estimation, one approach,\\nthe method of moments, will be based on using the law of large numbers to estimate the mean µ or a function of µ.\\n\\nCare needs to be taken to ensure that the simulated random variables indeed have a mean. For example, use the\\nrunif command to simulate uniform transform variables, and choose a transformation Y = g(U ) that results in an\\nintegral\\n\\n(cid:90) 1\\n\\n0\\n\\ng(u) du\\n\\nthat does not converge. Then, if we simulate independent uniform random variables, the running average\\n\\n1\\nn\\n\\n(g(U1) + ··· + g(Un))\\n\\nwill not converge. This issue is the topic of the next exercise and example.\\n\\nExercise 10.3. Let U be a uniform random variable on the interval [0, 1]. Give the value for p for which the mean is\\nﬁnite and the values for which it is inﬁnite. Simulate the situation for a value of p for which the integral converges and\\na second value of p for which the integral does not converge and cheek has in Example 10.1 a plot of Sn/n versus n.\\n\\nExample 10.4. The standard Cauchy random variable X has density function\\n\\nfX (x) =\\n\\n1\\nπ\\n\\n1\\n\\n1 + x2\\n\\nx ∈ R.\\n\\nLet Y = |X|. In an attempt to compute the improper integral for EY = E|X|, note that\\n\\n(cid:90) b\\n−b |x|fX (x) dx = 2(cid:90) b\\n\\n0\\n\\n1\\nπ\\n\\nx\\n\\n1 + x2 dx =\\n\\n1\\nπ\\n\\nb\\n\\n0\\n\\n=\\n\\n1\\nπ\\n\\nln(1 + b2) → ∞\\n\\nln(1 + x2)(cid:12)(cid:12)(cid:12)\\n\\nas b → ∞. Thus, Y has inﬁnite mean. We now simulate 1000 independent Cauchy random variables.\\n\\n> n<-c(1:1000)\\n> y<-abs(rcauchy(1000))\\n> s<-cumsum(y)\\n> plot(s/n,xlab=\"n\",ylim=c(-6,6),type=\"l\")\\n\\nThese random variables do not have a ﬁnite mean. As you can see in Figure 10.2 that their running averages do\\nnot seem to be converging. Thus, if we are using a simulation strategy that depends on the law of large numbers, we\\nneed to check that the random variables have a mean.\\n\\nExercise 10.5. Using simulations, check the failure of the law of large numbers of Cauchy random variables. In the\\nplot of running averages, note that the shocks can jump either up or down.\\n\\n181\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nFigure 10.2: Four simulations of the running average Sn/n, n = 1, 2, . . . , 1000 for the absolute value of independent Cauchy random variables.\\nNote that the running average does not seem to be settling down and is subject to “shocks”. Because Cauchy random variables do not have a mean,\\nwe know, from the law of large numbers, that the running averages do not converge.\\n\\n10.2 Monte Carlo Integration\\n\\nMonte Carlo methods use stochastic simulations to approximate solutions to questions that are very difﬁcult to solve\\nanalytically. This approach has seen widespread use in ﬁelds as diverse as statistical physics, astronomy, population\\ngenetics, protein chemistry, and ﬁnance. Our introduction will focus on examples having relatively rapid computations.\\nHowever, many research groups routinely use Monte Carlo simulations that can take weeks of computer time to\\nperform.\\n\\nFor example, let X1, X2, . . . be independent random variables uniformly distributed on the interval [a, b] and write\\n\\n182\\n\\n02004006008001000024681012ns/n02004006008001000024681012n02004006008001000024681012n02004006008001000024681012n\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nfX for their common density..\\n\\nThen, by the law of large numbers, for n large we have that\\n\\nThus,\\n\\ng(X)n =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\ng(Xi) ≈ Eg(X1) =(cid:90) b\\n\\na\\n\\ng(x)fX (x) dx =\\n\\n(cid:90) b\\n\\na\\n\\ng(x) dx. ≈ (b − a)g(X)n.\\n\\nRecall that in calculus, we deﬁned the average of g to be\\n\\n1\\n\\nb − a(cid:90) b\\n\\na\\n\\ng(x) dx.\\n\\n1\\n\\nb − a(cid:90) b\\n\\na\\n\\ng(x) dx.\\n\\nWe can also interpret this integral as the expected value of g(X1).\\n\\nThus, Monte Carlo integration leads to a procedure for estimating integrals.\\n\\n• Simulate uniform random variables X1, X2, . . . , Xn on the interval [a, b].\\n• Evaluate g(X1), g(X2), . . . , g(Xn).\\n• Average this values and multiply by b − a to estimate the integral.\\n\\nExample 10.6. Let g(x) = (cid:112)1 + cos3(x) for x ∈ [0, π], to ﬁnd(cid:82) π\\n\\nfollowing R code.\\n\\n0 g(x) dx. The three steps above become the\\n\\n> x<-runif(1000,0,pi)\\n> g<-sqrt(1+cos(x)ˆ3)\\n> pi*mean(g)\\n[1] 2.991057\\n\\nExample 10.7. To ﬁnd the integral of g(x) = cos2(√x3 + 1) on the interval [−1, 2], we simulate n random variables\\n\\nuniformly using runif(n,-1,2) and then compute mean(cos(sqrt(xˆ3+1))ˆ2). The choices n = 25 and\\nn = 250 are shown in Figure 10.3\\n\\nThe variation in estimates for the integral can be described by the variance as given in equation (10.2).\\n\\nVar(g(X)n) =\\n\\n1\\nn\\n\\nVar(g(X1)).\\n\\nwhere σ2 = Var(g(X1)) = E(g(X1) − µg(X1))2 = (cid:82) b\\ndifﬁcult to estimate than(cid:82) b\\n\\na (g(x) − µg(X1))2fX (x) dx. Typically this integral is more\\na g(x) dx, our original integral of interest. However, we can see that the variance of the\\nestimator is inversely proportional to n, the number of random numbers in the simulation. Thus, the standard deviation\\nis inversely proportional to √n.\\n\\nMonte Carlo techniques are rarely the best strategy for estimating one or even very low dimensional integrals. R\\n\\ndoes integration numerically using the function and the integrate commands. For example,\\n\\n> g<-function(x){sqrt(1+cos(x)ˆ3)}\\n> integrate(g,0,pi)\\n2.949644 with absolute error < 3.8e-06\\n\\n183\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nFigure 10.3: Monte Carlo integration of g(x) = cos2(√x3 + 1) on the interval [−1, 2], with (left) n = 25 and (right) n = 250. g(X)n is the\\ng. This estimate is multiplied by 3, the length of the interval to give(cid:82) 2−1 g(x) dx. In this example, the estimate os the integral is 0.905 for n = 25\\n\\naverage heights of the n lines whose x values are uniformly chosen on the interval. By the law of large numbers, this estimates the average value of\\n\\nand 1.028 for n = 250. Using the integrate command, a more precise numerical estimate of the integral gives the value 1.000194.\\n\\nWith only a small change in the algorithm, we can also use this to evaluate high dimensional multivariate integrals.\\n\\nFor example, in three dimensions, the integral\\n\\nI(g) =(cid:90) b1\\n\\na1 (cid:90) b2\\n\\na2 (cid:90) b3\\n\\na3\\n\\ng(x, y, z) dz dy dx\\n\\ncan be estimated using Monte Carlo integration by generating three sequences of uniform random variables,\\n\\nX1, X2, . . . , Xn,\\n\\nY1, Y2, . . . , Yn,\\n\\nand Z1, Z2, . . . Zn\\n\\nThen,\\n\\nI(g) ≈ (b1 − a1)(b2 − a2)(b3 − a3)\\n\\nExample 10.8. Consider the function\\n\\ng(Xi, Yi, Zi).\\n\\n(10.3)\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\ng(x, y, z) =\\n\\n32x3\\n\\n3(y + z4 + 1)\\n\\nwith x, y and z all between 0 and 1.\\n\\nTo obtain a sense of the distribution of the approximations to the integral I(g), we perform 1000 simulations using\\n100 uniform random variable for each of the three coordinates to perform the Monte Carlo integration. The command\\nIg<-numeric(1000) creates a space for a vector of length 1000. This is added so that R creates a place ahead\\nof the simulations to store the results.\\n\\n> Ig<-numeric(1000)\\n> for(i in 1:1000){x<-runif(100);y<-runif(100);z<-runif(100);\\n\\ng<-32*xˆ3/(3*(y+zˆ4+1)); Ig[i]<-mean(g)}\\n\\n> hist(Ig)\\n\\n184\\n\\n-1.0-0.50.00.51.01.52.00.00.20.40.60.81.0xcos(sqrt(x^3 + 1))^2-1.0-0.50.00.51.01.52.00.00.20.40.60.81.0xcos(sqrt(x^3 + 1))^2\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nFigure 10.4: Histogram of 1000 Monte Carlo estimates for the integral(cid:82) 1\\n\\n(cid:82) 1\\n\\n0\\n\\n(cid:82) 1\\n\\n0 32x3/(y + z4 + 1) dx dy dz. The sample standard deviation is\\n\\n0\\n\\n0.188.\\n\\n> summary(Ig)\\n\\nMin. 1st Qu. Median\\n1.644\\n\\n1.507\\n\\n1.045\\n\\nMean 3rd Qu.\\n1.788\\n\\n1.650\\n\\nMax.\\n2.284\\n\\n> var(Ig)\\n[1] 0.03524665\\n> sd(Ig)\\n[1] 0.1877409\\n\\nThus, our Monte Carlo estimate the standard deviation of the estimated integral is 0.188.\\nAlternatively, this can be accomplished with the replicate command\\n\\n> g<-function(x,y,z) 32*xˆ3/(3*(y+zˆ4+1))\\n> Ig<-replicate(1000,mean(g(runif(100),runif(100),runif(100))))\\n\\nExercise 10.9. Estimate the variance and standard deviation of the Monte Carlo estimator for the integral in the\\nexample above based on n = 500 and 1000 random numbers.\\nExercise 10.10. How many observations are needed in estimating the integral in the example above so that the\\nstandard deviation of the average is 0.05?\\n\\nTo modify this technique for a region [a1, b1]× [a2, b2]× [a3, b3] use indepenent uniform random variables Xi, Yi,\\n\\nand Zi on the respective intervals, then\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\ng(Xi, Yi, Zi) ≈ Eg(X1, Y1, Z1) =\\n\\n1\\n\\n1\\n\\nb1 − a1\\n\\nb2 − a2\\n\\nThus, the estimate for the integral is\\n\\n(b1 − a1)(b2 − a2)(b3 − a3)\\n\\nn\\n\\n185\\n\\ng(x, y, z) dz dy dx.\\n\\na1 (cid:90) b2\\n\\na2 (cid:90) b3\\n\\na3\\n\\n1\\n\\nb3 − a3(cid:90) b1\\nn(cid:88)i=1\\n\\ng(Xi, Yi, Zi).\\n\\nHistogram of IgIgFrequency1.01.21.41.61.82.02.2050100150\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nExercise 10.11. Use Monte Carlo integration to estimate\\n\\n0 (cid:90) 2\\n(cid:90) 3\\n\\n−2\\n\\ndydx.\\n\\ncos(π(x + y))\\n\\n4(cid:112)1 + xy2\\n\\n(cid:90) 1\\n\\n0\\n\\ne−x/2\\n\\n(cid:112)x(1 − x)\\n\\n10.3 Importance Sampling\\nIn many of the large simulations, the dimension of the integral can be in the hundreds and the function g can be\\nvery close to zero for large regions in the domain of g. Simple Monte Carlo simulation will then frequently choose\\nvalues for g that are close to zero. These values contribute very little to the average. Due to this inefﬁciency, a more\\nsophisticated strategy is employed. In addition, in regions where g is rapidly changing, the answer can be sensitive\\nto the choice of points in the simulation. Importance sampling methods begin with the observation that a better\\nsampling strategy may be to concentrate the random points in those regions.\\n\\nFor example, for the integral\\n\\ndx,\\n\\n(10.4)\\n\\nthe integrand rapidly changing for values near x = 0 or x = 1. (See Figure 10.4) Thus, we can hope to have a more\\naccurate estimate by concentrating our sample points in these places.\\n\\nWith this in mind, we perform the Monte Carlo integration beginning with Y1, Y2, . . . independent random vari-\\nables with common densityfY . The goal is to ﬁnd a density fY that tracks the changes in g. The density fY is called\\nthe importance sampling function or the proposal density. With this choice of density, we deﬁne the importance\\nsampling weights so that\\n\\ng(y) = w(y)fY (y).\\n\\n(10.5)\\n\\nTo justify this choice, note that, the sample mean\\n\\nw(Y )n =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nw(Yi) ≈(cid:90) ∞\\n\\n−∞\\n\\nw(y)fY (y) dy =(cid:90) ∞\\n\\n−∞\\n\\ng(y)dy = I(g).\\n\\nThus, the average of the importance sampling weights, by the strong law of large numbers, still approximates the\\nintegral of g. This is an improvement over simple Monte Carlo integration if the variance decreases, i.e.,\\n\\nVar(w(Y1)) =(cid:90) ∞\\n\\n−∞\\n\\n(w(y) − I(g))2fY (y) dy = σ2\\n\\nf << σ2.\\n\\nAs the formula shows, this can be best achieved by having the weight w(y) be close to the integral I(g). Referring to\\nequation (10.5), we can now see that we should endeavor to have fY proportional to g.\\n\\nImportance leads to the following procedure for estimating integrals.\\n• Write the integrand g(x) = w(x)fY (x). Here fY is the density function for a random variable Y that is chosen\\n\\nto capture the changes in g.\\n\\n• Simulate variables Y1, Y2 . . . , Yn with density fY . This will sometimes require integrating the density function\\n−1\\nY (u), the quantile function.\\n−1\\nY (Ui) where U1, U2 . . . , Un, independent\\n\\nto obtain the distribution function FY (x), and then ﬁnding its inverse function F\\nThis sets up the use of the probability transform to obtain Yi = F\\nrandom variables uniformly distributed on the interval [0, 1].\\n\\n• Compute the average of w(Y1), w(Y2) . . . , w(Yn) to estimate the integral of g.\\nNote that the use of the probability transform removes the need to multiply b − a, the length of the interval.\\nExample 10.12. For the integral (10.4) we can use Monte Carlo simulation based on uniform random variables.\\n\\n186\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\n> Ig<-numeric(1000)\\n> for(i in 1:1000){x<-runif(100);g<-exp(-x/2)*1/sqrt(x*(1-x));Ig[i]<-mean(g)}\\n> summary(Ig)\\n\\nMin. 1st Qu. Median\\n2.425\\n\\n2.277\\n\\n1.970\\n\\nMean 3rd Qu.\\n2.583\\n\\n2.484\\n\\nMax.\\n8.586\\n\\n> sd(Ig)\\n[1] 0.3938047\\n\\nBased on a 1000 simulations, we ﬁnd a sample mean value of 2.484 and a sample standard deviation of 0.394.\\nBecause the integrand is changes rapidly near both x = 0 and x = 1, we choose look for a density fY to concentrate\\nthe random samples near the ends of the intervals.\\n\\nOur choice for the proposal density is a Beta(1/2, 1/2), then\\n\\non the interval [0, 1]. Thus the weight\\n\\nis the ratio g(x)/fY (x)\\n\\nfY (y) =\\n\\n1\\nπ\\n\\ny1/2−1(1 − y)1/2−1\\n\\nw(y) = πe\\n\\n−y/2\\n\\nFigure 10.5: The function g to be integrated (in black) and the Beta(1/2, 1/2) proposal density fX (in blue). Note how the proposal density\\nfollows the the integrand.\\n\\nAgain, we perform the simulation multiple times.\\n\\n> IS<-numeric(1000)\\n> for(i in 1:1000){y<-rbeta(100,1/2,1/2);w<-pi*exp(-y/2);IS[i]<-mean(w)}\\n> summary(IS)\\n\\nMin. 1st Qu. Median\\n2.483\\n\\n2.455\\n\\n2.321\\n\\n> var(IS)\\n[1] 0.0002105915\\n> sd(IS)\\n[1] 0.04377021\\n\\nMean 3rd Qu.\\n2.515\\n\\n2.484\\n\\nMax.\\n2.609\\n\\n187\\n\\n00.10.20.30.40.50.60.70.80.9100.511.522.533.544.55xg(x) = e−x/2/sqrt(x(1−x))fY(x)=1//sqrt(x(1−x))\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nBased on 1000 simulations, we ﬁnd a sample mean value again of 2.484 and a sample standard deviation of 0.044,\\nabout 1/9th the size of the Monte Carlo sample standard deviation. Part of the gain is illusory. Beta random variables\\ntake longer to simulate. If they require a factor more than 81 times longer to simulate, then the extra work needed\\nto create a good importance sample is not helpful in producing a more accurate estimate for the integral. Numerical\\nintegration gives\\n\\n> g<-function(x){exp(-x/2)*1/sqrt(x*(1-x))}\\n> integrate(g,0,1)\\n2.485054 with absolute error < 2e-06\\n\\nExercise 10.13. Evaluate the integral\\n\\ne−x\\n3√x\\n\\ndx\\n\\n(cid:90) 1\\n\\n0\\n\\n1000 times using n = 200 sample points using directly Monte Carlo integration and using importance sampling with\\nrandom variables having density\\n\\non the interval [0, 1]. For the second part, you will need to use the probability transform. Compare the means and\\nstandard deviations of the 1000 estimates for the integral. The integral is approximately 1.04969.\\n\\nfX (x) =\\n\\n2\\n3 3√x\\n\\nprovided that 1 − p > 0 or p < 1. For p > 1, we evaluate the integral in the interval [b, 1] and take a limit as b → 0,\\n\\n10.4 Answers to Selected Exercises\\n10.3. For p (cid:54)= 1, the expected value\\n\\nEU\\n\\n−p =(cid:90) 1\\n\\n0\\n\\n−p dp =\\nu\\n\\n1\\n1 − p\\n\\nFor p = 1,\\n\\n1\\n\\n0\\n\\n=\\n\\n1\\n1 − p\\n\\n< ∞\\n\\nu1−p(cid:12)(cid:12)(cid:12)\\n\\n(1 − b1−p) → ∞.\\n\\n−p dp =\\n\\nu\\n\\n(cid:90) 1\\n\\nb\\n\\n(cid:90) 1\\n\\nb\\n\\nu\\n\\nb\\n\\n1\\n\\n1\\n\\n=\\n\\n1\\n1 − p\\n\\n1\\n1 − p\\n\\nu1−p(cid:12)(cid:12)(cid:12)\\n−1 dp = ln u(cid:12)(cid:12)(cid:12)\\n(cid:90) 1\\nu1/2 du = 2u3/2(cid:12)(cid:12)1\\n\\nb\\n\\n)\\n\\n0 = 2\\n\\n= − ln b → ∞.\\n\\nWe use the case p = 1/2 for which the integral converges. and p = 2 in which the integral does not. Indeed,\\n\\n> par(mfrow=c(1,2))\\n> u<-runif(1000)\\n> x<-1/uˆ(1/2)\\n> s<-cumsum(x)\\n> plot(s/n,n,type=\"l\")\\n> x<-1/uˆ2\\n> s<-cumsum(x)\\n> plot(n,s/n,type=\"l\")\\n\\n10.5. Here are the R commands:\\n\\n188\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nFigure 10.6: Importance sampling using the density function fY to estimate(cid:82) 1\\n\\n0 g(x) dx. The weight w(x) = πe−x/2.\\n\\n> par(mfrow=c(2,2))\\n> x<-rcauchy(1000)\\n> s<-cumsum(x)\\n> plot (n,s/n,type=\"l\")\\n> x<-rcauchy(1000)\\n> s<-cumsum(x)\\n> plot (n,s/n,type=\"l\")\\n> x<-rcauchy(1000)\\n> s<-cumsum(x)\\n> plot (n,s/n,type=\"l\")\\n> x<-rcauchy(1000)\\n> s<-cumsum(x)\\n> plot (n,s/n,type=\"l\")\\n\\nThis produces in Figure 10.5. Notice the differences for the values on the x-axis\\n10.9. The standard deviation for the average of n observations is σ/√n where σ is the standard deviation for a single\\nobservation. From the output\\n> sd(Ig)\\n[1] 0.1877409\\n\\nWe have that 0.1877409 ≈ σ/√100 = σ/10. Thus, σ ≈ 1.877409. Consequently, for 500 observations, σ/√500 ≈\\n0.08396028. For 1000 observations σ/√1000 ≈ 0.05936889\\n10.10. For σ/√n = 0.05, we have that n = (σ/0.05)2 ≈ 1409.866. So we need approximately 1410 observations.\\n10.11. To view the surface for cos(π(x+y))\\n4√1+xy2\\n\\n, 0 ≤ x ≤ 3, −2 ≤ y ≤ 2, we type\\n\\n> x <- seq(0,3, len=30)\\n> y <- seq(-2,2, len=30)\\n> f <- outer(x, y, function(x, y)\\n(cos(pi*(x+y)))/(1+x*yˆ2)ˆ(1/4))\\n> persp(x,y,f,col=\"orange\",phi=45,theta=30)\\n\\nUsing 1000 random numbers uniformly distributed for both x and y, we have\\n\\n> x<-runif(1000,0,3)\\n> y<-runif(1000,-2,2)\\n> g<-(cos(pi*(x+y)))/(1+x*yˆ2)ˆ(1/4)\\n> 3*4*mean(g)\\n[1] 0.2452035\\n\\n189\\n\\nFigure 10.8: Surface plot of function\\nused in Exercise 10.11.\\n\\n020040060080010001.41.61.82.02.22.4ns/n020040060080010000100030005000ns/nxyf\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nFigure 10.7: Plots of running averages of Cauchy random variables.\\n\\nTo ﬁnish, we need to multiply the average of g as estimated by mean(g) by the area associated to the integral\\n(3 − 0) × (2 − (−2)) = 12.\\n10.13. For the direct Monte Carlo simulation, we have\\n\\n> Ig<-numeric(1000)\\n> for (i in 1:1000){x<-runif(200);g<-exp(-x)/xˆ(1/3);Ig[i]<-mean(g)}\\n> mean(Ig)\\n[1] 1.048734\\n> sd(Ig)\\n[1] 0.07062628\\n\\n190\\n\\n02004006008001000-2-101ns/n02004006008001000-2.5-1.5-0.50.5ns/n02004006008001000-15-10-50ns/n020040060080010000246ns/n\\x0cIntroduction to the Science of Statistics\\n\\nThe Law of Large Numbers\\n\\nFor the importance sampler, the integral is\\n\\n0\\n\\n3\\n\\n2(cid:90) 1\\nFX (x) =(cid:90) x\\n\\n0\\n\\n−xfX (x) dx.\\n\\ne\\n\\n2\\n3 3√t\\n\\n= x2/3.\\n\\nx\\n\\n0\\n\\ndt = t2/3(cid:12)(cid:12)(cid:12)\\n\\nTo simulate independent random variables with density fX, we ﬁrst need the cumulative distribution function for X,\\n\\nThen, to ﬁnd the probability transform, note that\\n\\nu = FX (x) = x2/3\\n\\nand x = F\\n\\n−1\\nX (u) = u3/2.\\n\\nThus, to simulate X, we simulate a uniform random variable U on the interval [0, 1] and evaluate U 3/2. This leads to\\nthe following R commands for the importance sample simulation:\\n\\n> ISg<-numeric(1000)\\n> for (i in 1:1000){u<-runif(200);x<-uˆ(3/2); w<-3*exp(-x)/2;ISg[i]<-mean(w)}\\n> mean(ISg)\\n[1] 1.048415\\n> sd(ISg)\\n[1] 0.02010032\\n\\nThus, the standard deviation using importance sampling is about 2/7-ths the standard deviation using simple Monte\\nCarlo simulation. Consequently, we can decrease the number of samples using importance sampling by a factor of\\n(2/7)2 ≈ 0.08.\\n\\n191\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n192\\n\\n\\x0cTopic 11\\n\\nThe Central Limit Theorem\\n\\nThe occurrence of the Gaussian probability density 1 = e−x2 in repeated experiments, in errors of\\nmeasurements, which result in the combination of very many and very small elementary errors, in diffusion\\nprocesses etc., can be explained, as is well-known, by the very same limit theorem, which plays a central\\nrole in the calculus of probability. - George Polya\\n\\n11.1 Introduction\\nIn the discussion leading to the law of large numbers, we saw visually that the sample means from a sequence of inde-\\npendent random variables converge to their common distributional mean as the number of random variables increases.\\nIn symbols,\\n\\n1\\nn\\n\\nSn = ¯Xn → µ as n → ∞.\\n\\nUsing the Pythagorean identity for independent random variables, we obtained the more precise statement that the\\nstandard deviation of of the sample mean, ¯Xn, is inversely proportional to √n, the square root of the number of obser-\\nvations. For example, for simulations based on observations of independent random variables, uniformly distributed\\non the interval [0, 1], we see, as anticipated, the running averages converging to\\n\\nµ =(cid:90) 1\\n\\n0\\n\\nxfX (x) dx =(cid:90) 1\\n\\n0\\n\\nx dx =\\n\\n1\\n\\n0\\n\\n=\\n\\n1\\n2\\n\\n,\\n\\nx2\\n\\n2 (cid:12)(cid:12)(cid:12)\\n\\nthe distributional mean.\\ndifference between the running average and the mean by a factor of √n and investigate the graph of\\n\\nNow, we zoom around the mean value of µ = 1/2. Because the standard deviation σ ¯Xn ∝ 1/√n, we magnify the\\n\\n√n(cid:18) 1\\n\\nn\\n\\nSn − µ(cid:19) =\\n\\nSn − nµ\\n\\n√n\\n\\n(11.1)\\n\\nversus n. The results of a simulation are displayed in Figure 11.1.\\n\\nAs we see in Figure 11.2, even if we extend this simulation for larger values of n, we continue to see ﬂuctuations\\nabout the center of roughly the same size and the size of the ﬂuctuations for a single realization of a simulation cannot\\nbe predicted in advance.\\n\\nThus, we focus on addressing a broader question: Does the distribution of the size of these ﬂuctuations have any\\nregular and predictable structure? This question and the investigation that led to led to its answer, the central limit\\ntheorem, constitute one of the most important episodes in mathematics.\\nExercise 11.1. Repeat the exercise above times. looking at the centered and magniﬁed running averages, (11.1) for\\n2000 steps for U (0, 1) random variables. Give 1000 simulations of the ﬁnal value for the simulations in (11.1), and\\ndescribe the histogram.\\n\\n193\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nFigure 11.1: a. Running average of independent random variables, uniform on [0, 1].\\nmagniﬁed by √n.\\n\\nb. Running average centered at the mean value of 1/2 and\\n\\nFigure 11.2: Plot of (11.1, the running average centered at the mean value of 1/2, and magniﬁed by √n extended to 2000 steps.\\n\\n11.2 The Classical Central Limit Theorem\\n\\nLet’s begin by examining the distribution for the sum of X1, X2 . . . Xn, independent and identically distributed random\\nvariables\\n\\nSn = X1 + X2 + ··· + Xn,\\n\\n194\\n\\n0501001502000.30.40.50.60.7ns/n050100150200-0.3-0.2-0.10.00.10.20.3n(s - n/2)/sqrt(n)0500100015002000-0.6-0.4-0.20.00.2n(s - n/2)/sqrt(n)\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nwhat distribution do we see? Let’s look ﬁrst to the simplest case, Xi Bernoulli random variables. In this case, the\\nsum Sn is a binomial random variable. We examine two cases - in the ﬁrst we keep the number of trials the same at\\nn = 100 and vary the success probability p. In the second case, we keep the success probability the same at p = 1/2,\\nbut vary the number of trials.\\n\\nFigure 11.3: a. Successes in 100 Bernoulli trials with p = 0.2, 0.4, 0.6 and 0.8. b. Successes in Bernoulli trials with p = 1/2 and n = 20, 40\\nand 80.\\n\\nThe curves in Figure 11.3 look like bell curves. Their center and spread vary in ways that are predictable. The\\n\\nbinomial random variable Sn has\\n\\nThus, if we take the standardized version of these sums of Bernoulli random variables\\n\\nmean np and standard deviation(cid:112)np(1 − p).\\n\\nZn =\\n\\n,\\n\\nSn − np\\n\\n(cid:112)np(1 − p)\\n\\nthen these bell curve graphs would lie on top of each other.\\n\\nFor our next example, we look at the density of the sum of standardized exponential random variables. The\\nexponential density is strongly skewed and so we have have to wait for larger values of n before we see the bell curve\\nemerge. In order to make comparisons, we examine standardized versions of the random variables with mean µ and\\nvariance σ2.\\n\\nTo accomplish this,\\n\\n• we can either standardize using the sum Sn having mean nµ and standard deviation σ√n, or\\n• we can standardize using the sample mean ¯Xn having mean µ and standard deviation σ/√n.\\n\\nThis yields two equivalent versions of the z-score.\\n\\nZn =\\n\\nSn − nµ\\nσ√n\\n\\n=\\n\\n¯Xn − µ\\nσ/√n\\n\\n=\\n\\n√n\\nσ\\n\\n( ¯Xn − µ).\\n\\n(11.2)\\n\\nIn Figure 11.4, we see the densities approaching that of the bell curve for a standard normal random variables.\\nEven for the case of n = 32, we see a small amount of skewness that is a remnant of the skewness in the exponential\\ndensity.\\n\\n195\\n\\n0204060801000.000.020.040.060.080.10successesprobability0204060801000.000.020.040.060.080.10successesprobability0204060801000.000.020.040.060.080.10successesprobability0204060801000.000.020.040.060.080.10successesprobability01020304050600.000.050.100.15successesprobability01020304050600.000.050.100.15successesprobability01020304050600.000.050.100.15successesprobability\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nFigure 11.4: Displaying the central limit theorem graphically. Density of the standardized version of the sum of n independent\\nexponential random variables for n = 2 (dark blue), 4 (green), 8 (red), 16 (light blue), 32 (magenta), and 64 (orange). Note how\\nthe skewness of the exponential distribution slowly gives way to the bell curve shape of the normal distribution.\\n\\nExercise 11.2. Show that the skewness for the sum of n independent Exp(λ) random variables is 2/√n. Thus the\\nskewness of the normalized sums converges to 0 as n → ∞. Hint: The sum Sn is a Γ(n, λ) random variable.\\nExercise 11.3. More generally, show the skewness for the sum of n independent random variables having a common\\ndistribution is γ1/√n where γ1 is the skewness of any one of the random variables in the sum. Consequently, the\\nskewness of Sn a Bin(n, p) random variable is (1 − 2p)/(cid:112)np(1 − p).\\nTheorem 11.4. Let {Xi; i ≥ 1} be independent random variables having a common distribution. Let µ be their mean\\nand σ2 be their variance. Then Zn, the standardized scores deﬁned by equation (11.2), converges in distribution to Z\\na standard normal random variable. This statement is shorthand for the more precise statement that the distribution\\nfunction FZn converges to Φ, the distribution function of the standard normal.\\n\\nThe theoretical result behind these numerical explorations is called the classical central limit theorem:\\n\\nlim\\nn→∞ FZn (z) = lim\\n\\nn→∞ P{Zn ≤ z} =\\n\\n1\\n\\n√2π(cid:90) z\\n\\n−∞\\n\\n−x2/2 dx = Φ(z).\\n\\ne\\n\\nIn practical terms the central limit theorem states that P{a < Zn ≤ b} ≈ P{a < Z ≤ b} = Φ(b) − Φ(a).\\nThis theorem is an enormously useful tool in providing good estimates for probabilities of events depending on\\n\\neither Sn or ¯Xn. We shall begin to show this in the following examples.\\n\\nOne recent rule of thumb for n the number of observations necessary to use the central limit theorem is to recognize\\nthat the more skewed the distribution, the more observations are need to obtain the bell curve shape. We saw this above\\nin the normalized sums of independent exponential distributions. Based on skewness, Sugden et al. (2000) provide\\nan extension of a method introduced by Cochran for n∗, the minimum sample size needed. Here is their formula for\\nobservations from a simple random sample.\\n\\n(11.3)\\n\\n∗\\n\\nn\\n\\n= 28 + 25γ2\\n1\\n\\n196\\n\\n-3-2-101230.00.10.20.30.40.5zdensity-3-2-101230.00.10.20.30.40.5zdensity-3-2-101230.00.10.20.30.40.5zdensity-3-2-101230.00.10.20.30.40.5zdensity-3-2-101230.00.10.20.30.40.5zdensity-3-2-101230.00.10.20.30.40.5zdensity\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nwhere γ1 is the skewness.\\n\\nExample 11.5. For exponential random variables, the mean, µ = 1/λ and the standard deviation, σ = 1/λ and\\ntherefore\\n\\nThe skewness γ1 = 2 and so the Sugden recommendation for the minimum sample size,\\n\\nZn =\\n\\nSn − n/λ\\n√n/λ\\n\\n=\\n\\nλSn − n\\n\\n√n\\n\\n.\\n\\n∗\\n\\nn\\n\\n= 28 + 25 × 22 = 128.\\n\\nLet T144 be the sum of 144 independent with parameter λ = 1. Thus, µ = 1 and σ = 1. Note that n = 144 is\\n\\nsufﬁciently large for the use of a normal approximation.\\n\\n2(cid:27) = P{Z144 < −0.5} ≈ 0.309.\\nP{T144 < 150} = P(cid:26) T144 − 150\\nExample 11.6. Pictures on your smartphone have a mean size of 400 kilobytes (KB) and a standard deviation of 50\\nKB. You want to store 100 pictures on your cell phone. If we assume that the size of the pictures X1, X2,··· , X100\\nare independent, then ¯X has mean µ ¯X = 400 KB and standard deviation σ ¯X = 50/√100 = 5 KB. So, the probability\\nthat the average picture size is between 394 and 406 kilobytes is\\n\\n(cid:27) = P(cid:26) T144 − 150\\n\\n144 − 150\\n\\n< −\\n\\n12\\n\\n12\\n\\n12\\n\\n<\\n\\n1\\n\\nP{394 ≤ ¯X ≤ 406} = P(cid:26) 394 − 400\\nS100 be the total storage space needed for the 100 pictures has mean 100 × 400 = 40, 000 KB and standard\\ndeviation σS100 = 50√100 = 500 KB. To estimate the space required to be 99% certain that the pictures will have\\nstorage space on the phone, note that\\n\\n(cid:27) = P{−1.2 ≤ Z100 ≤ 1.2} ≈ 0.230.\\n\\n406 − 400\\n\\n¯X − 400\\n\\n≤\\n\\n≤\\n\\n5\\n\\n5\\n\\n5\\n\\n> qnorm(0.99)\\n[1] 2.326348\\n\\nThus,\\n\\nkilobyres.\\n\\nZ100 =\\n\\nS100 − 40000\\n\\n500\\n\\n≥ 2.326348, S100 − 40000 ≥ 1163.174, S100 ≥ 41163.174\\n\\nExercise 11.7. If your smartphone has 42000 KB of storage space for pictures, Use the central limit theorem to\\nestimate the number of pictures you can have necessary to have a 1% chance of running out of space.\\n\\nExercise 11.8. Simulate 1000 times, ¯x, the sample mean of 100 random variables, uniformly distributed on [0, 1].\\nShow a histogram for these simulations to see the approximation to a normal distribution. Find the mean and standard\\ndeviations for the simulations and compare them to their distributional values. Use both the simulation and the central\\nlimit theorem to estimate the 35th percentile of ¯X.\\n\\n11.2.1 Bernoulli Trials and the Continuity Correction\\n\\nExample 11.9. For Bernoulli random variables, µ = p and σ = (cid:112)p(1 − p). Sn is the number of successes in n\\n\\nBernoulli trials. In this situation, the sample mean is the fraction of trials that result in a success. This is generally\\ndenoted by ˆp to indicate the fact that it is a sample proportion.\\n\\nThe normalized versions of Sn and ˆp are equal to\\n\\nZn =\\n\\n=\\n\\nSn − np\\n\\n(cid:112)np(1 − p)\\n\\n197\\n\\n,\\n\\nˆpn − p\\n\\n(cid:112)p(1 − p)/n\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nFigure 11.5: Mass function for a Bin(100, 0.3) random variable (black) and approximating normal density N (100 · 0.3, √100 · 0.3 · 0.7).\\nFor example, in 100 tosses of a fair coin, µ = 1/2 and σ =(cid:112)1/2(1 − 1/2) = 1/2. Thus,\\n\\nZ100 =\\n\\nS100 − 50\\n\\n.\\n\\n5\\n\\nTo ﬁnd P{S100 > 65}, convert the event {S100 > 65} to an event concerning Z100.\\n\\nP{S100 > 65} = P{S100 − 50 > 15} = P(cid:26) S100 − 50\\n\\n5\\n\\n> 3(cid:27) = P{Z100 > 3} ≈ P{Z > 3} = 0.0013.\\n\\n> 1-pnorm(3)\\n[1] 0.001349898\\n\\nWe could also write,\\n\\nand\\n\\nZ100 =\\n\\nˆp − 1/2\\n1/20\\n\\n= 20(ˆp − 1/2).\\n\\nP{ˆp ≤ 0.40} = P{ˆp−1/2 ≤ 0.40−1/2} = P{20(ˆp−1/2) ≤ 20(0.4−1/2)} = P{Z100 ≤ −2} ≈ P{Z ≤ −2} = 0.023.\\n> pnorm(-2)\\n[1] 0.02275013\\n\\nRemark 11.10. We can improve the normal approximation to the binomial random variable by employing the conti-\\nnuity correction. For a binomial random variable X, the distribution function\\n\\nP{X ≤ x} = P{X < x + 1} =\\n\\nx(cid:88)y=0\\n\\nP{X = y}\\n\\ncan be realized as the area of x + 1 rectangles, height P{X = y}, y = 0, 1, . . . , x and width 1. These rectangles look\\nlike a Riemann sum for the integral up to the value x + 1/2. For the example in Figure 11.5, P{X ≤ 32} = P{X <\\n33} is the area of 33 rectangles. This right side of rectangles is at the value 32.5. Thus, for the approximating normal\\nrandom variable Y , this suggests computing P{Y ≤ 32.5}. In this example the exact value\\n\\n198\\n\\n2526272829303132333400.010.020.030.040.050.060.070.080.09number of successes\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n> pbinom(32,100,0.3)\\n[1] 0.7107186\\n\\nComparing this to possible choices for the normal approximations\\n\\n> n<-100\\n> p<-0.3\\n> mu<-n*p\\n> sigma<-sqrt(p*(1-p))\\n> prob<-pnorm((x-mu)/(sigma*sqrt(n)))\\n> x<-c(32,32.5,33)\\n> data.frame(x,prob)\\n\\nx\\n\\nprob\\n1 32.0 0.6687397\\n2 32.5 0.7073105\\n3 33.0 0.7436546\\n\\nThis shows a difference of 0.0034 for the choice x = 32.5 and larger differences for the choices x = 32 or x = 33.\\n\\nExample 11.11. Opinion polls are generally designed to be modeled as Bernoulli trials. The number of trials n is set\\nto give a prescribed value m of two times the standard deviation of ˆp. This value of m is an example of a margin of\\nerror. The standard deviation\\n\\ntakes on its maximum value for p = 1/2. For this case,\\n\\n(cid:112)p(1 − p)/n\\n2(cid:19) /n =\\n2(cid:18)1 −\\n\\n1\\n\\n1\\n√n\\n\\nm = 2(cid:115) 1\\n\\nThus,\\n\\nn =\\n\\n1\\nm2\\n\\nWe display the results in R for typical values of m.\\n\\n> m<-seq(0.01,0.05,0.01)\\n> n<-1/mˆ2\\n> data.frame(m,n)\\n\\nm\\n\\nn\\n1 0.01 10000.000\\n2500.000\\n2 0.02\\n1111.111\\n3 0.03\\n625.000\\n4 0.04\\n5 0.05\\n400.000\\n\\nSo, a 5% margin of error can be achieved with a modest sample size of n = 400, whereas a 1% margin of error\\n\\nrequires 10,000 samples.\\n\\nExercise 11.12. We have two approximation methods for a large number n of Bernoulli trials - Poisson, which applies\\nthen p is small and their product λ = np is moderate and normal when the mean number of successes np or the mean\\nnumber of failures n(1−p) is sufﬁciently large. Investigate the approximation of the distribution, X, a Poisson random\\nvariable, by the distribution of a normal random variable, Y , for the case λ = 16. Use the continuity correction to\\ncompare\\n\\nP{X ≤ x}\\n\\n1\\n2}.\\n\\nto P{Y ≤ x +\\n199\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n11.3 Propagation of Error\\nPropagation of error or propagation of uncertainty is a strategy to estimate the impact on the standard deviation\\nof the consequences of a nonlinear transformation of a measured quantity whose measurement is subject to some\\nuncertainty.\\n\\nFor any random variable Y with mean µY and standard deviation σY , we will be looking at linear functions aY + b\\n\\nfor Y . Using the linearity of expectation and the quadratic identity for variance, we have that\\n\\nE[a + bY ] = a + bµY ,\\n\\nVar(a + bY ) = b2Var(Y ).\\n\\n(11.4)\\n\\nExercise 11.13. Show that\\n\\nE[a + b(Y − µY )] = a,\\n\\nVar(a + b(Y − µY )) = b2Var(Y ).\\n\\nWe will apply this to the linear approximation of g(Y ) about the point µY .\\n\\ng(Y ) ≈ g(µY ) + g\\n\\n(cid:48)\\n\\n(µY )(Y − µY ).\\n\\nIf we take expected values, then\\n\\nEg(Y ) ≈ E[g(µY ) + g\\n\\nThe variance\\n\\n(cid:48)\\n\\n(cid:48)\\n\\n(µY )(Y − µY )] = g(µY ).\\n\\nVar(g(Y )) ≈ Var(g(µY ) + g\\n\\n(µY )(Y − µY )) = g\\n(cid:48)\\n\\nσg(Y ) ≈ |g\\n\\n(µY )|σY\\n\\n(cid:48)\\n\\n(µY )2σ2\\nY .\\n\\nThus, the standard deviation\\n\\ngives what is known as the propagation of error.\\n\\nIf Y is meant to be some measurement of a quantity q with a measurement subject to error, then saying that\\n\\n(11.5)\\n\\n(11.6)\\n\\nq = µY = EY\\n\\nis stating that Y is an unbiased estimator of q. In other words, Y does not systematically overestimate or under-\\nestimate q. The standard deviation σY gives a sense of the variability in the measurement apparatus. However, if\\nwe measure Y but want to give not an estimate for q, but an estimate for a function of q, namely g(q), its standard\\ndeviation is approximation by formula (11.6).\\n\\nExample 11.14. Let Y be the measurement of a side of a cube with length (cid:96). Then Y 3 is an estimate of the volume\\nof the cube. If the measurement error has standard deviation σY , then, taking g(y) = y3, we see that the standard\\ndeviation of the error in the measurement of the volume\\n\\nIf we estimate q with Y , then\\n\\nσY 3 ≈ 3q2σY .\\n\\nTo estimate the coefﬁcient volume expansion α3 of a material, we begin with a material of known length (cid:96)0 at\\n\\ntemperature T0 and measure the length (cid:96)1 at temperature T1. Then, the coefﬁcient of linear expansion\\n\\nσY 3 ≈ 3Y 2σY .\\n\\nIf the measure length of (cid:96)1 is Y . We estimate this by\\n\\nα1 =\\n\\n(cid:96)1 − (cid:96)0\\n\\n(cid:96)0(T1 − T0)\\n\\nˆα1 =\\n\\nY − (cid:96)0\\n\\n(cid:96)0(T1 − T0)\\n200\\n\\n.\\n\\n.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nThen, if a measurement Y of (cid:96)1 has variance σ2\\n\\nY , then\\n\\nNow, we estimate\\n\\nand\\n\\nVar(ˆα1) =\\n\\nσ2\\nY\\n\\n(cid:96)2\\n0(T1 − T0)2\\n\\nσ ˆα1 =\\n\\nσY\\n\\n(cid:96)0|T1 − T0|\\n\\n.\\n\\nα3 =\\n\\n0\\n\\n(cid:96)3\\n1 − (cid:96)3\\n(cid:96)3\\n0(T1 − T0)\\n\\nby\\n\\nˆα3 =\\n\\n0\\n\\nY 3 − (cid:96)3\\n(cid:96)3\\n0(T1 − T0)\\n\\nσ ˆα3 ≈ 3Y 2\\n\\nσY\\n\\n.\\n\\n(cid:96)3\\n0|T1 − T0|\\n\\nExercise 11.15. In a effort to estimate the angle θ of the sun, the length (cid:96) of a shadow from a 10 meter ﬂag pole is\\nmeasured. If σˆ(cid:96) is the standard deviation for the length measurement, use propagation of error to estimate σˆθ, the\\nstandard deviation in the estimate of the angle.\\n\\nOften, the function g is a function of several variables. We will show the multivariate propagation of error in\\nthe two dimensional case noting that extension to the higher dimensional case is straightforward. Now, for random\\nvariables Y1 and Y2 with means µ1 and µ2 and variances σ2\\n2, the linear approximation about the point (µ1, µ2)\\nis\\n\\n1 and σ2\\n\\ng(Y1, Y2) ≈ g(µ1, µ2) +\\n\\n(µ1, µ2)(Y1 − µ1) +\\n\\n(µ1, µ2)(Y2 − µ2).\\n\\n∂g\\n∂y1\\n\\n∂g\\n∂y2\\n\\nAs before,\\n\\nFor Y1 and Y2 independent, we also have that the random variables\\n\\nEg(Y1, Y2) ≈ g(µ1, µ2).\\n\\n∂g\\n∂y1\\n\\n(µ1, µ2)(Y1 − µ1)\\n\\nand\\n\\n∂g\\n∂y2\\n\\n(µ1, µ2)(Y2 − µ2)\\n\\nare independent. Because the variance of the sum of independent random variables is the sum of their variances, we\\nhave the approximation\\n\\nand consequently, the standard deviation,\\n\\nσ2\\ng(Y1,Y2) = Var(g(Y1, Y2)) ≈ Var(\\n=(cid:18) ∂g\\nσg(Y1,Y2) ≈(cid:115)(cid:18) ∂g\\n\\n∂y1\\n\\n∂y1\\n\\n∂g\\n∂y1\\n\\n(µ1, µ2)(Y1 − µ1)) + Var(\\n(µ1, µ2)(cid:19)2\\n\\n1 +(cid:18) ∂g\\n\\n(µ1, µ2)(cid:19)2\\n\\n∂y2\\n\\nσ2\\n\\n∂g\\n∂y2\\n\\n(µ1, µ2)(Y2 − µ2))\\n\\nσ2\\n2.\\n\\n(11.7)\\n\\n(µ1, µ2)(cid:19)2\\n\\nσ2\\n\\n1 +(cid:18) ∂g\\n\\n∂y2\\n\\n(µ1, µ2)(cid:19)2\\n\\nσ2\\n2.\\n\\nExercise 11.16. Repeat the exercise in the case that the height h if the ﬂag poll is also unknown and is measured\\nindependently of the shadow length with standard deviation σˆh. Comment on the case in which the two standard\\ndeviations are equal.\\n\\nExercise 11.17. Generalize the formula for the variance to the case of g(Y1, Y2, . . . , Yd) for independent random\\nvariables Y1, Y2, . . . , Yd.\\nExample 11.18. In the previous example, we now estimate the volume of an (cid:96)0 × w0 × h0 rectangular solid with\\nthe measurements Y1, Y2, and Y3 for, respectively, the length (cid:96)0, width w0, and height h0 with respective standard\\ndeviations σ(cid:96), σw, and σh. Here, we take g((cid:96), w, h) = (cid:96)wh, then\\n\\n∂g\\n∂(cid:96)\\n\\n((cid:96), w, h) = wh,\\n\\n∂g\\n∂w\\n\\n((cid:96), w, h) = (cid:96)h,\\n\\n((cid:96), w, h) = (cid:96)w,\\n\\n∂g\\n∂h\\n\\n201\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n(cid:19)2\\n\\n(cid:18) ∂g\\n\\n∂w\\n\\nσ2\\n(cid:96) +\\n\\n(cid:19)2\\n\\n((cid:96)0, w0, h0)\\n\\n(cid:18) ∂g\\n\\n∂h\\n\\nσ2\\n\\nw +\\n\\n(cid:19)2\\n\\nσ2\\nh\\n\\n((cid:96)0, w0, h0)\\n\\n((cid:96)0, w0, h0)\\n\\nσg(Y1,Y2,Y3)\\n\\n(cid:115)(cid:18) ∂g\\n(cid:113)\\n\\n∂(cid:96)\\n\\nand\\n\\n≈\\n\\n=\\n\\n(wh)2σ2\\n\\n(cid:96) + ((cid:96)h)2σ2\\n\\nw + ((cid:96)w)2σ2\\nh.\\n\\n11.4 Delta Method\\nLet’s use repeated independent measurements, Y1, Y2, . . . Yn to estimate a\\nquantity q by its sample mean ¯Y .\\nIf each measurement has mean µY and\\nvariance σ2\\nY /n. By the central limit\\ntheorem,\\n\\nY , then ¯Y has mean q = µY and variance σ2\\n\\nZn =\\n\\n¯Y − µY\\nσY /√n\\n\\n(11.8)\\n\\nhas a distribution that can be approximated by a standard normal. We can apply\\nthe propagation of error analysis based on a linear approximation of g( ¯Y ) to\\nobtain\\n\\ng( ¯Y ) ≈ g(µY ),\\n\\nand Var(g( ¯Y )) ≈ g\\n\\n(cid:48)\\n\\n(µY )2 σ2\\nY\\nn\\n\\n.\\n\\nThus, the reduction in the variance in the estimate of q “propagates” to a reduc-\\ntion in variance in the estimate of g(q).\\n\\nHowever, the central limit theorem gives us some additional information.\\n\\nReturning to the linear approximation (11.5)\\n\\ng( ¯Y ) ≈ g(µY ) + g\\n\\n(cid:48)\\n\\n(µY )( ¯Y − µY ).\\n\\n(11.9)\\n\\nThe delta method combines the central limit theorem and the propagation\\n\\nThe central limit theorem tells us that ¯Y has a nearly normal distribution. Thus,\\nthe linear approximation to g( ¯Y ) also has nearly a normal distribution. More-\\nover, with repeated measurements, the variance of ¯Y is the variance of a single\\nmeasurement divided by n. As a consequence, the linear approximation under\\nrepeated measurements yields a better approximation because the reduction in\\nvariance implies that the difference ¯Y − µY is more likely to be small.\\nof error. To see use (11.9) to write,\\n\\ndelta\\nFigure\\nmethod. Here µ = 1.5 and the blue\\ncurve g(x) = x2. Thus, g( ¯X) is approx-\\nimately normal with approximate mean\\n2.25 and σg( ¯X) ≈ 3σ ¯X. The bell curve\\non the y-axis is the reﬂection of the bell\\ncurve on the x-axis about the (black)\\ntangent line y = g(µ) + g(cid:48)(µ)(x − µ).\\nThe last equality uses (11.8). The ± sign depends on the sign of the derivative g(cid:48)(µY ). Because the negative of a\\nstandard normal is also a standard normal, we have the desired approximation to the standard normal.\\nThen, Zn converges in distribution to a standard normal random variable. In this way, the delta method greatly\\n\\ng(cid:48)(µY )( ¯Y − µY )\\n|g(cid:48)(µY )|σY /√n\\n\\ng( ¯Y ) − g(µy)\\n\\n11.6: Illustrating\\n\\n= ±Zn.\\n\\nσg( ¯Y )\\n\\n≈\\n\\nthe\\n\\nextends the applicability of the central limit theorem.\\n\\nLet’s return to our previous example on thermal expansion.\\n\\nExample 11.19. Let Y1, Y2, . . . , Yn be repeated unbiased measurement of a side of a cube with length (cid:96)1 and temper-\\nature T1. We use the sample mean ¯Y of these measurements to estimate the length at temperature T1 for the coefﬁcient\\nof linear expansion.\\n\\nˆα1 =\\n\\n.\\n\\n¯Y − (cid:96)0\\n\\n(cid:96)0(T1 − T0)\\n202\\n\\n11.5211.522.533.54\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nThen, if each measurement Yi has variance σ2\\nY ,\\n\\nVar(ˆα1) =\\n\\nσ2\\nY\\n\\n(cid:96)2\\n0(T1 − T0)2n\\n\\nσ ˆα1 =\\n\\nσY\\n\\n(cid:96)0|T1 − T0|√n\\n\\n.\\n\\nNow, we estimate the coefﬁcient of volume expansion by\\n\\nˆα3 =\\n\\n¯Y 3 − (cid:96)3\\n(cid:96)3\\n0(T1 − T0)\\n\\n0\\n\\nand\\n\\nBy the delta method,\\n\\nσ ˆα3 ≈\\n\\n.\\n\\n(cid:96)3\\n\\n3 ¯Y 2σY\\n0|T1 − T0|√n\\nˆα3 − α3\\n\\nσ ˆα3\\n\\nZn =\\n\\nhas a distribution that can be well approximated by a standard normal random\\nvariable.\\n\\nmaterial\\nalumium\\n\\nbradd\\nconcrete\\ndiamond\\ngasoline\\n\\nglass\\nwater\\n\\ncoefﬁcient of\\n\\nlinear expansion\\n\\n23.1\\n19\\n12\\n1\\n317\\n8.5\\n69\\n\\nTable I: Coefﬁcient of linear expansion at\\n20◦C in units 10−6/◦C.\\n\\nThe next natural step is to take the approach used for the propagation of error in a multidimensional setting and ex-\\ntend the delta method. Focusing on the three dimensional case, we have three independent sequences (Y1,1, . . . , Y1,n1 ),\\n(Y2,1, . . . , Y2,n2) and (Y3,1, . . . , Y3,n3) of independent random variables. The observations in the i-th sequence have\\ni for i = 1, 2 and 3. We shall use ¯Y1, ¯Y2 and ¯Y3 to denote the sample means for the three sets\\nmean µi and variance σ2\\nof observations. Then, ¯Yi has\\n\\nFrom the propagation of error linear approximation, we obtain\\n\\nmean µi\\n\\nand\\n\\nvariance\\n\\nσ2\\ni\\nni\\n\\nfor i = 1, 2, 3.\\n\\nEg( ¯Y1, ¯Y2, ¯Y3) ≈ g(µ1, µ2, µ3).\\n\\nFor the variance, look to the multidimensional propagation of error variance formula (11.7) replacing the measure-\\nments Yi by the sample mean ¯Yi .\\n\\ng( ¯Y1, ¯Y2, ¯Y3) = Var(g( ¯Y1, ¯Y2, ¯Y3))\\nσ2\\n(µ1, µ2, µ3)2 σ2\\n1\\nn1\\n\\n∂g\\n∂y1\\n\\n≈\\n\\n+\\n\\n∂g\\n∂y2\\n\\n(µ1, µ2, µ3)2 σ2\\n2\\nn2\\n\\n+\\n\\n∂g\\n∂y3\\n\\n(µ1, µ2, µ3)2 σ2\\n3\\nn3\\n\\n.\\n\\n(11.10)\\n\\nTo obtain the normal approximation associated with the delta method, we need to have the additional fact that the sum\\nof independent normal random variables is also a normal random variable. Thus, we have that, for n large,\\n\\nZn =\\n\\ng( ¯Y1, ¯Y2, ¯Y3) − g(µ1, µ2, µ3)\\n\\nσg( ¯Y1, ¯Y2, ¯Y3)\\n\\nis approximately a standard normal random variable.\\nExample 11.20. Fecundity is the reproductive rate of a community or of a population. Fecundity can change over\\ntime due to both genetic and environmental circumstances. In avian biology, the fecundity B is deﬁned as the number\\nof female ﬂedglings per female per year. B > 1 indicates a growing population and B < 1, a declining population.\\nB is a product of three quantities,\\n\\nwhere\\n\\nB = F · p · N,\\n\\n• F equals the mean number of female ﬂedglings per successful nest,\\n\\n203\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n• p equals nest survival probability, and\\n• N equals the mean number of nests built per female per year.\\n\\nLet’s\\n\\n• collect measurement F1, . . . , FnF on nF nests to count female ﬂedglings in a successful nest, and determine the\\n\\nsample mean ¯F ,\\n\\n• check np nests for survival probability, and determine the sample proportion ˆp, and\\n• follow nN females to count the number N1, . . . , NnN of successful nests per year and determine the sample\\n\\nmean ¯N.\\n\\nOur experimental design is structured so that measurements are independent. Then, taking the appropriate partial\\nderivatives in (11.10) to B = g(F, p, N ) = F · p · N, we obtain an estimate for the variance of ˆB = g( ¯F , ˆp, ¯N ),\\n\\nσ2\\n\\nˆB ≈(cid:18) ∂B\\n\\n∂F\\n\\n(µF , p, µN )(cid:19)2 σ2\\n+ (µF µN )2 σ2\\np\\nnp\\n\\nF\\nnF\\n\\n= (µpµN )2 σ2\\nF\\nnF\\n\\n∂p\\n+ (µpµF )2 σ2\\nN\\nnN\\n\\n.\\n\\n+(cid:18) ∂B\\n\\n(µF , p, µN )(cid:19)2 σ2\\n\\np\\nnp\\n\\n+(cid:18) ∂B\\n\\n∂N\\n\\n(µF , p, µN )(cid:19)2 σ2\\n\\nN\\nnN\\n\\n.\\n\\n(11.11)\\n\\nThe checks of nest survival form a sequence of Bernoulli trials. Thus, µp = p and σ2\\nrandom variable, we can write the expression above upon dividing by B2 as\\n\\np = p(1 − p) for a Bernoulli\\n\\n(cid:16) σ ˆB\\nB (cid:17)2\\n\\n≈\\n\\n=\\n\\n1\\n\\nnF (cid:18) σF\\nµF(cid:19)2\\nnF (cid:18) σF\\nµF(cid:19)2\\n\\n1\\n\\n+\\n\\n+\\n\\n1\\n\\n1\\n\\n+\\n\\nnp(cid:18) σp\\nµp(cid:19)2\\np (cid:19) +\\nnp(cid:18) 1 − p\\n\\nnN (cid:18) σN\\nµN(cid:19)2\\nnN (cid:18) σN\\nµN(cid:19)2\\n\\n1\\n\\n1\\n\\n.\\n\\nThis gives the individual contributions to the variance of B from each of the three data collecting activities - female\\nﬂedglings, nest survivability, nest building. The values of nF , np, and nN can be adjusted in the collection of data to\\nadjust the variance of ˆB under a variety of experimental designs.\\n\\nEstimates for σ2\\nˆB\\n\\ncan be found from the ﬁeld data. Compute sample means\\n\\n¯F ,\\n\\nˆp,\\n\\nand\\n\\n¯N ,\\n\\nand sample variance\\n\\ns2\\nF ,\\n\\nUsing (11.11), we estimate the variance in fecundity\\n\\nˆp(1 − ˆp) and s2\\nN .\\n\\ns2\\nˆB ≈\\n\\n1\\nnF\\n\\n(ˆp ¯N sF )2 +\\n\\n1\\nnp\\n\\n( ¯F ¯N )2 ˆp(1 − ˆp) +\\n\\n1\\nnN\\n\\n(ˆp ¯F sN )2\\n\\nIf we make multiple measurements on the collection of female adult birds. The three observations (female off-\\nspring, net survivability, and number of nests) may be correlated, but the observations made on different female adults\\nare independent. This leads to the following extension of the delta method.\\nExercise 11.21. In the case in which the n observations Yi = (Yi,1, Yi,2, . . . Yi,n) are independent vector, but the\\nentries in this vector may not be independent, show that the formula for the delta method is\\n\\nVar(g( ¯Y1, ¯Y2, . . . , ¯Yn)) ≈\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nn(cid:88)j=1\\n\\n∂\\n∂yi\\n\\ng(µ1, µ2 . . . , µn)\\n\\n∂\\n∂yj\\n\\ng(µ1, µ2 . . . , µn)ρi,jσiσj.\\n\\n(11.12)\\n\\nHere the distributional means µi, sample means ¯Yi, and distributional standard deviations σi for the i-th set of obser-\\nvations are as above and where ρi,j is the correlation of i-th and j-th set of observations.\\n\\n204\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nExercise 11.22. Use this formula for σ2\\nnot independent.\\n\\nB,n in the case that measurements for F , p, and N for a given female adult are\\n\\nWe will now move to a fundamental issue in statistics - estimation. The analysis of the properties an estimator,\\nnamely, its accuracy and its precision, are based to a large extent on the tools in probability theory that we have\\ndeveloped here - the law of large numbers, the central limit theorem and their extensions.\\n\\nWe ﬁnish the discussion on the central limit theorem with a summary of some of its applications.\\n\\n11.5 Summary of Normal Approximations\\nThe standardized score or z-score of some random quantity is\\n\\nZ =\\n\\nrandom quantity − mean\\n\\nstandard deviation\\n\\n.\\n\\nThe central limit theorem\\nand extensions like the delta\\nmethod tell us when the z-score\\nhas an approximately standard\\nnormal distribution. Thus, us-\\ning R, we can ﬁnd good ap-\\nproximations by computing the\\nprobabilities of P{Z < z},\\npnorm(z) and P{Z > z} us-\\ning 1-pnorm(z) or P{z1 <\\nZ < z2} using the difference\\npnorm(z2) - pnorm(z1).\\n\\n11.5.1 Sample Sum\\nIf we have a sum Sn of n\\nindependent random variables,\\nX1, X2, . . . Xn whose common\\ndistribution has mean µ and vari-\\nance σ2, then\\n\\nFigure 11.7: The density function for Sn for a random sample of size n = 10 (red), 20\\n(green), 30 (blue), and 40 (purple).\\nIn this example, the observations are normally dis-\\ntributed with mean µ = 1 and standard deviation σ = 10.\\n\\n• the mean ESn = nµ,\\n• the variance Var(Sn) = nσ2,\\n• the standard deviation is σ√n.\\n\\nThus, Sn is approximately normal with mean nµ and variance nσ2. The z-score in this case is\\n\\nZn =\\n\\nSn − nµ\\nσ√n\\n\\n.\\n\\nWe can approximate P{Sn < x} by noting that this is the same as computing the probability\\n\\nZn =\\n\\nSn − nµ\\nσ√n\\n\\n<\\n\\nx − nµ\\nσ√n\\n\\n= z\\n\\nand ﬁnding P{Zn < z} using the standard normal distribution.\\n\\n205\\n\\n-100-500501001500.0000.0020.0040.0060.0080.0100.012x-100-500501001500.0000.0020.0040.0060.0080.0100.012x-100-500501001500.0000.0020.0040.0060.0080.0100.012x-100-500501001500.0000.0020.0040.0060.0080.0100.012x\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n11.5.2 Sample Mean\\nFor a sample mean\\n\\n¯X = (X1 + X2 + ··· + Xn)/n,\\n• the mean E ¯X = µ,\\n• the variance Var( ¯X) = σ2/n,\\n• the standard deviation is σ/√n.\\n\\nThus, ¯X is approximately normal with mean µ and variance\\nσ2/n. The z-score in this case is\\n\\nZn =\\n\\n¯X − µ\\nσ/√n\\n\\n.\\n\\nThus,\\n\\n¯X < x is equivalent to Zn =\\n\\n¯X − µ\\nσ/√n\\n\\n<\\n\\nx − µ\\nσ/√n\\n\\n.\\n\\n11.5.3 Sample Proportion\\nFor Bernoulli trials X1, X2, . . . Xn with success probability\\np, let ˆp = (X1 + X2 +··· + Xn)/n be the sample proportion.\\nThen\\n\\n.\\n\\nFigure 11.8: The density function for ¯X − µ for a random\\nsample of size n = 1 (black), 10 (red), 20 (green), 30 (blue),\\nand 40 (purple). In this example, the observations are nor-\\nmally distributed with standard deviation σ = 10.\\n\\n• the mean E ˆp = p,\\n• the variance Var(ˆp) = p(1 − p)/n,\\n• the standard deviation is(cid:112)p(1 − p)/n.\\nThus, ˆp is approximately normal with mean p and variance p(1 − p)/n. The z-score in this case is\\n\\nFor the special case of Bernoulli trials, normal approximations often use a continuity correction.\\n\\nZn =\\n\\n.\\n\\nˆp − p\\n\\n(cid:112)p(1 − p)/n\\n\\n11.5.4 Delta Method\\nFor the delta method in one variable using ¯X and a function g, for a sample mean ¯X = (X1 + X2 + ··· + Xn)/n, we\\nhave\\n\\n• the mean Eg( ¯X) ≈ g(µ),\\n• the variance Var(g( ¯X)) ≈ g(cid:48)(µ)2σ2/n,\\n• the standard deviation is |g(cid:48)(µ)|σ/√n.\\n\\nThus, g( ¯X) is approximately normal with mean g(µ) and variance g(cid:48)(µ)2σ2/n. The z-score is\\n\\nZn =\\n\\ng( ¯X) − g(µ)\\n|g(cid:48)(µ)|σ/√n\\n\\n.\\n\\n206\\n\\n-10-505100.00.10.20.30.4x-10-505100.00.10.20.30.4x-10-505100.00.10.20.30.4x-10-505100.00.10.20.30.4x-10-505100.00.10.20.30.4x\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nFor the two variable delta method, we now\\nhave two independent sequences of independent\\nrandom variables, X1,1, X1,2, . . . X1,n1 whose\\ncommon distribution has mean µ1 and variance σ2\\n1\\nand X2,1, X2,2, . . . X2,n2 whose common distribu-\\ntion has mean µ2 and variance σ2\\n2. For a function\\ng of the sample means, we have that\\n\\n• the mean Eg( ¯X1, ¯X2) ≈ g(µ1, µ2),\\n• the variance\\n\\nVar(g( ¯X1, ¯X2)) = σ2\\n\\nFigure 11.9: Two variable delta method. σg,\\nthe standard deviation of\\ng( ¯X1, ¯X2), is approximated by the length of the hypotenuse in the triangle\\nshown above.\\n\\ng ≈(cid:18) ∂\\n\\n∂x\\n\\ng(µ1, µ2)(cid:19)2 σ2\\n\\n1\\nn1\\n\\n+(cid:18) ∂\\n\\n∂y\\n\\ng(µ1, µ2)(cid:19)2 σ2\\n\\n2\\nn2\\n\\n,\\n\\n• the standard deviation is σg.\\n\\nThus, g( ¯X1, ¯X2) is approximately normal with mean g(µ1, µ2) and variance σ2\\n\\ng,n. The z-score is\\n\\nZn =\\n\\ng( ¯X1, ¯X2) − g(µ1, µ2)\\n\\nσg,n\\n\\n.\\n\\nThe generalization of the delta method to higher dimensional data will add terms to the variance formula. Fuse or\\n\\nmultiple observations on n individuals, (?? to determine the standard deviation in the z-score.\\n\\n11.6 Answers to Selected Exercises\\n11.1 Here is the code for one of the plots of of (Sn − n/2)/√n in Figure 11.9.\\n\\n> n<-1:2000\\n> x<-runif(2000)\\n> s<-cumsum(x)\\n> plot(n,(s-n/2)/sqrt(n),type=\"l\",\\n\\nylim=c(-0.5,0.5),col=\"orange\")\\n\\nFor the 1000 simulations of (S2000 − 1000)/√2000 with a summary\\n\\nand a histogram, we have\\n\\nFigure 11.10: Left.Six plots of of (Sn − n/2)/√n for U (0, 1) random variables. Right Histogram of z-scores for one simulation.\\n\\n207\\n\\n\\x00\\x00\\x00\\x00@@xg(µ1,µ2)\\x00\\x00\\x00\\x00\\x001pn1\\x00\\x00\\x00\\x00@@yg(µ1,µ2)\\x00\\x00\\x00\\x00\\x002pn20500100015002000-0.4-0.20.00.20.4n(s - n/2)/sqrt(n)0500100015002000-0.4-0.20.00.20.4n(s - n/2)/sqrt(n)0500100015002000-0.4-0.20.00.20.4n(s - n/2)/sqrt(n)0500100015002000-0.4-0.20.00.20.4n(s - n/2)/sqrt(n)0500100015002000-0.4-0.20.00.20.4n(s - n/2)/sqrt(n)0500100015002000-0.4-0.20.00.20.4n(s - n/2)/sqrt(n)0500100015002000-0.4-0.20.00.20.4n(s - n/2)/sqrt(n)Histogram of scorescoreFrequency-1.0-0.50.00.51.0050100150200250\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n> score<-numeric(1000)\\n> for (i in 1:1000){x<-runif(2000);\\n\\ns<-sum(x);score[i]<-(s-1000)/sqrt(2000)}\\n\\n> mean(score)\\n[1] -0.001879942\\n> sd(score)\\n[1] 0.28578\\n> hist(score)\\n\\nThis histogram has a bell shape with the mean at 0 and a standard deviation of 0.2858.\\n\\n11.2. The sum Sn of exponential random variables is Γ(n, λ) and thus has mean n/λ and standard deviation √n/λ.\\nThe skewness is the third moment of the standardized sum,\\n\\nSn − n/λ\\n\\n√nλ\\n\\n=\\n\\nλSn − n\\n\\n√n\\n\\n=\\n\\nTn − n\\n√n\\n\\nwhere Tn = λSn is Γ(n, 1). (Check this!) Thus, the skewness\\n\\nE(cid:34)(cid:18) Tn − n\\n\\n√n (cid:19)3(cid:35) =\\n\\n1\\nn3/2 (ET 3\\n\\nn − 3nET 2\\n\\nn + 3n2ETn − n3).\\n\\nHere we use the linearity properties of expectation. Now, the ﬁrst moment, ETn = n. For the second moment,\\n\\nET 2\\n\\nn =(cid:90) ∞\\n\\n0\\n\\nx2xn−1e\\n\\n−xdx =\\n\\n1\\n\\nΓ(n)\\n\\nΓ(n + 2)\\n\\nΓ(n)\\n\\n(cid:90) ∞\\n\\n0\\n\\n1\\n\\nΓ(n + 2)\\n\\nx(n+2)−1e\\n\\n−xdx = (n + 1)n.\\n\\nNotice that we are integrating the density function of a Γ(n + 2, 1) random variable. Similarly,\\n\\nET 3\\n\\nn =(cid:90) ∞\\n\\n0\\n\\nx3xn−1e\\n\\n−xdx =\\n\\n1\\n\\nΓ(n)\\n\\nΓ(n + 3)\\n\\nΓ(n)\\n\\n(cid:90) ∞\\n\\n0\\n\\n1\\n\\nΓ(n + 3)\\n\\nx(n+3)−1e\\n\\n−xdx = (n + 2)(n + 1)n.\\n\\nReturning to the skewness, we substitute for each of the ﬁrst three moments of Tn\\n\\nE(cid:34)(cid:18) Tn − n\\n\\n√n (cid:19)3(cid:35) =\\n\\n=\\n\\n=\\n\\n1\\nn3/2 ((n + 2)(n + 1)n − 3n(n + 1)n + 3n2n − n3)\\n1\\n√n\\n1\\n√n\\n\\n((n + 2)(n + 1) − 3(n + 1)n + 2n3)\\n(n2 + 3n + 2 − 3n2 − 3n + 2n2) =\\n\\n2\\n√n\\n\\n.\\n\\n11.3. Let µ and σ denote the common mean and standard deviation the standardized random variable\\n\\n1\\n\\n(cid:18) Sn − nµ\\nσ√n (cid:19)3\\n\\nσ (cid:33)3\\ni = (Xi − µ)/σ is the standardized version of Xi. In particular, EX∗\\n\\nn3/2(cid:18) Sn − nµ\\n\\nn3/2(cid:32) n(cid:88)i=1\\n\\nXi − µ\\n\\n(cid:19)3\\n\\n=\\n\\n=\\n\\nσ\\n\\n1\\n\\nwhere X∗\\nrandom variables. In so doing, we see that the expected value of the cube of a sum involves terms γ1 = EX∗3\\nj X∗\\ni X∗\\nskewness of a single random variable in the sum, terms EX∗2\\n\\nWe expand the cube of the sum in (11.13) and use the property of the expectation of the product of independent\\n, the\\nk =\\n\\nj = EX∗2\\n\\ni EX∗\\n\\ni X∗\\n\\ni = 0.\\n\\nj with i (cid:54)= j, and terms EX∗\\n\\ni\\n\\n=\\n\\n1\\n\\nn3/2(cid:32) n(cid:88)i=1\\n\\n∗\\n\\ni(cid:33)3\\n\\nX\\n\\n,\\n\\n(11.13)\\n\\n208\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\ni EX∗\\n\\nEX∗\\ncontribute to the expected value. Thus,\\n\\nj EX∗\\n\\nk where i, j, and k all differ. Each of the last two types of terms equals 0 and consequently does not\\n\\n1\\n\\n] =\\n\\nn3/2\\n\\n∗3\\nE[X\\ni\\n\\nn(cid:88)i=1\\n\\n1\\nn3/2 nγ1 =\\n\\nσ√n (cid:19)3(cid:35) =\\n\\nE(cid:34)(cid:18) Sn − nµ\\nThe binomial random variable Sn can be realized as the sum of n independent Bernoulli random variables with\\nskewness γ1 = (1 − 2p)/(cid:112)p(1 − p).\\n11.7. For Z a standard normal random variable to determine z∗ that satisﬁes P{Z < z∗\\ncommand\\n> qnorm(0.01)\\n[1] -2.326348\\nThus, we look for the value n that gives a standardized score of z∗.\\n\\n} = 0.01, we use the R\\n\\n1\\n√n\\n\\nγ1.\\n\\n−2.326348 = z\\n\\n∗\\n\\n=\\n\\n=\\n\\n50√n\\n\\n8n − 840\\n\\n400n − 42000\\n√n\\n−2.326348√n = 8n − 840 = 8(n − 105)\\n−0.2907935√n = n − 105\\n0 = n + 0.2907935√n − 105\\n\\nBy the quadratic formula, we solve for √n, keeping only the positve root.\\n\\n√n =\\n\\n0.2907935 +(cid:112)(0.2907935)2 − 4 · 1 · 105\\n\\n2 · 1\\n\\n= 10.10259\\n\\nand n = 102.0622. So, take n = 102.\\n11.8. The R code for the simulations is\\n> xbar<-numeric(1000)\\n> for (i in 1:1000)\\n\\n{x<-runif(100);xbar[i]<-mean(x)}\\n\\n> hist(xbar)\\n> mean(xbar)\\n[1] 0.498483\\n> sd(xbar)\\n[1] 0.02901234\\n> quantile(xbar,0.35)\\n\\n35%\\n0.488918\\n> qnorm(0.35)\\n[1] -0.3853205\\n\\nThe mean of a U [0, 1] random variable is µ = 1/2 and its variance\\nis σ2 = 1/12. Thus the mean of ¯X is 1/2, its standard deviation is\\n\\n(cid:112)1/(12 · 100) = 0.0289, close to the simulated values.\\n\\nUse qnorm(0.35) to see that the 35th percentile corresponds to\\n\\na z-score of -0.3853205. Thus, the 35th percentile for ¯X is approximately\\n\\nFigure 11.11: Histogram of the sample means of 100\\nrandom variables, uniformly distributed on [0, 1].\\n\\nµ + z0.35\\n\\nσ\\n√n\\n\\n= 0.5 − 0.3853205\\n\\n1\\n\\n√1200\\n\\n= 0.4888768,\\n\\nagreeing to four decimal places the value given by the simulations of xbar.\\n\\nAlternatively, we can use the qnorm command with the appropriate mean and standard deviation.\\n\\n209\\n\\nHistogram of xbarxbarFrequency0.400.450.500.550.60050100150200250\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n> qnorm(0.35,0.5,sqrt(1/1200))\\n[1] 0.4888768\\n11.12. A Poisson random variable with parameter λ = 16 has mean 16 and standard deviation 4 = √16. Thus, we\\nﬁrst look at the maximum difference in the distribution function of a P ois(4) random variable, X, and a N (16, 4)\\nrandom variable, Y , by comparing P{X ≤ x} to P{Y ≤ x + 1\\n> x<-4:28\\n> max(abs(pnorm(x+0.5,16,4)-ppois(x,16)))\\n[1] 0.01648312\\n\\n2} in a range around the mean value of 16.\\n\\nThe maximum difference between the distribution function is approximately 1.6%. To compare the density functions,\\nwe have the R commands. (See Figure 11.11.)\\n\\n> poismass<-dpois(x,16)\\n> plot(x,poismass,ylim=c(0,0.1),\\n\\nylab=\"probability\")\\n\\n> par(new=TRUE)\\n> x<-seq(4,28,0.01)\\n> normd<-dnorm(x,16,4)\\n> plot(x,normd,ylim=c(0,0.1),\\n\\nylab=\"probability\",type=\"l\",col=\"red\")\\n\\n11.13. Using (11.4)\\n\\nE[a + b(Y − µY )] = E[a − bµy + bY ] = a − bµY + bµY = b.\\nand\\n\\nVar(a + b(Y − µY )) = Var(a − bµy + bY ) = b2Var(Y ).\\n\\n11.15. Using right triangle trigonometry, we have that\\n\\nθ = g((cid:96)) = tan\\n\\n−1(cid:18) (cid:96)\\n\\n10(cid:19) . Thus,\\n\\n(cid:48)\\n\\ng\\n\\n((cid:96)) =\\n\\nFigure 11.12: Circles indicate the mass function for a\\nP ois(16) random variable. The red curve is the density func-\\ntion of a N (16, 4) random variable. The plots show that the\\nPoisson random variable is slightly more skewed to the right\\nthat the normal.\\n1 + ((cid:96)/10)2 =\\n\\n100 + (cid:96)2 .\\n\\n1/10\\n\\n10\\n\\nSo, σˆθ ≈ 10/(100 + (cid:96)2) · σ(cid:96). For example, set σ(cid:96) = 0.1 meter and (cid:96) = 5. Then, σˆθ ≈ 10/125 · 0.1 = 1/125 radians\\n= 0.49◦.\\n11.16. In this case,\\n\\nFor the partial derivatives, we use the chain rule\\n\\n∂g\\n∂(cid:96)\\n\\nThus,\\n\\n((cid:96), h) =\\n\\n1\\n\\nh(cid:19) =\\n1 + ((cid:96)/h)2(cid:18) 1\\nσˆθ ≈(cid:115)(cid:18) h\\nh2 + (cid:96)2(cid:19)2\\n\\nIf σh = σ(cid:96), let σ denote their common value. Then\\n\\nh\\n\\nθ = g((cid:96), h) = tan1(cid:18) (cid:96)\\nh(cid:19) .\\n1 + ((cid:96)/h)2(cid:18)−(cid:96)\\nh2 + (cid:96)2(cid:113)h2σ2\\n\\nh2 + (cid:96)2(cid:19)2\\n\\n((cid:96), h) =\\n\\nh2 + (cid:96)2\\n\\nσ2\\nh =\\n\\n∂g\\n∂h\\n\\n1\\n\\n1\\n\\nσ2\\n\\n(cid:96) +(cid:18) (cid:96)\\nh2 + (cid:96)2(cid:112)h2σ2 + (cid:96)2σ2 =\\n\\n1\\n\\nσ\\n\\n√h2 + (cid:96)2\\n\\n.\\n\\n210\\n\\nσˆθ ≈\\n\\nh2(cid:19) = −\\n\\n(cid:96)\\n\\nh2 + (cid:96)2\\n\\n(cid:96) + (cid:96)2σ2\\nh.\\n\\n5101520250.000.020.040.060.080.10x5101520250.000.020.040.060.080.10x\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\nIn other words, σˆθ is inversely proportional to the length of the hypotenuse.\\n11.17. Let µi be the mean of the i-th measurement. Then\\n\\nσg(Y1,Y2,·,Ydt) ≈(cid:115)(cid:18) ∂g\\n\\n∂y1\\n\\n(µ1, . . . , µd)(cid:19)2\\n\\nσ2\\n\\n1 +(cid:18) ∂g\\n\\n∂y2\\n\\n(µ1, . . . , µd)(cid:19)2\\n\\nσ2\\n\\n2 + ··· +(cid:18) ∂g\\n\\n∂yd\\n\\n(µ1, . . . , µd)(cid:19)2\\n\\nσ2\\nd.\\n\\n11.21. Recall that for random variables X1, X2, . . . Xn and constants c1, c2, . . . cn,\\n\\nVar(c0 + c1X1 + c2X2 + ··· + cnXn) =\\n\\ncicjCov(Xi, Xj) =\\n\\ncicjρi,jσXiσXj .\\n\\nwhere ρi,j is the correlation of Xi and Xj. Note that the correlation of a random variable with itself, ρi,i = 1. For the\\ndelta method,\\n\\nci =\\n\\ng(µ1, µ2 . . . , µn), Xi = ¯Yi,\\n\\nand σXi =\\n\\n∂\\n∂yi\\n\\nn(cid:88)i=1\\n\\nn(cid:88)j=1\\n\\nσi\\n√ni\\n\\n.\\n\\nn(cid:88)i=1\\n\\nn(cid:88)j=1\\n\\n. Then\\n\\nVar(g( ¯Y1, ¯Y2, . . . , Yn)) ≈\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nn(cid:88)j=1\\n\\n∂\\n∂yi\\n\\ng(µ1, µ2 . . . , µn)\\n\\n∂\\n∂yj\\n\\ng(µ1, µ2 . . . , µn)ρi,jσiσj.\\n\\n11.22. Let µF , p, µN be the means of the variables under consideration. Then we have the linear approximation,\\n\\ng( ¯F , ˆp, ¯N ) ≈ g(F, p, N ) +\\n\\n∂g\\n∂F\\n\\n(F, p, N )( ¯F − F ) +\\n\\n∂g\\n∂p\\n\\n(F, p, N )(ˆp − p) +\\n\\n∂g\\n∂N\\n\\n(F, p, N )( ¯N − µN ).\\n\\n= g(F, p, N ) + pN ( ¯F − F ) + F N (ˆp − p) + F p( ¯N − µN )\\n\\nMatching this to the covariance formula, we have\\n\\nc0 = g(F, p, N ),\\n\\nc1 = pN,\\n\\nc2 = F N,\\n\\nc3 = F p,\\n\\nX1 = ¯F , X2 = ˆp, X3 = ¯N .\\n\\nThus,\\n\\nσ2\\nB,n =\\n\\n(pN σF )2 +\\n\\n1\\nnF\\n+2F pN 2ρF,p\\n\\n1\\nnp\\nσF σp\\n√nF np\\n\\n(F N σp)2 +\\n\\n1\\nnN\\n\\n(pF σN )2\\n\\n+ 2F p2N ρF,N\\n\\nσF σN\\n√nF nN\\n\\n+ 2F 2pN ρp,N\\n\\nσpσN\\n√nF nN\\n\\n.\\n\\nThe subscripts on the correlation coefﬁcients ρ have the obvious meaning.\\n\\n211\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Central Limit Theorem\\n\\n212\\n\\n\\x0cPart III\\n\\nEstimation\\n\\n213\\n\\n\\x0c\\x0cTopic 12\\n\\nOverview of Estimation\\n\\nInference is the problem of turning data into knowledge, where knowledge often is expressed in terms of\\nentities that are not present in the data per se but are present in models that one uses to interpret the data.\\nStatistical rigor is necessary to justify the inferential leap from data to knowledge, and many difﬁculties\\narise in attempting to bring statistical principles to bear on massive data. Overlooking this foundation\\nmay yield results that are, at best, not useful, or harmful at worst. In any discussion of massive data\\nand inference, it is essential to be aware that it is quite possible to turn data into something resembling\\nknowledge when actually it is not. Moreover, it can be quite difﬁcult to know that this has happened. -\\npage 2, Frontiers in Massive Data Analysis by the National Research Council, 2013.\\n\\nThe balance of this book is devoted to developing formal procedures of statistical inference. In this introduction\\nto inference, we will be basing our analysis on the premise that the data have been collected according to carefully\\nplanned procedures informed by the appropriate probability models. We will focus our presentation on parametric\\nestimation and hypothesis testing based on a given family of probability models chosen in line with the science under\\ninvestigation and with the data collection procedures.\\n\\n12.1 Introduction\\nIn the simplest possible terms, the goal of estimation theory is to answer the question:\\n\\nWhat is that number?\\n\\nWhat is the length, the reaction rate, the fraction displaying a particular behavior, the temperature, the kinetic\\nenergy, the Michaelis constant, the speed of light, mutation rate, the melting point, the probability that the dominant\\nallele is expressed, the elasticity, the force, the mass, the free energy, the mean number of offspring, the focal length,\\nmean lifetime, the slope and intercept of a line?\\n\\nThe next step is to perform an experiment that is well designed to estimate one (or more) numbers. However, before\\nwe can embark on such a design, we must be informed by the principles of estimation in order to have an understanding\\nof the properties of a good estimator and to present our uncertainties concerning the estimate. Statistics has provided\\ntwo distinct approaches - typically called classical or frequentist and Bayesian. We shall give an overview of both\\napproaches. However, this text will emphasize the classical approach.\\n\\nLet’s begin with a deﬁnition:\\n\\nDeﬁnition 12.1. A statistic is a function of the data that does not depend on any unknown parameter.\\n\\nExample 12.2. We have to this point, seen a variety of statistics.\\n\\n• sample mean, ¯x\\n\\n215\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\n• sample variance, s2\\n• sample standard deviation, s\\n• sample median, sample quartiles Q1, Q3, percentiles and other quantiles\\n• standardized scores (xi − ¯x)/s\\n• order statistics x(1), x(2), . . . x(n), including sample maximum and minimum\\n• sample moments\\n\\nxm =\\n\\nxm\\nk , m = 1, 2, 3, . . . .\\n\\n1\\nn\\n\\nn(cid:88)k=1\\n\\nHere, we will look at a particular type of parameter estimation, in which we consider X = (X1, . . . , Xn), inde-\\npendent random variables chosen according to one of a family of probabilities Pθ where θ is element from the param-\\neter space Θ. Based on our analysis, we choose an estimator ˆθ(X). If the data x takes on the values x1, x2, . . . , xn,\\nthen\\n\\nˆθ(x1, x2, . . . , xn)\\n\\nis called the estimate of θ. Thus we have three closely related objects,\\n\\n1. θ - the parameter, an element of the parameter space Θ. This is a number or a vector.\\n\\n2. ˆθ(x1, x2, . . . , xn) - the estimate. This again is a number or a vector obtained by evaluating the estimator on the\\n\\ndata x = (x1, x2, . . . , xn).\\n\\n3. ˆθ(X1, . . . , Xn) - the estimator. This is a random variable. We will analyze the distribution of this random\\n\\nvariable to decide how well it performs in estimating θ.\\n\\nThe ﬁrst of these three objects is a number. The second is a statistic. The third can be analyzed and its properties\\ndescribed using the theory of probability. Keeping the relationship among these three objects in mind is essential in\\nunderstanding the fundamental issues in statistical estimation.\\n\\nExample 12.3. For Bernoulli trials X = (X1, . . . , Xn), each Xi, i = 1, . . . , n can take only two values 0 and 1. We\\nhave\\n\\n1. p, a single parameter, the probability of success, with parameter space [0, 1]. This is the probability that a single\\n\\nBernoulli takes on the value 1.\\n\\n2. ˆp(x1, . . . , xn) is the sample proportion of successes in the data set.\\n\\n3. ˆp(X1, . . . , Xn), the sample mean of the random variables\\n\\nˆp(X1, . . . , Xn) =\\n\\n1\\nn\\n\\n(X1 + ··· + Xn) =\\n\\n1\\nn\\n\\nSn\\n\\nis an estimator of p. In this case the Xi are Bernoulli trials. Consequently, we can give the distribution of this\\nestimator because Sn is a binomial random variable.\\n\\nExample 12.4. Given pairs of observations (x, y) = ((x1, y1), (x2, y2), . . . , (xn, yn)) that display a general linear\\npattern, we use ordinary least squares regressn for\\n\\n1. parameters - the slope β and intercept α of the regression line. So, the parameter space is R2, pairs of real\\n\\nnumbers.\\n\\n216\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\n2. They are estimated using the statistics ˆβ and ˆα in the equations\\n\\nˆβ(x, y) =\\n\\ncov(x, y)\\n\\nvar(x)\\n\\n,\\n\\n¯y = ˆα(x, y) + ˆβ(x, y)¯x.\\n\\n3. Later, when we consider statistical inference for linear regression, we will analyze the distribution of the esti-\\n\\nmators.\\n\\nExercise 12.5. Let X = (X1, . . . , Xn) be independent uniform random variables on the interval [0, θ] with θ un-\\nknown. Give some estimators of θ from the statistics above.\\n\\n12.2 Classical Statistics\\nIn classical statistics, the state of nature is assumed to be ﬁxed, but unknown to us. Thus, one goal of estimation is to\\ndetermine which of the Pθ is the source of the data. The estimate is a statistic\\n\\nˆθ : data → Θ.\\n\\nIntroduction to estimation in the classical approach to statistics is based on two fundamental questions:\\n• How do we determine estimators?\\n• How do we evaluate estimators?\\nWe can ask if this estimator in any way systematically under or over estimate the parameter, if it has large or small\\nvariance, and how does it compare to a notion of best possible estimator. How easy is it to determine and to compute\\nand how does the procedure improve with increased sample size?\\n\\nThe raw material for our analysis of any estimator is the distribution of the random variables that underlie the\\ndata under any possible value θ of the parameter. To simplify language, we shall use the term density function to\\nrefer to both continuous and discrete random variables. Thus, to each parameter value θ ∈ Θ, there exists a density\\nfunction which we denote\\n\\nWe focus on experimental designs based on a simple random sample. To be more precise, the data are assumed\\n\\nto be a sample from a sequence of random variables\\n\\nfX (x|θ).\\n\\nX1(ω), . . . , Xn(ω),\\n\\ndrawn from a family of distributions having common density fX (x|θ) where the parameter value θ is unknown and\\nmust be estimated. Because the random variables are independent, the joint density is the product of the marginal\\ndensities.\\n\\nfX (x|θ) =\\n\\nfX (xk|θ) = fX (x1|θ)fX (x2|θ)··· fX (xn|θ).\\n\\nn(cid:89)k=i\\n\\nIn this circumstance, the data x are known and the parameter θ is unknown. Thus, we write the density function as\\n\\nand call L the likelihood function.\\n\\nBecause the algebra and calculus of the likelihood function are a bit unfamiliar, we will look at several examples.\\n\\nL(θ|x) = fX (x|θ)\\n\\nExample 12.6 (Parametric families of densities).\\n\\n217\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\n1. For Bernoulli trials with a known number of trials n but unknown success probability parameter p has joint\\n\\ndensity\\n\\n(cid:80)n\\n(cid:80)n\\nfX (x|p) = px1(1 − p)1−x1px2 (1 − p)1−x2 ··· pxn (1 − p)1−xn = p\\nk=1(1−xk)\\nk=1 xk (1 − p)\\n\\n(cid:80)n\\nk=1 xk (1 − p)n−(cid:80)n\\n\\n= p\\n\\nk=1 xk = pn¯x(1 − p)n(1−¯x)\\n\\n2. Normal random variables with known variance σ0 but unknown mean µ has joint density\\n\\nfX (x|µ) =\\n\\n=\\n\\n1\\n\\nσ0√2π\\n1\\n\\n(σ0√2π)n\\n\\nexp(cid:18)−\\nexp(cid:32)−\\n\\n(x1 − µ)2\\n\\n2σ2\\n\\n0 (cid:19) ·\\nn(cid:88)k=1\\n\\n1\\n\\nσ0√2π\\n(xk − µ)2(cid:33)\\n\\n1\\n2σ2\\n0\\n\\nexp(cid:18)−\\n\\n(x2 − µ)2\\n\\n2σ2\\n\\n0 (cid:19)···\\n\\n1\\n\\nσ0√2π\\n\\nexp(cid:18)−\\n\\n(xn − µ)2\\n\\n2σ2\\n0\\n\\n(cid:19)\\n\\n3. Normal random variables with unknown mean µ and variance σ has density\\n\\nfX (x|µ, σ) =\\n\\n1\\n\\n(σ√2π)n\\n\\nexp(cid:32)−\\n\\n1\\n2σ2\\n\\n(xk − µ)2(cid:33) .\\nn(cid:88)k=1\\n\\n4. Beta random variables with parameters α and β has joint desity\\n\\nfX (x|α, β) =(cid:18) Γ(α + β)\\nΓ(α)Γ(β)(cid:19)n\\nΓ(α)Γ(β)(cid:19)n(cid:32) n(cid:89)i=1\\n=(cid:18) Γ(α + β)\\n\\n(x1 · x2 ··· xn)α−1((1 − x1) · (1 − x2)··· (1 − xn))β−1\\nxi(cid:33)α−1(cid:32) n(cid:89)i=1\\n\\n(1 − xi)(cid:33)β−1\\n\\nExercise 12.7. Give the likelihood function for n observations of independent Γ(α, β) random variables.\\n\\nThe choice of a point estimator ˆθ is often the ﬁrst step. For the next three topics, we consider two approaches\\nfor determining estimators - method of moments and maximum likelihood.\\nIn between the introduction of these\\ntwo estimation procedures, we will develop analyses of the quality of the estimator. With this in view, we will\\nprovide methods for approximating the bias and the variance of the estimators. Typically, this information is, in\\npart, summarized though what is know as an interval estimator. This is a procedure that determines a subset of the\\nparameter space with high probability that it contains the real state of nature. We see this most frequently in the use of\\nconﬁdence intervals.\\n\\n12.3 Bayesian Statistics\\nFor a few tosses of a coin always that always turn up tails, the estimate ˆp = 0 for the probability of heads did not\\nseem reasonable to Thomas Bayes. He wanted a way to place our uncertainly of the value for p into the procedure for\\nestimation.\\n\\nToday, the Bayesian approach to statistics takes into account not only the density\\n\\nfX|Θ(x|ψ)\\n\\nfor the data collected for any given experiment but also external information to determine a prior density π on the\\nparameter space Θ. Thus, in this approach, both the parameter and the data are modeled as random. Estimation is\\nbased on Bayes formula. We now want to take Bayes theorem, previously derived for a ﬁnite partition and obtain a\\nformula useful for Bayesian estimation. The ﬁrst step will yield a formula based on a discrete mixture. We will then\\nneed to introduce the notion of a continuous mixture to give us the ﬁnal formula, (12.4).\\n\\n218\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nr\\n\\nFigure 12.1: Plot of density π(ψ) in black for continuous mixture and the approximating discrete mixture with weights ˜π( ˜ψ) proportional to the\\nheights of the red vertical lines.\\n\\nLet ˜Θ be a random variable having the given prior density π. In the case in which both ˜Θ and the data take on only\\n\\na ﬁnite set of values, ˜Θ is a discrete random variable and π is a mass function\\n\\nπ{ψ} = P{ ˜Θ = ψ}.\\n\\nLet Cψ = { ˜Θ = ψ} be the event that ˜Θ takes on the value ψ and A = {X = x} be the values taken on by the data.\\nThen {Cψ, ψ ∈ Θ} form a partition of the probability space. Bayes formula is\\n\\n(cid:80)\\n= P (A|Cθ)P (Cθ)\\nψ P (A|Cψ)P (Cψ)\\n(cid:80)\\nfΘ|X (θ|x) = P{ ˜Θ = θ|X = x} = P{X=x| ˜Θ=θ}P{ ˜Θ=θ}\\n\\nP (Cθ|A)\\n\\nψ P{X=x| ˜Θ=ψ}P{ ˜Θ=ψ} =\\n\\nor\\nfX|Θ(x|θ)π{θ}\\n\\n(cid:80)ψ fX|Θ(x|ψ)π{ψ}\\n\\n(12.1)\\n\\n.\\n\\nGiven data x, the function of θ, fΘ|X (θ|x) = P{ ˜Θ = θ|X = x} is called the posterior density.\\nRemark 12.8. As we learned in tte section on Random Variables and Distribution functions, the expression\\n\\n(cid:88)ψ\\n\\nfX|Θ(x|ψ)π{ψ}\\n\\nis the mixture of the densities fX|Θ(x|ψ) for ψ in the ﬁnite set with weights π(ψ). Typically the parameter space Θ\\nis continuous and so we want to use the density π of a continuous random variable. To determine an expression for a\\ncontinuous mixture, we will be guided by the ideas used deriving the formula for the expected value for a continuous\\nrandom variable based on the formula for a discrete random variable. Beginning with the property\\n\\n(cid:90)θ\\n\\nπ(ψ)dψ = 1 we have, for a Riemann sum, (cid:88)˜ψ\\n\\nπ( ˜ψ)∆ψ ≈ 1.\\n\\nNow, write\\n\\n˜π{ ˜ψ} = π( ˜ψ)∆ψ.\\n\\n219\\n\\n(12.2)\\n\\n-1.0-0.50.00.51.00.00.10.20.30.40.50.60.7psidensity-1.0-0.50.00.51.00.00.10.20.30.40.50.60.7\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nThen ˜π is (approximately) the density function for a discrete random variable. If we take a mixture of fX|Θ(x| ˜ψ)\\n\\nwith weights ˜π( ˜ψ), we have the mixture density\\n\\n(cid:88)˜ψ\\n\\nfX|Θ(x|ψ)˜π{ ˜ψ} =(cid:88)˜ψ\\n\\nfX|Θ(x|ψ)π( ˜ψ)∆ψ.\\n\\nThis last sum is a Riemann sum and so taking limits as ∆ψ → 0, we have that the Riemann sum converges to the\\ndeﬁnite integral. This gives us the continuous mixture\\n\\nfX (x) =(cid:90)Θ\\n\\nfX|Θ(x|ψ)π(ψ) dψ.\\n\\n(12.3)\\n\\nExercise 12.9. Show that the expression (12.3) for fX (x) is a valid density function\\n\\nReturning to the expression (12.1), substituting (12.2), we have in the interval from θ to θ + ∆θ,\\n\\nfΘ|X (θ|x)∆θ =\\n\\nfX|Θ(x|θ)π(θ)∆θ\\n\\n(cid:80)ψ fX|Θ(x|ψ)π(ψ)∆ψ\\n\\n.\\n\\nAfter dividing by ∆θ and taking a limit as ∆ψ → 0, we have, for π, a density for a continuous random variable, that\\nthe sum in Bayes formula becomes an integral for a continuous mixture,\\nfX|Θ(x|θ)π(θ)\\n\\n(12.4)\\n\\nSometimes we shall write (12.4) as\\n\\nfΘ|X (θ|x) =\\n\\n(cid:82) fX|Θ(x|ψ)π(ψ) dψ\\n\\nfΘ|X (θ|x) = c(x)fX|Θ(x|θ)π(θ)\\n\\nwhere c(x), the reciprocal of continuous mixture (12.3) in the denominator of (12.4), is the value necessary so that the\\nintegral of the posterior density fΘ|X (θ|x) with respect to θ equals 1. We might also write\\n\\nfΘ|X (θ|x) ∝ fX|Θ(x|θ)π(θ)\\n\\n(12.5)\\n\\nwhere c(x) is the constant of proportionality.\\n\\nEstimation, e.g., point and interval estimates, in the Bayesian approach is based on the data and an analysis using\\nthe posterior density. For example, one way to estimate θ is to use the mean of the posterior distribution, or more\\nbrieﬂy, the posterior mean,\\n\\nˆθ(x) = E[θ|x] =(cid:90) θfΘ|X (θ|x) dθ.\\n\\nExample 12.10. As suggested in the original question of Thomas Bayes, we will make independent ﬂips of a biased\\ncoin and use a Bayesian approach to make some inference for the probability of heads. We ﬁrst need to set a prior\\ndistribution for ˜P . The beta family Beta(α, β) of distributions takes values in the interval [0, 1] and provides a\\nconvenient prior density π. Thus,\\n\\nπ(p) = cα,βp(α−1)(1 − p)(β−1),\\n\\n0 < p < 1.\\n\\nAny density on the interval [0, 1] that can be written a a power of p times a power of 1 − p times a constant chosen so\\nthat\\n\\nis a member of the beta family. This distribution has\\n\\n1 =(cid:90) 1\\n\\n0\\n\\nπ(p) dp\\n\\nmean\\n\\nα\\n\\nα + β\\n\\nand\\n\\nvariance\\n\\nαβ\\n\\n(α + β)2(α + β + 1)\\n\\n.\\n\\n(12.6)\\n\\n220\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nThus, the mean is the ratio of α and α + β. If the two parameters are each multiplied by a factor of k, then the\\nmean does not change. However, the variance is reduced by a factor close to k. The prior gives a sense of our prior\\nknowledge of the mean through the ratio of α to α + β and our uncertainly through the size of α and β\\n\\nThus the posterior distribution of the parameter ˜P given the data x, using (12.5), we have.\\n\\nIf we perform n Bernoulli trials, x = (x1, . . . , xn), then the joint density\\n\\n(cid:80)n\\nk=1 xk (1 − p)n−(cid:80)n\\nfX (x|p) = p\\n(cid:80)n\\nk=1 xk (1 − p)n−(cid:80)n\\n= cα,βpα+(cid:80)n\\nf ˜P|X (p|x) ∝ fX| ˜P (x|p)π(p) = p\\n\\nk=1 xk .\\n\\nk=1 xk · cα,βp(α−1)(1 − p)(β−1).\\n\\nk=1 xk−1(1 − p)β+n−(cid:80)n\\n\\nk=1 xk−1.\\n\\nConsequently, the posterior distribution is also from the beta family with parameters\\n\\nα +\\n\\nn(cid:88)k=1\\n\\nxk\\n\\nand β + n −\\n\\nxk = β +\\n\\nn(cid:88)k=1\\n\\nn(cid:88)k=1\\n\\n(1 − xk).\\n\\nα + # successes\\n\\nand β + # failures.\\n\\nNotice that the posterior mean can be written as\\n\\nα +(cid:80)n\\n\\nk=1 xk\\nα + β + n\\n\\n=\\n\\n=\\n\\n=\\n\\nα\\n\\nα + β + n\\n\\n+ (cid:80)n\\n\\nk=1 xk\\nα + β + n\\n\\nα\\n\\nα + β ·\\n\\nα\\n\\nα + β ·\\n\\nα + β\\n\\nα + β + n\\n\\nα + β\\n\\nα + β + n\\n\\n+\\n\\n1\\nn\\n\\nn(cid:88)k=1\\n\\nxk ·\\nn\\n\\n+ ¯x ·\\n\\nα + β + n\\n\\n.\\n\\nn\\n\\nα + β + n\\n\\nThis expression allow us to see that the posterior mean can be expresses as a weighted average α/(α + β) from the\\nprior mean and ¯x, the sample mean from the data. The relative weights are\\n\\nα + β from the prior\\n\\nand\\n\\nn, the number of observations.\\n\\nThus, if the number of observations n is small compared to α + β, then most of the weight is placed on the prior\\n\\nmean α/(α + β). As the number of observations n increase, then\\n\\nn\\n\\nα + β + n\\n\\nincreases towards 1. The weight result in a shift the posterior mean away from the prior mean and towards the sample\\nmean ¯x.\\n\\nThis brings forward two central issues in the use of the Bayesian approach to estimation.\\n• If the number of observations is small, then the estimate relies heavily on the quality of the choice of the prior\\n\\ndistribution π. Thus, an unreliable choice for π leads to an unreliable estimate.\\n\\n• As the number of observations increases, the estimate relies less and less on the prior distribution.\\n\\nIn this\\ncircumstance, the prior may simply be playing the roll of a catalyst that allows the machinery of the Bayesian\\nmethodology to proceed.\\n\\nExercise 12.11. Show that this answer is equivalent to having α heads and β tails in the data set before actually\\nﬂipping coins.\\n\\n221\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nExample 12.12. If we ﬂip a coin n = 14 times with 8 heads, then the classical estimate of the success probability p\\nis 8/14=4/7. For a Bayesian analysis with a beta prior distribution, using (12.6) we have a beta posterior distribution\\nwith the following parameters.\\n\\nprior\\n\\ndata\\n\\nα β mean\\n1/2\\n6\\n3/4\\n9\\n3\\n1/4\\n\\n6\\n3\\n9\\n\\nvariance\\n\\nheads\\n\\n1/52=0.0192\\n3/208=0.0144\\n3/208=0.0144\\n\\n8\\n8\\n8\\n\\ntails\\n6\\n6\\n6\\n\\nα\\n14\\n17\\n11\\n\\nβ\\n12\\n9\\n15\\n\\nposterior\\n\\nmean\\n\\nvariance\\n\\n14/(12+14)=7/13\\n17/(17+9) =17/26\\n11/(15+11)=11/26\\n\\n168/18542=0.0092\\n153/18252=0.0083\\n165/18542=0.0090\\n\\nFigure 12.2: Example of prior (black) and posterior (red) densities based on 14 coin ﬂips, 8 heads and 6 tails. Left panel: Prior is Beta(6, 6),\\nRight panel: Prior is Beta(9, 3). Note how the peak is narrowed. This shows that the posterior variance is smaller than the prior variance. In\\naddition, the peal moves from the prior towards ˆp = 4/7, the sample proportion of the number of heads.\\n\\nIn his original example, Bayes chose was the uniform distribution (α = β = 1) for his prior. In this case the\\n\\nposterior mean is\\n\\nFor the example above\\n\\n1\\n\\n2 + n(cid:32)1 +\\n\\nxk(cid:33) .\\n\\nn(cid:88)k=1\\n\\nprior\\n\\ndata\\n\\nα β mean\\n1\\n1/2\\n\\n1\\n\\nvariance\\n\\n1/12=0.0833\\n\\nheads\\n\\n8\\n\\ntails α β\\n6\\n7\\n\\n9\\n\\nposterior\\n\\nmean\\n\\nvariance\\n\\n9/(9+7)=9/16\\n\\n63/4352=0.0144\\n\\nThe Bayesian approach is amenable to sequential updating. For example, if we collect independent data in three\\n\\nbatches, say x = (x1, x2, x3), then the density for the entire data set x can be written\\n\\nTo set the notation, write\\n\\nfX|Θ(θ|x) = fX3|Θ(x3|θ) · fX2|Θ(x2|θ) · fX1|Θ(x1|θ).\\n\\n• X = (X1, X2, X3) for the the sequential sets of random variables associated to the observations,\\n• fΘ|X1(θ|x1) for the posterior density based on the data x1, and\\n\\n222\\n\\n0.00.20.40.60.81.001234x0.00.20.40.60.81.001234x0.00.20.40.60.81.001234x0.00.20.40.60.81.001234x\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\n• fΘ|X1,X2(θ|x1, x2) for the posterior density based on the data (x1, x2),\\n\\nThen, the posterior density\\n\\nfΘ|X (θ|x) ∝ fX|Θ(x1, x2, x3|θ)π(θ) = fX3|Θ(x3|θ) · fX2|Θ(x2|θ) · fX1|Θ(x1|θ)π(θ)\\n\\n= fX3|Θ(x3|θ) · fX2|Θ(x2|θ) · fΘ|X1(θ|x1)\\n= fX3|Θ(x3|θ) · fΘ|X1,X2(θ|x1, x2)\\n\\nThus,\\n\\n• The posterior density fΘ|X1(θ|x1) ∝ fX1|Θ(x1|θ)π(θ) serves are the prior density for (x2, x3).\\n• The posterior density fΘ|X1,X2(θ|x1, x2) ∝ fX2|Θ(x2|θ) · fΘ|X1 (θ|x1) serves are the prior density for x3.\\n\\nOf course, this strategy can be used for any number of sequential updates.\\nExample 12.13. Extending the example on the original use of Bayes estimation, the observations x1 consist of 8 heads\\nand 6 tails, .x2 consist of 8 heads and 4 tails, and x3 consist of 9 heads and 4 tails, We start with a Beta(1, 1) prior\\nand so all of the subsequent posteriors will have a Beta(α, β) distribution. The data and the parameter values are\\nshown in the table below.\\n\\nprior\\n\\n(x1, x2, x3)\\n\\n(x2, x3)\\n\\nx3\\n\\nα\\n1\\n9\\n17\\n\\nβ\\n1\\n7\\n11\\n\\ndata\\n\\nobservations\\n\\nheads\\n\\nx1\\nx2\\nx3\\n\\n8\\n8\\n9\\n\\ntails\\n6\\n4\\n4\\n\\nNotice that the posterior for one stage serves as the prior for the next.\\n\\nx1\\n\\nposterior\\nα\\n9\\n17\\n26\\n\\n(x1, x2)\\n\\n(x1, x2, x3)\\n\\nβ\\n7\\n11\\n15\\n\\nFigure 12.3: Bayesian updating. With a beta distributed prior and Bernoulli trials, the posterior densities are also beta distributed. Successive\\nupdates are shown in blue, magenta, and red.\\n\\nExample 12.14. Reliability engineering emphasizes dependability of a product by assessing the ability of a system\\nor component to function. We introduce the Bayesian perspective to reliability through a the consideration of the\\nreliability of simple devise. Our analysis is based on an extension the ideas of Bernoulli trials example above.\\n\\nA devise consists of two independent units. Let Ai, i = 1, 2 be the event that the i-th unit is operating appropriately\\nand deﬁne the probability pi = P (Ai). Then, the devise works with probability p1p2 = P (A1∩ A2). For each unit, we\\n\\n223\\n\\n0.00.20.40.60.81.0012345xdensity0.00.20.40.60.81.0012345xdensity0.00.20.40.60.81.0012345xdensity0.00.20.40.60.81.0012345xdensity\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nplace independent Beta(αi, βi) prior distributions. Next we test ni units of type i. Repeating the steps in a previous\\nexercise, we ﬁnd that yi units are functioning, then the posterior distribution are also in the beta family,\\n\\nBeta(α1 + y1, β1 + n1 − y1) and Beta(α2 + y2, β2 + n2 − y2),\\n\\nrespectively. This results in a joint posterior density\\n\\nfP1,P2|Y1,Y2(p1, p2|y1, y2) = c(α1, β1, n1)c(α2, β2, n2) pα1+x1\\n\\n1\\n\\n(1 − p1)β1+n1−y1 · pα2+x2\\n\\n2\\n\\n(1 − p2)β1+n2−y2.\\n\\nTo ﬁnd the posterior distribution of p = p1p2 that the devise functions, we integrate to ﬁnd the cumulative distribution\\nfunction.\\n\\nFP|Y1,Y2(p|y1, y2) =(cid:90) (cid:90){p1p2≤p}\\n\\nfP1,P2|Y1,Y2(p1, p2|y1, y2) dp2dp1.\\n\\nWe can also simulate using the rbeta command to estimate values for this distribution functions.\\n\\nTo provide a concrete example, assume a uniform prior (α1 = β1 = α2 = β2 = 1) and test twenty units of each\\n\\ntype (n1 = n2 = 20). If 15 and 17 of the devises work (y1 = 15, y2 = 17), then the posteriors distributions are\\n\\nBeta(16, 6) and Beta(18, 4),\\n\\nWe simulate this in R to ﬁnd the distribution of the posterior probability of p = p1p2.\\n\\n> p1<-rbeta(10000,16,6);p2<-rbeta(10000,18,4)\\n> p<-p1*p2\\n\\nWe then give a table of deciles for the posterior distribution function and present a histogram.\\n\\n> data.frame(quantile(p,d))\\n\\nquantile.p..d.\\n0.2825593\\n0.4660896\\n0.5094321\\n0.5422747\\n0.5712765\\n0.5968341\\n0.6209610\\n0.6477835\\n0.6776208\\n0.7187307\\n0.9234834\\n\\n0%\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\n70%\\n80%\\n90%\\n100%\\n> hist(p)\\n\\nThe posterior density fP|Y1,Y2 (p|y1, y2) is non-negative throughout the interval from 0 to 1, but is very small for\\nvalues near 0 and 1. Indeed, none of the 10,000 simulations give a posterior probability below 0.282 or above 0.923.\\nWe could take the mean of the simulated sample as a point estimate ˆp for p.\\n\\n> mean(p);sd(p)\\n[1] 0.5935356\\n[1] 0.09661065\\n\\nThis is very close to the means from the beta distributions.\\n\\nE ˆp = Ep1p2 = Ep1Ep2 =\\n\\n16\\n22 ·\\n\\n18\\n22\\n\\n=\\n\\n72\\n121\\n\\n= 0.595.\\n\\n224\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nFigure 12.4: Histogram of simulated posterior distribution of the reliability p = p1p2 where p1 and p2 have independent beta distributions based\\non the prior distributions and the data.\\n\\nExercise 12.15. The simulation variance is also indicated. Compare this answer with the answer given by the delta\\nmethod.\\n\\nExample 12.16. Suppose that the prior density is a normal random variable with mean θ0 and variance 1/λ. This\\nway of giving the variance may seem unusual, but we will see that λ is a measure of information. Thus, low variance\\nmeans high information. Our data x are a realization of independent normal random variables with unknown mean\\nθ. We shall choose the variance to be 1 to set a scale for the size of the variation in the measurements that yield the\\ndata x. We will present this example omitting some of the algebraic steps to focus on the central ideas.\\n\\nThe prior density is\\n\\nπ(θ) =(cid:114) λ\\n\\n2π\\n\\nexp(cid:18)−\\n\\nλ\\n2\\n\\n(θ − θ0)2(cid:19) .\\n\\nWe rewrite the density for the data to empathize the difference between the parameter θ for the mean and the ¯x, the\\nsample mean.\\n\\nfX|Θ(x|θ) =\\n\\n=\\n\\n1\\n\\n(2π)n/2 exp(cid:32)−\\n(2π)n/2 exp(cid:32)−\\n\\n1\\n\\n1\\n2\\n\\nn\\n2\\n\\n(xi − θ)2(cid:33)\\nn(cid:88)i=1\\nn(cid:88)i=1\\n(θ − ¯x)2 −\\n\\n1\\n2\\n\\n(xi − ¯x)2(cid:33) .\\n\\nThe posterior density is proportional to the product fX|Θ(x|θ)π(θ), Because the posterior is a function of θ, we\\n\\n225\\n\\nHistogram of ppFrequency0.30.40.50.60.70.80.90500100015002000\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nneed only keep track of the terms which involve θ. Consequently, we write the posterior density as\\n\\nfΘ|X (θ|x) = c(x) exp(cid:18)−\\n\\n1\\n2\\n\\n= ˜c(x) exp(−\\n\\n(n(θ − ¯x)2 + λ(θ − θ0)2)(cid:19)\\n(θ − θ1(x))2).\\n\\n2\\n\\nn + λ\\n\\nwhere\\n\\nθ1(x) =\\n\\nλ\\n\\nλ + n\\n\\nθ0 +\\n\\nn\\n\\nλ + n\\n\\n¯x.\\n\\n(12.7)\\n\\nNotice that the posterior distribution is normal with mean θ1(x) that results from the weighted average with\\n\\nrelative weights\\n\\nλ from the information from the prior\\n\\nand n from the data.\\n\\nThe variance in inversely proportional to the total information λ + n. Thus, if n is small compared to λ, then θ1(x) is\\nnear θ0. If n is large compared to λ, θ1(x) is near ¯x.\\n\\nFigure 12.5: Example of prior (black) and posterior (red) densities for a normal prior distribution and normally distributed data. In this ﬁgure the\\nprior density is N (1, 1/2). Thus, θ0 = 1 and λ = 2. Here the data consist of 3 observations having sample mean ¯x = 2/3. Thus, the posterior\\nmean from equation (12.7) is θ1(x) = 4/5 and the variance is 1/(2+3) = 1/5.\\n\\nExercise 12.17. Fill in the steps in the derivation of the posterior density in the example above.\\n\\nExercise 12.18. Use sequential updating for the normal family of distribution in the example above. The prior and\\nthe summary statistics are below.\\n\\nprior\\n\\ndata\\n\\n(x1, x2, x3)\\n\\n(x2, x3)\\n\\nx3\\n\\nµ σ2\\n0\\n4\\n\\nobservations n\\n6\\n3\\n3\\n\\nx1\\nx2\\nx3\\n\\n¯x\\n\\n1.216\\n1.911\\n0.811\\n\\ns2\\n\\n1.042\\n0.432\\n0.348\\n\\nposterior\\n\\nµ σ2\\n\\nx1\\n\\n(x1, x2)\\n\\n(x1, x2, x3)\\n\\n226\\n\\n-3-2-101230.00.51.01.52.0x-3-2-101230.00.51.01.52.0x\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nShow that the answer is the same if we aggregate the data ﬁrst.\\n\\nFor these two examples, we see that the prior distribution and the posterior distribution are members of the same\\nparameterized family of distributions, namely the beta family and the normal family. In these two cases, we say that\\nthe prior density and the density of the data form a conjugate pair. In the case of coin tosses, we ﬁnd that the beta and\\nthe Bernoulli families form a conjugate pair. In Example 12.11, we learn that the normal density is conjugate to itself.\\nTypically, the computation of the posterior density is much more computationally intensive that what was shown\\nin the two examples above. The choice of conjugate pairs is enticing because the posterior density is a determined\\nfrom a simple algebraic computation.\\n\\nBayesian statistics is seeing increasing use in the sciences, including the life sciences, as we see the explosive in-\\ncrease in the amount of data. For example, using a classical approach, mutation rates estimated from genetic sequence\\ndata are, due to the paucity of mutation events, often not very precise. However, we now have many data sets that can\\nbe synthesized to create a prior distribution for mutation rates and will lead to estimates for this and other parameters\\nof interest that will have much smaller variance than under the classical approach.\\n\\nExercise 12.19. Show that the gamma family of distributions is a conjugate prior for the Poisson family of distribu-\\ntions. Give the posterior mean based on n observations.\\n\\n12.4 Answers to Selected Exercises\\n12.5. Double the average, 2 ¯X. Take the maximum value of the data, max1≤i≤n xi. Double the difference of the\\nmaximum and the minimum, 2(max1≤i≤n xi − min1≤i≤n xi).\\n12.7. The density of a gamma random variable\\n\\nThus, for n observations\\n\\nf (x|α, β) =\\n\\nβα\\nΓ(α)\\n\\nxα−1e\\n\\n−βx.\\n\\n=\\n\\nL(θ|x) = f (x1|α, β)f (x2|α, β)··· f (xn|α, β)\\n−βx2 ···\\n\\n−βx1 βα\\nΓ(α)\\n\\nβα\\nΓ(α)\\nβnα\\n\\nxα−1\\nΓ(α)n (x1x2 ··· xn)α−1e\\n\\nβα\\nΓ(α)\\n−β(x1+x2+···+xn)\\n\\nxα−1\\n\\n=\\n\\ne\\n\\ne\\n\\n1\\n\\n2\\n\\nxα−1\\n\\nn\\n\\ne\\n\\n−βxn\\n\\n12.9. We need to verify that the density f is a non-negative function and that the integral over the sample space is 1.\\nNote that both fX|Θ(x|ψ) and π(ψ) and thus their product is positive. Consequently,\\nfor all x.\\n\\nfX|Θ(x|ψ)π(ψ) dψ ≥ 0\\n\\nfX (x) =(cid:90)Θ\\n\\nNext, we reverse the order of the double integral,\\n\\n(cid:90)Rn\\n\\nfX (x)dx =(cid:90)Rn(cid:18)(cid:90)Θ\\n\\nfX|Θ(x|ψ)π(ψ) dψ(cid:19) dx =(cid:90)Θ(cid:18)(cid:90)Rn\\n\\nfX|Θ(x|ψ) dx(cid:19) π(ψ)dψ.\\n\\nBecause fX|Θ(x|ψ), is a density function, the integral inside the parentheses is 1. Now use the fact that π is a\\nprobability density,\\n\\n(cid:90)Rn\\n\\nfX (x)dx =(cid:90)Θ\\n\\nπ(ψ)dψ = 1.\\n\\n227\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\n12.10. In this case the total number of observations is α + β + n and the total number of successes is α +(cid:80)n\\n\\nTheir ratio is the posterior mean.\\n\\ni=1 xi.\\n\\n12.15. Our goal is to estimate the variance of p = g(p1, p2) = p1p2 using the delta method. Let µi, σ2\\nmean and variance for unit i = 1, 2, respectively. For this we write\\n\\ni , be the posterior\\n\\nσ2\\ng(µ1,µ2) ≈\\n\\n=\\n\\n∂g\\n∂p1\\n16\\n22 ·\\n\\n∂g\\n∂p2\\n18\\n22 ·\\n\\n+\\n\\n=\\n\\n1 +\\n\\n(4 + 6) =\\n\\n1 + µ1σ2\\n2\\n\\n2 = µ2σ2\\n16 · 18\\n223 · 23\\n\\n(µ1, µ2)σ2\\n16 · 6\\n222 · 23\\n\\n(µ1, µ2)σ2\\n18 · 4\\n222 · 23\\ng(µ1,µ2) ≈ 0.1084 is about 12% higher than the estimate from the simulation.\\n((xi − ¯x) + (¯x − θ))2(cid:33)\\n\\n(2π)n/2 exp(cid:32)−\\n\\n(xi − θ)2(cid:33) =\\n\\n8 · 9 · 5\\n113 · 23\\n\\n= 0.01176.\\n\\n1\\n2\\n\\n1\\n2\\n\\n1\\n\\nn(cid:88)i=1\\n\\nThe estimated standard deviation σ2\\n\\nfX|Θ(x|θ) =\\n\\n1\\n\\n(2π)n/2 exp(cid:32)−\\n\\nn(cid:88)i=1\\n\\nThen we expand the square in the sum to obtain\\n\\n12.17. To include some of the details in the computation, we ﬁrst add and subtract ¯x in the sum for the joint density,\\n\\nn(cid:88)i=1\\n\\n((xi − ¯x) + (¯x − θ))2 =\\n\\n(xi − ¯x)2 + 2(cid:32) n(cid:88)i=1\\n\\n(xi − ¯x)(cid:33) (¯x − θ) +\\n\\nn(cid:88)i=1\\n\\n(¯x − θ)2\\n\\n(xi − ¯x)2 + 0 + n(¯x − θ)2\\n\\nThis gives the joint density\\n\\nfX|Θ(x|θ) =\\n\\nThe posterior density is\\n\\nn\\n2\\n\\n(θ − ¯x)2 −\\n\\n(xi − ¯x)2(cid:33) .\\n\\n1\\n2\\n\\nn(cid:88)i=1\\n\\nfΘ|X (θ|x) = c(x)fX|Θ(x|θ) · fΘ(θ)\\nn\\n2\\n\\n= c(x)\\n\\n1\\n\\n(2π)n/2 exp(cid:32)−\\n(2π)n/2(cid:114) λ\\n\\n2π\\n\\n(θ − ¯x)2 −\\nexp(cid:32)−\\n\\n1\\n2\\n\\n1\\n2\\n\\n(xi − ¯x)2(cid:33) ·(cid:114) λ\\nn(cid:88)i=1\\n(xi − ¯x)2(cid:33)(cid:33) exp(cid:18)−\\nn(cid:88)i=1\\n\\n1\\n2\\n\\n2π\\n\\nλ\\n2\\n\\n(θ − θ0)2(cid:19)\\n\\nexp(cid:18)−\\n(n(θ − ¯x)2 + λ(θ − θ0)2)(cid:19)\\n\\n1\\n\\n=(cid:32)c(x)\\n= c1(x) exp(cid:18)−\\n\\n1\\n2\\n\\n(n(θ − ¯x)2 + λ(θ − θ0)2)(cid:19) .\\n\\nHere c1(x) is the function of x in parenthesis. We now expand the expressions in the exponent,\\n\\nn(θ − ¯x)2 + λ(θ − θ0)2 = (nθ2 − 2n¯xθ + n¯x2) + (λθ2 − 2λθ0θ + λθ2\\n0)\\n= (n + λ)θ2 − 2(n¯x + λθ0)θ + (n¯x2 + λθ2\\n0)\\n= (n + λ)(cid:18)θ2 − 2\\nθ(cid:19) + (n¯x2 + λθ2\\n= (n + λ)(cid:0)θ2 − 2θ1(x)θ + θ1(x)2(cid:1) − (n + λ)θ1(x)2 + (n¯x2 + λθ2\\n= (n + λ)(θ − θ1(x))2 − (n + λ)θ1(x)2 + (n¯x2 + λθ2\\n0)\\n\\nn¯x + λθ0\\n\\nn + λ\\n\\n0)\\n\\n0)\\n\\n228\\n\\n=\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n(2π)n/2 exp(cid:32)−\\n\\n1\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nusing the deﬁnition of θ1(x) in (12.7) and completing the square.\\n\\nfΘ|X (θ|x) = c1(x) exp(cid:18)−\\n=(cid:18)c1(x) exp(cid:18)−\\n\\n1\\n2\\n\\n1\\n2\\n\\n((n¯x2 + λθ2\\n\\n((n¯x2 + λθ2\\n\\n0) − (n + λ)θ1(x)2 + (n + λ)(θ − θ1(x))2)(cid:19)\\n0) − (n + λ)θ(x)2)(cid:19)(cid:19) exp(−\\n\\nn + λ\\n\\n2\\n\\n(θ − θ1(x))2)\\n\\n= c2(x) exp(−\\n\\n2\\n\\n(θ − θ1(x))2)\\n\\nn + λ\\n\\nwhere c2(x) is the function of x in parenthesis. This give a posterior density that is normal, mean θ1(x) and variance\\nn + λ.\\n12.18. Using the formula in (12.7) for the mean. For the information, we have the transformation λ (cid:55)→ n + λ for n\\nobservations. With these two idesa, we compute the sequential updates.\\n\\nstatistics\\nprior\\nλ = 1/4, σ2 = 4,\\nn = 6,\\nλ = 25/4, σ2 = 4/25, µ = 1.16736\\nn = 3,\\nλ = 37/4, σ2 = 4/37, µ = 1.408541, n = 3,\\n\\nµ = 0,\\n\\nposterior\\n\\n¯x = 1.216, λ = 25/4, µ = 24/25 · 1.216 = 1.16736\\n¯x = 1.911, λ = 37/4, µ = 25/37 · 1.16736 + 12/37 · 1.911 = 1.408541\\n¯x = 0.811, λ = 49/4, µ = 37/49 · 1.408541 + 12/49 · 0.811 = 1.262204\\n\\nTo accomplish this in one step, note that\\n\\nn = 6 + 3 = 3 = 12,\\n\\n¯x =\\n\\n1\\n12\\n\\n(6 · 1.216 + 3 · 1.911 + 3 · 0.811) = 1.2885.\\n\\nprior\\nλ = 1/4, σ2 = 4, µ = 0,\\n\\nstatistics\\nn = 12, ¯x = 1.2885,\\n\\nposterior\\nλ = 49/4, µ = 48/49 · 1.2885 = 1.262204\\n\\nUnder either method, the posterior mean is 1.262204 and the posterior variance is 4/49. Notice that the sample\\n\\nvariance played no role in the computation. Thus, the complete table is:\\n\\nprior\\n\\nµ\\n0\\n\\n1.16736\\n1.408541\\n\\n(x1, x2, x3)\\n\\n(x2, x3)\\n\\nx3\\n\\ndata\\n\\nσ2\\n4\\n\\n4/25\\n4/37\\n\\nobservations n\\n6\\n3\\n3\\n\\nx1\\nx2\\nx3\\n\\n¯x\\n\\n1.216\\n1.911\\n0.811\\n\\ns2\\n\\n1.042\\n0.432\\n0.348\\n\\nposterior\\nµ\\n\\nx1\\n\\n(x1, x2)\\n\\n(x1, x2, x3)\\n\\n1.16736\\n1.408541\\n1.262204\\n\\nσ2\\n4/25\\n4/37\\n4/49\\n\\n12.19. For n observations x1, x2, . . . , xn of independent Poisson random variables having parameter λ, the joint\\ndensity is the product of the n marginal densities.\\n−λ =\\n\\nλx1+x2+···+xn e\\n\\n−nλ =\\n\\n−nλ.\\n\\nλn¯xe\\n\\n1\\n\\n1\\n\\ne\\n\\ne\\n\\ne\\n\\nfX (x|λ) =\\n\\nλx1\\nx1!\\n\\n−λ ·\\n\\nλx2\\nx2!\\n\\n−λ ···\\n\\nλxn\\nxn!\\n\\nx1!x2!··· xn!\\n\\nx1!x2!··· xn!\\n\\nThe prior density on λ has a Γ(α, β) density\\n\\nπ(λ) =\\n\\nβα\\nΓ(α)\\n\\nλα−1e\\n\\n−βλ.\\n\\nThus, the posterior density\\n\\nfΛ|X (λ|x) = c(x)λα−1e\\n\\n−βλ · λn¯xe\\n\\n−nλ = c(x)λα+n¯x−1e\\n\\n−(β+n)λ\\n\\nis the density of a Γ(α + n¯x, β + n) random variable. Its mean can be written as the weighted average\\n\\nα + n¯x\\nβ + n\\n\\n=\\n\\nα\\nβ ·\\n\\nβ\\n\\nβ + n\\n\\n+ ¯x ·\\n\\nn\\n\\nβ + n\\n\\n229\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nOverview of Estimation\\n\\nof the prior mean α/β and the sample mean ¯x. The weights are, respectively, proportional to β and the number of\\nobservations n.\\n\\nThe ﬁgure above demonstrate the case with a Γ(2, 1) prior density on λ and a sum x1 +x2 +x3 +x4 +x5 = 6 for 5\\nvalues for independent observations of a Poisson random random variable. Thus the posterior has a Γ(2 + 6, 1 + 5) =\\nΓ(8, 6) distribution.\\n\\n230\\n\\n0123450.00.20.40.60.81.0xdensity0123450.00.20.40.60.81.0xdensity\\x0cTopic 13\\n\\nMethod of Moments\\n\\n13.1 Introduction\\n\\nMethod of moments estimation is based solely on the law of large numbers, which we repeat here:\\n\\nLet M1, M2, . . . be independent random variables having a common distribution possessing a mean µM . Then the\\n\\nsample means converge to the distributional mean as the number of observations increase.\\n\\n¯Mn =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nMi → µM as n → ∞.\\n\\nTo show how the method of moments determines an estimator, we ﬁrst consider the case of one parameter. We\\nstart with independent random variables X1, X2, . . . chosen according to the probability density fX (x|θ) associated\\nto an unknown parameter value θ. The common mean of the Xi, µX, is a function k(θ) of θ. For example, if the Xi\\nare continuous random variables, then\\n\\nThe law of large numbers states that\\n\\nµX =(cid:90) ∞\\n\\n−∞\\n\\nxfX (x|θ) dx = k(θ).\\n\\n¯Xn =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nXi → µX as n → ∞.\\n\\nThus, if the number of observations n is large, the distributional mean, µ = k(θ), should be well approximated by\\n\\nthe sample mean, i.e.,\\n\\nThis can be turned into an estimator ˆθ by setting\\n\\n¯X ≈ k(θ).\\n\\n¯X = k(ˆθ).\\n\\nand solving for ˆθ.\\n\\nWe shall next describe the procedure in the case of a vector of parameters and then give several examples. We\\n\\nshall see that the delta method can be used to estimate the variance of method of moment estimators.\\n\\n231\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Method of Moments\\n\\n13.2 The Procedure\\nMore generally, for independent random variables X1, X2, . . . chosen according to the probability distribution derived\\nfrom the parameter value θ and m a real valued function, if k(θ) = Eθm(X1), then\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nm(Xi) → k(θ)\\n\\nas n → ∞.\\n\\nThe method of moments results from the choices m(x) = xm. Write\\n\\nµm = EX m = km(θ).\\n\\n(13.1)\\n\\nfor the m-th moment.\\n\\nOur estimation procedure follows from these 4 steps to link the sample moments to parameter estimates.\\n• Step 1. If the model has d parameters, we compute the functions km in equation (13.1) for the ﬁrst d moments,\\n\\nµ1 = k1(θ1, θ2 . . . , θd), µ2 = k2(θ1, θ2 . . . , θd),\\n\\n. . . , µd = kd(θ1, θ2 . . . , θd),\\n\\nobtaining d equations in d unknowns.\\n\\n• Step 2. We then solve for the d parameters as a function of the moments.\\n\\nθ1 = g1(µ1, µ2,··· , µd),\\n\\nθd = gd(µ1, µ2,··· , µd).\\n• Step 3. Now, based on the data x = (x1, x2, . . . , xn), we compute the ﬁrst d sample moments,\\n\\nθ2 = g2(µ1, µ2,··· , µd),\\n\\n. . . ,\\n\\n(13.2)\\n\\nx =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nxi,\\n\\nx2 =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nx2\\ni ,\\n\\n. . . ,\\n\\nxd =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nxd\\ni .\\n\\nUsing the law of large numbers, we have, for each moment, m = 1, . . . , d, that µm ≈ xm.\\nNB Sometimes, the central moments are more convenient. For the case of d = 2, the entails using\\n\\nin place of m1 and m2.\\n\\nm1\\n\\nand\\n\\nσ2 = m2 − m2\\n\\n1\\n\\n• Step 4. We replace the distributional moments µm by the sample moments xm, then the solutions in (13.2) give\\n\\nus formulas for the method of moment estimators (ˆθ1, ˆθ2, . . . , ˆθd). For the data x, these estimates are\\n\\nˆθ1(x) = g1(¯x, x2,··· , xd),\\n\\nˆθ2(x) = g2(¯x, x2,··· , xd),\\n\\n. . . ,\\n\\nˆθd(x) = gd(¯x, x2,··· , xd).\\n\\nHow this abstract description works in practice can be best seen through examples.\\n\\n13.3 Examples\\nExample 13.1. Let X1, X2, . . . , Xn be a simple random sample of Pareto random variables with density\\n\\nThe cumulative distribution function is\\n\\nfX (x|β) =\\n\\nβ\\nxβ+1 ,\\n\\nx > 1.\\n\\n−β,\\n\\nFX (x) = 1 − x\\n232\\n\\nx > 1.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Method of Moments\\n\\nThe mean and the variance are, respectively,\\n\\nIn this situation, we have one parameter, namely β. Thus, in step 1, we will only need to determine the ﬁrst moment\\n\\nµ =\\n\\n,\\n\\nσ2 =\\n\\nβ\\nβ − 1\\n\\nβ\\n\\n(β − 1)2(β − 2)\\n\\n.\\n\\nto ﬁnd the method of moments estimator ˆβ for β.\\n\\nFor step 2, we solve for β as a function of the mean µ.\\n\\nβ = g1(µ) =\\n\\nConsequently, a method of moments estimator for β is obtained by replacing the distributional mean µ by the sample\\nmean ¯X.\\n\\nA good estimator should have a small variance . To use the delta method to estimate the variance of ˆβ,\\n\\nµ1 = µ = k1(β) =\\n\\nβ\\nβ − 1\\n\\nµ\\nµ − 1\\n\\n.\\n\\nˆβ =\\n\\n¯X\\n¯X − 1\\n\\n.\\n\\nσ2\\nˆβ ≈ g\\n\\n1(µ)2 σ2\\n(cid:48)\\nn\\n\\n.\\n\\nwe compute\\n\\n(cid:48)\\n1(µ) = −\\ng\\n\\n1\\n\\n(µ − 1)2 ,\\n\\n1\\n\\ng\\n\\n(cid:48)\\n\\n1(cid:18) β\\nβ − 1(cid:19) = −\\n\\n( β\\nβ−1 − 1)2\\nThus, ˆβ has mean approximately equal to β and variance\\n\\ngiving in terms of β,\\n\\n= −\\n\\n(β − 1)2\\n\\n(β − (β − 1))2 = −(β − 1)2.\\n\\nn(β − 1)2(β − 2)\\nAs a example, let’s consider the case with β = 3 and n = 100. Then,\\n\\nσ2\\nˆβ ≈ g\\n\\n1(µ)2 σ2\\n(cid:48)\\nn\\n\\n= (β − 1)4\\n\\nβ\\n\\n=\\n\\nβ(β − 1)2\\nn(β − 2)\\n\\nσ2\\nˆβ ≈\\n\\n3 · 22\\n100 · 1\\n\\n=\\n\\n12\\n100\\n\\n=\\n\\n3\\n25\\n\\n,\\n\\nand σ ˆβ ≈\\n\\n√3\\n5\\n\\n= 0.346.\\n\\nTo simulate this, we ﬁrst need to simulate Pareto random variables. Recall that the probability transform states that\\nif the Xi are independent Pareto random variables, then Ui = FX (Xi) are independent uniform random variables on\\nthe interval [0, 1]. Thus, we can simulate Xi with F\\n\\nu = FX (x) = 1 − x\\n\\n−3,\\n\\n−1/3 = v\\n\\n−1/3, where v = 1 − u.\\n\\nNote that if Ui are uniform random variables on the interval [0, 1] then so are Vi = 1−Ui. Consequently, 1/ β√V1, 1/ β√V2,···\\n\\nhave the appropriate Pareto distribution. ..\\n\\n−1\\nX (Ui). If\\nthen x = (1 − u)\\n\\n> paretobar<-numeric(1000)\\n> for (i in 1:1000){v<-runif(100);pareto<-1/vˆ(1/3);paretobar[i]<-mean(pareto)}\\n> hist(paretobar)\\n> betahat<-paretobar/(paretobar-1)\\n> hist(betahat)\\n> mean(betahat)\\n[1] 3.053254\\n> sd(betahat)\\n[1] 0.3200865\\n\\n233\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Method of Moments\\n\\nThe sample mean for the estimate for β at 3.053 is close to the simulated value of 3. In this example, the estimator\\nˆβ is biased upward, In other words, on average the estimate is greater than the parameter, i. e., Eβ ˆβ > β. The\\nsample standard deviation value of 0.320 is close to the value 0.346 estimated by the delta method. When we examine\\nunbiased estimators, we will learn that this bias could have been anticipated.\\nExercise 13.2. The muon is an elementary particle with an electric charge of −1 and a spin (an intrinsic angular\\nmomentum) of 1/2. It is an unstable subatomic particle with a mean lifetime of 2.2 µs. Muons have a mass of about\\n200 times the mass of an electron. Since the muon’s charge and spin are the same as the electron, a muon can be\\nviewed as a much heavier version of the electron. The collision of an accelerated proton (p) beam having energy\\n600 MeV (million electron volts) with the nuclei of a production target produces positive pions (π+) under one of two\\npossible reactions.\\n\\nFrom the subsequent decay of the pions (mean lifetime 26.03 ns), positive muons (µ+), are formed via the two body\\ndecay\\n\\np + p → p + n + π+ or p + n → n + n + π+\\n\\nπ+ → µ+ + νµ\\n\\nµ+ → e+ + νe + ¯νµ\\n\\nwhere νµ is the symbol of a muon neutrino. The decay of a muon into a positron (e+), an electron neutrino (νe),\\nand a muon antineutrino (¯νµ)\\n\\nhas a distribution angle t with density given by\\n\\nf (t|α) =\\n\\n1\\n2π\\n\\n(1 + α cos t),\\n\\n0 ≤ t ≤ 2π,\\n\\nwith t the angle between the positron trajectory and the µ+-spin and anisometry parameter α ∈ [−1/3, 1/3] depends\\nthe polarization of the muon beam and positron energy. Based on the measurement t1, . . . tn, give the method of\\nmoments estimate ˆα for α. (Note: In this case the mean is 0 for all values of α, so we will have to compute the second\\nmoment to obtain an estimator.)\\n\\nExample 13.3 (Lincoln-Peterson method of mark and recapture). The size of an animal population in a habitat of\\ninterest is an important question in conservation biology. However, because individuals are often too difﬁcult to ﬁnd,\\n\\n234\\n\\nHistogram of paretobarparetobarFrequency1.41.61.82.0050100150200250Histogram of betahatbetahatFrequency2.02.53.03.54.04.5050100150200250\\x0cIntroduction to the Science of Statistics\\n\\nThe Method of Moments\\n\\na census is not feasible. One estimation technique is to capture some of the animals, mark them and release them back\\ninto the wild to mix randomly with the population.\\n\\nSome time later, a second capture from the population is made. In this case, some of the animals were not in the\\n\\nﬁrst capture and some, which are tagged, are recaptured. Let\\n\\n• t be the number captured and tagged,\\n• k be the number in the second capture,\\n• r be the number in the second capture that are tagged, and let\\n• N be the total population size.\\nThus, both t and k are under the control of the experimenter. The value of r is random and the populations size\\nN is the parameter to be estimated. We will use a method of moments strategy to estimate N. First, note that we can\\nguess the the estimate of N by considering two proportions.\\n\\nthe proportion of the tagged ﬁsh in the second capture ≈ the proportion of tagged ﬁsh in the population\\n\\nr\\nk ≈\\n\\nt\\nN\\n\\nThis can be solved for N to ﬁnd N ≈ kt/r. The advantage of obtaining this as a method of moments estimator is\\n\\nthat we evaluate the precision of this estimator by determining, for example, its variance. To begin, let\\n\\nXi =(cid:26) 1\\n\\n0\\n\\nif the i-th individual in the second capture has a tag.\\nif the i-th individual in the second capture does not have a tag.\\n\\nThe Xi are Bernoulli random variables with success probability\\nt\\nN\\n\\nP{Xi = 1} =\\n\\n.\\n\\nThey are not Bernoulli trials because the outcomes are not independent. We are sampling without replacement.\\n\\nFor example,\\n\\nP{the second individual is tagged|ﬁrst individual is tagged} =\\n\\nt − 1\\nN − 1\\n\\n.\\n\\nIn words, we are saying that the probability model behind mark and recapture is one where the number recaptured is\\nrandom and follows a hypergeometric distribution. The number of tagged individuals is X = X1 + X2 + ··· + Xk\\nand the expected number of tagged individuals is\\n\\n+ ··· +\\nThe proportion of tagged individuals, ¯X = (X1 + ··· + Xk)/k, has expected value\\n\\nµ = EX = EX1 + EX2 + ··· + EXk =\\n\\n+\\n\\nt\\nN\\n\\nt\\nN\\n\\nt\\nN\\n\\n=\\n\\nkt\\nN\\n\\n.\\n\\nThus,\\n\\nE ¯X =\\n\\nµ\\nk\\n\\n=\\n\\nt\\nN\\n\\n.\\n\\nNow in this case, we are estimating µ, the mean number recaptured with r, the actual number recaptured. So, to\\n\\nobtain the estimate ˆN. we replace µ with the previous equation by r.\\n\\nN =\\n\\nkt\\nµ\\n\\n.\\n\\nTo simulate mark and capture, consider a population of 2000 ﬁsh, tag 200, and capture 400. We perform 1000\\nsimulations of this experimental design. (The R command replicate repeats a chosen number of times (here 1000)\\nthe stated expression and stores, in this case, in the vector r).\\n\\nˆN =\\n\\nkt\\nr\\n\\n235\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nThe Method of Moments\\n\\n> t<-200;k<-400;N<-2000\\n> fish<-c(rep(1,t),rep(0,N-t))\\n> r<-replicate(1000,sum(sample(fish,k)))\\n> Nhat<-k*t/r\\n\\nThe command sample(fish,400) creates a vector of length 400 of zeros and ones for, respectively, untagged\\nand tagged ﬁsh. Thus, the sum command gives the number of tagged ﬁsh in the simulation. This is repeated 1000\\ntimes and stored in the vector r. Let’s look a summaries of r and the estimates ˆN of the population.\\n\\n> mean(r)\\n[1] 40.09\\n> sd(r)\\n[1] 5.245705\\n> mean(Nhat)\\n[1] 2031.031\\n> sd(Nhat)\\n[1] 276.6233\\n\\nTo estimate the population of pink salmon in Deep Cove Creek in southeastern Alaska, 1709 ﬁsh were tagged. Of\\n\\nthe 6375 carcasses that were examined, 138 were tagged. The estimate for the population size\\n\\nˆN =\\n\\n6375 × 1709\\n\\n138\\n\\n≈ 78948.\\n\\nExercise 13.4. Use the delta method to estimate Var( ˆN ) and σ ˆN . Apply this to the simulated sample and to the Deep\\nCove Creek data.\\nExample 13.5. Fitness is a central concept in the theory of evolution. Relative ﬁtness is quantiﬁed as the average\\nnumber of surviving progeny of a particular genotype compared with average number of surviving progeny of com-\\npeting genotypes after a single generation. Consequently, the distribution of ﬁtness effects, that is, the distribution of\\nﬁtness for newly arising mutations is a basic question in evolution. A basic understanding of the distribution of ﬁtness\\neffects is still in its early stages. Eyre-Walker (2006) examined one particular distribution of ﬁtness effects, namely,\\ndeleterious amino acid changing mutations in humans. His approach used a gamma-family of random variables and\\ngave the estimate of ˆα = 0.23 and ˆβ = 5.35.\\n\\n236\\n\\nHistogram of rrFrequency2030405060050150250350Histogram of NhatNhatFrequency1500200025003000050150250350\\x0cIntroduction to the Science of Statistics\\n\\nThe Method of Moments\\n\\nA Γ(α, β) random variable has mean α/β and variance α/β2. Because we have two parameters, the method of\\n\\nmoments methodology requires us, in step 1, to determine the ﬁrst two moments.\\n\\nE(α,β)X1 =\\n\\nα\\nβ\\n\\nand E(α,β)X 2\\n\\n1 = Var(α,β)(X1) + E(α,β)[X1]2 =\\n\\nα\\n\\nβ2 +(cid:18) α\\nβ(cid:19)2\\n\\n=\\n\\nα(1 + α)\\n\\nβ2\\n\\n=\\n\\nα\\nβ2 +\\n\\nα2\\nβ2 .\\n\\nThus, for step 1, we ﬁnd that\\n\\nµ1 = k1(α, β) =\\n\\nα\\nβ\\n\\n, µ2 = k2(α, β) =\\n\\nα\\nβ2 +\\n\\nα2\\nβ2 .\\n\\nFor step 2, we solve for α and β. Note that\\n\\nand\\n\\nSo set\\n\\nto obtain estimators\\n\\nµ2 − µ2\\nµ1\\n\\n=\\n\\nµ2 − µ2\\n\\n1\\n\\n1 =\\n\\nα\\nβ2 ,\\n\\nα/β\\nα/β2 = β,\\n\\nµ1 ·\\n\\nµ1\\n\\nµ2 − µ2\\n\\n1\\n\\n=\\n\\nα\\nβ · β = α,\\n\\nor α =\\n\\nµ2\\n1\\n\\nµ2 − µ2\\n\\n1\\n\\n.\\n\\n¯X =\\n\\nˆβ =\\n\\n¯X\\n\\nX 2 − ( ¯X)2\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n=\\n\\n¯X\\nS2\\n\\nXi\\n\\nand X 2 =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nX 2\\ni\\n\\nand\\n\\nˆα = ˆβ ¯X =\\n\\n( ¯X)2\\n\\nX 2 − ( ¯X)2\\n\\n=\\n\\n( ¯X)2\\nS2 .\\n\\nFigure 13.1: The density of a Γ(0.23, 5.35) random variable.\\n\\n237\\n\\n0.00.20.40.60.81.0024681012xdgamma(x, 0.23, 5.35)\\x0cIntroduction to the Science of Statistics\\n\\nThe Method of Moments\\n\\nThe result shows how using the sample variance can simplify the algebra in ﬁnding the method of moments estimator.\\nTo investigate the method of moments on simulated data using R, we consider 1000 repetitions of 100 independent\\n\\nobservations of a Γ(0.23, 5.35) random variable.\\n> xbar <- numeric(1000)\\n> x2bar <- numeric(1000)\\n> for (i in 1:1000){x<-rgamma(100,0.23,5.35);xbar[i]<-mean(x);x2bar[i]<-mean(xˆ2)}\\n> betahat <- xbar/(x2bar-(xbar)ˆ2)\\n> alphahat <- betahat*xbar\\n> mean(alphahat)\\n[1] 0.2599894\\n> sd(alphahat)\\n[1] 0.06672909\\n> mean(betahat)\\n[1] 6.315644\\n> sd(betahat)\\n[1] 2.203887\\n\\nTo obtain a sense of the distribution of the estimators ˆα and ˆβ, we give histograms.\\n\\n> hist(alphahat,probability=TRUE)\\n> hist(betahat,probability=TRUE)\\n\\nAs we see, the variance in the estimate of β is quite large. We will revisit this example using maximum likelihood\\nestimation in the hopes of reducing this variance. The use of the delta method is more difﬁcult in this case because\\nit must take into account the correlation between ¯X and X 2 for independent gamma random variables. Indeed, from\\nthe simulation, we have an estimate..\\n> cor(xbar,x2bar)\\n[1] 0.8120864\\nMoreover, the two estimators ˆα and ˆβ are fairly strongly positively correlated. Again, we can estimate this from the\\nsimulation.\\n> cor(alphahat,betahat)\\n[1] 0.7606326\\n\\nIn particular, an estimate of ˆα and ˆβ are likely to be overestimates or underestimates in tandem.\\n\\n238\\n\\nHistogram of alphahatalphahatDensity0.10.20.30.40.50123456Histogram of betahatbetahatDensity0510150.000.050.100.15\\x0cIntroduction to the Science of Statistics\\n\\nThe Method of Moments\\n\\n13.4 Answers to Selected Exercises\\n13.2. Let T be the random variable that is the angle between\\nthe positron trajectory and the µ+-spin. Then integrate by parts\\ntwice to obtain\\n\\nµ2 = EαT 2 =\\n\\n1\\n\\n2π(cid:90) π\\n\\n−π\\n\\nt2(1 + α cos t)dt =\\n\\nπ2\\n3 − 2α\\n\\nThus, α = (µ2 − π2/3)/2. This leads to the method of mo-\\nments estimate\\n\\nˆα =\\n\\n1\\n\\n2(cid:18)t2 −\\n\\nπ2\\n\\n3 (cid:19)\\n\\nwhere t2 is the sample second moment.\\n13.4. Let X be the random variable for the number of tagged ﬁsh. Then, X is a hypergeometric random variable with\\n\\nFigure 13.2: Densities f (t|α) for the values of α = −1\\n(yellow). −1/3 (red), 0 (black), 1/3 (blue), 1 (light blue).\\n\\nmean µX =\\n\\nkt\\nN\\n\\nand variance σ2\\n\\nX = k\\n\\nt\\nN\\n\\nN − t\\nN\\n\\nN = g(µX ) =\\n\\n. Thus, g\\n\\n(cid:48)\\n\\nkt\\nµX\\n\\n(µX ) = −\\n\\nkt\\nµ2\\nX\\n\\nN − k\\nN − 1\\n.\\n\\nThe variance of ˆN\\n\\n(cid:48)\\n\\n(µ)2σ2\\n\\nX(cid:19)2\\nX =(cid:18) kt\\nVar( ˆN ) ≈ g\\n=(cid:18) kt\\nX(cid:19)2\\n\\nt\\nµ2\\nN\\nkt − kµX\\nkt − µX t\\nkt − µX\\n\\nN − t\\nN\\n\\nµX t\\nkt\\n\\nkt\\n\\nk\\n\\nk\\n\\nN − k\\nN − 1\\n\\nµ2\\n\\n=(cid:18) kt\\nX(cid:19)2\\nX(cid:19)2\\n=(cid:18) kt\\n\\nµ2\\n\\nk\\n\\nµX\\nk\\n\\nk\\n\\nkt/µX − k\\nkt/µX − 1\\n\\nt\\n\\nkt/µX\\nk − µX\\n\\nk\\n\\nkt/µX − t\\nkt/µX\\nk(t − µX )\\nkt − µX\\n\\nµ2\\nk2t2\\nµ3\\nX\\n\\n=\\n\\n(k − µX )(t − µX )\\n\\nkt − µX\\n\\nNow if we replace µX by its estimate r we obtain\\n\\nσ2\\nˆN ≈\\n\\nk2t2\\nr3\\n\\n(k − r)(t − r)\\n\\n.\\n\\nkt − r\\n\\nFor t = 200, k = 400 and r = 40, we have the estimate σ ˆN = 268.4. This compares to the estimate of 276.6 from\\n\\nsimulation.\\n\\nFor t = 1709, k = 6375 and r = 138, we have the estimate σ ˆN = 6373.4.\\n\\n239\\n\\n-0.3-0.2-0.10.00.10.20.3-0.3-0.2-0.10.00.10.20.3xy-0.3-0.2-0.10.00.10.20.3-0.3-0.2-0.10.00.10.20.3xy-0.3-0.2-0.10.00.10.20.3-0.3-0.2-0.10.00.10.20.3xy-0.3-0.2-0.10.00.10.20.3-0.3-0.2-0.10.00.10.20.3xy-0.3-0.2-0.10.00.10.20.3-0.3-0.2-0.10.00.10.20.3xy\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\n240\\n\\n\\x0cTopic 14\\n\\nUnbiased Estimation\\n\\n14.1 Introduction\\nIn creating a parameter estimator, a fundamental question is whether or not the estimator differs from the parameter\\nin a systematic manner. Let’s examine this by looking a the computation of the mean and the variance of 16 ﬂips of a\\nfair coin.\\n\\nGive this task to 10 individuals and ask them report the number of heads. We can simulate this in R as follows .\\n\\n> (x<-rbinom(10,16,0.5))\\n\\n[1]\\n\\n8 5 9 7 7 9 7 8 8 10\\n\\nOur estimate is obtained by taking these 10 answers and averaging them. Intuitively we anticipate an answer\\n\\naround 8. For these 10 observations, we ﬁnd, in this case, that\\n\\n> sum(x)/10\\n[1] 7.8\\n\\nThe result is a bit below 8. Is this systematic? To assess this, we appeal to the ideas behind Monte Carlo to twice\\n\\nperform a 1000 simulations of the example above.\\n\\n> meanx<-replicate(1000,mean(rbinom(10,16,0.5)))\\n> mean(meanx)\\n[1] 7.9799\\n> meanx<-replicate(1000,mean(rbinom(10,16,0.5)))\\n> mean(meanx)\\n[1] 8.0049\\n\\nFrom this, we surmise that we the estimate of the sample mean ¯x neither systematically overestimates or un-\\nderestimates the distributional mean. From our knowledge of the binomial distribution, we know that the mean\\nµ = np = 16 · 0.5 = 8. In addition, the sample mean ¯X also has mean\\n\\nE ¯X =\\n\\n1\\n10\\n\\n(8 + 8 + 8 + 8 + 8 + 8 + 8 + 8 + 8 + 8) =\\n\\n80\\n10\\n\\n= 8\\n\\nverifying that we have no systematic error.\\n\\nThe phrase that we use is that the sample mean ¯X is an unbiased estimator of the distributional mean µ. Here is\\n\\nthe precise deﬁnition.\\nDeﬁnition 14.1. For observations X = (X1, X2, . . . , Xn) based on a distribution having parameter value θ, and for\\nd(X) an estimator for k(θ), the bias is the mean of the difference d(X) − k(θ), i.e.,\\n\\nbd(θ) = Eθd(X) − k(θ).\\n\\n241\\n\\n(14.1)\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nIf bd(θ) = 0 for all values of the parameter, then d(X) is called an unbiased estimator. Any estimator that is not\\nunbiased is called biased.\\nExample 14.2. Let X1, X2, . . . , Xn be Bernoulli trials with success parameter p and set the estimator for p to be\\nd(X) = ¯X, the sample mean. Then,\\n\\nEp ¯X =\\n\\n1\\nn\\n\\n(EX1 + EX2 + ··· + EXn) =\\n\\n1\\nn\\n\\n(p + p + ··· + p) = p\\n\\nThus, ¯X is an unbiased estimator for p. In this circumstance, we generally write ˆp instead of ¯X. In addition, we can\\nuse the fact that for independent random variables, the variance of the sum is the sum of the variances to see that\\n\\nVar(ˆp) =\\n\\n=\\n\\n1\\nn2 (Var(X1) + Var(X2) + ··· + Var(Xn))\\n1\\nn2 (p(1 − p) + p(1 − p) + ··· + p(1 − p)) =\\n\\n1\\nn\\n\\np(1 − p).\\n\\nExample 14.3. If X1, . . . , Xn form a simple random sample with unknown ﬁnite mean µ, then ¯X is an unbiased\\nestimator of µ. If the Xi have variance σ2, then\\n\\nVar( ¯X) =\\n\\nσ2\\nn\\n\\n.\\n\\nWe can assess the quality of an estimator by computing its mean square error, deﬁned by\\n\\nEθ[(d(X) − k(θ))2].\\n\\n(14.2)\\n\\n(14.3)\\n\\nEstimators with smaller mean square error are generally preferred to those with larger. Next we derive a simple\\nrelationship between mean square error and variance. If we write Y = d(X) − k(θ) in (14.3) and recall that the\\nvariance Varθ(Y ) = EθY 2 − (EθY )2 .\\n\\nThen\\n\\nEθY = Eθ(d(X) − k(θ)) = Eθd(X) − k(θ) = bd(θ)\\n\\nand the mean square error\\n\\nand Varθ(Y ) = Varθ(d(X))\\n\\nEθ[(d(X) − k(θ))2] = EθY 2 = Var(Y ) + (EY )2 = Varθ(d(X)) + bd(θ)2\\n\\n(14.4)\\nThus, the representation of the mean square error as equal to the variance of the estimator plus the square of the\\nbias is called the bias-variance decomposition. Mean square error can be considered as a measure of the accuracy\\nof an estimator. If the variance is small, then we can say that the estimator is precise. It may still not be very accurate\\nif the bias is large, but will be accurate only if the estimator is both precise and has low bias. In addition:\\n\\n• The mean square error for an unbiased estimator is its variance.\\n• Bias always increases the mean square error.\\n\\n14.2 Computing Bias\\nFor the variance σ2, we have been presented with two choices:\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2\\n\\nand\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2.\\n\\n(14.5)\\n\\nUsing bias as our criterion, we can now resolve between the two choices for the estimators for the variance σ2.\\nAgain, we use simulations to make a conjecture, we then follow up with a computation to verify our guess. For 16\\ntosses of a fair coin, we know that the variance is np(1 − p) = 16 · 1/2 · 1/2 = 4\\nFor the example above, we begin by simulating the coin tosses and compute the sum of squares(cid:80)10\\n\\ni=1(xi − ¯x)2,\\n\\n242\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\n> ssx<-numeric(1000)\\n> for (i in 1:1000){x<-rbinom(10,16,0.5);ssx[i]<-sum((x-mean(x))ˆ2)}\\n> mean(ssx)\\n[1] 35.8511\\n\\nThe choice is to divide either by 10, for the ﬁrst\\n\\nchoice, or 9, for the second.\\n\\n> mean(ssx)/10;mean(ssx)/9\\n[1] 3.58511\\n[1] 3.983456\\n\\nulations support dividing by 10 rather than 9.\\n\\nthe sum of squares(cid:80)10\\nNore generally, show that(cid:80)n\\n\\nExercise 14.4. Repeat the simulation above, compute\\ni=1(xi − 8)2. Show that these sim-\\ni=1(Xi − µ)2/n is an\\nunbiased estimator for σ2 for independent random vari-\\nable X1, . . . , Xn whose common distribution has mean\\nµ and variance σ2.\\n\\nIn this case, because we know all the aspects of the\\nsimulation, and thus we know that the answer ought to\\nbe near 4. Consequently, division by 9 appears to be the\\nappropriate choice. Let’s check this out, beginning with\\nwhat seems to be the inappropriate choice to see what goes wrong..\\n\\nFigure 14.1: Sum of squares about ¯x for 1000 simulations.\\n\\nExample 14.5. If a simple random sample X1, X2, . . . , has unknown ﬁnite variance σ2, then, we can consider the\\nsample variance\\n\\nTo ﬁnd the mean of S2, we divide the difference between an observation Xi and the distributional mean into two steps\\n- the ﬁrst from Xi to the sample mean ¯X and and then from the sample mean to the distributional mean, i.e.,\\n\\nS2 =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n(Xi − ¯X)2.\\n\\nWe shall soon see that the lack of knowledge of µ is the source of the bias. Make this substitution and expand the\\nsquare to obtain\\n\\nXi − µ = (Xi − ¯X) + ( ¯X − µ).\\n\\nn(cid:88)i=1\\n\\n(Xi − µ)2 =\\n\\n=\\n\\n=\\n\\n=\\n\\n(Xi − ¯X)( ¯X − µ) +\\n\\nn(cid:88)i=1\\n((Xi − ¯X) + ( ¯X − µ))2\\nn(cid:88)i=1\\nn(cid:88)i=1\\n(Xi − ¯X)2 + 2\\nn(cid:88)i=1\\nn(cid:88)i=1\\n(Xi − ¯X) + n( ¯X − µ)2\\n(Xi − ¯X)2 + 2( ¯X − µ)\\nn(cid:88)i=1\\n(Xi − ¯X)2 + n( ¯X − µ)2\\n\\nn(cid:88)i=1\\n\\n( ¯X − µ)2\\n\\n(Check for yourself that the middle term in the third line equals 0.) Subtract the term n( ¯X − µ)2 from both sides and\\ndivide by n to obtain the identity\\n\\n243\\n\\nHistogram of ssxssxFrequency020406080100120050100150200250\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nUsing the identity above and the linearity property of expectation we ﬁnd that\\n\\n1\\nn\\n\\nn\\n\\nn(cid:88)i=1\\n(Xi − ¯X)2 =\\nES2 = E(cid:34) 1\\n= E(cid:34) 1\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\n1\\nn\\n\\n1\\nn\\n\\n=\\n\\n=\\n\\nn\\n\\n=\\n\\n1\\nn\\n\\nnσ2 −\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n(Xi − µ)2 − ( ¯X − µ)2.\\n(Xi − ¯X)2(cid:35)\\nn(cid:88)i=1\\n(Xi − µ)2 − ( ¯X − µ)2(cid:35)\\nn(cid:88)i=1\\nE[(Xi − µ)2] − E[( ¯X − µ)2]\\n\\nVar(Xi) − Var( ¯X)\\nn − 1\\nn\\n\\nσ2 =\\n\\n1\\nn\\n\\nσ2 (cid:54)= σ2.\\n\\nThe last line uses (14.2). This shows that S2 is a biased estimator for σ2. Using the deﬁnition in (14.1), we can\\n\\nsee that it is biased downwards.\\n\\nb(σ2) =\\n\\nn − 1\\nn\\n\\nσ2 − σ2 = −\\n\\n1\\nn\\n\\nσ2.\\n\\nNote that the bias is equal to −Var( ¯X). In addition, because\\nE(cid:2)S2(cid:3) =\\n\\nE(cid:20) n\\n\\nS2(cid:21) =\\n\\nn\\nn − 1\\n\\nn − 1\\n\\nand\\n\\nS2\\n\\nu =\\n\\nS2 =\\n\\nn\\nn − 1\\n\\n1\\n\\nn − 1\\n\\nσ2(cid:19) = σ2\\n\\nn\\n\\nn\\n\\nn − 1(cid:18) n − 1\\nn(cid:88)i=1\\n(Xi − ¯X)2\\n\\nSu =(cid:112)S2\\n\\nis an unbiased estimator for σ2. As we shall learn in the next section, because the square root is concave downward,\\n\\nu as an estimator for σ is downwardly biased.\\n\\nExample 14.6. We have seen, in the case of n Bernoulli trials having x successes, that ˆp = x/n is an unbiased\\nestimator for the parameter p. This is the case, for example, in taking a simple random sample of genetic markers at\\na particular biallelic locus. (A locus with exactly two alleles.) Let one allele denote the wildtype (the typical alleles\\nas it occurs in nature) and the second a variant. If the circumstances in which variant is recessive, then an individual\\nexpresses the variant phenotype only in the case that both chromosomes contain this marker. In the case of independent\\nalleles from each parent, the probability of the variant phenotype is p2. Na¨ıvely, we could use the estimator ˆp2. (Later,\\nwe will see that this is the maximum likelihood estimator.) To determine the bias of this estimator, note that\\n\\nE ˆp2 = (E ˆp)2 + Var(ˆp) = p2 +\\n\\n1\\nn\\n\\np(1 − p).\\n\\n(14.6)\\n\\nThus, the bias b(p) = p(1 − p)/n and the estimator ˆp2 is biased upward.\\n\\nExercise 14.7. For Bernoulli trials X1, . . . , Xn,\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n(Xi − ˆp)2 = ˆp(1 − ˆp).\\n\\n244\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nBased on this exercise, and the computation above yielding an unbiased estimator, S2\\n\\nu, for the variance,\\n\\nE(cid:20) 1\\n\\nn − 1\\n\\nˆp(1 − ˆp)(cid:21) =\\n\\nE(cid:34) 1\\n\\nn − 1\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n(Xi − ˆp)2(cid:35) =\\n\\n1\\nn\\n\\nE[S2\\n\\nu] =\\n\\n1\\nn\\n\\nVar(X1) =\\n\\n1\\nn\\n\\np(1 − p).\\n\\nIn other words,\\n\\n1\\n\\nˆp(1 − ˆp)\\n\\nn − 1\\n\\nis an unbiased estimator of p(1 − p)/n. Returning to (14.6),\\nˆp(1 − ˆp)(cid:21) =(cid:18)p2 +\\n\\nE(cid:20)ˆp2 −\\n\\nn − 1\\n\\n1\\n\\n1\\nn\\n\\np(1 − p)(cid:19) −\\n\\n1\\nn\\n\\np(1 − p) = p2.\\n\\nu = ˆp2 −\\nis an unbiased estimator of p2.\\n\\n(cid:98)p2\\n\\nThus,\\n\\n0.4333,\\n\\n1\\n\\nn − 1\\n\\nˆp(1 − ˆp) = ˆp\\n\\n(n − 1)ˆp − 1 + ˆp\\n\\nn − 1\\n\\n= ˆp\\n\\nnˆp − 1\\nn − 1\\n\\n=\\n\\nx(x − 1)\\nn(n − 1)\\n\\n.\\n\\nTo compare the two estimators for p2, assume that we ﬁnd 13 variant alleles in a sample of 30, then ˆp = 13/30 =\\n\\nThe bias for the estimate ˆp2, in this case 0.0085, is subtracted to give the unbiased estimate (cid:98)p2\\nThe heterozygosity of a biallelic locus is h = 2p(1−p). From the discussion above, we see that h has the unbiased\\n\\nestimator\\n\\nu.\\n\\n30(cid:19)2\\nˆp2 =(cid:18) 13\\n\\n= 0.1878,\\n\\nand\\n\\nˆh =\\n\\n2n\\nn − 1\\n\\nˆp(1 − ˆp) =\\n\\n2n\\n\\nn − 1(cid:16) x\\n\\nu =\\n\\n13 · 12\\n30 · 29\\n\\n(cid:98)p2\\nn (cid:19) =\\nn(cid:17)(cid:18) n − x\\n\\n= 0.1793.\\n\\n2x(n − x)\\nn(n − 1)\\n\\n.\\n\\n14.3 Compensating for Bias\\nIn the methods of moments estimation, we have used g( ¯X) as an estimator for g(µ). If g is a convex function, we\\ncan say something about the bias of this estimator. In Figure 14.2, we see the method of moments estimator for the\\nestimator g( ¯X) for a parameter β in the Pareto distribution. The choice of β = 3 corresponds to a mean of µ = 3/2 for\\nthe Pareto random variables. The central limit theorem states that the sample mean ¯X is nearly normally distributed\\nwith mean 3/2. Thus, the distribution of ¯X is nearly symmetric around 3/2. From the ﬁgure, we can see that the\\ninterval from 1.4 to 1.5 under the function g maps into a longer interval above β = 3 than the interval from 1.5 to 1.6\\nmaps below β = 3. Thus, the function g spreads the values of ¯X above β = 3 more than below. Consequently, we\\nanticipate that the estimator ˆβ will be upwardly biased.\\n\\nTo address this phenomena in more general terms, we use the characterization of a convex function as a differen-\\ntiable function whose graph lies above any tangent line. If we look at the value µ for the convex function g, then this\\nstatement becomes\\n\\nNow replace x with the random variable ¯X and take expectations.\\n\\ng(x) − g(µ) ≥ g\\n\\n(µ)(x − µ).\\n\\n(cid:48)\\n\\nEµ[g( ¯X) − g(µ)] ≥ Eµ[g\\n\\n(cid:48)\\n\\n(µ)( ¯X − µ)] = g\\n\\n(cid:48)\\n\\n(µ)Eµ[ ¯X − µ] = 0.\\n\\nConsequently,\\n\\n(14.7)\\n\\nand g( ¯X) is biased upwards. The expression in (14.7) is known as Jensen’s inequality.\\nExercise 14.8. Show that the estimator Su is a downwardly biased estimator for σ.\\n\\nEµg( ¯X) ≥ g(µ)\\n\\n245\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nFigure 14.2: Graph of a convex function. Note that the tangent line is below the graph of g. Here we show the case in which µ = 1.5 and\\nβ = g(µ) = 3. Notice that the interval from x = 1.4 to x = 1.5 has a longer range than the interval from x = 1.5 to x = 1.6 Because g spreads\\nthe values of ¯X above β = 3 more than below, the estimator ˆβ for β is biased upward. We can use a second order Taylor series expansion to correct\\nmost of this bias.\\n\\nTo estimate the size of the bias, we look at a quadratic approximation for g centered at the value µ\\n\\nAgain, replace x in this expression with the random variable ¯X and then take expectations. Then, the bias\\n\\ng(x) − g(µ) ≈ g\\n\\n(cid:48)\\n\\n(µ)(x − µ) +\\n\\n(cid:48)(cid:48)\\n\\ng\\n\\n1\\n2\\n\\n(µ)(x − µ)2.\\n\\n(cid:48)\\n\\n(µ)( ¯X − µ)] +\\n\\n(cid:48)(cid:48)\\n\\nE[g\\n\\n1\\n2\\n\\n(µ)( ¯X − µ)2] =\\n\\n(cid:48)(cid:48)\\n\\ng\\n\\n1\\n2\\n\\n(µ)Var( ¯X) =\\n\\n(cid:48)(cid:48)\\n\\ng\\n\\n1\\n2\\n\\n(µ)\\n\\nσ2\\nn\\n\\n.\\n\\n(14.8)\\n\\nbg(µ) = Eµ[g( ¯X)] − g(µ) ≈ Eµ[g\\n(Remember that Eµ[g(cid:48)(µ)( ¯X − µ)] = 0.)\\n\\nThus, the bias has the intuitive properties of being\\n• large for strongly convex functions, i.e., ones with a large value for the second derivative evaluated at the mean\\n\\nµ,\\n\\n• large for observations having high variance σ2, and\\n• small when the number of observations n is large.\\nIn addition, this provides an estimate of the mean square error for a method of moment estimator ˆθ(X) =\\ng( ¯X) based on the relationship θ = g(µ). Based on the variance-bias identity in (14.3), we use the delta method to\\napproximate Varθ(g( ¯X)) and (14.8) to approximate the bias. Consequently,\\n\\nEθ[(ˆθ(X) − θ)2] = Eθ[(g( ¯X) − g(µ))2] = Varθ(g( ¯X)) + bg(µ)2 ≈ g\\n\\n(cid:48)\\n\\n(µ)2 σ2\\nn\\n\\n+\\n\\n(cid:48)(cid:48)\\n\\n1\\n\\n4(cid:18)g\\n\\n(µ)\\n\\nσ2\\n\\nn(cid:19)2\\n\\n.\\n\\nFinally, we make the substitution µ = k(θ) to give an expression for the approximation of the mean square error for\\nˆθ as a function of θ. Notice that the contribution to mean square error from the variance of the estimator is inversely\\n\\n246\\n\\n1.251.31.351.41.451.51.551.61.651.71.7522.533.544.55x!g(x) = x/(x!1)y=g(µ)+g’(µ)(x!µ)\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nproportional to n, the sample size. The contribution from the bias decreases more rapidly, inversely proportional to\\nn2,\\nExercise 14.9. If a method of moments estimator ˆθ is a linear function of the sample mean ¯x, then it is unbiased,\\nExercise 14.10. Use (14.8) to estimate the bias in using ˆp2 as an estimate of p2 is a sequence of n Bernoulli trials and\\nnote that it matches the value (14.6).\\nExample 14.11. For the method of moments estimator for the Pareto random variable, we determined that\\n\\nand that ¯X has\\n\\ng(µ) =\\n\\nµ\\nµ − 1\\n\\n.\\n\\nmean µ = β\\nβ−1\\n\\nand\\n\\nvariance\\n\\nσ2\\nn =\\n\\nβ\\n\\nn(β−1)2(β−2)\\n\\nBy taking derivatives, we see that g(cid:48)(µ) = −1/(µ − 1)2 and g(cid:48)(cid:48)(µ) = 2(µ − 1)−3 > 0 and, because µ > 1, g is a\\n\\nconvex function. Next, we have\\n\\ng\\n\\n(cid:48)(cid:18) β\\nβ − 1(cid:19) =\\n\\n−1\\n\\nβ−1 − 1(cid:17)2 = −(β − 1)2\\n(cid:16) β\\n\\nand g\\n\\n(cid:48)(cid:48)(cid:18) β\\nβ − 1(cid:19) =\\n\\n2\\n\\nβ−1 − 1(cid:17)3 = 2(β − 1)3.\\n(cid:16) β\\n\\nThus, the bias\\n\\nbg(β) ≈\\n\\n(cid:48)(cid:48)\\n\\ng\\n\\n1\\n2\\n\\n(µ)\\n\\nσ2\\nn\\n\\n=\\n\\n1\\n2\\n\\n2(β − 1)3\\n\\nβ\\n\\nn(β − 1)2(β − 2)\\n\\n=\\n\\nβ(β − 1)\\nn(β − 2)\\n\\n.\\n\\nSo, for β = 3 and n = 100, the bias is approximately 0.06. Compare this to the estimated value of 0.053 from the\\nsimulation in the previous section. The mean square error,\\n\\nEθ[(g( ¯X) − g(µ))2] ≈ g\\n\\n(cid:48)\\n\\n(µ)2 σ2\\nn\\n\\n+ bg(β)2 =\\n\\nβ(β − 1)2\\nn(β − 2)\\n\\n+\\n\\nβ2(β − 1)2\\nn2(β − 2)2 =\\n\\nβ(β − 1)2\\n\\nn(β − 2) (cid:18)1 +\\n\\nβ\\n\\nn(β − 2)(cid:19) .\\n\\nExample 14.12. For estimating the population in mark and recapture, we used the estimate\\n\\nfor the total population. Here µ is the mean number recaptured, k is the number captured in the second capture event\\nand t is the number tagged. The second derivative\\n\\nN = g(µ) =\\n\\nkt\\nµ\\n\\nand hence the method of moments estimate is biased upwards. In this siutation, n = 1 and the number recaptured is a\\nhypergeometric random variable. Hence its variance\\n\\n(cid:48)(cid:48)\\n\\ng\\n\\n(µ) =\\n\\n2kt\\nµ3 > 0\\n\\nσ2 =\\n\\nkt\\nN\\n\\n(N − t)(N − k)\\n\\nN (N − 1)\\n\\n.\\n\\nThus, the bias\\n\\nbg(N ) =\\n\\n1\\n2\\n\\n2kt\\nµ3\\n\\nkt\\nN\\n\\n(N − t)(N − k)\\n\\n(N − t)(N − k)\\n\\n=\\n\\n=\\n\\n(kt/µ − t)(kt/µ − k)\\n\\n=\\n\\nkt(k − µ)(t − µ)\\n\\n.\\n\\nN (N − 1)\\n\\nµ(N − 1)\\n\\nµ(kt/µ − 1)\\n\\nµ2(kt − µ)\\n\\nIn the simulation example, N = 2000, t = 200, k = 400 and µ = 40. This gives an estimate for the bias of 36.02. We\\ncan compare this to the bias of 2031.03-2000 = 31.03 based on the simulation in Example 13.2.\\n\\nThis suggests a new estimator by taking the method of moments estimator and subtracting the approximation of\\n\\nthe bias.\\n\\nˆN =\\n\\nkt\\nr −\\n\\nkt(k − r)(t − r)\\n\\nr2(kt − r)\\n\\n=\\n\\nkt\\n\\nr (cid:18)1 −\\n\\n(k − r)(t − r)\\n\\nr(kt − r) (cid:19) .\\n\\n247\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nThe delta method gives us that the standard deviation of the estimator is |g(cid:48)(µ)|σ/√n. Thus the ratio of the bias\\n\\nof an estimator to its standard deviation as determined by the delta method is approximately\\n\\ng(cid:48)(cid:48)(µ)σ2/(2n)\\n|g(cid:48)(µ)|σ/√n\\n\\n=\\n\\n1\\n2\\n\\ng(cid:48)(cid:48)(µ)\\n|g(cid:48)(µ)|\\n\\nσ\\n√n\\n\\n.\\n\\nIf this ratio is (cid:28) 1, then the bias correction is not very important. In the case of the example above, this ratio is\\n\\n36.02\\n268.40\\n\\n= 0.134\\n\\nand its usefulness in correcting bias is small.\\nExample 14.13. As noted earlier, because S2 is an unbiased estimator of σ2 and the square root function is concave\\ndownward. Thus, S is biased downwards as an estimator of σ. To illustrate this, we ﬁrst simulate both ESn and ES2\\nn\\nfor values n from 2 to 25 using normal random variables with mean µ = 0 and standard deviation σ = 1.\\n\\n> s<-numeric(24); s2<-numeric(24)\\n> for (i in 1:24){s[i]<-mean(replicate(10000,sd(rnorm(i+1))))}\\n> for (i in 1:24){s2[i]<-mean(replicate(10000,var(rnorm(i+1))))}\\n\\nWe can also ﬁnd an analytical expression for ESn.\\n\\nExercise 14.14. Note that for Z1, Z2, . . . , Zn independent standard normal variables, then\\n\\nVn =\\n\\nn(cid:88)i=1\\n(Zi − ¯Z)2\\n\\nis χ2\\n\\nn−1. Let\\n\\nS2\\n\\nn =\\n\\n1\\n\\nn − 1\\n\\n(Zi − ¯Z)2. Show that ESn =(cid:114) 2\\nn(cid:88)i=1\\n\\nn − 1\\n\\nΓ(n/2)\\n\\nΓ((n − 1)/2)\\n\\n.\\n\\n(14.9)\\n\\nFigure 14.3: Simulated values ESn (red) and ES2\\n\\nn (black) versus degrees of freedom df = n − 1. The values of ESn (solid black) from (14.9).\\n\\n248\\n\\n05101520250.750.800.850.900.951.001.05df05101520250.750.800.850.900.951.001.05df05101520250.750.800.850.900.951.001.05df\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nThen, we add the values of ES to form a table that has ES along with the simulated values of ES and ES2.\\n\\n> df<-1:24\\n> means<-sqrt(2/df)*gamma((df+1)/2)/gamma(df/2)\\n> head(data.frame(df,means,s,s2))\\n\\ndf\\n\\ns\\n\\nmeans\\n\\ns2\\n1 0.7978846 0.7913008 1.0008486\\n2 0.8862269 0.8920162 0.9811749\\n3 0.9213177 0.9190756 1.0145019\\n4 0.9399856 0.9419472 1.0006425\\n5 0.9515329 0.9498657 1.0055365\\n6 0.9593688 0.9563602 0.9985865\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n\\nAs we can see from both the analytical expression and the simulations, ES < σ = 1 and approaches 1 as the\\n\\ndegrees of freedom df = n − 1 increase. Notice that the simulated values for the variance is close to 1 = ES2.\\nExercise 14.15. The Stirling approximation states that\\n\\nUse this in (14.9) to show that\\n\\nΓ(t) ≈(cid:112)2π(t − 1)(cid:18) t − 1\\n\\ne (cid:19)t−1\\n\\n,\\n\\nt = 1.2. . . .\\n\\nn→∞ ESn = 1.\\nlim\\n\\n14.4 Consistency\\nDespite the desirability of using unbiased estimation, sometimes such an estimator is hard to ﬁnd and at other times\\nimpossible. However, note that in the examples above both the size of the bias and the variance in the estimator\\ndecrease inversely proportional to n, the number of observations. Thus, these estimators improve, under both of these\\ncriteria, with more observations. A concept that describes properties such as these is called consistency.\\nDeﬁnition 14.16. Given data X1, X2, . . . and a real valued function h of the parameter space, a sequence of estima-\\ntors dn, based on the ﬁrst n observations, is called consistent if for every choice of θ\\n\\nwhenever θ is the true state of nature.\\n\\nlim\\nn→∞ dn(X1, X2, . . . , Xn) = k(θ)\\n\\nThus, the bias of the estimator disappears in the limit of a large number of observations. In addition, the distribution\\n\\nof the estimators dn(X1, X2, . . . , Xn) become more and more concentrated near k(θ).\\n\\nFor the next example, we need to recall the sequence deﬁnition of continuity: A function g is continuous at a real\\n\\nnumber x provided that for every sequence {xn; n ≥ 1} with\\n\\nxn → x, then, we have that g(xn) → g(x).\\n\\nA function is called continuous if it is continuous at every value of x in the domain of g. Thus, we can write the\\nexpression above more succinctly by saying that for every convergent sequence {xn; n ≥ 1},\\n\\nn→∞ g(xn) = g( lim\\nlim\\n\\nn→∞ xn).\\n\\nExample 14.17. For a method of moment estimator, let’s focus on the case of a single parameter (d = 1). For\\nindependent observations, X1, X2, . . . , having mean µ = k(θ), we have that\\n\\nE ¯Xn = µ,\\n\\n249\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\ni. e. ¯Xn, the sample mean for the ﬁrst n observations, is an unbiased estimator for µ = k(θ). Also, by the law of large\\nnumbers, we have that\\n\\nAssume that k has a continuous inverse g = k−1. In particular, because µ = k(θ), we have that g(µ) = θ. Next,\\n\\nusing the methods of moments procedure, deﬁne, for n observations, the estimators\\n\\nlim\\nn→∞\\n\\n¯Xn = µ.\\n\\nˆθn(X1, X2, . . . , Xn) = g(cid:18) 1\\n\\nn\\n\\n(X1 + ··· + Xn)(cid:19) = g( ¯Xn).\\n\\nfor the parameter θ. Using the continuity of g, we ﬁnd that\\n\\nlim\\nn→∞\\n\\nˆθn(X1, X2, . . . , Xn) = lim\\n\\nn→∞ g( ¯Xn) = g( lim\\nn→∞\\n\\n¯Xn) = g(µ) = θ\\n\\nand so we have that g( ¯Xn) is a consistent sequence of estimators for θ.\\n\\n14.5 Cram´er-Rao Bound\\nThis topic is somewhat more advanced and can be skipped for the ﬁrst reading. This section gives us an introduction to\\nthe log-likelihood and its derivative, the score functions. We shall encounter these functions again when we introduce\\nmaximum likelihood estimation. In addition, the Cram´er Rao bound, which is based on the variance of the score\\nfunction, known as the Fisher information, gives a lower bound for the variance of an unbiased estimator. These\\nconcepts will be necessary to describe the variance for maximum likelihood estimators.\\n\\nAmong unbiased estimators, one important goal is to ﬁnd an estimator that has as small a variance as possible, A\\nmore precise goal would be to ﬁnd an unbiased estimator d that has uniform minimum variance. In other words,\\nd(X) has has a smaller variance than for any other unbiased estimator ˜d for every value θ of the parameter.\\n\\nfor all θ ∈ Θ.\\nThe efﬁciency e( ˜d) of unbiased estimator ˜d is the minimum value of the ratio\\n\\nVarθd(X) ≤ Varθ ˜d(X)\\n\\nVarθd(X)\\nVarθ ˜d(X)\\n\\nover all values of θ. Thus, the efﬁciency is between 0 and 1 with a goal of ﬁnding estimators with efﬁciency as near to\\none as possible.\\n\\nFor unbiased estimators, the Cram´er-Rao bound tells us how small a variance is ever possible. The formula is a bit\\nmysterious at ﬁrst. However, we shall soon learn that this bound is a consequence of the bound on correlation that we\\nhave previously learned\\n\\nRecall that for two random variables Y and Z, the correlation\\n\\ntakes values between -1 and 1. Thus, ρ(Y, Z)2 ≤ 1 and so\\n\\nCov(Y, Z)\\n\\n.\\n\\nρ(Y, Z) =\\n\\n(cid:112)Var(Y )Var(Z)\\nCov(Y, Z)2 ≤ Var(Y )Var(Z).\\n\\nExercise 14.18. If EZ = 0, the Cov(Y, Z) = EY Z\\n\\n250\\n\\n(14.10)\\n\\n(14.11)\\n\\n\\x0c1 =(cid:90)Rn\\n\\nk(θ) = Eθd(X) =(cid:90)Rn\\n\\nIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nWe begin with data X = (X1, . . . , Xn) drawn from an unknown probability Pθ. The parameter space Θ ⊂ R.\\n\\nDenote the joint density of these random variables\\n\\nf (x|θ), where x = (x1 . . . , xn).\\n\\nIn the case that the data come from a simple random sample then the joint density is the product of the marginal\\ndensities.\\n\\nFor continuous random variables, the two basic properties of the density are that f (x|θ) ≥ 0 for all x and that\\n\\nf (x|θ) = f (x1|θ)··· f (xn|θ)\\n\\n(14.12)\\n\\nf (x|θ) dx.\\n\\n(14.13)\\n\\nNow, let d be the unbiased estimator of k(θ), then by the basic formula for computing expectation, we have for\\n\\ncontinuous random variables\\n\\nd(x)f (x|θ) dx.\\n\\n(14.14)\\n\\nIf the functions in (14.13) and (14.14) are differentiable with respect to the parameter θ and we can pass the\\nderivative through the integral, then we ﬁrst differentiate both sides of equation (14.13), and then use the logarithm\\nfunction to write this derivate as the expectation of a random variable,\\n\\n∂θ\\n\\n∂f (x|θ)\\n\\n∂ ln f (x|θ)\\n\\n∂f (x|θ)/∂θ\\n\\n0 =(cid:90)Rn\\n\\ndx =(cid:90)Rn\\n\\nf (x|θ)\\nFrom a similar calculation using (14.14),\\n\\nf (x|θ) dx = Eθ(cid:20) ∂ ln f (X|θ)\\n(cid:21) .\\nZ = ∂ ln f (X|θ)/∂θ. From equations (14.16) and then (14.11), we ﬁnd that\\n\\nf (x|θ) dx =(cid:90)Rn\\n(θ) = Eθ(cid:20)d(X)\\n\\n∂ ln f (X|θ)\\n\\n∂θ\\n\\n∂θ\\n\\n∂θ\\n\\nk\\n\\n(cid:48)\\n\\nNow, return to the review on correlation with Y = d(X), the unbiased estimator for k(θ) and the score function\\n\\n(cid:21) .\\n\\n(14.15)\\n\\n(14.16)\\n\\n(θ)2 = Eθ(cid:20)d(X)\\n\\n∂ ln f (X|θ)\\n\\n∂ ln f (X|θ)\\n\\n∂θ\\n\\n(cid:19) ≤ Varθ(d(X))Varθ(cid:18) ∂ ln f (X|θ)\\n\\n∂θ\\n\\n(cid:19) ,\\n\\n(14.17)\\n\\n∂θ\\n\\n(cid:21)2\\n\\n= Covθ(cid:18)d(X),\\nVarθ(d(X)) ≥\\nI(θ) = Varθ(cid:18) ∂ ln f (X|θ)\\n\\n∂θ\\n\\nk(cid:48)(θ)2\\nI(θ)\\n\\n.\\n\\n(cid:19) = Eθ(cid:34)(cid:18) ∂ ln f (X|θ)\\n\\n∂θ\\n\\n(cid:19)2(cid:35)\\n\\n(cid:48)\\n\\nk\\n\\nor,\\n\\nwhere\\n\\nis called the Fisher information. For the equality, recall that the variance Var(Z) = EZ 2 − (EZ)2 and recall from\\nequation (14.15) that the random variable Z = ∂ ln f (X|θ)/∂θ has mean EZ = 0.\\nEquation (14.17), called the Cram´er-Rao lower bound or the information inequality, states that the lower bound\\nfor the variance of an unbiased estimator is the reciprocal of the Fisher information. In other words, the higher the\\ninformation, the lower is the possible value of the variance of an unbiased estimator.\\nExercise 14.19. Let X be uniform on the interval [0, θ], θ > 0. Show that\\n\\ndx (cid:54)= 0\\nThus, in the case, we cannot pass the derivative through the integral,\\n\\n∂θ\\n\\n∂f (x|θ)\\n\\n(cid:90) θ\\n\\n0\\n\\n251\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nIf we return to the case of a simple random sample, then take the logarithm of both sides of equation (14.12)\\n\\nand then differentiate with respect to the parameter θ,\\n\\nln f (x|θ) = ln f (x1|θ) + ··· + ln f (xn|θ)\\n\\n∂ ln f (x|θ)\\n\\n∂θ\\n\\n=\\n\\n∂ ln f (x1|θ)\\n\\n∂θ\\n\\n+ ··· +\\n\\n∂ ln f (xn|θ)\\n\\n∂θ\\n\\n.\\n\\nThe random variables {∂ ln f (Xk|θ)/∂θ; 1 ≤ k ≤ n} are independent and have the same distribution. Using the fact\\nthat the variance of the sum is the sum of the variances for independent random variables, we see that In, the Fisher\\ninformation for n observations is n times the Fisher information of a single observation.\\n\\nIn(θ) = Var(cid:18) ∂ ln f (X1|θ)\\n\\n∂θ\\n\\n+ ··· +\\n\\n∂ ln f (Xn|θ)\\n\\n∂θ\\n\\n(cid:19) = nVar(\\n\\n∂ ln f (X1|θ)\\n\\n∂θ\\n\\n) = nE[(\\n\\n∂ ln f (X1|θ)\\n\\n∂θ\\n\\n)2].\\n\\nNotice the correspondence. Information is linearly proportional to the number of observations. If our estimator\\nis a sample mean or a function of the sample mean, then the variance is inversely proportional to the number of\\nobservations.\\n\\nExample 14.20. For independent Bernoulli random variables with unknown success probability θ, the density is\\n\\nThe mean is θ and the variance is θ(1 − θ). Taking logarithms, we ﬁnd that\\n\\nf (x|θ) = θx(1 − θ)(1−x).\\n\\nx\\nθ −\\nThe Fisher information associated to a single observation\\n\\nln f (x|θ) =\\n\\nln f (x|θ) = x ln θ + (1 − x) ln(1 − θ),\\nx − θ\\n∂\\nθ(1 − θ)\\n∂θ\\n\\n1 − x\\n1 − θ\\n\\n=\\n\\n.\\n\\nI(θ) = E(cid:34)(cid:18) ∂\\n\\n∂θ\\n\\nln f (X|θ)(cid:19)2(cid:35) =\\n\\n=\\n\\n1\\n\\nθ2(1 − θ)2 E[(X − θ)2] =\\nθ2(1 − θ)2 θ(1 − θ) =\\n\\n1\\n\\n1\\n\\nθ(1 − θ)\\n\\n1\\n\\n.\\n\\nθ2(1 − θ)2 Var(X)\\n\\nThe information for n observations In(θ) = n/(θ(1 − θ)). Thus, by the Cram´er-Rao lower bound, any unbiased\\nestimator of θ based on n observations must have variance al least θ(1 − θ)/n. Now, notice that if we take d(x) = ¯x,\\nthen\\n\\nEθ ¯X = θ,\\n\\nand Varθd(X) = Var( ¯X) =\\n\\nθ(1 − θ)\\n\\nn\\n\\n.\\n\\nThese two equations show that ¯X is a unbiased estimator having uniformly minimum variance.\\n\\nExercise 14.21. For independent normal random variables with known variance σ2\\nuniformly minimum variance unbiased estimator.\\n\\n0 and unknown mean µ, ¯X is a\\n\\nExercise 14.22. If we have that the parameter θ appears in the density as a function η = η(θ), then we have two\\nforms for the Fisher information, Iθ and Iη for each parameterization. Show that\\n\\nIθ(θ) = Iη(η(θ))(cid:18) dη(θ)\\ndθ (cid:19)2\\n\\n252\\n\\n(14.18)\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nExercise 14.23. Take two derivatives of ln f (x|θ) to show that\\n\\nI(θ) = Eθ(cid:34)(cid:18) ∂ ln f (X|θ)\\n\\n∂θ\\n\\n(cid:19)2(cid:35) = −Eθ(cid:20) ∂2 ln f (X|θ)\\n\\n∂θ2\\n\\n(cid:21) .\\n\\n(14.19)\\n\\nThis identity is often a useful alternative to compute the Fisher Information.\\n\\nExample 14.24. For an exponential random variable,\\n\\nln f (x|λ) = ln λ − λx,\\n\\n∂2f (x|λ)\\n\\n∂λ2\\n\\n1\\nλ2 .\\n\\n= −\\n\\nThus, by (14.19),\\n\\n1\\nλ2 .\\nNow, ¯X is an unbiased estimator for h(λ) = 1/λ with variance\\n\\nI(λ) =\\n\\nBy the Cram´er-Rao lower bound, we have that\\n\\n1\\nnλ2 .\\n\\ng(cid:48)(λ)2\\nnI(λ)\\n\\n=\\n\\n1/λ4\\nnλ2 =\\n\\n1\\nnλ2 .\\n\\nBecause ¯X has this variance, it is a uniformly minimum variance unbiased estimator.\\nExample 14.25. To give an estimator that does not achieve the Cram´er-Rao bound, let X1, X2, . . . , Xn be a simple\\nrandom sample of Pareto random variables with density\\n\\nfX (x|β) =\\n\\nβ\\nxβ+1 ,\\n\\nx > 1.\\n\\nThe mean and the variance\\n\\nµ =\\n\\n,\\n\\nσ2 =\\n\\nβ\\nβ − 1\\n\\nβ\\n\\n(β − 1)2(β − 2)\\n\\n.\\n\\nThus, ¯X is an unbiased estimator of µ = β/(β − 1)\\nVar( ¯X) =\\n\\nTo compute the Fisher information, note that\\n\\nβ\\n\\nn(β − 1)2(β − 2)\\n\\n.\\n\\nln f (x|β) = ln β − (β + 1) ln x and thus\\n\\n∂2 ln f (x|β)\\n\\n∂β2\\n\\n1\\nβ2 .\\n\\n= −\\n\\nUsing (14.19), we have that\\n\\nNext, for\\n\\nµ = g(β) =\\n\\nThus, the Cram´er-Rao bound for the estimator is\\n\\nβ\\nβ − 1\\n\\n(cid:48)\\n\\ng\\n\\n,\\n\\n(β) = −\\n\\n1\\n\\n(β − 1)2 ,\\n\\nand\\n\\n(cid:48)\\n\\ng\\n\\n(β)2 =\\n\\n1\\n\\n(β − 1)4 .\\n\\nand the efﬁciency compared to the Cram´er-Rao bound is\\n\\ng(cid:48)(β)2/In(β)\\n\\nVar( ¯X)\\n\\nβ2\\n\\n=\\n\\nn(β − 1)4 ·\\n\\nn(β − 1)2(β − 2)\\n\\nβ\\n\\n=\\n\\nβ(β − 2)\\n(β − 1)2 = 1 −\\n\\n1\\n\\n(β − 1)2 .\\n\\nThe Pareto distribution does not have a variance unless β > 2. For β just above 2, the efﬁciency compared to its\\n\\nCram´er-Rao bound is low but improves with larger β.\\n\\nI(β) =\\n\\n1\\nβ2 .\\n\\ng(cid:48)(β)2\\nIn(β)\\n\\n=\\n\\nβ2\\n\\nn(β − 1)4 .\\n\\n253\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\n14.6 A Note on Exponential Families and Efﬁcient Estimators\\nFor an efﬁcient estimator, we need ﬁnd the cases that lead to equality in the correlation inequality (14.10). Recall that\\nequality occurs precisely when the correlation is ±1. This occurs when the estimator d(X) and the score function\\n∂ ln fX (X|θ)/∂θ are linearly related with probability 1.\\n\\n∂\\n∂θ\\n\\nln fX (X|θ) = a(θ)d(X) + b(θ).\\n\\n(14.20)\\n\\nAfter integrating, we obtain,\\n\\nln fX (X|θ) =(cid:90) a(θ)dθ d(X) +(cid:90) b(θ)dθ + j(X) = η(θ)d(X) + B(θ) + j(X)\\n\\nNote that the constant of integration of integration is a function of X. Now exponentiate both sides of this equation\\n\\nfX (X|θ) = c(θ)h(X) exp(η(θ)d(X)).\\n\\n(14.21)\\n\\nHere c(θ) = exp B(θ) and h(X) = exp j(X).\\n\\nDeﬁnition 14.26. Density functions satisfying equation (14.21) form an exponential family with natural parameter\\nη(θ) and sufﬁcient statistic d(x).\\n\\nThus, if we have independent random variables X1, X2, . . . Xn, then the joint density is the product of the densi-\\n\\nties, namely,\\n\\nIn addition, as a consequence of this linear relation in (14.20), the mean of he sufﬁcient statistic\\n\\nf (X|θ) = c(θ)nh(X1)··· h(Xn) exp(η(θ)(d(X1) + ··· + d(Xn)).\\n\\n(14.22)\\n\\nd(X) =\\n\\n1\\nn\\n\\n(d(X1) + ··· + d(Xn))\\n\\nis an efﬁcient estimator for k(θ).\\n\\nExample 14.27 (Poisson random variables).\\n\\nf (x|λ) =\\n\\nλx\\nx!\\n\\n−λ = e\\n\\ne\\n\\n−λ 1\\nx!\\n\\nexp(x ln λ).\\n\\nThus, Poisson random variables are an exponential family with c(λ) = exp(−λ), h(x) = 1/x!, and natural parameter\\nη(λ) = ln λ. Because\\n\\nλ = Eλ ¯X,\\n\\n¯X is an unbiased estimator of the parameter λ.\\n\\nThe score function\\n\\nThe Fisher information for one observation is\\n\\n∂\\n∂λ\\n\\nln f (x|λ) =\\n\\n∂\\n∂λ\\n\\n(x ln λ − ln x! − λ) =\\n\\nx\\nλ − 1.\\n\\nI(λ) = Eλ(cid:34)(cid:18) X\\n\\nλ − 1(cid:19)2(cid:35) =\\n\\n1\\nλ2 Eλ[(X − λ)2] =\\n\\n1\\nλ\\n\\n.\\n\\nThus, In(λ) = n/λ is the Fisher information for n observations. In addition,\\n\\nVarλ( ¯X) =\\n\\nλ\\nn\\n\\n254\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nand d(x) = ¯x has efﬁciency\\n\\nVar( ¯X)\\n1/In(λ)\\n\\n= 1.\\n\\nThis could have been predicted. The density of n independent observations is\\n\\nf (x|λ) =\\n\\ne−λ\\nx1!\\n\\nλx1 ···\\n\\ne−λ\\nxn!\\n\\nλxn =\\n\\ne−nλλx1···+xn\\nx1!··· xn!\\n\\n=\\n\\ne−nλλn¯x\\nx1!··· xn!\\n\\nand so the score function\\n\\nUnbiased Estimation\\n\\n∂\\n∂λ\\n\\nln f (x|λ) =\\n\\n(−nλ + n¯x ln λ) = −n +\\n\\n∂\\n∂λ\\n\\nn¯x\\nλ\\n\\nshowing that the estimate ¯x and the score function are linearly related.\\nExercise 14.28. Show that a Bernoulli random variable with parameter p is an exponential family with c(p) =\\n\\n1 − p, h(x) = 1 and the natural parameter η(p) = ln(cid:16) p\\n\\n1−p(cid:17), the log-odds, and sufﬁcient statistic x.\\n\\nExercise 14.29. Show that a normal random variable with known variance σ2\\nfamily.\\n\\n0 and unknown mean µ is an exponential\\n\\nIf we parameterize using the natural parameter η, then\\n\\nfX (x|η) = c(η)h(x) exp(ηd(x))\\n\\nln fX (x|η) = ln c(η) + ln h(x) + ηd(x)\\nln fX (x|η) =\\n\\nln c(η) + d(x)\\n\\n∂\\n∂η\\n\\n∂\\n∂η\\n\\nRecall that the mean of the score function is 0. Thus,\\n\\n0 = Eη(cid:20) ∂\\n\\n∂η\\n\\nln fX (X|η)(cid:21) =\\n\\nln c(η) + Eηd(x)\\n\\n∂\\n∂η\\n∂\\n∂η\\n\\nEηd(x) = −\\n\\nln c(η)\\n\\n(14.23)\\n\\nExercise 14.30. For a Bernoulli random variable, show that\\n\\nc(η) =\\n\\n1\\n\\n1 + eη\\n\\nExercise 14.31. For a Bernoulli random variable, use (14.23) to show that EpX = p.\\n\\nNow differentiate the score function and take expectation\\n\\n∂2\\n∂η2 ln fX (x|η) =\\n\\nEη(cid:20) ∂2\\n\\n∂η2 ln fX (X|η)(cid:21) =\\n\\n∂2\\n∂η2 ln c(η)\\n∂2\\n∂η2 ln c(η)\\n\\nIη(η) = −\\n\\n∂2\\n∂η2 ln c(η),\\n\\n(14.24)\\n\\nthe Fisher information with respect to the natural parameter η.\\nExercise 14.32. For a Bernoulli random variable, use (14.24) to show that.\\n\\nand check that (14.18) holds.\\n\\nIη(η) =\\n\\neη\\n\\n(1 + eη)2\\n\\n255\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\n14.7 Answers to Selected Exercises\\n14.4. Repeat the simulation, replacing mean(x) by 8.\\n\\n> ssx<-numeric(1000)\\n> for (i in 1:1000){x<-rbinom(10,16,0.5);ssx[i]<-sum((x-8)ˆ2)}\\n> mean(ssx)/10;mean(ssx)/9\\n[1] 3.9918\\n[1] 4.435333\\n\\nNote that division by 10 gives an answer very close to the correct value of 4. To verify that the estimator is\\n\\nunbiased, we write\\n\\nE(cid:34) 1\\n\\nn\\n\\n(Xi − µ)2(cid:35) =\\nn(cid:88)i=1\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nE[(Xi − µ)2] =\\n\\nVar(Xi) =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nσ2 = σ2.\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n14.7. For a Bernoulli trial note that X 2\\n\\ni = Xi. Expand the square to obtain\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\n(Xi − ˆp)2 =\\n\\nX 2\\ni − ˆp\\n\\nXi + nˆp2 = nˆp − 2nˆp2 + nˆp2 = n(ˆp − ˆp2) = nˆp(1 − ˆp).\\n\\nDivide by n to obtain the result.\\n14.8. Recall that ES2\\ndown functions, the direction of the inequality in Jensen’s inequality is reversed. Setting t = S2\\n\\nu = σ2. Check the second derivative to see that g(t) = √t is concave down for all t. For concave\\n\\nu, we have that\\n\\nESu = Eg(S2\\n\\nu) ≤ g(ES2\\n\\nu) = g(σ2) = σ\\n\\nand Su is a downwardly biased estimator of σ.\\n14.9. In oder to have a linear method of moments estimator\\n\\nwe must have, for mean µ,\\n\\nThus,\\n\\nˆθ = a + b¯x,\\n\\nθ = a + bµ.\\n\\nEθ ˆθ = E[a + b ¯X] = a + bE ¯X = a + bµ = θ\\n\\nand ˆθ is unbiased.\\n14.10. Set g(p) = p2. Then, g(cid:48)(cid:48)(p) = 2. Recall that the variance of a Bernoulli random variable σ2 = p(1 − p) and\\nthe bias\\n\\nbg(p) ≈\\n\\n14.14. Vn is χ2\\n\\nn−1,\\n\\n(cid:48)(cid:48)\\n\\ng\\n\\n(p)\\n\\n1\\n2\\n\\nσ2\\nn\\n\\n=\\n\\n1\\n2\\n\\np(1 − p)\\n\\n2\\n\\np(1 − p)\\n\\n.\\n\\n=\\n\\nn\\n\\nn\\n\\nE(cid:112)Vn =(cid:90) ∞\\n\\n0\\n\\n√vf (v|n − 1) dv =\\n\\n2(n−1)/2Γ((n − 1)/2)(cid:90) ∞\\n\\n1\\n\\n0\\n\\nvn/2−1e\\n\\n−v/2 dv =\\n\\n−v/2 dv\\n\\n√vv(n−1)/2−1e\\n√2Γ(n/2)\\n\\nΓ((n − 1)/2)(cid:90) ∞\\n\\n0\\n\\nf (v|n) dv\\n\\n1\\n\\n2n/2Γ(n/2)(cid:90) ∞\\n\\n0\\n\\n=\\n\\n=\\n\\n√2Γ(n/2)\\nΓ((n − 1)/2)\\n√2Γ(n/2)\\nΓ((n − 1)/2)\\n\\n256\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nNow, S2\\n\\nn = 1\\n\\nn−1 V . Thus\\n\\nESn =\\n\\n14.15. Recalling that\\n\\nΓ(n/2)\\n\\nΓ((n − 1)/2)\\n\\n.\\n\\n1\\n\\n√n − 1\\n\\nn − 1\\n\\nE(cid:112)Vn =(cid:114) 2\\nx→∞(cid:16)1 −\\nx(cid:17)x\\n\\nlim\\n\\nα\\n\\n−α,\\n\\n= e\\n\\nwe substitute Stirling’s approximation and drop terms from the expression that have limit 1.\\n\\nlim\\nn→∞ ESn = lim\\n\\nΓ(n/2)\\n\\nΓ((n − 1)/2)\\n\\n= lim\\n\\n= lim\\n\\nn − 1\\n\\nn→∞(cid:114) 2\\nn→∞(cid:114) 2\\nn→∞(cid:114) 2\\nn→∞(cid:115) 2\\n\\n= lim\\n\\n= lim\\nn→∞\\n\\n= lim\\nn→∞\\n\\ne\\n\\ne\\n\\ne\\n\\n1\\n\\n(cid:19)n/2−1\\nn − 1(cid:112)2π(n/2 − 1)(cid:18) n/2 − 1\\n(cid:112)2π((n − 1)/2 − 1)(cid:18)\\nn − 1(cid:115) n/2 − 1\\n(n − 1)/2 − 1(cid:19)(n−1)/2−1\\n(cid:19)n/2−1(cid:18)\\n(n − 1)/2 − 1(cid:18) n/2 − 1\\nn→∞(cid:115) 1\\n(n/2 − 1)n/2−1/2\\n((n − 1)/2 − 1)(n−1)/2−1/2 = lim\\n(n − 1)/2 − 1(cid:19)n/2−1/2\\nn − 3(cid:19)n/2(cid:18) n − 2\\n√e (cid:18) n − 2\\n√e (cid:18) n/2 − 1\\n(1 − 2/n)n/2\\n(1 − 3/n)n/2 =\\n\\nn − 3(cid:19)−1/2\\n\\n(n − 1)/2 − 1\\n(n − 1)/2\\n\\ne−1\\ne−3/2 = 1,\\n\\ne(n − 1)\\n\\n= lim\\nn→∞\\n\\n1\\n√e\\n\\ne1/2\\n\\n1\\n\\n1\\n\\n1\\n\\ne\\n\\ne\\n\\n(n − 1)/2 − 1(cid:19)(n−1)/2−1\\n\\n(n/2 − 1)n/2−1/2\\n\\n((n − 1)/2 − 1)n/2−1/2\\n\\n14.18. Cov(Y, Z) = EY Z − EY · EZ = EY Z whenever EZ = 0.\\n14.19. The density f (x|θ) = 1/θ. Thus,\\n\\nNote that by the Leibnitz integral rule,\\n\\n∂f (x|θ)\\n\\n0\\n\\n∂θ\\n\\n(cid:90) θ\\n∂θ(cid:90) θ\\n\\n∂\\n\\n0\\n\\n0\\n\\n∂\\n\\n∂θ(cid:18) 1\\ndx =(cid:90) θ\\nf (x|θ) dx =(cid:90) θ\\n\\n0\\n\\nθ(cid:19) dx =(cid:90) θ\\n\\n0 −\\n\\n1\\nθ2 dx = −\\n\\nθ\\nθ2 = −\\n\\n1\\nθ (cid:54)= 0.\\n\\n∂f (x|θ)\\n\\n∂θ\\n\\ndx + f (θ|θ)\\n\\ndθ\\ndθ\\n\\n1\\nθ\\n\\n= −\\n\\n+\\n\\n1\\nθ\\n\\n= 0.\\n\\n14.21. For independent normal random variables with known variance σ2\\n(x − µ)2\\n\\n1\\n\\n,\\n\\n0 and unknown mean µ, the density\\n\\nσ0√2π\\n\\nf (x|µ) =\\nexp−\\nln f (x|µ) = − ln(σ0√2π) −\\n\\n2σ2\\n0\\n(x − µ)2\\n\\n2σ2\\n0\\n\\n.\\n\\nThus, the score function\\n\\nln f (x|µ) =\\nand the Fisher information associated to a single observation\\n\\n∂\\n∂µ\\n\\n1\\nσ2\\n0\\n\\n(x − µ).\\n\\nI(µ) = E(cid:34)(cid:18) ∂\\n\\n∂µ\\n\\nln f (X|µ)(cid:19)2(cid:35) =\\n\\n1\\nσ4\\n0\\n\\nE[(X − µ)2] =\\n\\n1\\nσ4\\n0\\n\\nVar(X) =\\n\\n1\\nσ2\\n0\\n\\n.\\n\\n257\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\nAgain, the information is the reciprocal of the variance. Thus, by the Cram´er-Rao lower bound, any unbiased estimator\\nbased on n observations must have variance al least σ2\\n\\n0/n. However, if we take d(x) = ¯x, then\\n\\nVarµd(X) =\\n\\nσ2\\n0\\nn\\n\\n.\\n\\nand ¯x is a uniformly minimum variance unbiased estimator.\\n\\n14.22. The information with respect to θ,\\n\\nIθ(θ) = Eθ(cid:34)(cid:18) ∂\\n= Eη(θ)(cid:34)(cid:18) ∂\\n\\n∂θ\\n\\n∂η\\n\\nln f (X|θ)(cid:19)2(cid:35) = Eη(θ)(cid:34)(cid:18) ∂\\n\\n∂θ\\n\\nln f (X|η(θ))(cid:19)2(cid:35)\\ndθ (cid:19)2(cid:35) = Iη(η(θ))(cid:18) dη(θ)\\ndθ (cid:19)2\\n\\ndη(θ)\\n\\n.\\n\\nln f (X|η(θ)) ·\\n\\nThe second equality uses the chain rule.\\n14.23. First, we take two derivatives of ln f (x|θ).\\n\\n∂ ln f (x|θ)\\n\\n∂θ\\n\\n=\\n\\n∂f (x|θ)/∂θ\\n\\nf (x|θ)\\n\\n(14.25)\\n\\nand\\n\\n∂2 ln f (x|θ)\\n\\n∂θ2\\n\\n=\\n\\n=\\n\\n∂2f (x|θ)/∂θ2\\n\\nf (x|θ)\\n\\n∂2f (x|θ)/∂θ2\\n\\nf (x|θ)\\n\\n=\\n\\n∂2f (x|θ)/∂θ2\\n\\nf (x|θ)\\n\\n−(cid:18) ∂f (x|θ)/∂θ)\\nf (x|θ) (cid:19)2\\n\\n−\\n\\n(∂f (x|θ)/∂θ)2\\n\\nf (x|θ)2\\n\\n−(cid:18) ∂ ln f (x|θ)\\n\\n∂θ\\n\\n(cid:19)2\\n\\nupon substitution from identity (14.25). Thus, the expected values satisfy\\n\\nEθ(cid:20) ∂2 ln f (X|θ)\\n\\n∂θ2\\n\\n(cid:21) = Eθ(cid:20) ∂2f (X|θ)/∂θ2\\n\\nf (X|θ)\\n\\n(cid:21) − Eθ(cid:34)(cid:18) ∂ ln f (X|θ)\\n\\n∂θ\\n\\n(cid:19)2(cid:35) .\\n\\nvariable,\\n\\nConsquently, the exercise is complete if we show that Eθ(cid:104) ∂2f (X|θ)/∂θ2\\nEθ(cid:20) ∂2f (X|θ)/∂θ2\\n\\nf (x|θ) dx =(cid:90)Rn\\n\\n(cid:21) =(cid:90)Rn\\n\\n∂2f (x|θ)/∂θ2\\n\\nf (X|θ)\\n\\nf (x|θ)\\n\\n∂2f (x|θ)\\n\\nf (X|θ)\\n\\n∂θ2\\n\\n(cid:105) = 0. However, for a continuous random\\n\\ndx =\\n\\n∂2\\n\\n∂θ2(cid:90)Rn\\n\\nf (x|θ) dx =\\n\\n∂2\\n∂θ2 1 = 0.\\n\\nNote that the computation require that we be able to pass two derivatives with respect to θ through the integral sign.\\n\\n14.28. The Bernoulli density\\n\\nf (x|p) = px(1 − p)1−x = (1 − p)(cid:18) p\\n\\n1 − p(cid:19)x\\nThus, c(p) = 1 − p, h(x) = 1, the natural parameter π(p) = ln(cid:16) p\\n\\n1 − p(cid:19)(cid:19) .\\n\\n= (1 − p) exp(cid:18)x ln(cid:18) p\\n1−p(cid:17), and the sufﬁcient statistic d(x) = x.\\n\\n14.29. The normal density\\n\\nf (x|µ) =\\n\\n1\\n\\nσ0√2π\\n\\nexp−\\n\\n(x − µ)2\\n\\n2σ2\\n0\\n\\n=\\n\\n1\\n\\nσ0√2π\\n\\n−µ2/2σ0e\\n\\n−x2/2σ0 exp\\n\\ne\\n\\nxµ\\nσ2\\n0\\n\\n258\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nUnbiased Estimation\\n\\n√\\nThus, c(µ) = 1\\nσ0\\n\\n2π e−µ2/2σ0, h(x) = e−x2/2σ0 and the natural parameter π(µ) = µ/σ2\\n0.\\n\\n14.30.. For a Bernoulli density,\\n\\n1 − p(cid:19)\\nη = ln(cid:18) p\\n\\np\\n1 − p\\n\\neη =\\neη − eηp = p\\n\\neη = (1 + eη)p\\n\\nThus,\\n\\n14.31. The sufﬁcient statistic d(x) = x, then\\n\\np =\\n\\neη\\n\\n1 + eη\\n\\nc(η) = 1 − p =\\n\\n1\\n\\n1 + eη .\\n\\nEX = ln c(η) = − ln(1 + eη)\\n−\\n1 + eη = p\\n\\nln c(η) =\\n\\n∂\\n∂η\\n\\neη\\n\\n14.32. Take a second derivative so see that the formula for Iη holds. Now, check that\\n\\nIη(η(p)) =\\n\\neη(p)\\n\\n1 + eη(p)\\n\\n1\\n\\n1 + eη(p) = p(1 − p)\\n\\nand\\n\\ndη\\ndp\\n\\n=\\n\\n1\\np\\n\\n+\\n\\n1\\n1 − p\\n\\n=\\n\\n1\\n\\np(1 − p)\\n\\n.\\n\\nNow substitute into (14.18).\\n\\n259\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\n260\\n\\n\\x0cTopic 15\\n\\nMaximum Likelihood Estimation\\n\\nThe solution of the problems of calculating from a sample the parameters of the hypothetical popula-\\ntion, which we have put forward in the method of maximum likelihood, consists, then, simply of choosing\\nsuch values of these parameters as have the maximum likelihood. - R. A. Fisher, Phil. Trans. Royal Soc.\\nSer. A. 222, (1922),\\n\\n15.1 Introduction\\n\\nThe principle of maximum likelihood is relatively straightforward to state. As before, we begin with observations\\nX = (X1, . . . , Xn) of random variables chosen according to one of a family of probabilities Pθ. In addition, f (x|θ),\\nx = (x1, . . . , xn) will be used to denote the density function for the data when θ is the true state of nature.\\nThen, the principle of maximum likelihood yields a choice of the estimator ˆθ as the value for the parameter that\\n\\nmakes the observed data most probable.\\n\\nDeﬁnition 15.1. The likelihood function is the density function regarded as a function of θ.\\n\\nThe maximum likelihood estimate (MLE),\\n\\nL(θ|x) = f (x|θ), θ ∈ Θ.\\n\\nˆθ(x) = arg max\\n\\nθ\\n\\nL(θ|x).\\n\\n(15.1)\\n\\n(15.2)\\n\\nThus, we are presuming that a unique global maximum exists.\\nWe will learn that especially for large samples, the maximum likelihood estimators have many desirable properties.\\nHowever, especially for high dimensional data, the likelihood can have many local maxima. Thus, ﬁnding the global\\nmaximum can be a major computational challenge.\\n\\nThis class of estimators has an important invariance property. If ˆθ(x) is a maximum likelihood estimate for\\nθ, then g(ˆθ(x)) is a maximum likelihood estimate for g(θ). For example, if θ is a parameter for the variance and ˆθ\\n\\ndeviation. This ﬂexibility in estimation criterion seen here is not available in the case of unbiased estimators.\\n\\nis the maximum likelihood estimate for the variance, then(cid:112)ˆθ is the maximum likelihood estimate for the standard\\nFor independent observations, the likelihood is the product of density functions. Because the logarithm of a product\\nis the sum of the logarithms, ﬁnding zeroes of the score function, ∂ ln L(θ|x)/∂θ, the derivative of the logarithm of\\nthe likelihood, will be easier. Having the parameter values be the variable of interest is somewhat unusual, so we will\\nnext look at several examples of the likelihood function.\\n\\n261\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nFigure 15.1: Likelihood function (top row) and its logarithm (bottom row) for Bernoulli trials. The left column is based on 20 trials having 8 and\\n11 successes. The right column is based on 40 trials having 16 and 22 successes. Notice that the maximum likelihood is approximately 10−6 for 20\\ntrials and 10−12 for 40. In addition, note that the peaks are more narrow for 40 trials rather than 20. We shall later be able to associate this property\\nto the variance of the maximum likelihood estimator.\\n\\n262\\n\\n0.20.30.40.50.60.70.0e+005.0e-071.0e-061.5e-06pl0.20.30.40.50.60.70.0e+005.0e-071.0e-061.5e-06pl0.20.30.40.50.60.70.0e+001.0e-122.0e-12pl0.20.30.40.50.60.70.0e+001.0e-122.0e-12pl0.20.30.40.50.60.7-20-18-16-14plog(l)0.20.30.40.50.60.7-20-18-16-14plog(l)0.20.30.40.50.60.7-33-31-29-27plog(l)0.20.30.40.50.60.7-33-31-29-27plog(l)\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\n15.2 Examples\\nExample 15.2 (Bernoulli trials). If the experiment consists of n Bernoulli trials with success probability p, then\\n\\nL(p|x) = px1(1 − p)(1−x1) ··· pxn (1 − p)(1−xn) = p(x1+···+xn)(1 − p)n−(x1+···+xn).\\n\\nln L(p|x) = ln p(\\n\\nn(cid:88)i=1\\n\\n∂\\n∂p\\n\\nThis equals zero when p = ¯x.\\n\\nxi) + ln(1 − p)(n −\\n\\nxi) = n(¯x ln p + (1 − ¯x) ln(1 − p)).\\n\\nn(cid:88)i=1\\n1 − p(cid:19) = n\\n\\n1 − ¯x\\n\\n¯x − p\\np(1 − p)\\n\\nln L(p|x) = n(cid:18) ¯x\\n\\np −\\n\\nExercise 15.3. Check that this is a maximum.\\n\\nThus,\\n\\nˆp(x) = ¯x.\\nIn this case the maximum likelihood estimator is also unbiased.\\n\\nExample 15.4 (Normal data). Maximum likelihood estimation can be applied to a vector valued parameter. For a\\nsimple random sample of n normal random variables, we can use the properties of the exponential function to simplify\\nthe likelihood function.\\n\\nL(µ, σ2|x) =(cid:18) 1\\n√2πσ2\\n\\nThe log-likelihood\\n\\nexp −(x1 − µ)2\\n\\n2σ2\\n\\n(cid:19)···(cid:18) 1\\n√2πσ2\\n\\nexp −(xn − µ)2\\n\\n2σ2\\n\\nln L(µ, σ2|x) = −\\n\\nn\\n2\\n\\n(ln 2π + ln σ2) −\\n\\n1\\n2σ2\\n\\n(cid:19) =\\nn(cid:88)i=1\\n\\n1\\n\\n(cid:112)(2πσ2)n\\n(xi − µ)2.\\n\\nexp−\\n\\n1\\n2σ2\\n\\nn(cid:88)i=1\\n\\n(xi − µ)2.\\n\\nThe score function is now a vector. (cid:16) ∂\\n\\nmaximum likelihood estimators ˆµ and ˆσ2\\n\\n∂σ2 ln L(µ, σ2|x)(cid:17). Next we ﬁnd the zeros to determine the\\n∂µ ln L(µ, σ2|x), ∂\\nn(cid:88)i=1\\n\\n1\\nˆσ2 n(¯x − ˆµ) = 0\\n\\n(xi − ˆµ) =\\n\\n1\\nˆσ2\\n\\nln L(ˆµ, ˆσ2|x) =\\n\\n∂\\n∂µ\\n\\nBecause the second partial derivative with respect to µ is negative,\\n\\nˆµ(x) = ¯x\\n\\nis the maximum likelihood estimator. For the derivative of the log-likelihood with respect to the parameter σ2,\\n\\n∂\\n∂σ2 ln L(µ, σ2|x) = −\\n\\nn\\n2σ2 +\\n\\n1\\n\\n2(σ2)2\\n\\nn(cid:88)i=1\\n\\n(xi − µ)2 = −\\n\\nn\\n\\n2(σ2)2(cid:32)σ2 −\\n\\n1\\nn\\n\\n(xi − µ)2(cid:33) = 0.\\nn(cid:88)i=1\\n\\nRecalling that ˆµ(x) = ¯x, we obtain\\n\\nˆσ2(x) =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2.\\n\\nNote that the maximum likelihood estimator is a biased estimator.\\n\\nExample 15.5 (Lincoln-Peterson method of mark and recapture). Let’s recall the variables in mark and recapture:\\n\\n263\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\n• t be the number captured and tagged,\\n• k be the number in the second capture,\\n• r the the number in the second capture that are tagged, and let\\n• N be the total population.\\nHere t and k is set by the experimental design; r is an observation that may vary. The total population N is\\n\\nunknown. The likelihood function for N is the hypergeometric distribution.\\n\\nExercise 15.6. Show that the maximum likelihood estimator\\n\\nL(N|r) = (cid:0)t\\n\\nr(cid:1)(cid:0)N−t\\nk−r(cid:1)\\n(cid:0)N\\nk(cid:1)\\nˆN =(cid:20) tk\\nr(cid:21) .\\n\\nwhere [·] mean the greater integer less than.\\n\\nThus, the maximum likelihood estimator is, in this case, obtained from the method of moments estimator by round-\\n\\ning down to the next integer.\\n\\nLet look at the example of mark and capture from the previous topic. There N = 2000, the number of ﬁsh in the\\npopulation, is unknown to us. We tag t = 200 ﬁsh in the ﬁrst capture event, and obtain k = 400 ﬁsh in the second\\ncapture.\\n\\n> N<-2000\\n> t<-200\\n> fish<-c(rep(1,t),rep(0,N-t))\\nThis creates a vector of length N with t ones representing tagged ﬁsh and and N − t zeroes representing the untagged\\nﬁsh.\\n\\n> k<-400\\n> r<-sum(sample(fish,k))\\n> r\\n[1] 42\\n\\nThis samples k for the recaptured and adds up the ones to obtained, in this simulation, the number r = 42 of recaptured\\nﬁsh. For the likelihood function, we look at a range of values for N that is symmetric about 2000. Here, the maximum\\nlikelihood estimate ˆN = [200 · 400/42] = 1904. ..\\n> N<-c(1800:2200)\\n> L<-dhyper(r,t,N-t,k)\\n> plot(N,L,type=\"l\",ylab=\"L(N|42)\",col=\"green\")\\n\\nThe likelihood function for this example is shown in Figure 15.2.\\n\\nExample 15.7 (Linear regression). Our data are n observations with one explanatory variable and one response\\nvariable. The model is that the responses yi are linearly related to the explanatory variable xi with an “error” \\x01i, i.e.,\\n\\nHere we take the \\x01i to be independent mean 0 normal random variables. The (unknown) variance is σ2. Consequently,\\nour model has three parameters, the intercept α, the slope β, and the variance of the error, σ2.\\n\\nyi = α + βxi + \\x01i\\n\\n264\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nFigure 15.2: Likelihood function L(N|42) for mark and recapture with t = 200 tagged ﬁsh, k = 400 in the second capture with r = 42 having\\ntags and thus recapture. Note that the maximum likelihood estimator for the total ﬁsh population is ˆN = 1904.\\n\\nThus, the joint density for the \\x01i is\\n\\n1\\n\\n1\\n\\n1\\n\\nexp−\\n\\nexp−\\n\\n\\x012\\n1\\n2σ2 ·\\n\\n√2πσ2\\n\\n√2πσ2\\n\\n√2πσ2\\n\\n\\x012\\n2\\n2σ2 ···\\nSince \\x01i = yi − (α + βxi), the likelihood function\\n1\\n(cid:112)(2πσ2)n\\n(ln 2π + ln σ2) −\\n\\nln L(α, β, σ2|y, x) = −\\n\\nL(α, β, σ2|y, x) =\\n\\nexp−\\n\\nn\\n2\\n\\n1\\n2σ2\\n\\nThe logarithm\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\n1\\n2σ2\\n\\nexp−\\n\\n1\\n2σ2\\n\\n\\x012\\ni\\n\\nn(cid:88)i=1\\n\\nexp−\\n\\n\\x012\\nn\\n2σ2 =\\n\\n1\\n\\n(cid:112)(2πσ2)n\\n(yi − (α + βxi))2.\\n\\n(yi − (α + βxi))2.\\n\\n(15.3)\\n\\nConsequently, maximizing the likelihood function for the parameters α and β is equivalent to minimizing\\n\\nSS(α.β) =\\n\\n(yi − (α + βxi))2.\\n\\nn(cid:88)i=1\\n\\nThus, the principle of maximum likelihood is equivalent to the least squares criterion for ordinary linear regression.\\nThe maximum likelihood estimators α and β give the regression line\\n\\nwith\\n\\nˆyi = ˆα + ˆβxi.\\n\\nˆβ =\\n\\ncov(x, y)\\nvar(x)\\n\\n,\\n\\nand ˆα determined by solving\\n\\n¯y = ˆα + ˆβ ¯x.\\n\\n265\\n\\n180019002000210022000.0450.0500.0550.0600.0650.070NL(N|42)Likelihood Function for Mark and Recapture\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nExercise 15.8. Show that the maximum likelihood estimator for σ2 is\\n\\nFrequently, software will report the unbiased estimator. For ordinary least square procedures, this is\\n\\nˆσ2\\nM LE =\\n\\n1\\nn\\n\\n(yi − ˆyi)2.\\n\\nˆσ2\\nU =\\n\\n1\\n\\nn − 2\\n\\n(yi − ˆyi)2.\\n\\nn(cid:88)k=1\\nn(cid:88)k=1\\n\\n(15.4)\\n\\nFor the measurements on the lengths in centimeters of the femur and humerus for the ﬁve specimens of Archeopteryx,\\n\\nwe have the following R output for linear regression. ..\\n> femur<-c(38,56,59,64,74)\\n> humerus<-c(41,63,70,72,84)\\n> summary(lm(humerus˜femur))\\n\\nCall:\\nlm(formula = humerus ˜ femur)\\n\\nResiduals:\\n\\n5\\n-0.8226 -0.3668 3.0425 -0.9420 -0.9110\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\nCoefficients:\\n\\nEstimate Std. Error t value Pr(>|t|)\\n-0.821 0.471944\\n15.941 0.000537 ***\\n\\n(Intercept) -3.65959\\nfemur\\n1.19690\\n---\\nSignif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1\\n\\n4.45896\\n0.07509\\n\\n1\\n\\nResidual standard error: 1.982 on 3 degrees of freedom\\nMultiple R-squared: 0.9883,Adjusted R-squared: 0.9844\\nF-statistic: 254.1 on 1 and 3 DF,\\np-value: 0.0005368\\n\\nThe residual standard error of 1.982 centimeters is obtained by squaring the 5 residuals, dividing by 3 = 5− 2 and\\n\\ntaking a square root.\\nExample 15.9 (weighted least squares). If we know the relative size of the variances of the \\x01i, then we have the model\\n\\nwhere the \\x01i are, again, independent mean 0 normal random variable with unknown variance σ2. In this case,\\n\\nyi = α + βxi + γ(xi)\\x01i\\n\\nare independent normal random variables, mean 0 and (unknown) variance σ2. the likelihood function\\n\\n\\x01i =\\n\\n1\\n\\nγ(xi)\\n\\n(yi − α + βxi)\\n\\nL(α, β, σ2|y, x) =\\n\\nw(xi)(yi − (α + βxi))2\\n\\n1\\n\\n(cid:112)(2πσ2)n\\n\\nexp−\\n\\n1\\n2σ2\\n\\nn(cid:88)i=1\\n\\nwhere w(x) = 1/γ(x)2. In other words, the weights are inversely proportional to the variances. The log-likelihood is\\n\\nln L(α, β, σ2|y, x) = −\\n\\nn\\n2\\n\\nln 2πσ2 −\\n\\n1\\n2σ2\\n\\nn(cid:88)i=1\\n\\nw(xi)(yi − (α + βxi))2.\\n\\n266\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nExercise 15.10. Show that the maximum likelihood estimators ˆαw and ˆβw have formulas\\n\\nˆβw =\\n\\ncovw(x, y)\\nvarw(x)\\n\\n,\\n\\n¯yw = ˆαw + ˆβw ¯xw\\n\\nwhere ¯xw and ¯yw are the weighted means\\n\\nThe weighted covariance and variance are, respectively,\\n\\ncovw(x, y) = (cid:80)n\\n\\ni=1 w(xi)(xi − ¯xw)(yi − ¯yw)\\n\\nThe maximum likelihood estimator for σ2 is\\n\\n,\\n\\ni=1 w(xi)xi\\ni=1 w(xi)\\n\\n¯xw = (cid:80)n\\n(cid:80)n\\n(cid:80)n\\nM LE = (cid:80)n\\n\\ni=1 w(xi)\\n\\nˆσ2\\n\\n.\\n\\ni=1 w(xi)yi\\ni=1 w(xi)\\n\\n¯yw = (cid:80)n\\n(cid:80)n\\nvarw(x) = (cid:80)n\\n\\n,\\n\\ni=1 w(xi)(xi − ¯xw)2\\n\\n,\\n\\ni=1 w(xi)\\n\\n(cid:80)n\\n\\nk=1 w(xi)(yi − ˆyi)2\\n\\n.\\n\\ni=1 w(xi)\\n\\n(cid:80)n\\n\\nIn the case of weighted least squares, the predicted value for the response variable is\\n\\nˆyi = ˆαw + ˆβwxi.\\n\\nExercise 15.11. Show that ˆαw and ˆβw are unbiased estimators of α and β. In particular, ordinary (unweighted) least\\nsquare estimators are unbiased.\\n\\nIn computing the optimal values using introductory differential calculus, the maximum can occur at either critical\\npoints or at the endpoints. The next example show that the maximum value for the likelihood can occur at the end\\npoint of an interval.\\nExample 15.12 (Uniform random variables). If our data X = (X1, . . . , Xn) are a simple random sample drawn from\\nuniformly distributed random variable whose maximum value θ is unknown, then each random variable has density\\n\\nTherefore, the joint density or the likelihood\\n\\n0\\n\\nf (x|θ) =(cid:26) 1/θ\\nf (x|θ) = L(θ|x) =(cid:26) 1/θn\\n\\n0\\n\\nif 0 ≤ x ≤ θ,\\notherwise.\\n\\nif 0 ≤ xi ≤ θ for all i,\\notherwise.\\n\\nConsequently, the joint density is 0 whenever any of the xi > θ. Restating this in terms of likelihood, no value\\nof θ is possible that is less than any of the xi. Conseuently, any value of θ less than any of the xi has likelihood 0.\\nSymbolically,\\n\\nL(θ|x) =(cid:26) 0\\n\\n1/θn\\n\\nfor θ < maxi xi = x(n),\\nfor θ ≥ maxi xi = x(n).\\n\\nRecall the notation x(n) for the top order statistic based on n observations.\\n\\nThe likelihood is 0 on the interval (0, x(n)) and is positive and decreasing on the interval [x(n),∞). Thus, to\\n\\nmaximize L(θ|x), we should take the minimum value of θ on this interval. In other words,\\n\\nBecause the estimator is always less than the parameter value it is meant to estimate, the estimator\\n\\nˆθ(x) = x(n).\\n\\nˆθ(X) = X(n) < θ,\\n\\n267\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nFigure 15.3: Likelihood function for uniform random variables on the interval [0, θ]. The likelihood is 0 up to max1≤i≤n xi and 1/θn afterwards.\\n\\nThus, we suspect it is biased downwards, i. e..\\n\\n(15.5)\\nIn order to compute the expected value in (15.5), note that X(n) = max1≤i≤n Xi ≤ x if and only if each of the\\n\\nEθX(n) < θ.\\n\\nXi ≤ x. Thus, for 0 ≤ x ≤ θ, the distribution function for X(n) is\\n\\nFX(n)(x|θ) = Pθ{ max\\n1≤i≤n\\n\\nXi ≤ x} = Pθ{X1 ≤ x, X2 ≤ x, . . . , Xn ≤ x}\\n\\neach of these random variables have the same distribution function\\n\\n= Pθ{X1 ≤ x}Pθ{X2 ≤ x}··· Pθ{Xn < x}\\n\\nThus, the distribution function for X(n) is the product FX1(x|θ)FX2(x|θ)··· FXn (x|θ), i.e.,\\n\\nTake the derivative to ﬁnd the density,\\n\\nThe mean\\n\\nfor x ≤ 0,\\nfor 0 < x ≤ θ,\\nfor θ < x.\\n\\nfor x ≤ 0,\\nfor 0 < x ≤ θ,\\nfor θ < x.\\n\\nfor x ≤ 0,\\nfor 0 < x ≤ θ,\\nfor θ < x.\\n\\n1\\n\\n0\\n\\n0\\nx\\nθ\\n1\\n\\nFXi(x|θ) = Pθ{Xi ≤ x} =\\uf8f1\\uf8f2\\uf8f3\\nFX(n) (x|θ) =\\uf8f1\\uf8f2\\uf8f3\\nθ(cid:1)n\\n(cid:0) x\\nfX(n) (x|θ) =\\uf8f1\\uf8f2\\uf8f3\\nxfX(n) (x|θ) dx =(cid:90) θ\\nEθX(n) =(cid:90) θ\\nθn (cid:90) θ\\n\\n0\\nnxn−1\\n\\nxn dx =\\n\\n0\\nn\\n\\n=\\n\\nθn\\n\\n0\\n\\n0\\n\\n0\\n\\nnxn−1\\n\\nθn\\n\\nx\\n\\nn\\n\\n(n + 1)θn xn+1(cid:12)(cid:12)(cid:12)\\n\\nθ\\n\\n0\\n\\n268\\n\\ndx\\n\\n=\\n\\nn\\n\\nn + 1\\n\\nθ.\\n\\n00.511.522.5300.20.40.60.81θL(θ|x)1/θnobservations xi in thisinterval\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nThis conﬁrms the bias of the estimator X(n) and gives us a strategy to ﬁnd an unbiased estimator. Note that the choice\\n\\nd(X) =\\n\\nn + 1\\n\\nn\\n\\nX(n)\\n\\nyields an unbiased estimator of θ.\\n\\n15.3 Summary of Estimators\\nLook to the text above for the deﬁnition of variables.\\n\\nparameter\\n\\np\\n\\nN\\n\\nµ\\nσ2\\n\\nσ\\n\\nβ\\n\\nα\\nσ2\\n\\nσ\\n\\nθ\\n\\nestimate\\nBernoulli trials\\n\\nˆp = 1\\n\\ni=1 xi = ¯x\\n\\nmark recapture\\n\\nnormal observations\\n\\nˆµ = 1\\nmle = 1\\nˆσ2\\nu = 1\\nˆσ2\\n\\nn(cid:80)n\\nˆN =(cid:2) kt\\nr(cid:3)\\nn(cid:80)n\\ni=1 xi = ¯x\\nn(cid:80)n\\ni=1(xi − ¯x)2\\nn−1(cid:80)n\\ni=1(xi − ¯x)2\\nˆσmle =(cid:113) 1\\nn(cid:80)n\\ni=1(xi − ¯x)2\\nlinear regression\\nˆβ = cov(x,y)\\nvar(x)\\nˆα = ¯y − ˆβ ¯x\\nn(cid:80)n\\ni=1(yi − (ˆα + ˆβx))2\\nn−2(cid:80)n\\ni=1(yi − (ˆα + ˆβx))2\\nˆσmle =(cid:113) 1\\nn(cid:80)n\\ni=1(yi − (ˆα + ˆβx))2\\nˆσu =(cid:113) 1\\nn−2(cid:80)n\\ni=1(yi − (ˆα + ˆβx))2\\nuniform [0, θ]\\n\\nmle = 1\\nˆσ2\\nu = 1\\nˆσ2\\n\\nˆθ = maxi xi\\n\\nˆθ = n+1\\n\\nn maxi xi\\n\\nunbiased\\n\\nbiased upward\\n\\nunbiased\\n\\nbiased downward\\n\\nunbiased\\n\\nbiased downward\\n\\nunbiased\\nunbiased\\n\\nbiased downward\\n\\nunbiased\\n\\nbiased downward\\n\\nbiased downward\\n\\nbiased downward\\n\\nunbiased\\n\\n15.4 Asymptotic Properties\\nMuch of the attraction of maximum likelihood estimators is based on their properties for large sample sizes. We\\nsummarizes some the important properties below, saving a more technical discussion of these properties for later.\\n\\n1. Consistency. If θ0 is the state of nature and ˆθn(X) is the maximum likelihood estimator based on n observations\\n\\nfrom a simple random sample, then\\n\\nˆθn(X) → θ0\\n\\nas n → ∞.\\n\\nIn words, as the number of observations increase, the distribution of the maximum likelihood estimator becomes\\nmore and more concentrated about the true state of nature.\\n\\n269\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\n2. Asymptotic normality and efﬁciency. Under some assumptions that allows, among several analytical proper-\\n\\nties, the use of a central limit theorem, we have that\\n\\nconverges in distribution as n → ∞ to a normal random variable with mean 0 and variance 1/I(θ0), the Fisher\\ninformation for one observation. Thus,\\n\\n√n(ˆθn(X) − θ0)\\n\\nVarθ0(ˆθn(X)) ≈\\n\\n1\\n\\nnI(θ0)\\n\\n,\\n\\nthe lowest variance possible under the Cr´amer-Rao lower bound. This property is called asymptotic efﬁciency.\\nWe can write this in terms of the z-score. Let\\n\\nThen, as with the central limit theorem, Zn converges in distribution to a standard normal random variable.\\n\\nZn =\\n\\n.\\n\\nˆθn(X) − θ0\\n1/(cid:112)nI(θ0)\\n\\n3. Properties of the log likelihood surface. For large sample sizes, the variance of a maximum likelihood estima-\\n\\ntor of a single parameter is approximately the reciprocal of the the Fisher information\\n\\nI(θ) = −E(cid:20) ∂2\\n\\n∂θ2 ln L(θ|X)(cid:21) .\\n\\nThe Fisher information can be approximated by the observed information based on the data x,\\n\\nJ(ˆθ) = −\\n\\n∂2\\n∂θ2 ln L(ˆθ(x)|x),\\n\\nthe negative of the curvature of the log-likelihood at the maximum likelihood estimate ˆθ(x). If the curvature is\\nsmall near the maximum likelihood estimator, then the likelihood surface is nearty ﬂat and the variance is large.\\nIf the curvature is large, the likelihood decreases quickly at the maximum and the variance is small.\\n\\n15.5 Comparison of Estimation Procedures\\n\\nFor n independent observations, x1, x2, . . . xn from a distribution having mean µ and standard deviation σ, and a\\nsingle parameter θ. Let θ0 denote the true parameter value:\\n\\nestimate\\n\\nbias\\n\\nvariance\\n\\nmethod of\\nmoments\\n\\nIf µ = k(θ), then ˆθ = g(¯x), where g = k−1.\\n\\nb(θ0) ≈ g(cid:48)(cid:48)(µ) σ2\\ndelta method\\nVarθ0(ˆθ) ≈ g(cid:48)(µ)2 σ2\\n\\nn\\n\\nn\\n\\nmaximum\\nlikelihood\\n\\n*\\n\\nˆθ = arg maxθ L(θ|x)\\nFisher information\\nVarθ0(ˆθ) ≈ 1\\n\\nnI(θ0)\\n\\n* If g is continuous at µ, then both estimators are consistent. For a vector of parameters, we will need to perform a\\nhigher dimenstional delta method or invert the Fisher information matrix to estimate variance.\\n\\n270\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nFigure 15.4: Distribution of estimators. For sufﬁciently large number of observations n, the estimator ˆθ is normally distributed as indicated by\\nthe bell curves in the ﬁgure. (left) The method of moments estimator has mean θ0 + b(θ0) that is shifted by the bias b(θ0) ≈ g(cid:48)(cid:48)(µ)σ2/n . The\\nstandard deviation |g(cid:48)(θ0)σ/√n| is determined using the delta method. (right) The maximum likelihood estimator is consistent. So the mean of\\n\\nI(θ0)n. Here I(θ0) is the Fisher information evaluated at the true parameter value θ0.\\n\\nthe estimator converges to θ0. The standard deviation is1/\\n\\n(cid:112)\\n\\nWe now look at these properties in some detail by revisiting the example of the distribution of ﬁtness effects.\\nFor this example, we have two parameters - α and β for the gamma distribution and so, we will want to extend the\\nproperties above to circumstances in which we are looking to estimate more than one parameter.\\n\\n15.6 Multidimensional Estimation\\nFor a multidimensional parameter space θ = (θ1, θ2, . . . , θn), the Fisher information I(θ) is now a matrix . As with\\none-dimensional case, the ij-th entry has two alternative expressions, namely,\\n\\nI(θ)ij = Eθ(cid:20) ∂\\n\\n∂θi\\n\\nln L(θ|X)\\n\\n∂\\n∂θj\\n\\nln L(θ|X)(cid:21) = −Eθ(cid:20) ∂2\\n\\n∂θi∂θj\\n\\nln L(θ|X)(cid:21) .\\n\\nRather than taking reciprocals to obtain an estimate of the variance, we ﬁnd the matrix inverse I(θ)−1. This inverse will\\nprovide estimates of both variances and covariances. To be precise, for n observations, let ˆθi,n(X) be the maximum\\nlikelihood estimator of the i-th parameter. Then\\n\\nVarθ(ˆθi,n(X)) ≈\\n\\nI(θ)\\n\\n−1\\nii\\n\\n1\\nn\\n\\nCovθ(ˆθi,n(X), ˆθj,n(X)) ≈\\n\\nI(θ)\\n\\n−1\\nij .\\n\\n1\\nn\\n\\nWhen the i-th parameter is θi, the asymptotic normality and efﬁciency can be expressed by noting that the z-score\\n\\nZi,n =\\n\\nˆθi(X) − θi\\n−1\\nii /√n\\nI(θ)\\n\\n.\\n\\nis approximately a standard normal. As we saw in one dimension, we can replace the information matrix with the\\nobserved information matrix,\\n\\nJ(ˆθ)ij = −\\n\\nln L(ˆθ(x)|x).\\n\\n∂2\\n\\n∂θi∂θj\\n\\n271\\n\\nmethod of momentsθ0θ0+b(θ)|g\\'(θ)|σnmaximum likelihoodθ01I(θ0)n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\ndα ln Γ( ˆα)) +(cid:80)n\\n\\nFigure 15.5: The graph of n(ln ˆα − ln ¯x − d\\nderivative is decreasing states that the score function moves from increasing to decreasing with α and conﬁrming that ˆα is a maximum.\\nExample 15.13. To obtain the maximum likelihood estimate for the gamma family of random variables, write the\\nlikelihood\\n\\ni=1 ln xi crosses the horizontal axis at ˆα = 0.2376. The fact that the graph of the\\n\\n(x1x2 ··· xn)α−1e\\n\\n−β(x1+x2+···+xn) .\\n\\nL(α, β|x) =(cid:18) βα\\n\\nΓ(α)\\n\\nxα−1\\n\\n1\\n\\ne\\n\\n−βx1(cid:19)···(cid:18) βα\\n\\nΓ(α)\\n\\nxα−1\\n\\nn\\n\\ne\\n\\nand its logarithm\\n\\n−βxn(cid:19) =(cid:18) βα\\nΓ(α)(cid:19)n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\nln Γ(ˆα)) +\\n\\nd\\ndα\\n\\nln L(α, β|x) = n(α ln β − ln Γ(α)) + (α − 1)\\n\\nln xi − β\\n\\nTo determine the parameters that maximize the likelihood, we solve the equations\\n\\nxi.\\n\\nn(cid:88)i=1\\n\\n∂\\n∂α\\n\\nand\\n\\nln L(ˆα, ˆβ|x) = n(ln ˆβ −\\n\\nln xi = 0\\n\\n∂\\n∂β\\n\\nln L(ˆα, ˆβ|x) = n\\n\\nˆα\\nˆβ −\\n\\nxi = 0,\\n\\nor\\n\\n¯x =\\n\\nˆα\\nˆβ\\n\\n.\\n\\nn(cid:88)i=1\\n\\nˆµ =\\n\\nˆα\\nˆβ\\n\\n= ¯x,\\n\\nand the sample mean is the maximum likelihood estimate for the distributional mean.\\n\\nSubstituting ˆβ = ˆα/¯x into the ﬁrst equation results the following relationship for ˆα\\n\\nn(ln ˆα − ln ¯x −\\n\\nd\\ndα\\n\\nln Γ(ˆα)) +\\n\\nln xi = 0\\n\\nn(cid:88)i=1\\n\\nwhich can be solved numerically. The derivative of the logarithm of the gamma function\\n\\nln Γ(α)\\n\\nψ(α) =\\n\\nd\\ndα\\n\\n272\\n\\nRecall that the mean µ of a gamma distribution is α/β. Thus. by the invariance property of maximum likelihood\\n\\nestimators\\n\\n0.150.200.250.300.35-1.0-0.50.00.51.01.52.0alphadiff0.150.200.250.300.35-1.0-0.50.00.51.01.52.0\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nis know as the digamma function and is called in R with digamma..\\n\\nFor the example for the distribution of ﬁtness effects α = 0.23 and β = 5.35 with n = 100, a simulated data set\\n\\nyields ˆα = 0.2376 and ˆβ = 5.690 for maximum likelihood estimator. (See Figure 15.4.)\\n\\nTo determine the variance of these estimators, we ﬁrst compute the Fisher information matrix. Taking the appro-\\npriate derivatives, we ﬁnd that each of the second order derivatives are constant and thus the expected values used to\\ndetermine the entries for Fisher information matrix are the negative of these constants.\\n\\nI(α, β)11 = −\\n\\n∂2\\n∂α2 ln L(α, β|x) = n\\n\\nd2\\ndα2 ln Γ(α),\\n\\nI(α, β)22 = −\\n\\n∂2\\n∂β2 ln L(α, β|x) = n\\n\\nα\\nβ2 ,\\n\\nI(α, β)12 = −\\n\\n∂2\\n\\n∂α∂β\\n\\nln L(α, β|x) = −n\\n\\n1\\nβ\\n\\n.\\n\\nThis give a Fisher information matrix\\n\\nI(α, β) = n(cid:32) d2\\n\\nβ2 (cid:33) .\\ndα2 ln Γ(α) − 1\\n\\nβ\\nα\\n\\nβ\\n\\n− 1\\n\\nThe second derivative of the logarithm of the gamma function\\n\\nψ1(α) =\\n\\nd2\\ndα2 ln Γ(α)\\n\\nis known as the trigamma function and is called in R with trigamma.\\n\\nThe inverse\\n\\nI(α, β)\\n\\n−1 =\\n\\n1\\n\\ndα2 ln Γ(α) − 1)(cid:18) α\\n\\nβ\\n\\nnα( d2\\n\\nβ\\n\\ndα2 ln Γ(α)(cid:19) .\\n\\nβ2 d2\\n\\nFor the example for the distribution of ﬁtness effects α = 0.23 and β = 5.35 and n = 100, and\\n\\nI(0.23, 5.35)\\n\\n−1 =\\n\\n100(0.23)(19.12804)(cid:18) 0.23\\n\\n5.35\\n\\n1\\n\\n5.35\\n\\n5.352(20.12804)(cid:19) =(cid:18) 0.0001202\\n\\n0.01216\\n\\n0.01216\\n\\n1.3095 (cid:19) .\\n\\nVar(0.23,5.35)(ˆα) ≈ 0.0001202, Var(0.23,5.35)( ˆβ) ≈ 1.3095.\\n\\nσ(0.23,5.35)(ˆα) ≈ 0.0110,\\n\\nσ(0.23,5.35)( ˆβ) ≈ 1.1443.\\n\\n273\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nFigure 15.6: (top) The log-likelihood near the maximum likelihood estimators. The domain is 0.1 ≤ α ≤ 0.4 and 4 ≤ β ≤ 8. (bottom) Graphs\\nof vertical slices through the log-likelihood function surface. (left) ˆα = 0.2376 and 4 ≤ β ≤ 8 varies. (right) ˆβ = 5.690 and 0.1 ≤ α ≤ 0.4. The\\nvariance of the estimator is approximately the negative reciprocal of the second derivative of the log-likelihood function at the maximum likelihood\\nestimators (known as the observed information). Note that the log-likelihood function is nearly ﬂat as β varies. This leads to the interpretation that\\na range of values for β are nearly equally likely and that the variance for the estimator for ˆβ will be high. On the other hand, the log-likelihood\\nfunction has a much greater curvature for the α parameter and the estimator ˆα will have a much smaller variance than ˆβ\\n\\n274\\n\\nIntroductiontotheScienceofStatisticsMaximumLikelihoodEstimationalphabetascore.eval0.10.20.30.4330335340345350355alphascore45678330335340345350355betascoreFigure15.5:(top)Thelog-likelihoodnearthemaximumlikelihoodestimators.Thedomainis0.1\\uf8ff↵\\uf8ff0.4and4\\uf8ff\\x00\\uf8ff8.(bottom)Graphsofverticalslicesthroughthelog-likelihoodfunctionsurface.(left)ˆ↵=0.2376and0.1\\uf8ff↵\\uf8ff0.4varies.(right)ˆ\\x00=5.690and4\\uf8ff\\x00\\uf8ff8.Thevarianceoftheestimatorisapproximatelythenegativereciprocalofthesecondderivativeofthelog-likelihoodfunctionatthemaximumlikelihoodestimators(knownastheobservedinformation).Notethatthelog-likelihoodfunctionisnearlyﬂatas\\x00varies.Thisleadstotheinterpretationthatarangeofvaluesfor\\x00arenearlyequallylikelyandthatthevariancefortheestimatorforˆ\\x00willbehigh.Ontheotherhand,thelog-likelihoodfunctionhasamuchgreatercurvatureforthe↵parameterandtheestimatorˆ↵willhaveamuchsmallervariancethanˆ\\x002300.10.20.30.4330335340345350355alphascore45678330335340345350355betascore\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nCompare this to the empirical values of 0.0662 and 2.046 for the method of moments. This gives the following\\n\\ntable of standard deviations for n = 100 observation\\n\\nmethod\\n\\nmaximum likelihood\\nmethod of moments\\n\\nratio\\n\\nˆα\\n\\n0.0110\\n0.0662\\n0.166\\n\\nˆβ\\n\\n1.1443\\n2.046\\n0.559\\n\\nThus, the standard deviation for the maximum likelihood estimator is respectively 17% and 56% that of method of\\nmoments estimator. We will look at the impact as we move on to our next topic - interval estimation and the conﬁdence\\nintervals.\\n\\nExercise 15.14. If the data are a simple random sample of 100 observations of a Γ(0.23, 5.35) random variable. Use\\nthe approximate normality of maximum likelihood estimators to estimate\\n\\nP{ˆα ≥ 0.2376}\\n\\nP{ ˆβ ≥ 5.690}.\\n\\n15.7 The Case of Exponential Families\\nAs with the Cramer-Rao bound for unbiased estimator, the case of exponential families forms an elegant example, in\\nthis case, for maximum likelihood estimation. Let ﬁrst write the density function for this family by\\n\\nusing the natural parameter η., Recall that the Fisher information\\n\\nfX (x|η) = c(η)h(x) exp(ηT (x)).\\n\\n(15.6)\\n\\nI(η) = −\\n\\n∂2\\n∂η2 ln c(η).\\n\\nExercise 15.15. The maximum likelihood estimate based on independent observations from a member of an exponen-\\ntial family is a function of T (x), the mean of the sufﬁcient statistic.\\n\\nWriting ˆη(x) = g(T (x)). Recall from the discussion of the Cram´er-Rao bound, that the estimator ˆη is efﬁcient.\\nIn other, words, if we have n independent observations from the density fX (x|η), then ˆη is an unbiased estimator of η\\nwith\\n\\nVarη(ˆη(X)) =\\n\\n1\\n\\nnI(η)\\n\\nReturning to the parameter space with θ ∈ Θ and the form of the exponential family with\\n\\nwe can use the invariance property of the the maximum likelihood estimate to say that\\n\\nfX (x|θ) = c(η(θ))h(x) exp(η(θ)T (x)),\\n\\nˆθ(x) = η\\n\\n−1(g(T (x)))\\n\\nprovided that η is a one-to-one function and thus has an inverse function η−1.\\nExercise 15.16. Use the delta method to show that\\n\\nIθ(θ) ≈ Iη(η(θ))(cid:18) dη(θ)\\ndθ (cid:19)2\\n\\nIn the discussion of exponential families and the Cramer-Rao bound, we learned that the approximation above is\\n\\nan equality.\\n\\n275\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\n15.8 Choice of Estimators\\nWith all of the desirable properties of the maximum likelihood estimator, the question arises as to why would one\\nchoose a method of moments estimator?\\n\\nOne answer is that the use maximum likelihood techniques relies on knowing the density function explicitly.\\nMoreover, the form of the density must be amenable to the analysis necessary to maximize the likelihood and ﬁnd the\\nFisher information.\\n\\nHowever, much less about the experiment is need in order to compute moments. Thus far, we have computed\\n\\nmoments using the density\\n\\nEθX m =(cid:90) ∞\\n\\n−∞\\n\\nxmfX (x|θ) dx.\\n\\nHowever, consider the case of determining parameters in the distribution in the number of proteins in a tissue. If\\n\\nthe tissue has several cell types, then we would need\\n\\n• the distribution of cell types, and\\n• a density function for the number of proteins in each cell type.\\nThese two pieces of information can be used to calculate the mean and variance for the number of cells with some\\nease. However, giving an explicit expression for the density and hence the likelihood function is more difﬁcult to\\nobtain. This leads to quite intricate computations to carry out the desired analysis of the likelihood function.\\n\\n15.9 Technical Aspects\\nWe can use concepts previously introduced to obtain the properties for the maximum likelihood estimator. For exam-\\nple, θ0 is more likely that a another parameter value θ\\n\\nL(θ0|X) > L(θ|X)\\n\\nif and only if\\n\\nln\\n\\nf (Xi|θ0)\\nf (Xi|θ)\\n\\n> 0.\\n\\nBy the strong law of large numbers, this sum converges to\\n\\n1\\nn\\n\\nn(cid:88)i=1\\nf (X1|θ)(cid:21) .\\n\\nf (X1|θ0)\\n\\nEθ0(cid:20)ln\\n\\nwhich is greater than 0. thus, for a large number of observations and a given value of θ, then with a probability nearly\\none, L(θ0|X) > L(θ|X) and so the maximum likelihood estimator has a high probability of being very near θ0. This\\nis a statement of the consistency of the estimator.\\n\\nFor the asymptotic normality and efﬁciency, we write the linear approximation of the score function\\n\\nd\\ndθ\\n\\nln L(θ|X) ≈\\n\\nd\\ndθ\\n\\nln L(θ0|X) + (θ − θ0)\\n\\nd2\\ndθ2 ln L(θ0|X).\\n\\nNow substitute θ = ˆθ and note that d\\n\\ndθ ln L(ˆθ|X) = 0. Then\\n\\n√n(ˆθn(X) − θ0) ≈ −√n\\n\\nd\\n\\ndθ ln L(θ0|X)\\ndθ2 ln L(θ0|X)\\n\\nd2\\n\\n=\\n\\n1√\\nn\\n− 1\\n\\nn\\n\\nd\\n\\ndθ ln L(θ0|X)\\ndθ2 ln L(θ0|X)\\n\\nd2\\n\\nNow assume that θ0 is the true state of nature. Then, the random variables d ln f (Xi|θ0)/dθ are independent with\\nmean 0 and variance I(θ0). Thus, the distribution of numerator\\n\\n1\\n√n\\n\\nd\\ndθ\\n\\nln L(θ0|X) =\\n\\n1\\n√n\\n\\nd\\ndθ\\n\\nn(cid:88)i=1\\n\\nln f (Xi|θ0)\\n\\n276\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\nconverges, by the central limit theorem, to a normal random variable with mean 0 and variance I(θ0). For the denom-\\ninator, −d2 ln f (Xi|θ0)/dθ2 are independent with mean I(θ0). Thus,\\nd2\\ndθ2 ln f (Xi|θ0)\\n\\nd2\\ndθ2 ln L(θ0|X) = −\\n\\n1\\nn\\n\\n1\\nn\\n\\n−\\n\\nconverges, by the law of large numbers, to I(θ0). Thus, the distribution of the ratio, √n(ˆθn(X) − θ0), converges to a\\nnormal random variable with variance I(θ0)/I(θ0)2 = 1/I(θ0).\\n\\nn(cid:88)i=1\\n\\n15.10 Answers to Selected Exercises\\n15.3. We have found that the score function\\n\\nThus\\n\\n∂\\n∂p\\n\\nln L(p|x) = n\\n\\n¯x − p\\np(1 − p)\\n\\n∂\\n∂p\\n\\nln L(p|x) > 0\\n\\nif p < ¯x,\\n\\nand\\n\\n∂\\n∂p\\n\\nln L(p|x) < 0\\n\\nif p > ¯x\\n\\nIn words, ln L(p|x) is increasing for p < ¯x and decreasing for p > ¯x. Thus, ˆp(x) = ¯x is a maximum.\\n15.6. We would like to maximize the likelihood given the number of recaptured individuals r. Because the domain for\\nN is the nonnegative integers, we cannot use calculus. However, we can look at the ratio of the likelihood values for\\nsuccessive value of the total population.\\n\\nN is more likely that N − 1 precisely when this ratio is larger than one. The computation below will show that\\nthis ratio is greater than 1 for small values of N and less than one for large values. Thus, there is a place in the middle\\nwhich has the maximum. We expand the binomial coefﬁcients in the expression for L(N|r) and simplify.\\n\\nL(N|r)\\nL(N − 1|r)\\n\\nL(N|r)\\nL(N − 1|r)\\n\\nThus, the ratio\\n\\nexceeds 1if and only if\\n\\nk (cid:1) = (cid:0)N−t\\n= (cid:0)t\\nr(cid:1)(cid:0)N−t\\nk−r(cid:1)/(cid:0)N\\nk(cid:1)\\nk−r(cid:1)(cid:0)N−1\\nk (cid:1)\\nk−r (cid:1)(cid:0)N\\n(cid:0)N−t−1\\nk−r (cid:1)/(cid:0)N−1\\n(cid:0)t\\nr(cid:1)(cid:0)N−t−1\\nk(cid:1) =\\n\\n(N − t)!(N − 1)!(N − t − k + r − 1)!(N − k)!\\n(N − t − 1)!N !(N − t − k + r)!(N − k − 1)!\\n\\n=\\n\\n(N−t)!\\n(N−t−1)!\\n\\n(k−r)!(N−t−k+r)!\\n(k−r)!(N−t−k+r−1)!\\n\\n(N−1)!\\n\\nN !\\n\\nk!(N−k−1)!\\nk!(N−k)!\\n(N − t)(N − k)\\nN (N − t − k + r)\\n\\n.\\n\\n=\\n\\nL(N|r)\\nL(N − 1|r)\\n\\n=\\n\\n(N − t)(N − k)\\nN (N − t − k + r)\\n\\n(N − t)(N − k) > N (N − t − k + r)\\n\\nN 2 − tN − kN + tk > N 2 − tN − kN + rN\\n\\ntk > rN\\ntk\\nr\\n\\n> N\\n\\nWriting [x] for the integer part of x, we see that L(N|r) > L(N − 1|r) for N < [tk/r] and L(N|r) ≤ L(N − 1|r)\\n\\nfor N ≥ [tk/r]. This give the maximum likelihood estimator\\nˆN =(cid:20) tk\\nr(cid:21) .\\n\\n277\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\n15.7. The log-likelihood function\\n\\nln L(α, β, σ2|y, x) = −\\n\\nn\\n2\\n\\n(ln(2π) + ln σ2) −\\n\\n1\\n2σ2\\n\\nn(cid:88)i=1\\n\\n(yi − (α + βxi))2\\n\\nleads to the ordinary least squares equations for the maximum likelihood estimates ˆα and ˆβ. Take the partial derivative\\nwith respect to σ2,\\n\\n∂\\n∂σ2 L(α, β, σ2|y, x) = −\\n\\nn\\n2σ2 +\\n\\n1\\n\\n2(σ2)2\\n\\n(yi − (α + βxi))2.\\n\\nn(cid:88)i=1\\n\\nThis partial derivative is 0 at the maximum likelihood estimates ˆσ2, ˆα and ˆβ.\\n\\nor\\n\\n0 = −\\n\\nn\\n2ˆσ2 +\\n\\nˆσ2 =\\n\\n1\\nn\\n\\n(yi − (ˆα + ˆβxi))2\\n\\n1\\n\\n2(ˆσ2)2\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n(yi − (ˆα + ˆβxi))2.\\n\\n15.8. Take the derivative with respect to σ2 in (15.3)\\n\\n∂\\n∂σ2 ln L(α, β, σ2|y, x) = −\\n\\nn\\n2σ2 +\\n\\n1\\n\\n2(σ2)2\\n\\nn(cid:88)i=1\\n\\n(yi − (α + βxi))2.\\n\\nNow set this equal to zero, substitute ˆα for α, ˆβ for β and solve for σ2 to obtain (15.4).\\n15.9. The maximum likelihood principle leads to a minimization problem for\\n\\nSSw(α, β) =\\n\\n\\x012\\ni =\\n\\nw(xi)(yi − (α + βxi))2.\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\n\\nFollowing the steps to derive the equations for ordinary least squares, take partial derivatives to ﬁnd that\\n\\n∂\\n∂β\\n\\nSSw(α, β) = −2\\n\\nn(cid:88)i=1\\n\\nw(xi)xi(yi − α − βxi)\\n\\n∂\\n∂α\\n\\nSSw(α, β) = −2\\n\\nn(cid:88)i=1\\n\\nw(xi)(yi − α − βxi).\\n\\nSet these two equations equal to 0 and call the solutions ˆαw and ˆβw.\\n\\nMultiply these equations by the appropriate factors to obtain\\n\\nw(xi)x2\\ni\\n\\n0 =\\n\\n0 =\\n\\nw(xi)xiyi − ˆαw\\n\\nw(xi)(yi − ˆαw − ˆβwxi) =\\n\\nn(cid:88)i=1\\nw(xi) − ˆβw\\n\\nn(cid:88)i=1\\nw(xi)xi(yi − ˆαw − ˆβwxi) =\\nw(xi)xi − ˆβw\\nn(cid:88)i=1\\nn(cid:88)i=1\\nn(cid:88)i=1\\nw(xi)xi(cid:33)\\nw(xi)(cid:33)(cid:32) n(cid:88)i=1\\nw(xi)xiyi(cid:33) − ˆαw(cid:32) n(cid:88)i=1\\n\\nw(xi)yi − ˆαw\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n0 =(cid:32) n(cid:88)i=1\\n− ˆβw(cid:32) n(cid:88)i=1\\n\\nw(xi)(cid:33)(cid:32) n(cid:88)i=1\\nw(xi)(cid:33)(cid:32) n(cid:88)i=1\\n\\ni(cid:33)\\n\\nw(xi)x2\\n\\nw(xi)xi\\n\\n(15.7)\\n\\n(15.8)\\n\\n(15.9)\\n\\n278\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\n0 =(cid:32) n(cid:88)i=1\\n\\nw(xi)xi(cid:33)(cid:32) n(cid:88)i=1\\n\\nw(xi)yi(cid:33) − ˆαw(cid:32) n(cid:88)i=1\\n\\nw(xi)(cid:33)(cid:32) n(cid:88)i=1\\n\\nw(xi)xi(cid:33) − ˆβw(cid:32) n(cid:88)i=1\\n\\nNow subtract the equation (15.10) from equation (15.9) and solve for ˆβ.\\n\\nw(xi)xi(cid:33)2\\n\\n(15.10)\\n\\nˆβ =\\n\\n((cid:80)n\\ni=1 w(xi)) ((cid:80)n\\ni=1 w(xi)xiyi) − ((cid:80)n\\ni − ((cid:80)n\\nn(cid:80)n\\n= (cid:80)n\\n(cid:80)n\\n\\ni=1 w(xi)x2\\ni=1 w(xi)(xi − ¯xw)(yi − ¯yw)\\n\\ni=1 w(xi)(xi − ¯xw)2\\n\\ni=1 w(xi) to obtain\\n\\n=\\n\\ncovw(x, y)\\nvarw(x)\\n\\n.\\n\\ni=1 w(xi)xi) ((cid:80)n\\n\\ni=1 w(xi)xi)2\\n\\ni=1 w(xi)yi)\\n\\nNext, divide equation (15.10) by(cid:80)n\\n\\n¯yw = ˆαw + ˆβw ¯xw.\\n\\n(15.11)\\n\\n15.10. Because the \\x01i have mean zero,\\n\\nE(α,β)yi = E(α,β)[α + βxi + γ(xi)\\x01i] = α + βxi + γ(xi)E(α,β)[\\x01i] = α + βxi.\\n\\nNext, use the linearity property of expectation to ﬁnd the mean of ¯yw.\\n\\n= α + β ¯xw.\\n\\n(15.12)\\n\\ni=1 w(xi)E(α,β)yi\\n\\ni=1 w(xi)(α + βxi)\\n\\ni=1 w(xi)\\n\\ni=1 w(xi)\\n\\n= (cid:80)n\\n\\n(cid:80)n\\n\\nE(α,β) ¯yw = (cid:80)n\\n\\n(cid:80)n\\nˆβw = E(α,β)(cid:20)covw(x, y)\\nvarw(x) (cid:21) =\\n(cid:80)n\\n\\nvarw(x)(cid:80)n\\n\\n=\\n\\n1\\n\\nTaken together, we have that E(α,β)[yi− ¯yw] = (α + βxi.)− (α + βxi) = β(xi− ¯xw). To show that ˆβw is an unbiased\\nestimator, we see that\\n(cid:21)\\n\\ni=1 w(xi)(xi − ¯xw)(yi − ¯yw)\\n\\nE(α,β)[covw(x, y)]\\n\\ni=1 w(xi)\\n\\nvarw(x)\\n\\nE(α,β)\\n\\n=\\n\\n1\\n\\nvarw(x)\\nβ\\n\\nE(α,β)(cid:20)(cid:80)n\\nvarw(x)(cid:80)n\\n\\n=\\n\\n(cid:80)n\\n\\ni=1 w(xi)\\n\\n(cid:80)n\\n\\ni=1 w(xi)(xi − ¯xw)(xi − ¯xw)\\n\\n= β.\\n\\ni=1 w(xi)(xi − ¯xw)E(α,β)[yi − ¯yw]\\n\\ni=1 w(xi)\\n\\nTo show that ˆαw is an unbiased estimator, recall that ¯yw = ˆαw + ˆβw ¯xw. Thus\\n\\nE(α,β) ˆαw = E(α,β)[¯yw − ˆβw ¯xw] = E(α,β) ¯yw − E(α,β)[ ˆβw]¯xw = α + β ¯xw − β ¯xw = α,\\n\\nusing (15.12) and the fact that ˆβw is an unbiased estimator of β\\n15.14. For ˆα, we have the z-score\\n\\nz ˆα =\\n\\nˆα − 0.23\\n√0.0001202 ≥\\n\\n0.2376 − 0.23\\n√0.0001202\\n\\n= 0.6841.\\n\\nThus, using the normal approximation,\\n\\nFor ˆβ, we have the z-score\\n\\nP{ˆα ≥ 0.2367} = P{z ˆα ≥ 0.6841} = 0.2470.\\n\\nz ˆβ =\\n\\nˆβ − 5.35\\n√1.3095 ≥\\n\\n5.690 − 5.35\\n√1.3095\\n\\n= 0.2971.\\n\\nHere, the normal approximation gives\\n\\nP{ ˆβ ≥ 5.690} = P{z ˆβ ≥ 0.2971} = 0.3832.\\n\\n279\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nMaximum Likelihood Estimation\\n\\n15.15. For n independent observations, the likelihood function\\n\\nL(π|x) = c(π)n\\n\\nn(cid:89)i=1\\n\\nln L(π|x) = n ln c(π) +\\n\\n∂\\n∂π\\n\\nln L(π|x) = n\\n\\nc(cid:48)(π)\\nc(π)\\n\\n+\\n\\n= n(cid:18) c(cid:48)(π)\\n\\nc(π)\\n\\nn(cid:88)i=1\\nln h(xi) +(cid:32)π\\n\\nT (xi)(cid:33)\\nn(cid:88)i=1\\n\\nh(xi) exp(cid:32)π\\nn(cid:88)i=1\\nn(cid:88)i=1\\n+ T (x)(cid:19)\\n\\nT (xi)\\n\\nT (xi)(cid:33)\\n\\nSet this equal to 0 to give ˆπ as a function of T (x).\\n15.16. By the delta method and the fact that ˆθ(X) is a maximum likelihood estimator,\\n\\nVarθ(ˆθ(X))\\n\\nVarθ(η(ˆθ(X))) ≈(cid:18) dη(θ)\\ndθ (cid:19)2\\nnIη(η) ≈(cid:18) dη(θ)\\ndθ (cid:19)2\\nIθ(θ) ≈ Iη(η(θ))(cid:18) dη(θ)\\ndθ (cid:19)2\\n\\nnIθ(θ)\\n\\n1\\n\\n1\\n\\n.\\n\\n280\\n\\n\\x0cTopic 16\\n\\nInterval Estimation\\n\\nThe form of this solution consists in determining certain intervals, which I propose to call the con-\\nﬁdence intervals..., in which we may assume are contained the values of the estimated characters of the\\npopulation, the probability of an error is a statement of this sort being equal to or less than 1− \\x01, where \\x01\\nis any number 0 < \\x01 < 1, chosen in advance. The number \\x01 I call the conﬁdence coefﬁcient. - Jerzy Ney-\\nman, 1934, On the Two Different Aspects of the Representative Method, Journal of the Royal Statistical\\nSociety\\n\\nOur strategy to estimation thus far has been to use a method to ﬁnd an estimator, e.g., method of moments, or\\nmaximum likelihood, and evaluate the quality of the estimator by evaluating its bias and the variance. Often, we\\nknow more about the distribution of the estimator and this allows us to take a more comprehensive statement about the\\nestimation procedure.\\n\\nInterval estimation is an exteneion to the variety of techniques we have examined. Given data x, we replace the\\npoint estimate ˆθ(x) for the parameter θ by a statistic that is subset ˆC(x) of the parameter space. We will consider both\\nthe classical and Bayesian approaches to choosing ˆC(x). As we shall learn, the two approaches have very different\\ninterpretations.\\n\\n16.1 Classical Statistics\\nIn this case, the random set ˆC(X) is chosen to have a prescribed high probability, γ, of containing the true parameter\\nvalue θ. In symbols,\\n\\nPθ{θ ∈ ˆC(X)} = γ.\\n\\nIn this case, the set ˆC(x) is called a γ-level conﬁdence set.\\nIn the case of a one dimensional parameter set, the typical choice\\nof conﬁdence set is a conﬁdence interval\\n\\nˆC(x) = (ˆθ(cid:96)(x), ˆθu(x)).\\n\\nOften this interval takes the form\\nˆC(x) = (ˆθ(x) − m(x), ˆθ(x) + m(x)) = ˆθ(x) ± m(x)\\n\\nwhere the two statistics,\\n\\n• ˆθ(x) is a point estimate, and\\n• m(x) is the margin of error.\\n\\nFigure 16.1: Upper tail critical values. α is the area under\\nthe standard normal density and to the right of the vertical line\\nat critical value zα\\n\\n281\\n\\n−3−2−1012300.050.10.150.20.250.30.350.4zαarea α\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\n16.1.1 Means\\nExample 16.1 (1-sample z interval). If X1.X2. . . . Xn are normal random variables with unknown mean µ but known\\nvariance σ2\\n\\n0. Then,\\n\\nZ =\\n\\n¯X − µ\\nσ0/√n\\n\\nis a standard normal random variable. For any α between 0 and 1, choose zα so that\\n\\nP{Z > zα} = α or equivalently P{Z ≤ zα} = 1 − α.\\n\\nThe value is known as the upper tail probability with critical value zα. We can compute this in R using, for example\\n\\n> qnorm(0.975)\\n[1] 1.959964\\n\\nfor α = 0.025.\\n\\nIf γ = 1 − 2α, then α = (1 − γ)/2. In this case, we have that\\n\\nP{−zα < Z < zα} = γ.\\n\\nLet µ0 is the state of nature. Taking in turn each the two inequalities in the line above and isolating µ0, we ﬁnd that\\n\\n¯X − µ0\\nσ0/√n\\n\\n= Z < zα\\n¯X − µ0 < zα\\n< µ0\\n\\nσ0√n\\n\\nσ0√\\nn\\n\\n¯X − zα\\n\\nSimilarly,\\n\\nimplies\\n\\nThus\\n\\nhas probability γ. Thus, for data x,\\n\\n¯X − µ0\\nσ0/√n\\n\\n= Z > −zα\\n\\nµ0 < ¯X + zα\\n\\nσ0√n\\n\\n¯X − zα\\n\\nσ0√n\\n\\n< µ0 < ¯X + zα\\n\\nσ0√n\\n\\n.\\n\\n¯x ± z(1−γ)/2\\n\\nσ0√n\\n\\nis a conﬁdence interval with conﬁdence level γ. In this case,\\n\\nˆµ(x) = ¯x is the estimate for the mean and m(x) = z(1−γ)/2σ0/√n is the margin of error.\\n\\nWe can use the z-interval above for the conﬁdence interval for µ for data that is not necessarily normally dis-\\ntributed as long as the central limit theorem applies. For one population intervals for means, n > 30 and data not\\nstrongly skewed is a good rule of thumb.\\n\\n282\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nGenerally, the standard deviation is not known and must be estimated. So, let X1, X2,··· , Xn be normal random\\nvariables with unknown mean and unknown standard deviation. Let S2 be the unbiased sample variance. If we are\\nforced to replace the unknown variance σ2 with its unbiased estimate s2, then the statistic is known as t:\\n\\nThe term s/√n which estimates the standard deviation of the sample mean is called the standard error. The\\n\\nremarkable discovery by William Gossett is that the distribution of the t statistic can be determined exactly. Write\\n\\nt =\\n\\n¯x − µ\\ns/√n\\n\\n.\\n\\nTn−1 =\\n\\n√n( ¯X − µ)\\n\\nS\\n\\n.\\n\\nThen, Gossett was able to establish the following three facts:\\n• The numerator is a standard normal random variable.\\n• The denominator is the square root of\\n\\nS2 =\\n\\n1\\n\\nn − 1\\n\\nn(cid:88)i=1\\n(Xi − ¯X)2.\\n\\nThis sum has chi-square distribution with n − 1 degrees of freedom.\\n\\n• The numerator and denominator are independent.\\n\\nWith this, Gossett was able to compute the density of the t distribution with n − 1 degrees of freedom. Gossett,\\nwho worked for the the brewery of Arthur Guinness in Dublin, was permitted to publish his results only if it appeared\\nunder a pseudonym. Gosset chose the name Student, thus the distribution is sometimes known as Student’s t.\\n\\nFigure 16.2: The density and distribution function for a standard normal random variable (black) and a t random variable with 4 degrees of freedom\\n(red). The variance of the t distribution is df /(df − 2) = 4/(4 − 2) = 2 is higher than the variance of a standard normal. This can be seen in the\\nbroader shoulders of the t density function or in the smaller increases in the t distribution function away from the mean of 0.\\n\\n283\\n\\n-4-20240.00.10.20.30.4x-4-20240.00.10.20.30.4x-4-20240.00.20.40.60.81.0x-4-20240.00.20.40.60.81.0x\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nFigure 16.3: Upper critical values for the t conﬁdence interval with γ = 0.90 (black), 0.95 (red), 0.98 (magenta) and 0.99 (blue) as a function of\\ndf, the number of degrees of freedom. Note that these critical values decrease to the critical value for the z conﬁdence interval and increases with\\nγ.\\n\\nAgain, for any α between 0 and 1, let upper tail probability tn−1,α satisfy\\n\\nP{Tn−1 > tn−1,α} = α or equivalently P{Tn−1 ≤ tn−1,α} = 1 − α.\\n\\nWe can compute this in R using, for example\\n\\n> qt(0.975,12)\\n[1] 2.178813\\nfor α = 0.025 and n − 1 = 12.\\nExample 16.2. For the data on the lengths of 200 Bacillus subtilis, we had a mean ¯x = 2.49 and standard deviation\\ns = 0.674. For a 96% conﬁdence interval α = 0.02 and we type in R,\\n\\n> qt(0.98,199)\\n[1] 2.067298\\n\\nThus, the interval is\\n\\n2.490 ± 2.0674\\n\\n0.674\\n√200\\n\\n= 2.490 ± 0.099\\n\\nor\\n\\n(2.391, 2.589)\\n\\nExample 16.3. We can obtain the data for the Michaelson-Morley experiment using R by typing\\n\\n> data(morley)\\n\\nThe data have 100 rows - 5 experiments (column 1) of 20 runs (column 2). The Speed is in column 3. The values\\nfor speed are the amounts over 299,000 km/sec. Thus, a t-conﬁdence interval will have 99 degrees of freedom. We can\\nsee a histogram by writing hist(morley$Speed). To determine a 95% conﬁdence interval, we ﬁnd\\n\\n284\\n\\n102030405001234df102030405001234df102030405001234df102030405001234df\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nFigure 16.4: Measurements of the speed of light. Actual values are 299,000 kilometers per second plus the value shown.\\n\\n> mean(morley$Speed)\\n[1] 852.4\\n> sd(morley$Speed)\\n[1] 79.01055\\n> qt(0.975,99)\\n[1] 1.984217\\n\\nThus, our conﬁdence interval for the speed of light is\\n\\n299, 852.4 ± 1.9842\\n\\n79.0\\n√100\\n\\n= 299, 852.4 ± 15.7\\n\\nor the interval (299836.7, 299868.1)\\n\\nThis conﬁdence interval does not include the presently determined values of 299,792.458 km/sec for the speed of light.\\nThe conﬁdence interval can also be found by tying t.test(morley$Speed). We will study this command in more\\ndetail when we describe the t-test.\\n\\nExercise 16.4. Give a 90% and a 98% conﬁdence interval for the example above.\\n\\nWe often wish to determine a sample size that will guarantee a desired margin of error. For a γ-level t-interval,\\n\\nthis is\\n\\nSolving this for n yields\\n\\nBecause the number of degrees of freedom, n − 1, for the t distribution is unknown, the quantity n appears on both\\nsides of the equation and the value of s is unknown. We search for a conservative value for n, i.e., a margin of error\\nthat will be no greater that the desired length. This can be achieved by overestimating tn−1,(1−γ)/2 and s. For the\\n\\nm = tn−1,(1−γ)/2\\n\\ns\\n√n\\n\\n.\\n\\nn =(cid:18) tn−1,(1−γ)/2 s\\n\\nm\\n\\n(cid:19)2\\n\\n.\\n\\n285\\n\\nHistogram of morley$Speedmorley$SpeedFrequency60070080090010001100051015202530\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nspeed of light example above, if we desire a margin of error of m = 10 km/sec for a 95% conﬁdence interval, then we\\nset tn−1,(1−γ)/2 = 2 and s = 80 to obtain\\n\\nn ≈(cid:18) 2 · 80\\n10 (cid:19)2\\n\\n= 256\\n\\nmeasurements are necessary to obtain the desired margin of error..\\n\\nThe next set of conﬁdence intervals are determined, in those case in which the distributional variance in known,\\nby ﬁnding the standardized score and using the normal approximation as given via the central limit theorem. In the\\ncases in which the variance is unknown, we replace the distribution variance with a variance that is estimated from the\\nobservations. In this case, the procedure that is analogous to the standardized score is called the studentized score.\\nExample 16.5 (matched pair t interval). We begin with two quantitative measurements\\n\\n(X1,1, . . . , X1,n) and\\n\\n(X2,1, . . . , X2,n),\\n\\non the same n individuals. Assume that the ﬁrst set of measurements has mean µ1 and the second set has mean µ2.\\n\\nIf we want to determine a conﬁdence interval for the difference µ1 − µ2, we can apply the t-procedure to the\\n\\ndifferences\\n\\nto obtain the conﬁdence interval\\n\\n(X1,1 − X2,1, . . . , X1,n − X2,n)\\nSd√n\\n\\n( ¯X1 − ¯X2) ± tn−1,(1−γ)/2\\n\\nwhere Sd is the standard deviation of the difference.\\n\\nExample 16.6 (2-sample z interval). If we have two independent samples of normal random variables\\n\\n(X1,1, . . . , X1,n1) and\\n\\n(X2,1, . . . , X2,n2),\\n\\nthe ﬁrst having mean µ1 and variance σ2\\nsample means\\n\\n1 and the second having mean µ2 and variance σ2\\n\\n2, then the difference in their\\n\\nis also a normal random variable with\\n\\n¯X2 − ¯X1\\n\\nmean µ1 − µ2\\n\\nand\\n\\nvariance σ2\\n1\\nn1\\n\\n+\\n\\nσ2\\n2\\nn2\\n\\n.\\n\\nTherefore,\\n\\n(cid:113) σ2\\nis a standard normal random variable. In the case in which the variances σ2\\nconﬁdence interval for the difference in parameters µ1 − µ2.\\n\\n+ σ2\\n2\\nn2\\n\\nZ =\\n\\n1\\nn1\\n\\n( ¯X1 − ¯X2) − (µ1 − µ2)\\n\\n( ¯X1 − ¯X2) ± z(1−γ)/2(cid:115) σ2\\n\\n1\\nn1\\n\\n+\\n\\nσ2\\n2\\nn2\\n\\n.\\n\\n1 and σ2\\n\\n2 are known, this gives us a γ-level\\n\\nExample 16.7 (2-sample t interval). If we know that σ2\\ndeviation. Let S2\\nweighted average of the sample variances with weights equal to their respective degrees of freedom.\\n\\n2, then we can pool the data to compute the standard\\n2 be the sample variances from the two samples. Then the pooled sample variance Sp is the\\n\\n1 and S2\\n\\n1 = σ2\\n\\nS2\\n\\np =\\n\\n(n1 − 1)S2\\n\\n1 + (n2 − 1)S2\\n\\n2\\n\\nn1 + n2 − 2\\n286\\n\\n.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nThis gives a statistic\\n\\nTn1+n2−2 =\\n\\n( ¯X1 − ¯X2) − (µ1 − µ2)\\n\\nSp(cid:113) 1\\n\\nn1\\n\\n+ 1\\nn2\\n\\n( ¯X1 − ¯X2) ± tn1+n2−2,(1−γ)/2Sp(cid:114) 1\\n\\nn1\\n\\n+\\n\\n1\\nn2\\n\\nthat has a t distribution with n1 + n2 − 2 degrees of freedom. Thus we have the γ level conﬁdence interval\\n\\nfor µ1 − µ2.\\n\\nIf we do not know that σ2\\n\\n1 = σ2\\n\\n2, then the corresponding studentized random variable\\n\\n( ¯X1 − ¯X2) − (µ1 − µ2)\\n\\nT =\\n\\n(cid:113) S2\\n\\n1\\nn1\\n\\n+ S2\\n2\\nn2\\n\\nno longer has a t-distribution.\\n\\ngiven by the Welch-Satterthwaite equation\\n\\nWelch and Satterthwaite have provided an approximation to the t distribution with effective degrees of freedom\\n\\nν =\\n\\nThis give a γ-level conﬁdence interval\\n\\n(cid:16) s2\\n\\n1\\nn1\\n\\ns4\\n1\\n\\n+ s2\\n\\n2\\n\\nn2(cid:17)2\\n\\ns4\\n2\\n\\n1·(n1−1) +\\nn2\\n\\n2·(n2−1)\\nn2\\n\\n.\\n\\n(16.1)\\n\\n¯x1 − ¯x2 ± tν,(1−γ/2(cid:115) s2\\n\\n1\\nn1\\n\\n+\\n\\ns2\\n2\\nn2\\n\\n.\\n\\nFor two sample intervals, the number of observations per group may need to be at least 40 for a good approxima-\\n\\ntion to the normal distribution.\\n\\nExercise 16.8. Show that the effective degrees is between the worst case of the minimum choice from a one sample\\nt-interval and the best case of equal variances.\\n\\nFor data on the life span in days of 88 wildtype and 99 transgenic mosquitoes, we have the summary\\n\\nmin{n1, n2} − 1 ≤ ν ≤ n1 + n2 − 2\\n\\nwildtype\\ntransgenic\\n\\nobservations mean\\n20.784\\n16.546\\n\\n88\\n99\\n\\nstandard\\ndeviation\\n\\n12.99\\n10.78\\n\\nUsing the conservative 95% conﬁdence interval based on min{n1, n2} − 1 = 87 degrees of freedom, we use\\n\\n> qt(0.975,87)\\n[1] 1.987608\\n\\nto obtain the interval\\n\\n(20.78 − 16.55) ± 1.9876(cid:114) 12.992\\n\\n88\\n\\n+\\n\\n10.782\\n\\n99\\n\\n= (0.744, 7.733)\\n\\nUsing the the Welch-Satterthwaite equation, we obtain ν = 169.665. The increase in the number of degrees of\\n\\nfreedom gives a slightly narrower interval (0.768, 7.710).\\n\\n287\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\n16.1.2 Linear Regression\\nFor ordinary linear regression, we have given least squares estimates for the slope β and the intercept α. For data\\n(x1, y1), (x2, y2) . . . , (xn, yn), our model is\\n\\nyi = α + βxi + \\x01i\\n\\nwhere \\x01i are independent N (0, σ) random variables. Recall that the estimator for the slope\\n\\nˆβ(x, y) =\\n\\ncov(x, y)\\nvar(x)\\n\\nis unbiased.\\n\\nExercise 16.9. Show that\\n\\nVar(α,β)( ˆβ) =\\n\\nσ2\\n\\n(n − 1)var(x)\\n\\n.\\n\\nVar(α,β)(ˆα) = σ2(cid:18) 1\\n\\n(n − 1)var(x)(cid:19) = σ2(cid:18) (n − 1)var(x) + n¯x2\\nCov(α,β)(ˆα, ˆβ) = −¯xVar(α,β)( ˆβ)\\nThe last equality for Var(α,β)(ˆα) uses formula (2.2) for the sample variance.\\n\\nn(n − 1)var(x) (cid:19) =\\n\\nand\\n\\n¯x2\\n\\n+\\n\\nn\\n\\nσ2x2\\n\\n(n − 1)var(x)\\n\\n,\\n\\nNotice that Var(α,β)(ˆα) increases with the distance that the mean of the x values is from 0. The correlation\\n\\nρ(α,β)(ˆα, ˆβ) =\\n\\nCov(α,β)(ˆα, ˆβ)\\n\\n(cid:113)Var(α,β)(ˆα)Var(α,β)( ˆβ)\\n\\n= −¯x(cid:115) Var(α,β)( ˆβ)\\n\\nVar(α,β)(ˆα)\\n\\n= −\\n\\n,\\n\\n¯x\\n\\n(cid:112)x2\\n\\nwhich does not depend on the data. If the mean of the explanatory variable ¯x > 0, then ˆα and ˆβ are negatively\\ncorrelated. For example, if we underestimate ˆβ for β > 0, then the line is more shallow and we will likely overestimate\\nˆα.\\n\\nExercise 16.10. Explore the fact that the correlation between ˆα and ˆβ does not depend on the data.\\n\\nIf σ is known, this suggests a z-interval for a γ-level conﬁdence interval\\n\\nGenerally, σ is unknown. However, the variance of the residuals,\\n\\nˆβ ± z(1−γ)/2\\n\\nσ\\n\\nsx√n − 1\\n\\n.\\n\\ns2\\nu =\\n\\n1\\n\\nn − 2\\n\\nn(cid:88)i=1\\n(yi − (ˆα − ˆβxi))2\\n\\n(16.2)\\n\\nis an unbiased estimator of σ2 and su/σ has a t distribution with n − 2 degrees of freedom. This gives the t-interval\\n\\nˆβ ± tn−2,(1−γ)/2\\n\\nsu\\n\\nsx√n − 1\\n\\n.\\n\\nAs the formula shows, the margin of error is proportional to the standard deviation of the residuals. It is inversely\\nproportional to the standard deviation of the x measurement. Thus, we can reduce the margin of error by taking a\\nbroader set of values for the explanatory variables.\\n\\nFor the data on the humerus and femur of the ﬁve specimens of Archeopteryx, we have ˆβ = 1.197. su = 1.982,\\n\\nsx = 13.2, and t3,0.025 = 3.1824, Thus, the 95% conﬁdence interval is 1.197 ± 0.239 or (0.958, 1.436).\\n\\n288\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\n16.1.3 Sample Proportions\\nExample 16.11 (proportions). For n Bernoulli trials with success parameter p, the sample proportion ˆp has\\n\\nmean p and\\n\\nvariance\\n\\np(1 − p)\\n\\nn\\n\\n.\\n\\nThe parameter p appears in both the mean and in the variance. Thus, we need to make a choice ˜p to replace p in the\\nconﬁdence interval\\n\\nˆp ± z(1−γ)/2(cid:114) ˜p(1 − ˜p)\\n\\nn\\n\\n.\\n\\n(16.3)\\n\\nOne simple choice for ˜p is simply to take the sample proportion ˆp. Based on extensive numerical experimentation, one\\nmore recent popular choice is\\n\\n˜p =\\n\\nx + 2\\nn + 4\\n\\nwhere x is the number of successes.\\n\\nFor population proportions, we ask that the mean number of successes np and the mean number of failures\\nn(1 − p) each be at least 10. We have this requirement so that a normal random variable is a good approximation to\\nthe appropriate binomial random variable.\\n\\nExample 16.12. For Mendel’s data the F2 generation consisted 428 for the dominant allele green pods and 152 for\\nthe recessive allele yellow pods. Thus, the sample proportion of green pod alleles is\\n\\nThe conﬁdence interval, using\\n\\nˆp =\\n\\n428\\n\\n428 + 152\\n\\n= 0.7379.\\n\\n˜p =\\n\\n428 + 2\\n\\n428 + 152 + 4\\n\\n= 0.7363\\n\\nis\\n\\n0.7379 ± z(1−γ)/2(cid:114) 0.7363 · 0.2637\\n\\n580\\n\\n= 0.7379 ± 0.0183z(1−γ)/2\\n\\nFor γ = 0.98, z0.01 = 2.326 and the conﬁdence interval is 0.7379 ± 0.0426 = (0.6953, 0.7805). Note that this\\n\\ninterval contains the predicted value of p = 3/4.\\n\\nA comparable formula gives conﬁdence intervals based on more than two independent samples\\n\\nExample 16.13. For the difference in two proportions p1 and p2 based on n1 and n2 independent trials. We have, for\\nthe difference p1 − p2, the conﬁdence interval\\n\\nˆp1 − ˆp2 ±(cid:115) ˆp1(1 − ˆp1)\\n\\nn1\\n\\n+\\n\\nˆp2(1 − ˆp2)\\n\\nn2\\n\\n.\\n\\nExample 16.14 (transformation of a single parameter). If\\n\\n(ˆθ(cid:96), ˆθu)\\n\\nis a level γ conﬁdence interval for θ and g is an increasing function, then\\n\\n(g(ˆθ(cid:96)), g(ˆθu))\\n\\nis a level γ conﬁdence interval for g(θ)\\n\\nExercise 16.15. For the example above, ﬁnd the conﬁdence interval for the yellow pod genotype.\\n\\n289\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\n16.1.4 Summary of Standard Conﬁdence Intervals\\nThe conﬁdence interval is an extension of the idea of a point estimation of the parameter to an interval that is likely to\\ncontain the true parameter value. A level γ conﬁdence interval for a population parameter θ is an interval computed\\nfrom the sample data having probability γ of producing an interval containing θ.\\n\\nFor an estimate of a population mean or proportion, a level γ conﬁdence interval often has the form\\n\\n∗\\n\\nestimate ± t\\n\\n× standard error\\n\\nwhere t∗ is the upper 1−γ\\ncritical value for the t distribution with the appropriate number of degrees of freedom. If\\nthe number of degrees of freedom is inﬁnite, we use the standard normal distribution to determine the critical value,\\nusually denoted by z∗.\\n\\n2\\n\\n× standard error decreases if\\n\\nThe margin of error m = t∗\\n• γ, the conﬁdence level, decreases\\n• the standard deviation decreases\\n• n, the number of observations, increases\\nThe procedures for ﬁnding the conﬁdence interval are summarized in the table below.\\n\\nprocedure\\none sample\\n\\ntwo sample\\n\\npooled two sample\\n\\none proportion\\ntwo proportion\\nlinear regression\\n\\nparameter\\n\\nµ\\n\\nµ1 − µ2\\nµ1 − µ2\\n\\np\\n\\np1 − p2\\n\\nβ\\n\\nestimate\\n\\n¯x\\n\\n¯x1 − ¯x2\\n¯x1 − ¯x2\\n\\nˆp\\n\\nˆp1 − ˆp2\\n\\nˆβ = cov(x, y)/var(x)\\n\\nstandard error\\n\\ndegrees of freedom\\n\\ns√n\\n+ s2\\n2\\nn2\\n+ 1\\nn2\\n\\nn1\\n\\n1\\nn1\\n\\n(cid:113) s2\\nsp(cid:113) 1\\n(cid:113) ˜p(1−˜p)\\n(cid:113) ˆp1(1−ˆp1)\\n\\nn1\\n\\nn\\n\\n, ˜p = x+2\\nn+4\\n+ ˆp2(1−ˆp2)\\n\\nn2\\n\\nsu\\n\\nsx√n−1\\n\\nn − 1\\n\\nSee (16.1)\\nn1 + n2 − 2\\n\\n∞\\n∞\\nn − 2\\n\\nThe ﬁrst conﬁdence interval for µ1 − µ2 is the two-sample t procedure. If we can assume that the two samples\\nhave a common standard deviation, then we pool the data to compute sp, the pooled standard deviation. Matched pair\\nprocedures use a one sample procedure on the difference in the observed values.\\n\\nFor these intervals, we need a sample size large enough so that the central limit theorem is a sufﬁciently good\\napproximation. For one population tests for means, n > 30 and data not strongly skewed is a good rule of thumb. For\\ntwo population tests, 40 observations for each group may be necessary. For population proportions, we ask that the\\nmean number of successes np and the mean number of failures n(1 − p) each be at least 10.\\nvalues of the explanatory variable.\\n\\nFor the standard error for ˆβ in linear regression, su is deﬁned in (16.2) and sx is the standard deviation of the\\n\\nInterpretation of the Conﬁdence Interval\\n\\n16.1.5\\nThe conﬁdence interval for a parameter θ is based on two statistics - ˆθ(cid:96)(x), the lower end of the conﬁdence interval\\nand ˆθu(x), the upper end of the conﬁdence interval. As with all statistics, these two statistics cannot be based on the\\nvalue of the parameter. In addition, the formulas for these two statistics are determined in advance of having the actual\\ndata. The term conﬁdence can be related to the production of conﬁdence intervals. We can think of the situation in\\n\\n290\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nFigure 16.5: One hundred conﬁdence build from repeatedly simulating 100 standard normal random variables and constructing 95% conﬁdence\\nintervals for the mean value - 0. Note that the 24th interval is entirely below 0 and so does not contain the actual parameter. The 11th, 80th and 91st\\nintervals are entirely above 0 and again do not contain the parameter.\\n\\nwhich we produce independent conﬁdence intervals repeatedly. Each time, we may either succeed or fail to include\\nthe true parameter in the conﬁdence interval. In other words, the inclusion of the parameter value in the conﬁdence\\ninterval is a Bernoulli trial with success probability γ.\\n\\nFor example, after having seen these 100 intervals in Figure 5, we can conclude that the lowest and highest intervals\\nare much less likely than 95% of containing the true parameter value. This phenomena can be seen in the presidential\\npolls for the 2012 election. Three days before the election we see the following spread between Mr. Obama and Mr.\\nRomney\\n\\n0% -1% 0% 1% 5% 0% -5% -1% 1% 1%\\n\\nwith the 95% conﬁdence interval having a margin of error ∼ 3% based on a sample of size ∼ 1000. Because these\\nvalues are highly dependent, the values of ±5% is less likely to contain the true spread.\\nExercise 16.16. Perform the computations needed to determine the margin of error in the example above.\\n\\nThe following example, although never likely to be used in an actual problem, may shed some insight into the\\n\\ndifference between conﬁdence and probability.\\n\\n291\\n\\n0102030405060708090100−0.5−0.4−0.3−0.2−0.100.10.20.30.40.5\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nExample 16.17. Let X1 and X2 be two independent observations from a uniform distribution on the interval [θ −\\n1, θ + 1] where θ is an unknown parameter. In this case, an observation is greater than θ with probability 1/2, and less\\nthan θ with probability 1/2. Thus,\\n\\n• with probability 1/4, both observations are above θ,\\n• with probability 1/4, both observations are below θ, and\\n• with probability 1/2, one observation is below θ and the other is above.\\n\\nIn the third case alone, the conﬁdence interval contains the parameter value. As a consequence of these considerations,\\nthe interval\\n\\n(ˆθ(cid:96)(X1, X2), ˆθu(X1, X2)) = (min{X1, X2}, max{X1, X2})\\n\\nis a 50% conﬁdence interval for the parameter.\\n\\nSometimes, max{X1, X2} − min{X1, X2} > 1. Because any subinterval of the interval [θ − 1, θ + 1] that has\\nlength at least 1 must contain θ, the midpoint of the interval, this conﬁdence interval must contain the parameter value.\\nIn other words, sometimes the 50% conﬁdence interval is certain to contain the parameter.\\n\\nExercise 16.18. For the example above, show that\\n\\nP{conﬁdence interval has length > 1} = 1/4.\\n\\nHint: Draw the square [θ − 1, θ + 1] × [θ − 1, θ + 1] and shade the region in which the conﬁdence interval has length\\ngreater than 1.\\n\\n16.1.6 Extensions on the Use of Conﬁdence Intervals\\nExample 16.19 (delta method). For estimating the distribution µ by the sample mean ¯X, the delta method provides\\nan alternative for the example above. In this case, the standard deviation of g( ¯X) is approximately\\n\\n|g(cid:48)(µ)|σ\\n√n\\n\\n.\\n\\n(16.4)\\n\\nWe replace µ with ¯X to obtain the conﬁdence interval for g(µ)\\n\\ng( ¯X) ± zα/2|g(cid:48)( ¯X)|σ\\n√n\\n\\n.\\n\\nUsing the notation for the example of estimating α3, the coefﬁcient of volume expansion based on independent\\n\\nlength measurements, Y1, Y2, . . . , Yn measured at temperature T1 of an object having length (cid:96)0 at temperature T0.\\n\\nExercise 16.20. Recall that the odds of an event having probability p is\\n\\n¯Y 3 − (cid:96)3\\n0|T1 − T0| ± z(1−γ)/2\\n(cid:96)3\\n\\n0\\n\\n3 ¯Y 2σY\\n\\nn\\n\\nUse the delta method to show that\\n\\nψ =\\n\\np\\n1 − p\\n\\n.\\n\\nσ2\\nˆψ ≈\\n\\n.\\n\\nψ(ψ + 1)2\\n\\nn\\n\\n292\\n\\n(16.5)\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nIn the example above on green peas,\\n\\nˆp\\n1 − ˆp\\nUsing (16.4), we obtain a 98% conﬁdence interval\\n\\nˆψ =\\n\\n=\\n\\n0.7379\\n1 − 0.7379\\n\\n= 2.8153.\\n\\nˆψ ± z0.01(cid:113)σ2\\n\\nˆψ ≈ 2.8153 ± 2.326(cid:114) 2.8153(1 + 2.8153)2\\n\\n580\\n\\n= (2.1970, 3.4337)\\n\\nwhich includes the predicted value ψ = 3.\\n\\nIf we transform the 98% conﬁdence interval (0.6953, 0.7805) for p to a conﬁdence interval for the odds ψ using\\nthe transformation (16.5), we obtain an interval (2.2819, 3.5558) that is slightly shifted upward from the conﬁdence\\ninterval obtained by the delta method.\\n\\nFor multiple independent samples, the simple idea using the transformation in the Example 12 no longer works.\\nFor example, to determine the conﬁdence interval using ¯X1 and ¯X2 above, the conﬁdence interval for g(µ1, µ2), the\\ndelta method gives the conﬁdence interval\\n\\ng( ¯X1, ¯X2) ± z(1−γ)/2(cid:115)(cid:18) ∂\\n\\n∂x\\n\\ng( ¯X1, ¯X2)(cid:19)2 σ2\\n\\n1\\nn1\\n\\n+(cid:18) ∂\\n\\n∂y\\n\\ng( ¯X1, ¯X2)(cid:19)2 σ2\\n\\n2\\nn2\\n\\n.\\n\\nExample 16.21. Let’s return to the example of n(cid:96) and nh measurements x and y of, respectively, the length (cid:96) and the\\nheight h of a right triangle with the goal of giving the angle\\n\\nθ = g((cid:96), h) = tan\\n\\n(cid:96)(cid:19)\\n−1(cid:18) h\\n\\nbetween these two sides. Here are the measurements:\\n\\n> x\\n\\n[1] 10.224484 10.061800 9.945213\\n[9]\\n\\n9.737811 9.956345\\n\\n> y\\n\\n9.982061\\n\\n9.961353 10.173944\\n\\n9.952279 9.855147\\n\\n[1] 4.989871 5.090002 5.021615 4.864633 5.024388 5.123419 5.033074 4.750892 4.985719\\n\\n[10] 4.912719 5.027048 5.055755\\n> mean(x);sd(x)\\n[1] 9.985044\\n[1] 0.1417969\\n> mean(y);sd(y)\\n[1] 4.989928\\n[1] 0.1028745\\n\\nThe angle θ is the arctangent, here estimated using the mean and given in radians\\n\\n>(thetahat<-atan(mean(y)/mean(x)))\\n[1] 0.4634398\\n\\nUsing the delta method, we have estimated the standard deviation of these measurements.\\n\\nh2 + (cid:96)2(cid:115)h2 σ2\\n\\n(cid:96)\\nn(cid:96)\\n\\n1\\n\\nσˆθ ≈\\n\\n+ (cid:96)2 σ2\\nh\\nnh\\n\\n.\\n\\nWe estimate this with the sample means and standard deviations\\n\\n¯y2 + ¯x2(cid:115)¯y2 s2\\n\\nx\\nn(cid:96)\\n\\n1\\n\\nsˆθ ≈\\n\\n+ ¯x2\\n\\ns2\\ny\\nnh\\n\\n= 0.0030.\\n\\n293\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nThis gives a γ level z- conﬁdence interval\\n\\nˆθ ± z(1−γ)/2sˆθ.\\n\\nFor a 95% conﬁdence interval, this is 0.4634 ± 0.0059 = (0.4575, 0.4693) radians or (26.22◦, 26.89◦).\\n\\nWe can extend the Welch and Satterthwaite method to include the delta method to create a t-interval with effective\\n\\ndegrees of freedom\\n\\n(cid:16) ∂g(¯x,¯y)2\\n\\ns4\\n1\\n\\n∂x\\n\\ns2\\n1\\nn1\\n\\n+ ∂g(¯x,¯y)2\\n1·(n1−1) + ∂g(¯x,¯y)4\\n\\n∂y\\n\\n∂y\\n\\nn2\\n\\ns2\\n2\\n\\nn2(cid:17)2\\n\\ns4\\n2\\n\\n2·(n2−1)\\nn2\\n\\n.\\n\\n∂g(¯x,¯y)4\\n\\n∂x\\n\\nν =\\n\\nWe compute to ﬁnd that ν = 19.4 and then use the t-interval\\n\\nˆθ ± tν,(1−γ)/2sˆθ.\\n\\nFor a 95% conﬁdence, this is sightly larger interval 0.4634± 0.0063 = (0.4571, 0.4697) radians or (26.19◦, 26.91◦).\\nExample 16.22 (maximum likelihood estimation). The Fisher information is the main tool used to set an conﬁdence\\ninterval for a maximum likelihood estimation .Two choices are typical. Let ˆθ be the maximum likelihood estimate for\\nthe parameter θ.\\n\\nFirst, we can use the Fisher information I(θ) and recall that ˆθ is approximately normally distributed, mean θ,\\n\\nstandard deviation 1/(cid:112)nI(θ). Replacing θ by its estimate gives a conﬁdence interval\\n\\nMore recently, the more popular method is to use the observed information based on the observations x =\\n\\n(x1, x2, . . . , xn).\\n\\nJ(θ) = −\\n\\n∂2\\n∂θ2 log L(θ|x) = −\\n\\n∂2\\n∂θ2 log fX (xi|θ).\\n\\nn(cid:88)i=1\\n\\nThis is the second derivative of the score function evaluated at the maximum likelihood estimator. Then, the conﬁdence\\ninterval is\\n\\nˆθ ± zα/2\\n\\n1\\n\\n.\\n\\n(cid:113)nI(ˆθ)\\n\\nˆθ ± zα/2\\n\\n1\\n\\n.\\n\\n(cid:113)J(ˆθ)\\n\\nTo compare the two approaches, ﬁrst note that EθJ(θ) = nI(θ), the Fisher information for n observations. Thus,\\n\\nby the law of large numbers,\\n\\nIf the estimator is consistent and I is continuous at θ, then\\n\\n1\\nn\\n\\nJ(θ) → I(θ) as n → ∞.\\n\\n1\\nn\\n\\nJ(ˆθ) → I(θ) as n → ∞.\\n\\n16.2 The Bootstrap\\nThe conﬁdence regions have been determined using aspects of the distribution of the data. In particular, these regions\\nhave often been speciﬁed by appealing to the central limit theorem and normal approximations. The notion behind\\nbootstrap techniques begins with the concession that the information about the source of the data is insufﬁcient to\\nperform the analysis to produce the necessary description of the distribution of the estimator. This is particularly true\\nfor small data sets or highly skewed data.\\n\\nThe strategy is to take the data and treat it as if it were the distribution underlaying the data and to use a resampling\\nprotocol to describe the estimator. For the example above, we estimated the angle in a right triangle by estimating (cid:96)\\n\\n294\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nFigure 16.6: Bootstrap distribution of ˆθ.\\n\\nand h, the lengths two adjacent sides by taking the mean of our measurements and then computing the arctangent of\\nthe ratio of these means. Using the delta method, our conﬁdence interval was based on a normal approximation of the\\nestimator.\\n\\nThe bootstrap takes another approach. We take the data\\n\\nx1, x2, . . . , xn1,\\n\\ny1, y2, . . . , yn2,\\n\\nthe empirical distribution of the measurements of (cid:96) and h and act as if it were the actual distribution. The next step\\nis the use the data and randomly create the results of the experiment many times over. In the example, we choose, with\\nreplacement n1 measurements from the x data and n2 measurements from the y data. We then compute the bootstrap\\nmeans\\n\\nand the estimate\\n\\n¯xb\\n\\nand\\n\\n¯yb\\n\\nˆθ(¯xb, ¯yb) = tan\\n\\n−1(cid:18) ¯yb\\n¯xb(cid:19) .\\n\\nRepeating this many times gives an empirical distribution for the estimator ˆθ. This can be accomplish in just a couple\\nlines of R code.\\n\\n> angle<-numeric10000)\\n> for (i in 1:10000){xb<-sample(x,length(x),replace=TRUE);\\n\\nyb<-sample(y,length(y),replace=TRUE);angle[i]<-atan(mean(yb)/mean(xb))*180/pi}\\n\\n> hist(angle)\\n\\nWe can use this bootstrap distribution of ˆθ to construct a conﬁdence interval.\\n\\n> q<-c(0.005,0.01,0.025,0.5,0.975,0.99,0.995)\\n> quantile(angle,q)\\n\\n0.5%\\n\\n99.5%\\n26.09837 26.14807 26.21860 26.55387 26.86203 26.91486 26.95065\\n\\n97.5%\\n\\n2.5%\\n\\n50%\\n\\n99%\\n\\n1%\\n\\n295\\n\\nHistogram of angleangleFrequency26.626.827.027.2050010001500\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nFigure 16.7: Bayesian credible interval. The 95% credible interval based on a Beta(9, 3) prior distribution and data consisting of 8 heads and 6\\ntails. This interval has probability 0.025 both above and below the end of the interval. Because the density is higher for the upper value, an narrow\\ncredible interval can be obtained by shifting the values upward so that the densities are equal at the endpoints of the interval and (16.6) is satisﬁed.\\n\\nA 95% conﬁdence interval (26.21◦, 26.86◦) can be accomplished using the 2.5th percentile as the lower end point\\nand the 97.5th percentile as the upper end point. This conﬁdence interval is very similar to the one obtained using the\\ndelta method.\\n\\nExercise 16.23. Give the 98% bootstrap conﬁdence interval for the angle in the example above.\\n\\nExercise 16.24. Bootstrap conﬁdences are based on a simulation. So, the answer will vary with each simulation.\\nRepeat the bootstrap above and compare.\\n\\n16.3 Bayesian Statistics\\nA Bayesian interval estimate is called a credible interval. Recall that for the Bayesian approach to statistics, both\\nthe data and the parameter are random Thus, the interval estimate is a statement about the posterior probability of the\\nparameter θ.\\n\\n(16.6)\\nHere ˜Θ is the random variable having a distribution equal to the prior probability π. We have choices in deﬁning\\n\\nP{ ˜Θ ∈ C(X)|X = x} = γ.\\n\\nthis interval. For example, we can\\n\\n• choose the narrowest interval, which involves choosing those values of highest posterior density.\\n• choosing the interval in which the probability of being below the interval is as likely as being above it.\\nWe can look at this by examining the two examples given in the Introduction to Estimation.\\n\\nExample 16.25 (coin tosses). In this example, we began with a beta prior distribution. Consequently, the posterior\\ndistribution will also be a member of the beta family of distributions. We then ﬂipped a coin n = 14 times with 8\\nheads. Here, again, is the summary.\\n\\nprior\\nα β mean\\n1/2\\n6\\n9\\n3/4\\n1/4\\n3\\n\\n6\\n3\\n9\\n\\ndata\\n\\nvariance\\n\\nheads\\n\\n1/52\\n3/208\\n3/208\\n\\n8\\n8\\n8\\n\\n296\\n\\nposterior\\n\\nβ mean\\n7/13\\n12\\n9\\n17/26\\n11/26\\n15\\n\\nα\\n14\\n17\\n11\\n\\ntails\\n6\\n6\\n6\\n\\n0.00.20.40.60.81.001234x0.00.20.40.60.81.001234x0.00.20.40.60.81.0012340.00.20.40.60.81.0012340.00.20.40.60.81.001234\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nWe use the R command qbeta to ﬁnd the credible interval. For the second case in the table above and with\\n\\nγ = 0.95, we ﬁnd values that give the lower and upper 2.5% of the posterior distribution.\\n\\n> qbeta(0.025,17,9)\\n[1] 0.4649993\\n> qbeta(0.975,17,9)\\n[1] 0.8202832\\n\\nThis gives a 95% credible interval of (0.4650, 0.8203). This is indicated in the ﬁgure above by the two vertical\\n\\nlines. Thus, the area under the density function from the vertical lines outward totals 5%.\\n\\nThe narrowest credible interval is (0.4737, 0.8276). At these values, the density equals 0.695. The density is lower\\nfor more extreme values and higher between these values. The beta distribution has a probability 0.0306 below the\\nlower value for the credible interval and 0.0194 above the upper value satisfying the criterion (16.6) with γ = 0.95.\\n\\nExample 16.26. For the example having both a normal prior distribution and normal data, we ﬁnd that we also have\\na normal posterior distribution. In particular, if the prior is normal, mean θ0, variance 1/λ and our data has sample\\nmean ¯x and each observation has variance 1.\\nThe classical statistics conﬁdence interval\\n\\n¯x ± zα/2\\n\\n1\\n√n\\n\\n.\\n\\nFor the Bayesian credible interval, note that the posterior distribution in normal with mean\\n\\nand variance 1/(n + λ). Thus the credible interval is\\n\\nθ1(x) =\\n\\nλ\\n\\nλ + n\\n\\nθ0 +\\n\\nn\\n\\nλ + n\\n\\n¯x.\\n\\nThus, the center of the interval is inﬂuenced by the prior mean. The prior variance results in a narrower interval.\\n\\nθ1(x) ± zα/2\\n\\n1\\n\\n√λ + n\\n\\n.\\n\\n16.4 Answers to Selected Exercises\\n16.4. Using R to ﬁnd upper tail probabilities, we ﬁnd that\\n\\n> qt(0.95,99)\\n[1] 1.660391\\n> qt(0.99,99)\\n[1] 2.364606\\n\\nFor the 90% conﬁdence interval\\n\\n299, 852.4 ± 1.6604\\nFor the 98% conﬁdence interval\\n\\n79.0\\n√100\\n\\n= 299852.4 ± 13.1\\n\\nor the interval\\n\\n(299839.3, 299865.5).\\n\\n299, 852.4 ± 2.3646\\n\\n79.0\\n√100\\n\\n= 299852.4 ± 18.7\\n\\nor the interval\\n\\n(299833.7, 299871.1).\\n\\n16.8. Let\\n\\nc =\\n\\ns2\\n1/n1\\ns2\\n2/n2\\n\\n. Then,\\n\\ns2\\n2\\nn2\\n\\n= c\\n\\ns2\\n1\\nn1\\n\\n.\\n\\n297\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nThen, substitute for s2\\n\\n2/n2 and divide by s2\\n\\n1/n1 to obtain\\n\\n(cid:16) s2\\n\\n1\\nn1\\n\\ns4\\n1\\n\\n+ s2\\n\\n2\\n\\nn2(cid:17)2\\n\\ns4\\n2\\n\\n1·(n1−1) +\\nn2\\n\\n2·(n2−1)\\nn2\\n\\n(cid:16) s2\\n\\n1\\nn1\\n\\n+ cs2\\n\\n1\\n\\nn1(cid:17)2\\n\\ns4\\n1\\n\\n1·(n1−1) + c2s4\\n\\n1·(n2−1)\\nn2\\n\\n1\\n\\nn2\\n\\n=\\n\\n=\\n\\n(1 + c)2\\nn1−1 + c2\\n1\\nn2−1\\n\\n=\\n\\n(n1 − 1)(n2 − 1)(1 + c)2\\n(n2 − 1) + (n1 − 1)c2\\n\\n.\\n\\nν =\\n\\nTake a derivative to see that\\n\\ndν\\ndc\\n\\n= (n1 − 1)(n2 − 1)\\n\\n((n2 − 1) + (n1 − 1)c2) · 2(1 + c) − (1 + c)2 · 2(n1 − 1)c\\n\\n((n2 − 1) + (n1 − 1)c2)2\\n\\n((n2 − 1) + (n1 − 1)c2) − (1 + c)(n1 − 1)c\\n\\n((n2 − 1) + (n1 − 1)c2)2\\n\\n= 2(n1 − 1)(n2 − 1)(1 + c)\\n\\n= 2(n1 − 1)(n2 − 1)(1 + c)\\n\\n(n2 − 1) − (n1 − 1)c\\n((n2 − 1) + (n1 − 1)c2)2\\n\\nSo the maximum takes place at c = (n2 − 1)/(n1 − 1) with value of ν.\\n\\nν =\\n\\n=\\n\\n=\\n\\n(n1 − 1)(n2 − 1)(1 + (n2 − 1)/(n1 − 1))2\\n(n2 − 1) + (n1 − 1)((n2 − 1)/(n1 − 1))2\\n(n1 − 1)(n2 − 1)((n1 − 1) + (n2 − 1))2\\n(n1 − 1)2(n2 − 1) + (n1 − 1)(n2 − 1)2\\n((n1 − 1) + (n2 − 1))2\\n= n1 + n2 − 2.\\n(n1 − 1) + (n2 − 1)\\n\\ns2\\n1\\ns2\\n2\\n\\n=\\n\\nn1\\nn2\\n\\nc =\\n\\nn1/(n1 − 1)\\nn2/(n2 − 1)\\n\\nNote that for this value\\n\\nand the variances are nearly equal. Notice that this is a global maximum with\\n\\nν → n1 − 1 as c → 0 and s1 (cid:28) s2\\nThe smaller of these two limits is the global minimum.\\n16.9. Recall that ˆβ is an unbiased estimator for β, thus E(α,β)\\n\\nand ν → n2 − 1 as c → ∞ and s2 (cid:28) s1.\\n\\nˆβ = β, and E(α,β)[( ˆβ − β)2] is the variance of ˆβ.\\n\\nˆβ(x, y) − β =\\n\\n=\\n\\n=\\n\\n=\\n\\n1\\n\\n1\\n\\n(n − 1)var(x)(cid:32) n(cid:88)i=1\\n(n − 1)var(x)(cid:32) n(cid:88)i=1\\n(n − 1)var(x)(cid:32) n(cid:88)i=1\\n(n − 1)var(x)(cid:32) n(cid:88)i=1\\n\\n1\\n\\n1\\n\\n(xi − ¯x)(xi − ¯x)(cid:33)\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)(yi − ¯y) − β\\n\\n(xi − ¯x)(yi − ¯y − β(xi − ¯x))(cid:33)\\n(xi − ¯x)((yi − βxi) − (¯y − β ¯x))(cid:33)\\n\\n(xi − ¯x)(yi − βxi) −\\n\\n(xi − ¯x)(¯y − β ¯x)(cid:33)\\n\\nn(cid:88)i=1\\n\\nThe second sum is 0. For the ﬁrst, we use the fact that yi − βxi = α + \\x01i. Thus,\\nVar(α,β)( ˆβ) = Var(α,β)(cid:32)\\n\\n1\\n\\n1\\n\\n(n − 1)var(x)\\n\\n(n − 1)2var(x)2\\n\\n(xi − ¯x)(α + \\x01i)(cid:33) =\\nn(cid:88)i=1\\n(n − 1)var(x)\\n\\nσ2\\n\\n(xi − ¯x)2σ2 =\\n\\n=\\n\\n1\\n\\n(n − 1)2var(x)2\\n\\nn(cid:88)i=1\\n\\n298\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2Var(α,β)(α + \\x01i)\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\nBecause the \\x01i are independent, we can use the Pythagorean identity that the variance of the sum is the sum of the\\n\\nvariances.\\n\\nSimilarly, ˆα is an unbiased estimator for α. Because ˆα = ¯y − ˆβ ¯x, we have by the law of cosines,\\n\\nVar(α,β)(ˆα) = Var(α,β)(¯y) + Var(α,β)( ˆβ ¯x) − 2Cov(α,β)(¯y, ˆβ ¯x)\\n= Var(α,β)(¯y) + ¯x2Var(α,β)( ˆβ) − 2¯xCov(α,β)(¯y, ˆβ)\\n\\nFor the ﬁrst term, note that Var(yi) = Var(\\x01n).\\n\\nVar(α,β)(¯y) = Var(α,β)(cid:32) 1\\n\\nn\\n\\nyi(cid:33) =\\n\\nn(cid:88)i=1\\n\\n1\\n\\nn2 Var(α,β)(cid:32) n(cid:88)i=1\\n\\n\\x01i(cid:33) =\\n\\n1\\nn2 nσ2 =\\n\\nσ2\\nn\\n\\n.\\n\\nFor the third term, note that\\n\\nand thus Cov(α,β)(yi, ¯y) =(cid:80)n\\n\\nthat\\n\\nCov(α,β)(yi, yj) =(cid:26) 0\\n\\nσ2\\n\\nif i (cid:54)= j,\\nif i = j.\\n\\nj=1 Cov(α,β)(yi, yj)/n = σ2/n. Using the binlinear properties of covariance, we ﬁnd\\n\\nCov(α,β)(yi, cov(x, y))\\n\\nCov(α,β)(yi, (xj − ¯x)(yj − ¯y))\\n\\nCov(α,β)(¯y, ˆβ) =\\n\\n=\\n\\n=\\n\\n=\\n\\n1\\n\\nnvar(x)\\n\\n1\\n\\nnvar(x)\\n\\n1\\n\\nnvar(x)\\n\\n1\\n\\nnvar(x)\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\nn(cid:88)i=1\\nn(cid:88)j=1\\n\\nn(cid:88)j=1\\nn(cid:88)j=1\\n\\n(xj − ¯x)(Cov(α,β)(yi, yj) − Cov(α,β)(yi, ¯y))\\n\\n(xj − ¯x)\\n\\nσ2(cid:18)1 −\\n\\n1\\n\\nn(cid:19) = 0\\n\\nn(cid:88)i=1\\n\\n(16.7)\\n\\nbecause(cid:80)n\\n\\nj=1(xj − ¯x) = 0. Combining the results, we obtain\\nVar(α,β)(ˆα) = σ2(cid:18) 1\\n\\n+\\n\\nn\\n\\n¯x2\\n\\n(n − 1)var(x)(cid:19) .\\n\\nFinally, we use ˆα = ¯y − ˆβ ¯x and (16.7) again.\\n\\nCov(α,β)(ˆα, ˆβ) = Cov(α,β)(¯y, ˆβ) − ¯xCov(α,β)( ˆβ, ˆβ) = 0 − ¯xVar(α,β)( ˆβ).\\n\\n16.10. We take x = 1, 2, . . . , 10. Then, the correlation of ˆα and ˆβ is\\n-0.8864053.\\n\\n> x<-1:10\\n> -mean(x)/sqrt(mean(xˆ2))\\n[1] -0.8864053\\n\\nWe make 100 different choices of intercept a and slope b uniformly be-\\ntween −2 and 2. The noise term has standard deviation 0.2.\\n299\\n\\nHistogram of rrFrequency-0.90-0.89-0.88-0.87-0.860510152025\\x0cIntroduction to the Science of Statistics\\n\\nInterval Estimation\\n\\n> a<-runif(100,-2,2); b<-runif(100,-2,2)\\n> ahat<-numeric(1000); bhat<-numeric(1000)\\n> r<-numeric(100)\\n> for (i in 1:100){for (j in 1:1000){y<-a[i]+b[i]*x+rnorm(10,0,0.2);\\nc<-lm(y˜x)$coef;ahat[j]<-c[1];bhat[j]<-c[2]};r[i]<-cor(ahat,bhat)}\\n\\n> mean(r)\\n[1] -0.8865578\\n> sd(r)\\n[1] 0.007148639\\n> hist(r)\\n\\nSo, the simulated correlations are all very close to the distributional values.\\n\\n16.11. For\\n\\nψ = g(p) =\\n\\np\\n1 − p\\nBy the delta method,\\n\\nwe have that p =\\n\\nψ\\n\\nψ + 1\\n\\nand\\n\\n(cid:48)\\n\\ng\\n\\n(p) =\\n\\n1\\n\\n(1 − p)2 .\\n\\nσ2\\nˆψ ≈ g\\n\\n(cid:48)\\n\\n(p)2 σ2\\np\\nn\\n\\n=\\n\\n1\\n\\n(1 − p)4\\n\\np(1 − p)\\n\\nn\\n\\n=\\n\\np\\n\\nn(1 − p)3 =\\n\\nψ(ψ + 1)2\\n\\nn\\n\\n.\\n\\n16.16. The conﬁdence interval for the proportion yellow pod genes 1 − p is (0.2195, 0.3047). The proportion of\\nyellow pod phenotype is (1 − p)2 and a 95% conﬁdence interval has as its endpoints the square of these numbers -\\n(0.0482, 0.0928).\\n16.17. The critical value z0.025 = 1.96. For ˆp = 0.468 and n = 1500, the number of successes is x = 702. The\\nmargin of error is\\n\\nn\\n\\n= 0.025.\\n\\nz0.025(cid:114) ˆp(1 − ˆp)\\n16.19. On the left is the square [θ − 1, θ + 1] × [θ − 1, θ + 1].\\nFor the random variables X1, X2, because they are independent and\\nuniformly distributed over a square of area 4, their joint density is 1/4\\non this square. The two diagonal line segments are the graph of |x1−\\nx2| = 1. In the shaded area, the region |x1−x2| > 1, is precisely the\\nregion in which max{x1, x2} − min{x1, x2} > 1. Thus, for these\\nvalues of the random variables, the conﬁdence interval has length\\ngreater than 1. The area of each of the shaded triangles is 1/2 · 1 ·\\n1 = 1/2. Thus, the total area of the two triangles, 1, represents a\\nprobability of 1/4.\\n16.23. A 98% conﬁdence interval (26.14◦, 26.91◦) can be accom-\\nplished using the 1st percentile as the lower end point and the 99th\\npercentile as the upper end point.\\n16.24. Here is the output of a second simulation.\\n\\n> q<-c(0.005,0.01,0.025,0.5,0.975,0.99,0.995)\\n> quantile(angle,q)\\n\\n0.5%\\n\\n99.5%\\n26.12021 26.16463 26.22423 26.55800 26.85488 26.90665 26.94847\\n\\n97.5%\\n\\n2.5%\\n\\n50%\\n\\n99%\\n\\n1%\\n\\nAll of the values of within 0.04 of those from the ﬁrst bootstrap.\\n\\n300\\n\\n−1−0.8−0.6−0.4−0.200.20.40.60.81−1−0.500.51\\x0cPart IV\\n\\nHypothesis Testing\\n\\n301\\n\\n\\x0c\\x0cTopic 17\\n\\nSimple Hypotheses\\n\\nI can point to the particular moment when I understood how to formulate the undogmatic problem of\\nthe most powerful test of a simple statistical hypothesis against a ﬁxed simple alternative. At the present\\ntime, the problem appears entirely trivial and within reach of a beginning undergraduate. But, with a\\ndegree of embarrassment, I must confess that it took something like half a decade of combined effort of\\nE.S.P. and myself to put things straight. - Jerzy Neymann in the Festschrift in honor of Herman Wold,\\n1970, E.S.P is Egon Sharpe Pearson\\n\\n17.1 Overview and Terminology\\nStatistical hypothesis testing is designed to address the question: Do the data provide sufﬁcient evidence to conclude\\nthat we must depart from our original assumption concerning the state of nature?\\n\\nThe logic of hypothesis testing is similar to the one a juror faces in a criminal trial: Is the evidence provided by the\\nprosecutor sufﬁcient for the jury to depart from its original assumption that the defendant is not guilty of the charges\\nbrought before the court?\\n\\nTwo of the jury’s possible actions are\\n• Find the defendant guilty.\\n• Find the defendant not guilty.\\nThe weight of evidence that is necessary to ﬁnd the defendant guilty depends on the type of trial. In a criminal\\ntrial the stated standard is that the prosecution must prove that the defendant is guilty beyond any reasonable doubt.\\nIn civil trials, the burden of proof may be the intermediate level of clear and convincing evidence or the lower level of\\nthe preponderance of evidence.\\n\\nGiven the level of evidence needed, a prosecutors task is to present the evidence in the most powerful and convinc-\\n\\ning manner possible. We shall see these notions reﬂected in the nature of hypothesis testing.\\n\\nThe simplest set-up for understanding the issues of statistical hypothesis, is the case of two values θ0, and θ1 in\\n\\nthe parameter space. We write the test, known as a simple hypothesis as\\n\\nH0 : θ = θ0\\n\\nversus H1 : θ = θ1.\\n\\nH0 is called the null hypothesis. H1 is called the alternative hypothesis.\\n\\nWe now frame the issue of hypothesis testing using the classical approach. In this approach, the possible actions\\n\\nare:\\n\\n• Reject the hypothesis. Rejecting the hypothesis when it is true is called a type I error or a false positive. Its\\nprobability α is called the size of the test or the signiﬁcance level. Sometimes, 1 − α, the true negatiive is\\n\\n303\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\ncalled the speciﬁcity. In symbols, we write\\n\\nα = Pθ0{reject H0}.\\n\\n• Fail to reject the hypothesis. Failing to reject the hypothesis when it is false is called a type II error or a false\\nnegative, has probability β. The power of the test, 1 − β, the probability of rejecting the test when it is indeed\\nfalse, is also called the true positive fraction or the the sensitivity. In symbols, we write\\n\\nβ = Pθ1{fail to reject H0}\\nhypothesis tests\\n\\nand\\n\\n1 − β = Pθ1{reject H0}.\\ncriminal trials\\n\\nH0 is true\\ntype I error\\n\\nH1 is true\\n\\nOK\\n\\nconvict\\n\\ninnocent\\n\\nthe defendant is\\nguilty\\nOK\\n\\nOK\\n\\ntype II error\\n\\ndo not convict\\n\\nOK\\n\\nreject H0\\n\\nfail to reject H0\\n\\nThus, the higher level necessary to secure conviction in a criminal trial corresponds to having lower signiﬁcance\\nlevels. This analogy should not be taken too far. The nature of the data and the decision making process is quite\\ndissimilar. For example, the prosecutor and the defense attorney are not always out to ﬁnd the most honest manner\\nto present information. In statistical inference for hypothesis testing, the goal is something that all participants in this\\nendeavor ought to share.\\n\\nIn addition, care should be taken not to be overly invested in a ﬁxed value α for the signiﬁcance level. As we con-\\ntinue to investigate the logic and methodology behind hypothesis testing, we will broaden and make more sophisticated\\nour approach to evaluating hypotheses.\\n\\nThe decision for the test is often based on ﬁrst determining a critical region C. Data x in this region is determined\\n\\nto be too unlikely to have occurred when the null hypothesis is true. Thus, the decision is\\n\\nGiven a choice α for the size of the test, the choice of a critical region C is called best or most powerful if for any\\n\\nother choice of critical region C∗ for a size α test, i.e., both critical region lead to the same type I error probability,\\n\\nreject H0\\n\\nif and only if x ∈ C.\\n\\nbut perhaps different type II error probabiities\\n\\nα = Pθ0{X ∈ C} = Pθ0{X ∈ C\\n\\n∗\\n\\n},\\n\\nβ = Pθ1{X /∈ C},\\n\\n= Pθ1{X /∈ C\\n\\n},\\n\\n∗\\n\\nβ\\n\\n∗\\n\\nwe have the lowest probability of a type II error, (β ≤ β∗) associated to the critical region C.\\nThe two approaches to hypothesis testing, classical and Bayesian, begin with distinct starting points and end with\\ndifferent interpretations for implications of the data. Interestingly, both approaches result in a decision that is based on\\nthe values of a likelihood ratio. In the classical approach, we shall learn, based on the Neyman-Pearson lemma, that\\nthe decision is based on a level for this ratio based on setting the type I error probabilities. In the Bayesian approach,\\nthe decision on minimizing risk, a concept that we will soon deﬁne precisely.\\n\\n17.2 The Neyman-Pearson Lemma\\nMany critical regions are either determined by the consequences of the Neyman-Pearson lemma or by using analogies\\nof this fundamental lemma. Rather than presenting a proof of this lemma, we will provide some intuition for the choice\\nof critical region through the following “game”.\\n\\nWe will conduct a single observation X that can take values from −11 to 11 and based on that observation, decide\\nwhether or not to reject the null hypothesis. Basing a decision on a single observation, of course, is not the usual\\n\\n304\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\ncircumstance for hypothesis testing. We will ﬁrst continue on this line of reasoning to articulate the logic behind the\\nNeyman-Pearson lemma before examining more typical and reasonable data collection protocols.\\n\\nTo begin the game, corresponding to values for x running from −11 to 11, write a row of the number from 0 up\\nto 10 and back down to 0 and add an additional 0 at each end. These numbers add to give 100. Now, scramble the\\nnumbers and write them under the ﬁrst row. This can be created and displayed quickly in R using the commands:\\n\\n> x<- -11:11\\n> L1<-c(0,0:10,9:0,0)\\n> L0<-sample(L0) #This provides a random perturbation of the values in L1.\\n> data.frame(x,L1,L0)\\n\\nThe top row, giving the values of L1, represents the likelihood for one observation under the alternative hypothesis.\\nThe bottom row, giving the values of L0, represents the likelihood under the null hypothesis. Note that the values for\\nL0 is a rearrangement of the values for L1. Here is the output.\\n\\nx\\n\\nL1(x)\\nL0(x)\\n\\n-11\\n0\\n3\\n\\n-10\\n0\\n8\\n\\n-9\\n1\\n7\\n\\n-8\\n2\\n5\\n\\n-7\\n3\\n7\\n\\n-6\\n4\\n1\\n\\n-5\\n5\\n3\\n\\n-4\\n6\\n10\\n\\n-3\\n7\\n6\\n\\n-2\\n8\\n0\\n\\n-1\\n9\\n6\\n\\n0\\n10\\n4\\n\\n1\\n9\\n2\\n\\n2\\n8\\n5\\n\\n3\\n7\\n0\\n\\n4\\n6\\n1\\n\\n5\\n5\\n0\\n\\n6\\n4\\n4\\n\\n7\\n3\\n0\\n\\n8\\n2\\n8\\n\\n9\\n1\\n2\\n\\n10\\n0\\n9\\n\\n11\\n0\\n9\\n\\nThe goal is to pick values x so that the accumulated points (the beneﬁt) increase as quickly as possible from the\\nlikelihood L1 keeping points (the cost) from L0 as low as possible. The natural start is to pick values of x so that\\nL0(x) = 0. Then, the beneﬁt begins to add up without any cost. We ﬁnd four such values for x and record their values\\nalong with running totals for L1 and L0.\\n\\nx\\n\\nL1 total\\nL0 total\\n\\n-2\\n8\\n0\\n\\n3\\n15\\n0\\n\\n5\\n20\\n0\\n\\n7\\n23\\n0\\n\\nBeing ahead by a score of 23 to 0 can be translated into a best critical region C in the following way. If we take\\n\\nC = {−2, 3, 5, 7}, then, because the L1-total is 23 points out of a possible 100, we ﬁnd the power of the test\\n\\nand the type II error β = P1{X /∈ C} = 0.77. Because the L0-total is 0 points, the size of the test,\\n\\n1 − β = P1{X ∈ C} = 0.23\\n\\nand there is no chance of type I error with this critical region.\\nUnderstanding the next choice is crucial. Candidates are\\n\\nα = P0{X ∈ C} = 0\\n\\nx = 4, with L1(4) = 6 against L0(4) = 1 and x = 1, with L1(1) = 9 against L0(1) = 2.\\n\\nThe choice 6 against 1 is better than 9 against 2. One way to see this is to note that choosing 6 against 1 twice will\\nput us in a better place than the single choice of 9 against 2. Indeed, after choosing 6 against 1, a choice of 3 against\\n1 puts us in at least as good a position than the single choice of 9 against 2. The central point is that the best choice\\ncomes to picking the remaining value for x that has the highest beneﬁt-to-cost ratio of L1(x) to L0(x)\\n\\nNow we can pick the next few candidates, keeping track of both the type I and type II error of the test with the\\n\\nchoice of critical region being the chosen values of x.\\n\\nx\\n\\n-2\\n\\n3\\nL1(x)/L0(x) ∞ ∞\\n15\\n0\\n\\nL1 total\\nL0 total\\n\\n8\\n0\\n\\nβ\\nα\\n\\n0.92\\n0.00\\n\\n0.85\\n0.00\\n\\n5\\n7\\n∞ ∞\\n23\\n20\\n0\\n0\\n0.77\\n0.00\\n\\n0.80\\n0.00\\n\\n4\\n6\\n29\\n1\\n\\n0.71\\n0.01\\n\\n1\\n9/2\\n38\\n3\\n\\n0.62\\n0.03\\n\\n-6\\n4\\n42\\n4\\n\\n0.58\\n0.04\\n\\n0\\n5/2\\n52\\n8\\n\\n0.48\\n0.08\\n\\n-5\\n5/3\\n57\\n11\\n0.43\\n0.11\\n\\n305\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\nFigure 17.1: Receiver Operating Characteristic. The graph of α = P{X ∈ C|H0 is true} (signiﬁcance) versus 1− β = P{X ∈\\nC|H1 is true} (power) in the example. The horizontal axis α is also called the false positive fraction (FPF). The vertical axis\\n1 − β is also called the true positive fraction (TPF).\\n\\nFrom this exercise we see how the likelihood ratio test is the choice for a most powerful test. For example, for\\nthese likelihoods, the last column states that for a α = 0.11 level test, the best region consists of those values of x so\\nthat\\n\\nL1(x)\\nL0(x) ≥\\n\\n5\\n3\\n\\n.\\n\\nThe type II error probability is β = 0.43 and thus the power is 1 − β = 0.57. In genuine examples, we will typically\\nlook for type II error probability much below 0.43 and we will make many observations. We now summarize carefully\\nthe insights from this game before examining more genuine examples. A proof of this theorem is provided in Section\\n17.4.\\n\\nTheorem 17.1 (Neyman-Pearson Lemma). Let L(θ|x) denote the likelihood function for the random variable X\\ncorresponding to the probability Pθ. If there exists a critical region C of size α and a nonnegative constant kα such\\nthat\\n\\nand\\n\\nL(θ1|x)\\nL(θ0|x) ≥ kα\\nL(θ1|x)\\nL(θ0|x)\\nthen C is the most powerful critical region of size α.\\n\\n< kα\\n\\nfor x ∈ C\\n\\nfor x /∈ C,\\n\\n(17.1)\\n\\nWe, thus, reject the null hypothesis if and only if the likelihood ratio exceeds a value kα with\\n\\nα = Pθ0(cid:26) L(θ1|X)\\n\\nL(θ0|X) ≥ kα(cid:27) .\\n\\nWe shall learn that many of the standard tests use critical values for the t-statistic, the chi-square statistic, or the F -\\nstatistic. These critical values are related to the critical value kα in extensions of the ideas of likelihood ratios. In a\\nfew pages, we will take a glance at the Bayesian approach to hypothesis testing.\\n\\n17.2.1 The Receiver Operating Characteristic\\nUsing R, we can complete the table for L0 total and L1 total.\\n\\n306\\n\\n0.00.20.40.60.81.00.20.40.60.81.0significancepower\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\n> o<-order(L1/L0,decreasing=TRUE)\\n> sumL1<-cumsum(L1[o])\\n> sumL0<-cumsum(L0[o])\\n> significance<-sumL0/100\\n> power<-sumL1/100\\n> plot(significance,power,type=\"s\")\\n> data.frame(x[o],L1[o],L0[o],sumL1,sumL0,power,significance)\\n\\nCompleting the curve, known as the receiver operating characteristic (ROC), is shown in the ﬁgure above.\\nThe ROC shows the inevitable trade-offs between Type I and Type II errors. For example, by the mere fact that the\\ngraph is increasing, we can see that by setting a more rigorous test achieved by lowering α, the level of signiﬁcance,\\n(decreasing the value on the horizontal axis) necessarily reduces 1− β, the power (decreasing the value on the vertical\\naxis.). The unusual and slightly mystifying name is due to the fact that the ROC was ﬁrst developed during World\\nWar II for detecting enemy objects in battleﬁelds, Following the surprise military attack on Pearl Harbor in 1941, the\\nUnited States saw the need to improve the prediction of the movement of aircraft from their radar signals.\\n\\nExercise 17.2. Consider the following (ignorant) example. Flip a coin that gives heads with probability α. Ignore\\nwhatever data you have collected and reject if the coin turns up heads. This test has signiﬁcance level α. Show that\\nthe receiver operating characteristic curve is the line through the origin having slope 1.\\n\\nThis shows what a minimum acceptable ROC curve looks like - any\\nhypothesis test ought be better than a coin toss that ignores the data. The\\nROC can be used as a test diagnostic. One commonly used is the area\\nunder the ROC, (AUC). For the example above, the AUC is 1/2. So any\\ntest should be improve on that value. The “nearly perfect test” would have\\nhave the power near to 1 for even very low signiﬁcance level. In this case\\nthe AUC is very nearly equal to 1.\\n\\n17.3 Examples\\nExample 17.3. Mimicry is the similarity of one species to another in a\\nmanner that enhances the survivability of one or both species - the model\\nand mimic . This similarity can be, for example, in appearance, behavior,\\nsound, or scent. One method for producing a mimic species is hybridiza-\\ntion. This results in the transferring of adaptations from the model species\\nto the mimic. The genetic signature of this has recently been discovered in\\nHeliconius butterﬂies. Padro-Diaz et al sequenced chromosomal regions\\nboth linked and unlinked to the red color locus and found a region that displays an almost perfect genotype by pheno-\\ntype association across four species in the genus Heliconius\\n\\nFigure 17.2: Heliconius butterﬂies\\n\\nLet’s consider a model butterﬂy species with mean wingspan µ0 = 10 cm and a mimic species with mean wingspan\\nµ1 = 7 cm. For both species, the wingspans have standard deviation σ0 = 3 cm. Collect 16 specimen to decide if the\\nmimic species has migrated into a given region. If we assume, for the null hypothesis, that the habitat under study is\\npopulated by the model species, then\\n\\n• a type I error is falsely concluding that the species is the mimic when indeed the model species is resident and\\n• a type II error is falsely concluding that the species is the model when indeed the mimic species has invaded.\\n\\nIf our action is to begin an eradication program if the mimic has invaded, then a type I error would result in the\\neradication of the resident model species and a type II error would result in the letting the invasion by the mimic take\\nits course.\\n\\n307\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\nTo begin, we set a signiﬁcance level. The choice of an α = 0.05 test means that we are accepting a 5% chance of\\nhaving this error. If the goal is to design a test that has the lowest type II error probability, then the Neyman-Pearson\\nlemma tells us that the critical region is determined by a threshold level kα for the likelihood ratio.\\n\\nWe next move to see how this critical region is determined.\\n\\nC =(cid:26)x;\\n\\nL(µ1|x)\\n\\nL(µ0|x) ≥ kα(cid:27) .\\n\\nExample 17.4. Let X = (X1, . . . , Xn) be independent normal observations with unknown mean and known variance\\n0. The hypothesis is\\nσ2\\n\\nH0 : µ = µ0\\n\\nversus H1 : µ = µ1.\\n\\n(17.2)\\n\\nFor the moment consider the case in which µ1 < µ0. We look to determine the critical region.\\n\\nL(µ1|x)\\nL(µ0|x)\\n\\n=\\n\\n=\\n\\nexp− (xn−µ1)2\\nexp− (xn−µ1)2\\n\\n2σ2\\n0\\n\\n2σ2\\n0\\n\\n0\\n\\n0\\n\\n1√2πσ2\\n1√2πσ2\\nexp− 1\\nexp− 1\\n1\\n2σ2\\n0\\n\\n2σ2\\n\\n2σ2\\n\\n0\\n\\n0\\n\\n2σ2\\n0\\n\\n2σ2\\n0\\n\\n1√2πσ2\\n1√2πσ2\\n\\n···\\n···\\ni=1(xi − µ1)2\\ni=1(xi − µ0)2\\n\\nexp− (x1−µ1)2\\nexp− (x1−µ0)2\\n0(cid:80)n\\n0(cid:80)n\\nn(cid:88)i=1(cid:0)(xi − µ1)2 − (xi − µ0)2(cid:1)\\nn(cid:88)i=1\\n\\n(2xi − µ1 − µ0)\\n\\n2σ2\\n0\\n\\nµ0 − µ1\\n\\n= exp−\\n\\n= exp−\\n\\nBecause the exponential function is increasing, the likelihood ratio test (17.1) is equivalent to\\n\\nexceeding some critical value. Continuing to simplify, this is equivalent to ¯x bounded by some critical value,\\n\\nµ1 − µ0\\n\\n2σ2\\n0\\n\\n(2xi − µ1 − µ0),\\n\\nn(cid:88)i=1\\n¯x ≤ ˜kα,\\n\\n(17.3)\\n\\nwhere ˜kα is chosen to satisfy\\n\\nPµ0{ ¯X ≤ ˜kα} = α.\\n\\n(Note that division by the negative number µ1 − µ0 reverses the direction of the inequality.) Pay particular attention\\nto the fact that the probability is computed under the null hypothesis specifying the mean to be µ0. In this case, ¯X is\\nN (µ0, σ0/√n) and consequently the standardized version of ¯X,\\n¯X − µ0\\nσ0/√n\\n\\n(17.4)\\n\\nZ =\\n\\n,\\n\\nis a standard normal. Set zα so that P{Z ≤ −zα} = α. (This can be determined in R using the qnorm command.)\\nThen, by rearranging (17.4), we can determine ˜kα.\\n\\n¯X ≤ µ0 − zα\\n\\nσ0√n\\n\\n= ˜kα.\\n\\n.\\n\\nEquivalently, we can use the standardized score Z as our test statistic and −zα as the critical value. Note that the\\nonly role played by µ1, the value of the mean under the alternative, is that is less than µ0. However, it will play a role\\nin determining the power of the test.\\n\\n308\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\nExercise 17.5. In the example above, give the value of ˜kα explicitly in terms of kα, µ0, µ1, σ2\\n\\n0 and n.\\n\\nReturning to the example of the model and mimic bird species, we now see, by the Neyman-Person lemma that\\n\\nthe critical region can be deﬁned as\\n\\nUnder the null hypothesis, ¯X has a normal distribution with mean µ0 = 10 and standard deviation σ/√n = 3/4.\\n\\nC =(cid:110)x; ¯x ≤ ˜kα(cid:111) =(cid:26)x;\\n\\n¯x − µ0\\n\\nσ/√n ≤ −zα(cid:27) .\\n\\nThis using the distribution function of the normal we can ﬁnd either ˜kα\\n> qnorm(0.05,10,3/4)\\n[1] 8.76636\\nor −zα,\\n> qnorm(0.05)\\n[1] -1.644854\\n\\nThus, the critical value is ˜kα = 8.767 for the test statistic ¯x and −zα = −1.645 for the test statistic z. Now let’s\\n\\nlook at data.\\n> x\\n\\n[1]\\n\\n8.9 2.4 12.1 10.0 9.2 3.7 13.9\\n\\n9.1\\n\\n8.8\\n\\n6.3 12.1 11.0 12.5\\n\\n4.5 8.2 10.2\\n\\n> mean(x)\\n[1] 8.93125\\n\\nThen\\n\\n¯x = 8.931\\n\\nz =\\n\\n8.93124 − 10\\n\\n3/√16\\n\\n= −1.425.\\n\\n˜kα = 8.766 < 8.931 or −zα = −1.645 < −1.425 and we fail to reject the null hypothesis.\\n\\nExercise 17.6. Modify the calculations in the example above to show that for the case µ0 < µ1, using the same value\\nof zα as above, the we reject the null hypothesis precisely when\\n\\nExercise 17.7. Give an intuitive explanation why the power should\\n\\n¯X ≥ µ0 + zα\\n\\nσ0√n\\n\\n. or Z ≥ zα\\n\\n• increase as a function of |µ1 − µ0|,\\n• decrease as a function of σ2\\n0, and\\n• increase as a function of n.\\nNext we determine the type II error probability for the situation given by the previous exercise. We will be guided\\n\\nby the fact that\\n\\n¯X − µ1\\nσ0/√n\\n\\nis a standard normal random variable for the case that the alternative hypothesis, H1 : µ = µ1, is true.\\n\\nFor µ1 > µ0, we ﬁnd that the type II error probability\\n\\nand the power\\n\\nβ = Pµ1{X /∈ C} = Pµ1{ ¯X < µ0 + zα\\n= Pµ1(cid:26) ¯X − µ1\\n< zα − |µ1 − µ0|\\n\\nσ0/√n\\n\\nσ0/√n (cid:27) = Φ(cid:18)zα − |µ1 − µ0|\\nσ0/√n (cid:19)\\n\\nσ0√n}\\n\\n1 − β = 1 − Φ(cid:18)zα + |µ1 − µ0|\\nσ0/√n (cid:19)\\n\\n309\\n\\n(17.5)\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\nExercise 17.8. For sample size determination for the simple hypothesis (17.2) show that n∗, the number of observa-\\ntions to obtain type I error probability α and type II error probability β must satisfy\\n\\n∗\\n\\nn\\n\\n≥\\n\\nσ2\\n0\\n\\n(µ1 − µ0)2 (zα + zβ)2.\\nNotice that n∗\\n• decreases as a function of |µ1 − µ0|,\\n• increases as a function of σ2\\n• decreases as a function of α and β. In other words, n∗\\nincreases as we decrease either type I or type II error.\\nExercise 17.9. Modify the calculations of power in (17.5)\\nabove to show that for the case µ1 < µ0 to show that\\n\\n0, and\\n\\n1 − β = Φ(cid:18)−zα −\\n\\nµ1 − µ0\\n\\nσ0/√n(cid:19) .\\n\\n(17.6)\\n\\nFigure 17.3: Sample size determination for the simple hy-\\npothesis (17.2) . Minimum sample sample size versus\\npower for signiﬁcance level α = 0.10 (black), 0.05 (red),\\n0.02 (purple), and 0.01 (blue).\\n\\nA type II error is falsely failing to conclude that the mimic\\nspecies have inhabited the study area when indeed they have.\\nTo compute the probability of a type II error, note that for α =\\n0.05, we substitute into (17.6),\\n\\n−zα +\\n\\nµ0 − µ1\\nσ0/√n\\n\\n= −1.645 +\\n\\n3\\n\\n3/√16\\n\\n= 2.355\\n\\nFigure 17.4: Left: (black) Density of ¯X for normal data under the null hypothesis - µ0 = 10 and σ0/√n = 3/√16 = 3/4. With an α = 0.05\\nlevel test, the critical value ˜kα = µ0 − zασ0/√n = 8.766. Thus, the area to the left of the vertical dashed line and below the black density\\nfunction is the signiﬁcance level α = Pµ0{ ¯X ≤ kα}. The alternatives shown are µ1 = 9 and 8 (in blue) and µ1 = 7 (in red). The areas below\\nthese curves and to the left of the dashed line is the power 1 − β = Pµ1{ ¯X ≤ kα}. These values are 0.3777, 0.8466, and 0.9907 for respective\\nalternatives µ1 = 9, 8 and 7. Right: The corresponding receiver operating characteristics curves of the power 1− β versus the signiﬁcance α using\\nequation (17.6). The power for an α = 0.05 test are indicated by the intersection of vertical dashed line and the receiver operating characteristics\\ncurves.\\n\\n310\\n\\n0.800.850.900.95010203040506070powernstar0.800.850.900.950102030405060700.800.850.900.950102030405060700.800.850.900.9501020304050607046810120.00.10.20.30.40.50.646810120.00.10.20.30.40.50.6xdensity46810120.00.10.20.30.40.50.6xdensity46810120.00.10.20.30.40.50.6xdensity46810120.00.10.20.30.40.50.6xdensity0.00.20.40.60.81.00.00.20.40.60.81.0alphapower0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0alphapower0.00.20.40.60.81.00.00.20.40.60.81.0alphapower0.00.20.40.60.81.00.00.20.40.60.81.0alphapower\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\nFigure 17.5: Power as a function of the number of observations for an α = 0.01 level test. The null hypothesis - µ0 = 10. The alternatives shown\\nare µ1 = 9 and 8 (in blue) and µ1 = 7 (in red). Here σ0 = 3. The low level for α is chosen to reﬂect the desire to have a stringent criterion for\\nrejecting the null hypothesis that the resident species is the model species.\\n\\npnorm(2.355)\\n\\n>\\n[1] 0.9907386\\n\\nand the type II error probability is β = 1 − 0.9907 = 0.0093, a bit under 1%.\\nLet’s expand the examination of equation (17.6). As we move the alternative value µ1 downward, the density of\\n¯X moves leftward. The values for µ1 = 9, 8, and 7 are displayed on the left in Figure 17.4. This shift in the values is\\na way of saying that the alternative is becoming more and more distinct as µ1 decreases. The mimic species becomes\\neasier and easier to detect. We express this by showing that the test is more and more powerful with decreasing values\\nof µ1. This is displayed by the increasing area under the density curve to the left of the dashed line from 0.377 for\\nthe alternative µ1 = 9 to 0.9907 for µ1 = 7. We can also see this relationship in the receiver operating characteristic\\ngraphed, the graph of the power 1 − β versus the signiﬁcance α. This is displayed for the signiﬁcance level α = 0.05\\nby the dashed line.\\n\\nExercise 17.10. Determine the power of the test for µ0 = 10 cm and µ1 = 9, 8, and 7 cm with the signiﬁcance level\\nα = 0.01. Does the power increase or decrease from its value when α = 0.01? Explain your answer. How would the\\ngraphs in Figure 17.4 be altered to show this case?\\n\\nOften, we wish to know in advance the number of observations n needed to obtain a given power. In this case,\\nwe use (17.5) with a ﬁxed value of α, the size of the test, and determine the power of the test as a function of n. We\\ndisplay this in Figure 17.5 with the value of α = 0.01. Notice how the number of observations needed to achieve a\\ndesired power is high when the wingspan of the mimic species is close to that of the model species.\\n\\nThe example above is called the z-test. If n is sufﬁciently large, then even if the data are not normally distributed,\\n¯X is well approximated by a normal distribution and, as long as the variance σ2\\n0 is known, the z-test is used in this\\ncase. In addition, the z-test can be used when g( ¯X1, . . . , ¯Xn) can be approximated by a normal distribution using the\\ndelta method.\\n\\nExample 17.11 (Bernoulli trials). Here X = (X1, . . . , Xn) is a sequence of Bernoulli trials with unknown success\\n\\n311\\n\\n0204060801000.00.20.40.60.81.0observationspower0204060801000.00.20.40.60.81.00204060801000.00.20.40.60.81.0\\x0cIntroduction to the Science of Statistics\\n\\nprobability p, the likelihood\\n\\nSimple Hypotheses\\n\\n(17.7)\\n\\n(17.8)\\n\\nL(p|x) = px1 (1 − p)1−x1 ··· pxn (1 − p)1−xn = px1+···+xn (1 − p)n−(x1+···+xn)\\n\\n= (1 − p)n(cid:18) p\\n\\n1 − p(cid:19)x1+···+xn\\n\\nH0 : p = p0\\n\\nversus H1 : p = p1\\n\\nFor the test\\n\\nthe likelihood ratio\\n\\nL(p1|x)\\nL(p0|x)\\n\\n=(cid:18) 1 − p1\\n1 − p0(cid:19)n(cid:18)(cid:18) p1\\n\\n1 − p1(cid:19)(cid:46)(cid:18) p0\\n\\n1 − p0(cid:19)(cid:19)x1+···+xn\\n\\n.\\n\\nExercise 17.12. Show that the likelihood ratio (17.7) results in a test to reject H0 whenever\\n\\nn(cid:88)i=1\\n\\nxi ≥ ˜kα when p0 < p1\\n\\nor\\n\\nxi ≤ ˜kα when p0 > p1.\\n\\nn(cid:88)i=1\\n\\nIn words, if the alternative is a higher proportion than the null hypothesis, we reject H0 when the data have too\\n\\nmany successes. If the alternative is lower than the null, we eject H0 when the data do not have enough successes .\\n\\ni=1 Xi has a Bin(n, p0) distribution under the null hypothesis.\\n\\nThus, in the case p0 < p1, we choose ˜kα so that\\n\\nIn either situation, the number of successes N =(cid:80)n\\nPp0(cid:40) n(cid:88)i=1\\n\\nXi ≥ ˜kα(cid:41) ≤ α.\\n\\nIn general, we cannot choose kα to obtain exactly the value α. Thus, we take the minimum value of kα to achieve the\\ninequality in (17.9).\\n\\nTo give a concrete example take p0 = 0.6 and n = 20 and look at a part of the cumulative distribution function.\\n\\nx\\n\\nFN (x) = P{N ≤ x}\\nIf we take α = 0.05, then\\n\\n···\\n···\\n\\n13\\n\\n14\\n\\n15\\n\\n16\\n\\n17\\n\\n18\\n\\n19\\n\\n0.7500\\n\\n0.8744\\n\\n0.9491\\n\\n0.9840\\n\\n0.9964\\n\\n0.9994\\n\\n0.99996\\n\\n20\\n1\\n\\n(17.9)\\n\\nP{N ≥ 16} = 1 − P{N ≤ 15} = 1 − 0.9491 = 0.0509 > 0.05\\nP{N ≥ 17} = 1 − P{N ≤ 16} = 1 − 0.9840 = 0.0160 < 0.05\\n\\nConsequently, we need to have at least 17 successes in order to reject H0.\\nExercise 17.13. Find the critical region in the example above for α = 0.10 and α = 0.01. For what values of α is\\nC = {16, 17, 18, 19, 20} a critical region for the likelihood ratio test.\\nExample 17.14. If np0 and n(1 − p0) are sufﬁciently large, then, by the central limit theorem,(cid:80)n\\nmately a normal distribution. If we write the sample proportion\\n\\ni=1 Xi has approxi-\\n\\nˆp =\\n\\n1\\nn\\n\\nXi,\\n\\nn(cid:88)i=1\\n\\nthen, under the null hypothesis, we can apply the central limit theorem to see that\\n\\nis approximately a standard normal random variable and we perform the z-test as in the previous exercise.\\nFor example, if we take p0 = 1/2 and p1 = 3/5 and α = 0.05, then with 110 heads in 200 coin tosses\\n\\nZ =\\n\\nˆp − p0\\n\\n(cid:112)p0(1 − p0)/n\\n= √2.\\n\\n0.55 − 0.50\\n0.05/√2\\n\\nZ =\\n\\n312\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\n> qnorm(0.95)\\n[1] 1.644854\\nThus, √2 < 1.645 = z0.05 and we fail to reject the null hypothesis.\\nExample 17.15. Honey bees store honey for the winter. This honey serves both as nourishment and insulation from\\nthe cold. Typically for a given region, the probability of survival of a feral bee hive over the winter is p0 = 0.7. We\\nare checking to see if, for a particularly mild winter, this probability moved up to p1 = 0.8. This leads us to consider\\nthe hypotheses\\n\\nH0 : p = p0\\n\\nversus H1 : p = p1.\\n\\nfor a test of the probability that a feral bee hive survives a winter. If we use the central limit theorem, then, under the\\nnull hypothesis,\\n\\nz =\\n\\nˆp − p0\\n\\n(cid:112)p0(1 − p0)/n\\n\\nhas a distribution approximately that of a standard normal random variable. For an α level test, the critical value is\\nzα where α is the probability that a standard normal is at least zα. If the signiﬁcance level is α = 0.05, then we will\\nreject H0 for any value of z > zα = 1.645\\n\\nFor this study, 112 colonies have been chosen and 88 survive. Thus ˆp = 0.7875 and\\n\\nConsequently, reject H0.\\n\\nz =\\n\\n0.7875 − 0.7\\n\\n(cid:112)0.7(1 − 0.7)/112\\n\\n= 1.979.\\n\\nFor both of these previous examples, the usual method is to compute the z-score with the continuity correction.\\n\\nWe shall soon see this with the use of prop.test in R.\\n\\n17.4 Summary\\n\\nFor a simple hypothesis\\n\\nwe have two possible action, reject H0 and fail to reject H0, this leads to two possible types of errors\\n\\nH0 : θ = θ0\\n\\nversus H1 : θ = θ1.\\n\\nprobability\\n\\nerror\\ntype I\\ntype II β = Pθ1{fail to reject H0}\\n\\nα = Pθ0{reject H0}\\n\\nlevel\\n\\nalternative names\\nsigniﬁcance\\n\\nfalse positive\\n\\nfalse negative\\n\\nThe probability 1 − β = Pθ1{reject H0} is called the true positive probability or power or sensitivity. The\\nprobability 1 − α = Pθ0{fail to reject H0} is called the speciﬁcity.\\nThe procedure is to set a signiﬁcance level α and ﬁnd a critical region C so that the type II error probability is as\\nsmall as possible. The Neyman-Pearson lemma lets us know that in many cases the critical region is determined by\\nsetting a level kα for the likelihood ratio.\\n\\nC =(cid:26)x;\\n\\nL(θ1|x)\\n\\nL(θ0|x) ≥ kα(cid:27)\\n\\nWe continue, showing the procedure in the examples above.\\n\\n313\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\nnormal observations µ1 ≥ µ0 Bernoulli trials p1 > p0\\n\\nSimplify likelihood ratio to obtain a\\ntest statistic T (x)\\n\\n¯x\\nz = ¯x−µ0\\nσ0/√n\\n\\n¯X ∼ N (µ0, σ0/√n)\\n\\nZ ∼ N (0, 1)\\n\\nUse the distribution of T (x) under\\nH0 to set a critical value ˜kα so that\\nPθ0{T (X) ≥ ˜kα} = α\\nDetermine type II error probability\\nβ = Pθ1{T (X) ≥ ˜kα}\\n17.5 Proof of the Neyman-Pearson Lemma\\nFor completeness in exposition, we include a proof of the Neyman-Pearson lemma.\\n\\nPµ1{ ¯X ≥ ˜kα}\\n\\ni=1 xi\\n\\n(cid:80)n\\n\\ni=1 Xi ∼ Bin(n, p0)\\n\\ni=1 Xi ≥ ˜kα}\\n\\n(cid:80)n\\nPp1{(cid:80)n\\n\\nLet C be the α critical region determined by the likelihood ratio test. In addition, let C∗ be a critical region for a\\n\\nsecond test of size α. In symbols,\\n\\n∗\\n\\nPθ0{X ∈ C\\n\\n} = Pθ0{X ∈ C} = α\\n\\n(17.10)\\nAs before, we use the symbols β and β∗ denote, respectively, the probability of type II error for the critical regions C\\nand C∗ respectively. The Neyman-Pearson lemma is the statement that β∗\\nDivide both critical regions C and C∗ into two disjoint subsets, the subset that the critical regions share S = C∩C∗\\nand the subsets E = C\\\\C∗ and E∗ = C∗\\n\\\\C that are exclusive to one region. In symbols, we write this as the disjoint\\nunions\\nC = S ∪ E,\\n\\n= S ∪ E\\n\\nand C\\n\\n≥ β.\\n\\n∗\\n\\n∗\\n\\n.\\n\\nThus under either parameter value θi, i = 1, 2,\\n\\nPθi{X ∈ C} = Pθi{X ∈ S} + Pθi{X ∈ E}\\n\\nand Pθi{X ∈ C\\n\\n∗\\n\\n} = Pθi{X ∈ S} + Pθi{X ∈ E\\n\\n∗\\n\\n}.\\n\\n(See Figure 17.5)\\n\\nFirst, we will describe the proof in words.\\n\\nFigure 17.6: Critical region C as determined by the Neyman-Pearson lemma is indicated by the circle on the left. The circle on the right C∗ is the\\ncritical region is for a second α level test. Thus, C = S ∪ E and C∗ = S ∪ E∗.\\n\\n314\\n\\n−0.8−0.6−0.4−0.200.20.40.60.8−0.6−0.4−0.200.20.40.6SEE*\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\n• The contribution to type I errors from data in S and for type II errors from data outside E ∪ E∗ are the same for\\nboth tests. Consequently, we can focus on differences in types of error by examining the case in which the data\\nland in either E and E∗.\\n• Because both test have level α, the probability that the data land in E or in E∗ are the same under the null\\nhypothesis.\\n• Under the likelihood ratio critical region, the null hypothesis is not rejected in E∗.\\n• Under the second test, the null hypothesis is not rejected in E.\\n• E∗ is outside likelihood ratio critical region. So, under the alternative hypothesis, the probability that the data\\nland in E∗ is at most kα times as large as it is under the null hypothesis. This contributes to the type II error for\\nthe likelihood ratio based test.\\n\\n• E is in the likelihood ratio critical region. So, under the alternative hypothesis, the probability that the data land\\nin E is at least kα times as large as it is under the null hypothesis. This contributes a larger amount to the type\\nII error for the second test than is added from E∗ to the likelihood ratio based test.\\n\\n• Thus, the type II error for the likelihood ratio based test is smaller than the type II error for the second test.\\nTo carry out the proof, ﬁrst consider the parameter value θ0 and subtract from both sides in (17.10) the probability\\n\\nPθ0{X ∈ S} that the data land in the shared critical regions and thus would be rejected by both tests to obtain\\n\\n∗\\n\\nPθ0{X ∈ E\\n∗\\n\\n} ≥ Pθ0{X ∈ E}\\n\\nPθ0{X ∈ E\\n\\n} − Pθ0{X ∈ E} ≥ 0.\\n\\nor\\n\\n(17.11)\\n\\n∗\\n\\n}.\\n\\nMoving to the parameter value θ1, the difference in the corresponding type II error probabilities is\\n\\n∗\\n\\nβ\\n\\n− β = Pθ1{X /∈ C\\n\\n} − Pθ1{X /∈ C}\\n\\n∗\\n\\n∗\\n\\n= (1 − Pθ1{X ∈ C\\n\\n}) − (1 − Pθ1{X ∈ C}) = Pθ1{X ∈ C} − Pθ1{X ∈ C\\n\\nNow subtract from both of the integrals the quantity Pθ1{X ∈ S}, the probability that the hypothesis would be falsely\\nrejected by both tests to obtain\\n(17.12)\\n\\n− β = Pθ1{X ∈ E} − Pθ1{X ∈ E\\nWe can use the likelihood ratio criterion on each of the two integrals above.\\n\\n}\\n\\nβ\\n\\n∗\\n\\n∗\\n\\n• For x ∈ E, then x is in the critical region and consequently L(θ1|x) ≥ kαL(θ0|x) and\\n\\nL(θ0|x) dx = kαPθ0{X ∈ E}.\\n• For x ∈ E∗, then x is not in the critical region and consequently L(θ1|x) ≤ kαL(θ0|x) and\\n∗\\nL(θ0|x) dx = kαPθ0{X ∈ E\\n\\nPθ1{X ∈ E\\n\\nL(θ1|x) dx ≥ kα(cid:90)E\\nL(θ1|x) dx ≤ kα(cid:90)E∗\\n\\nPθ1{X ∈ E} =(cid:90)E\\n} =(cid:90)E∗\\n\\n∗\\n\\n}.\\n\\nApply these two inequalities to (17.12)\\n∗\\n\\nβ\\n\\n∗\\n− β ≥ kα(Pθ0{X ∈ E\\n} − Pθ0{X ∈ E}).\\nThis difference is at least 0 by (17.11) and consequently β∗\\n≥ β, i. e., the critical region C∗ has at least as large type\\nII error probability as that given by the likelihood ratio test.\\nNB. The integral will be placed by sums in the case of discrete random variables. For those who know some measure\\ntheory, we can maintain the inequalities above if the integral is taken with respect to some reference measure µ.\\n\\n315\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\n17.6 An Brief Introduction to the Bayesian Approach\\nAs with other aspects of the Bayesian approach to statistics, hypothesis testing is closely aligned with Bayes theorem.\\nFor a simple hypothesis, we begin with a prior probability for each of the competing hypotheses.\\n\\nπ{θ0} = P{H0 is true}\\n\\nand π{θ1} = P{H1 is true}.\\n\\nNaturally, π{θ0} + π{θ1} = 1. Although this is easy to state, the choice of a prior ought to be grounded in solid\\nscientiﬁc reasoning.\\nAs before, we collect data and with it compute the posterior probabilities of the two parameter values θ0 and θ1.\\n\\nThis gives us the posterior probabilities that H0 is true and H1 is true.\\n\\nWe can see, in its formulation, the wide difference in perspective between the Bayesian and classical approaches.\\n• In the Bayesian approach, we begin with a prior probability that H0 is true.\\n\\nIn the classical approach, the\\n\\nassumption is that H0 is true.\\n\\n• In the Bayesian approach, we use the data and Bayes formula to compute the posterior probability that H1 is\\ntrue. In the classical approach, we use the data and a signiﬁcance level to make a decision to reject H0. The\\nquestion: What is the probability that H1 is true? has no meaning in the classical setting.\\n\\n• The decision to reject H0 in the Bayesian setting is based on minimizing risk using presumed losses for type\\nI and type II errors. In classical statistics, the choice of type I error probability is used to construct a critical\\nregion. This choice is made with a view to making the type II error probability as small as possible. We reject\\nH0 whenever the data fall in the critical region.\\n\\nBoth approaches use as a basic concept, the likelihood function L(θ|x) for the data x. Let ˜Θ be a random variable\\n\\ntaking on one of the two values θ0, θ1 and having a distribution equal to the prior probability π. Thus,\\n\\nRecall Bayes formula for events A and C,\\n\\nπ{θi} = P{ ˜Θ = θi},\\n\\ni = 0, 1.\\n\\nP (C|A) =\\n\\nP (A|C)P (C)\\n\\nP (A|C)P (C) + P (A|C c)P (C c)\\n\\n,\\n\\n(17.13)\\n\\nwe set C to be the event that the alternative hypothesis is true and A to be the event that the data take on the value x.\\nIn symbols,\\n\\nFocus for the moment on the case in which the data are discrete, we have the conditional probabilities for the alternative\\nhypothesis.\\n\\nC = { ˜Θ = θ1} = {H1 is true}\\n\\nand A = {X = x}.\\n\\nSimilarly, for the null hypothesis,\\n\\nP (A|C) = Pθ1{X = x} = fX (x|θ1) = L(θ1|x).\\n\\nP (A|C c) = Pθ0{X = x} = fX (x|θ0) = L(θ0|x).\\nThe posterior probability that H1 is true can be written symbolically in several ways.\\n\\nReturning to Bayes formula, we make the substitutions in (17.13),\\n\\nf ˜Θ|X (θ1|x) = P{H1 is true|X = x} = P{ ˜Θ = θ1|X = x}\\n\\nf ˜Θ|X (θ1|x) =\\n\\nL(θ1|x)π{θ1}\\n\\nL(θ0|x)π{θ0} + L(θ1|x)π{θ1}\\n\\n.\\n\\n316\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\nBy making a similar argument involving limits, we can reach the same identity for the density of continuous\\nrandom variables. The formula for the posterior probability can be more easily understood if we rewrite the expression\\nabove in terms of odds, i. e., as the ratio of probabilities.\\n\\nf ˜Θ|X (θ1|x)\\nf ˜Θ|X (θ0|x)\\n\\n=\\n\\nP{H1 is true|X = x}\\nP{H0 is true|X = x}\\n\\n=\\n\\nP{ ˜Θ = θ1|X = x}\\nP{ ˜Θ = θ0|X = x}\\n\\n=\\n\\nL(θ1|x)\\nL(θ0|x) ·\\n\\nπ{θ1}\\nπ{θ0}\\n\\n.\\n\\n(17.14)\\n\\nWith this expression we see that the posterior odds are equal to the likelihood ratio times the prior odds. In this case\\nthe likelihood ratio is called the Bayes factor of H1 in favor of H0.\\nL(θ1|x)\\nL(θ0|x)\\n\\nB =\\n\\n(This is the reciprocal of the ratio used in the Neyman Pearson lemma. In general, pay particular attention to the choice\\nof numerator and denominator in this ratio.)\\n\\nThe decision whether or not to reject H0 depends on the values assigned for the loss obtained in making an\\nincorrect conclusion. We begin by setting values for the loss. This can be a serious exercise in which a group of\\nexperts weighs the evidence for either adverse outcome. We will take a loss of 0 for making a correct decision, a loss\\nof (cid:96)I for a type I error and (cid:96)II for a type II error. We summarize this in a table.\\n\\nloss function table\\n\\ndecision H0 is true H1 is true\\n\\nH0\\nH1\\n\\n0\\n(cid:96)I\\n\\n(cid:96)II\\n0\\n\\nThe Bayes procedure is to make the decision that has the smaller posterior expected loss, also known as the risk.\\n\\nIf the decision is H0, the loss L0(x) takes on two values\\n\\nL0(x) =(cid:26) 0\\n\\nwith probability P{H0 is true|X = x},\\n(cid:96)II with probability P{H1 is true|X = x}.\\n\\nThe expected loss\\n\\nEL0(x) = (cid:96)IIP{H1 is true|X = x} = (cid:96)II(1 − P{H0 is true|X = x})\\n\\n(17.15)\\n\\nis simply the product of the loss and the probability of incorrectly choosing H1.\\n\\nIf the decision is H1, the loss L1(x) also takes on two values\\n\\nL1(x) =(cid:26) (cid:96)I with probability P{H0 is true|X = x},\\nwith probability P{H1 is true|X = x}.\\n\\n0\\n\\nIn this case, the expected loss\\n\\nis a product of the loss and the probability of incorrectly choosing H0.\\n\\nWe can now express the Bayesian procedure in symbols using the criterion of smaller posterior expected loss:\\n\\nEL1(x) = (cid:96)IP{H0 is true|X = x}\\n\\n(17.16)\\n\\ndecide on H1\\n\\nif and only if EL1(x) ≤ EL0(x).\\n\\nNow substituting for EL0(x) and EL1(x) in (17.15) and (17.16), we ﬁnd that we make the decision on H1 and\\n\\nreject H0 if and only if\\n\\n(cid:96)IP{H0 is true|X = x} ≤ (cid:96)II(1 − P{H0 is true|X = x})\\n\\n((cid:96)I + (cid:96)II)P{H0 is true|X = x} ≤ (cid:96)II\\n\\nP{H0 is true|X = x} ≤\\n\\n317\\n\\n(cid:96)II\\n\\n(cid:96)I + (cid:96)II\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nor stated in terms of odds\\n\\nSimple Hypotheses\\n\\n(17.17)\\n\\n(cid:96)I\\n(cid:96)II\\n\\n,\\n\\nP{H1 is true|X = x}\\nP{H0 is true|X = x} ≥\\n\\nwe reject H0 whenever the posterior odds exceeds the ratio of the losses for each type of error.\\n\\nAs we saw in (17.14), this ratio of posterior odds is dependent on the ratio of prior odds. Taking this into account,\\n\\nwe see that the criterion for rejecting H0 is a level test for the likelihood ratio:\\n\\nReject H0 if and only if the Bayes factor\\n\\nB =\\n\\nL(θ1|x)\\nL(θ0|x) ≥\\n\\n(cid:96)I/π{θ1}\\n(cid:96)II/π{θ0}\\n\\n.\\n\\n(17.18)\\n\\nThis is exactly the same type of criterion as that used in classical statistics. However, the rationale, thus the value\\nfor the ratio necessary to reject, is quite different. For example, the higher the value of the prior odds, the higher the\\nlikelihood ratio needed to reject H0 under the Bayesian framework.\\nExample 17.16. For normal observations with means µ0 for the null hypothesis and µ1 for the alternative hypothesis.\\nIf the variance has a known value, σ0, we have from Example 17.4, the likelihood ratio\\n\\nL(µ1|x)\\nL(µ0|x)\\n\\n= exp\\n\\nµ1 − µ0\\n\\n2σ2\\n0\\n\\n(2xi − µ1 − µ0) = exp(cid:18) µ1 − µ0\\nn(cid:88)i=1\\n\\n2σ2\\n0\\n\\nn(2¯x − µ1 − µ0)(cid:19) .\\n\\n(17.19)\\n\\nFor Example 17.3 on the model and mime butterﬂy species, µ0 = 10, µ1 = 7, σ0 = 3, and sample mean ¯x = 8.931\\nbased on n = 16 observations, we ﬁnd the likelihood ratio 0.1004. Thus,\\nP{ ˜M = µ1|X = x}\\nP{ ˜M = µ0|X = x}\\n\\nP{H1 is true|X = x}\\nP{H0 is true|X = x}\\n\\nπ{µ1}\\nπ{µ0}\\n\\n= 0.1004\\n\\n=\\n\\n.\\n\\nwhere ˜M is a random variable having a distribution equal to the prior probability π for the model and mimic butterﬂy\\nwingspan. Consequently, the posterior odds for the mimic vs. mime species is approximately ten times the prior odds.\\nFinally, the decision will depend on the ratio of (cid:96)II/(cid:96)I, i. e., the ratio of the loss due to eradication of the resident\\n\\nmodel species versus letting the invasion by the mimic take its course.\\n\\nExercise 17.17. Substitute the likelihood ratio in 17.19 into 17.18 and solve in terms of ¯x. Use this to determine\\nthreshold values for barx to reject H0 for prior probabilities π{µ0} = 0.05, 0.10, 0.20 and lost ratios (cid:96)I/(cid:96)II =\\n1/2, 1, 2. What situations give the lowest and highest threshold values for ¯x? Explain your answer.\\nExercise 17.18. Returning to a previous example, give the likelihood ratios for n = 20 Bernoulli trials with p0 = 0.6\\nand p1 = 0.7 for values x = 0, . . . , 20 for the number of successes. Give the values for the number of successes in\\nwhich the number of successes change the prior odds by a factor of 5 or more as given by the posterior odds.\\n\\n17.7 Answers to Selected Exercises\\n17.2 Flip a biased coin in which the probability of heads is α under both the\\nnull and alternative hypotheses and reject whenever heads turns up. Then\\n\\nα = Pθ0{heads} = Pθ1{heads} = 1 − β.\\n\\nThus, the receiver operating characteristic curve is the line through the origin\\nhaving slope 1.\\n17.4. The likelihood ratio\\n\\nL(µ1|x)\\nL(µ0|x)\\n\\n= exp−\\n\\nµ0 − µ1\\n\\n2σ2\\n0\\n\\nn(cid:88)i=1\\n\\n(2xi − µ1 − µ0) ≥ kα.\\n\\n318\\n\\nFigure 17.7: Receiver operating Charac-\\nteristic based on a biased coin toss. Thus,\\nany viable ROC should be above the line\\nthe the graph.\\n\\n0.00.20.40.60.81.00.00.20.40.60.81.0alpha1 - beta\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\nThus,\\n\\ni=1(2xi − µ1 − µ0) ≥ ln kα\\nln kα\\n\\nµ1−µ0\\n0 (cid:80)n\\n2σ2\\n(cid:80)n\\ni=1(2xi − µ1 − µ0) ≥ 2σ2\\nµ1−µ0\\n2¯x − µ1 − µ0 ≤ 2σ2\\n2(cid:16)\\nn(µ1−µ0) ln kα + µ1 + µ0(cid:17) = ˜kα\\n¯x ≤ 1\\n\\nn(µ1−µ0) ln kα\\n\\n2σ2\\n0\\n\\n0\\n\\n0\\n\\nNotice that since µ1 < µ0, division by µ1 − µ0 changes the direction of the inequality.\\n17.6. If cα is the critical value in expression in (17.3) then\\n\\nµ1 − µ0\\n\\n2σ2\\n0\\n\\nn(cid:88)i=1\\n\\n(2xi − µ1 − µ0) ≥ cα\\n\\nSInce µ1 > µ0, division by µ1 − µ0 does not change the direction of the inequality. The rest of the argument proceeds\\nas before. we obtain that ¯x ≥ ˜kα.\\n17.7.\\nIf power means easier to distinguish using the data, then this is true when the means are farther apart, the\\nmeasurements are less variable or the number of measurements increases. This can be seen explicitly is the power\\nequation (17.5).\\n17.8. We shall do the case µ1 > µ0. the other case is similar.\\nFrom equation (17.5),\\n\\nσ0/√n (cid:19)\\nβ = Φ(cid:18)zα − |µ1 − µ0|\\n\\n√\\n\\nThe goal is to choose n so that the argument argument zα +\\n|µ1−µ0|\\nn has probability β. However, we have that −zβ has\\nσ0/\\nlower tail probability β. In other words, β = Φ(−zβ). Be-\\ncause Φ, the cumulative distribution function for the standard\\nnormal, is one-to-one,\\n\\n−zβ = zα − |µ1 − µ0|\\nσ0/√n\\n\\n√n|µ1 − µ0|\\n\\nσ0\\n\\n= zα + zβ\\n\\n√n =\\n\\nn =\\n\\nσ0\\n\\n(zα + zβ)\\n\\n|µ1 − µ0|\\n(µ1 − µ0)2 (zα + zβ)2\\n\\nσ2\\n0\\n\\nThus, n∗, any integer al least as large as n will have the desired\\ntype I and type II errors.\\n17.9. For µ0 > µ1,\\n\\nFigure 17.8: Plot of standard normal density function\\nThe value −zβ has lower tail probabiility β. (β = 0.05\\nis shown.)\\n\\nβ = Pµ1{X /∈ C} = Pµ1{ ¯X > µ0 −\\n= Pµ1(cid:26) ¯X − µ1\\nµ1 − µ0\\n\\n> −zα −\\n\\nσ0/√n\\n\\nσ0/√n(cid:27) = 1 − Φ(cid:18)−zα −\\n\\nµ1 − µ0\\n\\nσ0/√n(cid:19)\\n\\nσ0√n\\n\\nzα}\\n\\nand the power\\n\\n1 − β = Φ(cid:18)−zα −\\n\\nµ1 − µ0\\n\\nσ0/√n(cid:19) .\\n\\n319\\n\\nz-4-3-2-101234standard normal density00.050.10.150.20.250.30.350.4-z-area -\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\n17.10. Interpreting equation (17.5) in R, we ﬁnd that\\n> mu0<-10;sigma0<-3;n<-16\\n> zalpha<-qnorm(0.99)\\n> mu1<-c(9,8,7)\\n> power<-1-pnorm(zalpha-abs(mu1-mu0)/(sigma0/sqrt(n)))\\n> data.frame(mu1,power)\\n\\nmu1\\n\\npower\\n9 0.1603514\\n8 0.6331918\\n7 0.9529005\\n\\n1\\n2\\n3\\n\\nNotice that the power has decreased from the case α = 0.05. This could be anticipated. In reducing the signiﬁcance\\nlevel from α = 0.05 to α = 0.01, we make the criterion for rejecting more stringent by reducing he critical region C.\\nThe effect can be seen in FIgure 17.4. On the left side ﬁgure, the vertical dashed line is moved left to reduce the area\\nunder the black curve to the left of the dashed line. This, in turn, reduces the area under the other curves to the left of\\nthe dashed line. On the right ﬁgure, the vertical dashed line is moved left to the value α = 0.01 and, because the ROC\\ncurve is increasing, the values for the power decreased.\\n17.12. For the likelihood ratio (17.7), take the logarithm to obtain\\n\\nL(p0|x)(cid:19) = n ln(cid:18) 1 − p1\\nln(cid:18) L(p1|x)\\n\\n1 − p0(cid:19) + (x1 + ··· + xn) ln(cid:18)(cid:18) p1\\n\\n1 − p1(cid:19)(cid:46)(cid:18) p0\\n\\n1 − p0(cid:19)(cid:19) ≥ ln kα.\\n\\nIf p0 < p1 then the ratio in the expression for the logarithm in the second term is greater than 1 and consequently, the\\ni=1 xi to give the test (17.8). For p0 > p1, the logarithm is negative\\n\\nlogarithm is positive. Thus, we isolate the sum(cid:80)n\\n\\nand the direction of the inequality in (17.8) is reversed.\\n17.13. If we take α = 0.10, then\\n\\nP{N ≥ 15} = 1 − P{N ≤ 14} = 1 − 0.8744 = 0.1256 > 0.10\\nP{N ≥ 16} = 1 − P{N ≤ 15} = 1 − 0.9491 = 0.0509 < 0.10\\n\\nConsequently, we need to have at least 16 successes in order to reject H0. If we take α = 0.01, then\\n\\nP{N ≥ 17} = 1 − P{N ≤ 16} = 1 − 0.9840 = 0.0160 > 0.01\\nP{N ≥ 18} = 1 − P{N ≤ 17} = 1 − 0.9964 = 0.0036 < 0.01\\n\\nConsequently, we need to have at least 18 successes in order to reject H0. For C = {16, 17, 18, 19, 20},\\n\\nP{N ∈ C} = 1 − P{N ≤ 15} = 1 − 0.9491 = 0.0509.\\n\\nThus, α must be less that 0.0509 for C to be a critical region. In addition, P{N ≥ 17} = 0.0160. Consequently, if\\nwe take any value for α < 0.0160, then the critical region will be smaller than C.\\n17.17. Making the substitution of 17.19 into 17.18, we have\\n\\nexp(cid:18) µ0 − µ1\\n\\nn(2¯x − µ1 − µ0)(cid:19) ≤\\nn(2¯x − µ1 − µ0) ≤ ln(cid:18) (cid:96)II/π{θ0}\\n(cid:96)I/π{θ1}(cid:19)\\n\\n(cid:96)II/π{θ0}\\n(cid:96)I/π{θ1}\\n\\n2σ2\\n0\\nµ0 − µ1\\n\\n2σ2\\n0\\n\\n2¯x − µ1 − µ0 ≤\\n\\n¯x ≤\\n\\n2σ2\\n0\\n\\nln(cid:18) (cid:96)II/π{θ0}\\n(cid:96)I/π{θ1}(cid:19)\\n(cid:96)I/π{θ1}(cid:19) + µ1 + µ0(cid:19)\\nln(cid:18) (cid:96)II/π{θ0}\\n\\nn(µ0 − µ1)\\n2σ2\\n1\\n0\\n\\n2(cid:18)\\n\\nn(µ0 − µ1)\\n\\n320\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nSimple Hypotheses\\n\\n> mu0<-10;mu1<-7;sigma<-3;n<-16\\n> pi0<-c(0.05,0.10,0.20)\\n> lr<-1/2\\n> threshold<-(2*sigmaˆ2/(n*(mu1-mu0))*log(lr*pi0/(1-pi0))+mu1+mu0)/2\\n> data.frame(pi0,threshold)\\n\\npi0 threshold\\n9.182047\\n9.041945\\n8.889895\\n\\n1 0.05\\n2 0.10\\n3 0.20\\n> threshold<-(2*sigmaˆ2/(n*(mu1-mu0))*log(lr*pi0/(1-pi0))+mu1+mu0)/2\\n> data.frame(pi0,threshold)\\n\\npi0 threshold\\n9.052082\\n8.911980\\n8.759930\\n\\n1 0.05\\n2 0.10\\n3 0.20\\n> lr<-2\\n> threshold<-(2*sigmaˆ2/(n*(mu1-mu0))*log(lr*pi0/(1-pi0))+mu1+mu0)/2\\n> data.frame(pi0,threshold)\\n\\npi0 threshold\\n8.922117\\n8.782015\\n8.629965\\n\\n1 0.05\\n2 0.10\\n3 0.20\\n> lr<-1\\nThe lowest threshold value ¯x = 8.62 is for the case π{θ0} = 0.20 and (cid:96)I/(cid:96)|I = 2. This is the highest prior probability\\nand the highest relative loss for a type I error. These both require stronger evidence to reject H0 and thus need a more\\nextreme and thus lower value for ¯x to reject H0.\\n\\nThe highest threshold value ¯x = 9.18 is for the case π{θ0} = 0.05 and (cid:96)I/(cid:96)II = 1/2. This is the lowest prior\\nprobability and the lowest relative loss for a type I error. These both require less evidence to reject H0 and thus need\\na less extreme and thus higher value for ¯x to reject H0.\\n17.19. Using the reciprocal likelihood ratio formula in Example 17.9, we compute the Bayes factor B for\\n\\n> x<-c(0:20)\\n> n<-20\\n> p0<-0.6\\n> p1<-0.7\\n> B<-((1-p1)/(1-p0))ˆn*((p1/(1-p1))/(p0/(1-p0)))ˆx\\n> data.frame(x[1:7],B[1:7],x[8:14],B[8:14],x[15:21],B[15:21])\\n\\nx.1.7.\\n\\nB.1.7. x.8.14.\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\n0 0.003171212\\n1 0.004932996\\n2 0.007673550\\n3 0.011936633\\n4 0.018568096\\n5 0.028883705\\n6 0.044930208\\n\\n7 0.06989143\\n8 0.10872001\\n9 0.16912001\\n10 0.26307558\\n11 0.40922867\\n12 0.63657794\\n13 0.99023235\\n\\nB.15.21.\\nB.8.14. x.15.21.\\n1.540361\\n14\\n2.396118\\n15\\n3.727294\\n16\\n5.798013\\n17\\n18\\n9.019132\\n19 14.029761\\n20 21.824072\\n\\nThus, values x ≤ 9 increase the posterior odds in favor of H0 by a factor greater than 5 (B < 1/5), values x ≥ 17\\n\\nincrease the posterior odds in favor of H1 by a factor greater than 5 (B > 5).\\n\\n321\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\n322\\n\\n\\x0cTopic 18\\n\\nComposite Hypotheses\\n\\nSimple hypotheses limit us to a decision between one of two possible states of nature. This limitation does not allow\\nus, under the procedures of hypothesis testing to address the basic question:\\n\\nDoes the length, the reaction rate, the fraction displaying a particular behavior or having a particular\\nopinion, the temperature, the kinetic energy, the Michaelis constant, the speed of light, mutation rate, the\\nmelting point, the probability that the dominant allele is expressed, the elasticity, the force, the mass, the\\nparameter value θ0 increase, decrease or change at all under under a different experimental condition?\\n\\n18.1 Partitioning the Parameter Space\\nThis leads us to consider composite hypotheses. In this case, the parameter space Θ is divided into two disjoint\\nregions, Θ0 and Θ1. The hypothesis test is now written\\n\\nversus H1 : θ ∈ Θ1.\\nAgain, H0 is called the null hypothesis and H1 the alternative hypothesis.\\n\\nH0 : θ ∈ Θ0\\n\\nFor the three alternatives to the question posed above, let θ be one of the components in the parameter space, then\\n• increase would lead to the choices Θ0 = {θ; θ ≤ θ0} and Θ1 = {θ; θ > θ0},\\n• decrease would lead to the choices Θ0 = {θ; θ ≥ θ0} and Θ1 = {θ; θ < θ0}, and\\n• change would lead to the choices Θ0 = {θ0} and Θ1 = {θ; θ (cid:54)= θ0}\\n\\nfor some choice of parameter value θ0. The effect that we are meant to show, here the nature of the change, is contained\\nin Θ1. The ﬁrst two options given above are called one-sided tests. The third is called a two-sided test,\\n\\nRejection and failure to reject the null hypothesis, critical regions, C, and type I and type II errors have the same\\nmeaning for a composite hypotheses as it does with a simple hypothesis. Signiﬁcance level and power will necessitate\\nan extension of the ideas for simple hypotheses.\\n\\n18.2 The Power Function\\nPower is now a function of the parameter value θ. If our test is to reject H0 whenever the data fall in a critical region\\nC, then the power function is deﬁned as\\n\\nthat gives the probability of rejecting the null hypothesis for a given value of the parameter.\\n\\nπ(θ) = Pθ{X ∈ C}.\\n\\n323\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nThe ideal power function has\\n\\nπ(θ) ≈ 0 for all θ ∈ Θ0 and π(θ) ≈ 1 for all θ ∈ Θ1\\n\\nWith this property for the power function, we would rarely reject the null hypothesis when it is true and rarely fail to\\nreject the null hypothesis when it is false.\\n\\nIn reality, incorrect decisions are made. Thus, for θ ∈ Θ0,\\n\\nπ(θ) is the probability of making a type I error,\\n\\ni.e., rejecting the null hypothesis when it is indeed true. For θ ∈ Θ1,\\n\\n1 − π(θ) is the probability of making a type II error,\\n\\ni.e., failing to reject the null hypothesis when it is false.\\n\\nThe goal is to make the chance for error small. The traditional method is analogous to that employed in the\\nNeyman-Pearson lemma. Fix a (signiﬁcance) level α, now deﬁned to be the largest value of π(θ) in the region Θ0\\ndeﬁned by the null hypothesis. In other words, by focusing on the value of the parameter in Θ0 that is most likely to\\nresult in an error, we insure that the probability of a type I error is no more that α irrespective of the value for θ ∈ Θ0.\\nThen, we look for a critical region that makes the power function as large as possible for values of the parameter\\nθ ∈ Θ1\\n\\nFigure 18.1: Power function for the one-sided test with alternative “greater”. The size of the test α is given by the height of the red segment.\\nNotice that π(µ) < α for all µ < µ0 and π(µ) > α for all µ > µ0\\n\\nExample 18.1. Let X1, X2, . . . , Xn be independent N (µ, σ0) random variables with σ0 known and µ unknown. For\\nthe composite hypothesis for the one-sided test\\n\\nwe use the test statistic from the likelihood ratio test and reject H0 if the statistic ¯x is too large. Thus, the critical\\nregion\\n\\nH0 : µ ≤ µ0\\n\\nversus H1 : µ > µ0,\\n\\nIf µ is the true mean, then the power function\\n\\nC = {x; ¯x ≥ k(µ0)}.\\n\\nπ(µ) = Pµ{X ∈ C} = Pµ{ ¯X ≥ k(µ0)}.\\n\\n324\\n\\n-0.50.00.51.00.00.20.40.60.81.0mupi-0.50.00.51.00.00.20.40.60.81.0\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nAs we shall see soon, the value of k(µ0) depends on the level of the test.\\n\\nAs the actual mean µ increases, then the probability that the sample mean ¯X exceeds a particular value k(µ0) also\\nincreases. In other words, π is an increasing function. Thus, the maximum value of π on the set Θ0 = {µ; µ ≤ µ0}\\ntakes place for the value µ0. Consequently, to obtain level α for the hypothesis test, set\\n\\nα = π(µ0) = Pµ0{ ¯X ≥ k(µ0)}.\\n\\nWe now use this to ﬁnd the value k(µ0). When µ0 is the value of the mean, we standardize to give a standard normal\\nrandom variable\\n\\nChoose zα so that P{Z ≥ zα} = α. Thus\\n\\nZ =\\n\\n¯X − µ0\\nσ0/√n\\n\\n.\\n\\nPµ0{Z ≥ zα} = Pµ0{ ¯X ≥ µ0 +\\n\\nand k(µ0) = µ0 + (σ0/√n)zα.\\n\\nIf µ is the true state of nature, then\\n\\nZ =\\n\\n¯X − µ\\nσ0/√n\\n\\nσ0√n\\n\\nzα}\\n\\nis a standard normal random variable. We use this fact to determine the power function for this test.\\n\\nσ0√n\\nπ(µ) = Pµ{ ¯X ≥\\n= Pµ(cid:26) ¯X − µ\\n\\nzα + µ0} = Pµ{ ¯X − µ ≥\\n\\nσ0√n\\nσ0/√n(cid:27) = 1 − Φ(cid:18)zα −\\n\\nµ − µ0\\n\\nσ0/√n ≥ zα −\\n\\nzα − (µ − µ0)}\\n\\nµ − µ0\\n\\nσ0/√n(cid:19)\\n\\n(18.1)\\n\\n(18.2)\\n\\nwhere Φ is the distribution function for a standard normal random variable.\\n\\nWe have seen the expression above in several contexts.\\n• If we ﬁx n, the number of observations and the alternative value µ = µ1 > µ0 and determine the power 1 − β\\nas a function of the signiﬁcance level α, then we have the receiver operating characteristic as in Figure 17.2.\\n• If we ﬁx µ1 the alternative value and the signiﬁcance level α, then we can determine the power as a function of\\n\\nthe number of observations as in Figure 17.3.\\n\\n• If we ﬁx n and the signiﬁcance level α, then we can determine the power function π(µ), the power as a function\\n\\nof the alternative value µ. An example of this function is shown in Figure 18.1.\\n\\nExercise 18.2. If the alternative is less than, show that\\n\\nπ(µ) = Φ(cid:18)−zα −\\n\\nµ − µ0\\n\\nσ0/√n(cid:19) .\\n\\nReturning to the example with a model species and its mimic. For the plot of the power function for µ0 = 10,\\n\\nσ0 = 3, and n = 16 observations,\\n\\n> zalpha<-qnorm(0.95)\\n> mu0<-10\\n> sigma0<-3\\n> mu<-(600:1100)/100\\n> n<-16\\n> z<--zalpha - (mu-mu0)/(sigma0/sqrt(n))\\n> pi<-pnorm(z)\\n> plot(mu,pi,type=\"l\")\\n\\n325\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nFigure 18.2: Power function for the one-sided test with alternative “less than”. µ0 = 10, σ0 = 3. Note, as argued in the text that π is a decreasing\\nfunction. (left) n = 16, α = 0.05 (black), 0.02 (red), and 0.01 (blue). Notice that lowering signiﬁcance level α reduces power π(µ) for each value\\nof µ. (right) α = 0.05, n = 15 (black), 40 (red), and 100 (blue). Notice that increasing sample size n increases power π(µ) for each value of\\nµ ≤ µ0 and decreases type I error probability for each value of µ > µ0. For all 6 power curves, we have that π(µ0) = α.\\n\\nIn Figure 18.2, we vary the values of the signiﬁcance level α and the values of n, the number of observations in\\n\\nthe graph of the power function π\\nExample 18.3 (mark and recapture). We may want to use mark and recapture as an experimental procedure to test\\nwhether or not a population has reached a dangerously low level. The variables in mark and recapture are\\n\\n• t be the number captured and tagged,\\n• k be the number in the second capture,\\n• r the the number in the second capture that are tagged, and let\\n• N be the total population.\\nIf N0 is the level that a wildlife biologist say is dangerously low, then the natural hypothesis is one-sided.\\n\\nH0 : N ≥ N0\\n\\nversus H1 : N < N0.\\n\\nThe data are used to compute r, the number in the second capture that are tagged. The likelihood function for N is\\nthe hypergeometric distribution,\\n\\nThe maximum likelihood estimate is ˆN = [tk/r]. Thus, higher values for r lead us to lower estimates for N. Let R be\\nthe (random) number in the second capture that are tagged, then, for an α level test, we look for the minimum value\\nrα so that\\n\\n(18.3)\\nAs N increases, then recaptures become less likely and the probability in (18.3) decreases. Thus, we should set the\\nvalue of rα according to the parameter value N0, the minimum value under the null hypothesis. Let’s determine rα\\n\\nπ(N ) = PN{R ≥ rα} ≤ α for all N ≥ N0.\\n\\nL(N|r) = (cid:0)t\\n\\nr(cid:1)(cid:0)N−t\\nk−r(cid:1)\\n(cid:0)N\\nk(cid:1)\\n\\n.\\n\\n326\\n\\n678910110.00.20.40.60.81.0mupi678910110.00.20.40.60.81.0mupi678910110.00.20.40.60.81.0mupi678910110.00.20.40.60.81.0mupi678910110.00.20.40.60.81.0mupi678910110.00.20.40.60.81.0mupi\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nfor several values of α using the example from the topic, Maximum Likelihood Estimation, and consider the case in\\nwhich the critical population is N0 = 2000.\\n> N0<-2000; t<-200; k<-400\\n> alpha<-c(0.05,0.02,0.01)\\n> ralpha<-qhyper(1-alpha,t,N0-t,k)\\n> data.frame(alpha,ralpha)\\n\\nalpha ralpha\\n49\\n51\\n53\\n\\n0.05\\n0.02\\n0.01\\n\\n1\\n2\\n3\\n\\nFor example, we must capture al least 49 that were tagged in order to reject H0 at the α = 0.05 level. In this case\\nthe estimate for N is ˆN = [kt/rα] = 1632. As anticipated, rα increases and the critical regions shrinks as the value\\nof α decreases.\\n\\nUsing the level rα determined using the value N0 for N, we see that the power function\\n\\nR is a hypergeometric random variable with mass function\\n\\nπ(N ) = PN{R ≥ rα}.\\n\\nfR(r) = PN{R = r} = (cid:0)t\\n\\nr(cid:1)(cid:0)N−t\\nk−r(cid:1)\\n(cid:0)N\\nk(cid:1)\\n\\n.\\n\\nThe plot for the case α = 0.05 is given using the R commands\\n\\n> N<-c(1300:2100)\\n> pi<-1-phyper(49,t,N-t,k)\\n> plot(N,pi,type=\"l\",ylim=c(0,1))\\n\\nWe can increase power by increasing the size of k, the number the value in the second capture. This increases the\\n\\nvalue of rα. For α = 0.05, we have the table.\\n> k<-c(400,600,800)\\n> N0<-2000\\n> ralpha<-qhyper(0.95,t,N0-t,k)\\n> data.frame(k,ralpha)\\n\\nk ralpha\\n49\\n70\\n91\\n\\n1 400\\n2 600\\n3 800\\n\\nWe show the impact on power π(N ) of both signiﬁcance level α and the number in the recapture k in Figure 18.3.\\n\\nExercise 18.4. Determine the type II error rate for N = 1600 with\\n\\n• k = 400 and α = 0.05, 0.02, and 0.01, and\\n• α = 0.05 and k = 400, 600, and 800.\\n\\nExample 18.5. For a two-sided test\\n\\nH0 : µ = µ0\\n\\nversus H1 : µ (cid:54)= µ0.\\n\\nIn this case, the parameter values for the null hypothesis Θ0 consist of a single value, µ0. We reject H0 if | ¯X − µ0| is\\ntoo large. Under the null hypothesis,\\n\\nZ =\\n\\n¯X − µ0\\nσ/√n\\n\\n327\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nFigure 18.3: Power function for Lincoln-Peterson mark and recapture test for population N0 = 2000 and t = 200 captured and tagged. (left)\\nk = 400 recaptured α = 0.05 (black), 0.02 (red), and 0.01 (blue). Notice that lower signiﬁcance level α reduces power. (right) α = 0.05,\\nk = 400 (black), 600 (red), and 800 (blue). As expected, increased recapture size increases power.\\n\\nis a standard normal random variable. For a signiﬁcance level α, choose zα/2 so that\\n\\nP{Z ≥ zα/2} = P{Z ≤ −zα/2} =\\n\\nα\\n2\\n\\n.\\n\\nThus, P{|Z| ≥ zα/2} = α. For data x = (x1, . . . , xn), this leads to a critical region\\n\\nIf µ is the actual mean, then\\n\\nC =(cid:26)x;(cid:12)(cid:12)(cid:12)\\n\\n¯x − µ0\\n\\nσ/√n(cid:12)(cid:12)(cid:12) ≥ zα/2(cid:27) .\\n\\n¯X − µ\\nσ0/√n\\n\\nis a standard normal random variable. We use this fact to determine the power function for this test\\n\\nπ(µ) = Pµ{X ∈ C} = 1 − Pµ{X /∈ C} = 1 − Pµ(cid:26)(cid:12)(cid:12)(cid:12)\\nσ0/√n(cid:12)(cid:12)(cid:12) < zα/2(cid:27)\\n< zα/2(cid:27) = 1 − Pµ(cid:26)−zα/2 −\\n= 1 − Pµ(cid:26)−zα/2 <\\n¯X − µ0\\nσ0/√n\\n= 1 − Φ(cid:18)zα/2 −\\nσ0/√n(cid:19)\\nσ0/√n(cid:19) + Φ(cid:18)−zα/2 −\\n\\n¯X − µ0\\n\\nµ − µ0\\n\\nµ − µ0\\n\\nµ − µ0\\nσ0/√n\\n\\n<\\n\\n¯X − µ\\nσ0/√n\\n\\n< zα/2 −\\n\\nµ − µ0\\n\\nσ0/√n(cid:27)\\n\\nIf we do not know if the mimic is larger or smaller that the model, then we use a two-sided test. Below is the R\\n\\ncommands for the power function with α = 0.05 and n = 16 observations.\\n\\n> zalpha = qnorm(.975)\\n> mu0<-10\\n> sigma0<-3\\n> mu<-(600:1400)/100\\n\\n328\\n\\n14001600180020000.00.20.40.60.81.0Npi14001600180020000.00.20.40.60.81.0Npi14001600180020000.00.20.40.60.81.0Npi14001600180020000.00.20.40.60.81.0Npi14001600180020000.00.20.40.60.81.0Npi14001600180020000.00.20.40.60.81.0Npi\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nFigure 18.4: Power function for the two-sided test. µ0 = 10, σ0 = 3. (left) n = 16, α = 0.05 (black), 0.02 (red), and 0.01 (blue). Notice that\\nlower signiﬁcance level α reduces power. (right) α = 0.05, n = 15 (black), 40 (red), and 100 (blue). As before, decreased signiﬁcance level\\nreduces power and increased sample size n increases power.\\n\\n> n<-16\\n> pi<-1-pnorm(zalpha-(mu-mu0)/(sigma0/sqrt(n)))\\n\\n+pnorm(-zalpha-(mu-mu0)/(sigma0/sqrt(n)))\\n\\n> plot(mu,pi,type=\"l\")\\n\\nWe shall see in the the next topic how these tests follow from extensions of the likelihood ratio test for simple\\n\\nhypotheses.\\n\\nThe next example is unlikely to occur in any genuine scientiﬁc situation. It is included because it allows us to\\n\\ncompute the power function explicitly from the distribution of the test statistic. We begin with an exercise.\\nExercise 18.6. For X1, X2, . . . , Xn independent U (0, θ) random variables, θ ∈ Θ = (0,∞). The density\\n\\nLet X(n) denote the maximum of X1, X2, . . . , Xn, then X(n) has distribution function\\n\\nθ\\n0\\n\\nif 0 < x ≤ θ,\\notherwise.\\n\\nfX (x|θ) =(cid:26) 1\\nFX(n)(x) = Pθ{X(n) ≤ x} =(cid:16) x\\nθ(cid:17)n\\n\\n.\\n\\nExample 18.7. For X1, X2, . . . , Xn independent U (0, θ) random variables, take the null hypothesis that θ lands in\\nsome normal range of values [θL, θR]. The alternative is that θ lies outside the normal range.\\n\\nH0 : θL ≤ θ ≤ θR versus H1 : θ < θL or θ > θR.\\n\\nBecause θ is the highest possible value for an observation, if any of our observations Xi are greater than θR, then\\nwe are certain θ > θR and we should reject H0. On the other hand, all of the observations could be below θL and the\\nmaximum possible value θ might still land in the normal range.\\n\\nConsequently, we will try to base a test based on the statistic X(n) = max1≤i≤n Xi and reject H0 if X(n) > θR\\nand too much smaller than θL, say ˜θ. We shall soon see that the choice of ˜θ will depend on n the number of observations\\nand on α, the size of the test.\\n\\n329\\n\\n681012140.00.20.40.60.81.0mupi681012140.00.20.40.60.81.0mupi681012140.00.20.40.60.81.0mupi681012140.00.20.40.60.81.0mupi681012140.00.20.40.60.81.0mupi681012140.00.20.40.60.81.0mupi\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nThe power function\\n\\nπ(θ) = Pθ{X(n) ≤ ˜θ} + Pθ{X(n) ≥ θR}\\n\\nWe compute the power function in three cases - low, middle and high values for the parameter θ. The second case\\nhas the values of θ under the null hypothesis. The ﬁrst and the third cases have the values for θ under the alternative\\nhypothesis. An example of the power function is shown in Figure 18.5.\\n\\nFigure 18.5: Power function for the test above with θL = 1, θR = 3, ˜θ = 0.9, and n = 10. The size of the test is π(1) = 0.3487.\\n\\nCase 1. θ ≤ ˜θ.\\nIn this case all of the observations Xi must be less than θ which is in turn less than ˜θ. Thus, X(n) is certainly less\\n\\nthan ˜θ and\\n\\nPθ{X(n) ≤ ˜θ} = 1 and Pθ{X(n) ≥ θR} = 0\\n\\nand therefore π(θ) = 1.\\nCase 2. ˜θ < θ ≤ θR.\\nHere X(n) can be less that ˜θ but never greater than θR.\\n\\nPθ{X(n) ≤ ˜θ} =(cid:32) ˜θ\\nθ(cid:33)n\\n\\nand Pθ{X(n) ≥ θR} = 0\\n\\nand therefore π(θ) = (˜θ/θ)n.\\n\\nCase 3. θ > θR.\\nRepeat the argument in Case 2 to conclude that\\n\\nPθ{X(n) ≤ ˜θ} =(cid:32) ˜θ\\nθ(cid:33)n\\n\\n330\\n\\n0123450.00.20.40.60.81.0thetapi\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nand that\\n\\nPθ{X(n) ≥ θR} = 1 − Pθ{X(n) < θR} = 1 −(cid:18) θR\\nθ (cid:19)n\\n\\nand therefore π(θ) = (˜θ/θ)n + 1 − (θR/θ)n.\\n\\nThe size of the test is the maximum value of the power function under the null hypothesis. This is case 2. Here, the\\n\\npower function\\n\\nθ(cid:33)n\\nπ(θ) =(cid:32) ˜θ\\n\\ndecreases as a function of θ. Thus, its maximum value takes place at θL and\\n\\nα = π(θL) =(cid:32) ˜θ\\nθL(cid:33)n\\n\\nTo achieve this level, we solve for ˜θ, obtaining ˜θ = θL\\n\\nn√α. Note that ˜θ increases with α. Consequently, we must\\nexpand the critical region in order to reduce the signiﬁcance level. Also, ˜θ increases with n and we can reduce the\\ncritical region while maintaining signiﬁcance if we increase the sample size.\\n\\nThe assessment of statistical power is an important aspect of experimental design. In practical terms, we can\\n\\nincrease power by either increasing effort or asking a less stringent question. For example, we can increase effort\\n\\n• (mathematics) by applying a more powerful test or a more rigorous design,\\n• (engineering) by designing a better measuring devise, reducing variance, or\\n• (exersion) by increasing sample size\\nWe can ask a less stringent question\\n• by increasing the signiﬁcance level and thus the ability to reject the null hypothesis or\\n• by increasing the difference between null value and the alternative value for detection of difference\\nThese practical considerations will be useful in understanding the change in power resulting from a change in the\\n\\nexperimental design and hypothesis testing.\\n\\n18.3 The p-value\\nThe report of reject the null hypothesis does not describe the strength of the evidence because it fails to give us the sense\\nof whether or not a small change in the values in the data could have resulted in a different decision. Consequently,\\none common method is not to choose, in advance, a signiﬁcance level α of the test and then report “reject” or “fail to\\nreject”, but rather to report the value of the test statistic and to give all the values for α that would lead to the rejection\\nof H0. The p-value is the probability of obtaining a result at least as extreme as the one that was actually observed,\\nassuming that the null hypothesis is true. In this way, we provide an assessment of the strength of evidence against\\nH0. Consequently, a very low p-value indicates strong evidence against the null hypothesis.\\n\\nExample 18.8. For the one-sided hypothesis test to see if the mimic had invaded,\\n\\nH0 : µ ≥ µ0\\n\\nversus H1 : µ < µ0.\\n\\nwith µ0 = 10 cm, σ0 = 3 cm and n = 16 observations. The test statistics is the sample mean ¯x and the critical region\\nis C = {x; ¯x ≤ k}\\n\\n331\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nFigure 18.6: Under the null hypothesis, ¯X has a normal distribution mean µ0 = 10 cm, standard deviation 3/√16 = 3/4 cm. The p-value,\\n0.077, is the area under the density curve to the left of the observed value of 8.931 for ¯x, The critical value, 8.767, for an α = 0.05 level test is\\nindicated by the red line. Because the p-vlaue is greater than the signiﬁcance level, we cannot reject H0.\\n\\nOur data had sample mean ¯x = 8.93125 cm. The maximum value of the power function π(µ) for µ in the subset\\n\\nof the parameter space determined by the null hypothesis occurs for µ = µ0. Consequently, the p-value is\\n\\nWith the parameter value µ0 = 10 cm, ¯X has mean 10 cm and standard deviation 3/√16 = 3/4. We can compute\\n\\nPµ0{ ¯X ≤ 8.93125}.\\n\\nthe p-value using R.\\n\\n> pnorm(8.93125,10,3/4)\\n[1] 0.0770786\\n\\nIf the p-value is below a given signiﬁcance level α, then we say that the result is statistically signiﬁcant at the\\nlevel α. For the previous example, we could not have rejected H0 at the α = 0.05 signiﬁcance level. Indeed, we\\ncould not have rejected H0 at any level below the p-value, 0.0770786. On the other hand, we would reject H0 for any\\nsigniﬁcance level above this value.\\n\\nMany statistical software packages (including R, see the example below) do not need to have the signiﬁcance level\\nin order to perform a test procedure. This is especially important to note when setting up a hypothesis test for the\\npurpose of deciding whether or not to reject H0. In these circumstances, the signiﬁcance level of a test is a value that\\nshould be decided before the data are viewed. After the test is performed, a report of the p-value adds information\\nbeyond simply saying that the results were or were not signiﬁcant.\\n\\nIt is tempting to associate the p-value to a statement about the probability of the null or alternative hypothesis being\\ntrue. Such a statement would have to be based on knowing which value of the parameter is the true state of nature.\\nAssessing whether of not this parameter value is in Θ0 is the reason for the testing procedure and the p-value was\\ncomputed in knowledge of the data and our choice of Θ0.\\n\\nIn the example above, the test is based on having a test statistic S(x) (namely ¯x) fall below a level kα, i.e., we\\n\\nhave decision\\n\\nreject H0 if and only if S(x) ≤ kα.\\n\\n332\\n\\n789101112130.00.10.20.30.40.50.6xdnorm(x, 10, 3/4)789101112130.00.10.20.30.40.50.6x789101112130.00.10.20.30.40.50.6x789101112130.00.10.20.30.40.50.6x\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nThis choice of kα is based on the choice of signiﬁcance level α and the choice of θ0 ∈ Θ0 so that π(θ0) = Pθ0{S(X) ≤\\nkα} = α, the lowest value for the power function under the null hypothesis. If the observed data x takes the value\\nS(x) = s, then the p-value equals\\n(18.4)\\nThis is the lowest value for the signiﬁcance level that would result in rejection of the null hypothesis if we had chosen\\nit in advance of seeing the data.\\nExample 18.9. Returning to the example on the proportion of hives that survive the winter, the appropriate composite\\nhypothesis test to see if more that the usual normal of hives survive is\\n\\nPθ0{S(X) ≤ s}.\\n\\nThe R output shows a p-value of 3%.\\nprop.test(88,112,0.7,alternative=\"greater\")\\n\\n>\\n\\nH0 : p ≤ 0.7\\n\\nversus H1 : p > 0.7.\\n\\n1-sample proportions test with continuity correction\\n\\n88 out of 112, null probability 0.7\\n\\ndata:\\nX-squared = 3.5208, df = 1, p-value = 0.0303\\nalternative hypothesis: true p is greater than 0.7\\n95 percent confidence interval:\\n\\n0.7107807 1.0000000\\n\\nsample estimates:\\n\\np\\n0.7857143\\nExercise 18.10. Is the hypothesis test above signiﬁcant at the 5% level? the 1% level?\\n\\nIn 2016, the American Statistical Association set for itself a task to make a statement on p-values. They note that\\nit is all too easy to set a test, create a test statistic and compute a p-value. Proper statistical practice is much more than\\nthis and includes\\n\\n• appropriately chosen techniques based on a thorough understanding of the phenomena under study,\\n• adequate visual and numerical summaries of the data,\\n• properly conducted analyses whose logic and quantitative approaches are clearly explained,\\n• correct interpretation of statistical results in context, and\\n• reproducibility of results via a thorough reporting.\\n\\nExpressing a p-value is one of many approaches to summarize the results of a statistical investigation. The notion\\nis that the smaller the p-value, the greater the statistical incompatibility of the data with the null hypothesis. This\\nincompatibility is meant to cast doubt on the null hypothesis.\\n\\nUnder the logic of classical statistics, the p-value cannot be turned into a statement about the truth of the null\\nhypothesis but rather is a statement about the data in relation to a speciﬁed statistical model stated as a hypothesis\\ntest. Moreover, the p-value is not meant to serve as a “bright line” between true and false. Part of this arises from the\\npedogogy of introducing of hypothesis testing in setting a signiﬁcance level α as a part of the test.\\n\\nThese issues are compounded in most scientiﬁc considerations where multiple hypothesis testing makes interpre-\\ntation of p-values difﬁcult and calls on the authors for complete transparency of all statistical procedures including\\ndata collection and hypothesis testing. In addition, even strong statistical evidence of the incompatibility of the data\\nwith the null hypothesis may have very little practical or scientiﬁc meaning.\\n\\nMany investigators engage in statistical analysis based on limited background and so often need to collaborate to\\nﬁnd other appropriate statistical approached to decision making under uncertainty. Some appear in this book, e.g.,\\nBayes factors, likelihood ratios, and false discovery rates, but there are many others.\\n\\n333\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\n18.4 Distribution of p-values and the Receiving Operating Characteristic\\nLet’s return to the case of a simple hypotheses.\\n\\nH0 : θ = θ0\\n\\nversus H1 : θ = θ1.\\n\\nAs before, for data x, let S(x) be a test statistic for this hypothesis, rejecting if the value of test statistic S(x) is\\ntoo low. If S(x) = s, the p-value is FS(X)(s|θ0) = Pθ0{S(X) ≤ s}. Recalling out introductory example on model\\nand mimic butterﬂies, the hypothesis on the mean wing span in centimeters, is\\n\\nH0 : µ = µ0\\n\\nversus H1 : µ = µ1.\\n\\nIn this situation, the test statistic S(X) = ¯X is N (µ0, σ/√n) under the null hypothesis.\\nIf the standard devia-\\ntion is sigma with n observations, using xbar to denote the sample mean, we ﬁnd the p-value with the command\\npnorm(xbar,mu0,sigma/sqrt(n)).\\n\\nFor a signiﬁcance test at level α, there exists a critical value kα so that\\n\\nα = Pθ0{S(X) ≤ kα} = FS(X)(kα|θ0)\\n\\nand we reject the null hypothesis at level α if the value of the test statistic is below the critical value, i. e., s < kα. Thus,\\nfor signiﬁcance level alpha, we determine kα with the command qnorm(alpha,mu0,sigma/sqrt(n)).\\n\\nFor the parameter value θ1, the power\\n\\n1 − β(α) = Pθ1{S(X) ≤ kα}.\\n\\nIn other words, 1 − β(α) is the probability, under the alternative parameter θ1 that the p-value is less than α.\\nDeﬁne FR(α) = 1 − β(α), then FR is a non-decreasing function on the interval [0, 1] with FR(0) = 0 and\\nFR(1) = 1. Thus, FR is a cumulative distribution function. Recall that the receiving operator characteristic is the\\nplot of the power as a function of signiﬁcance. In other words, it is the plot FR(α).\\nExercise 18.11. Show that the receiver operating characteristic gives the distribution function for the p-values for the\\nalternative parameter value θ1.\\n\\nThe area under the receiving operator characteristic, AUC,\\n\\n(cid:90) 1\\n\\n0\\n\\nFR(α) dα.\\n\\nis a general diagnostic for the overall power of a test. If the AUC is nearly 1, then the power has the very desirable\\nproperty of increasing quickly for low signiﬁcance levels.\\nExercise 18.12. Let Si, i = 0, 1 be independent random variables that have the distributions of S(X) under θi. The\\nthe area under the curve equals\\n\\n(cid:90) ∞\\n\\n−∞\\n\\nF1(s0)f0(s0) ds0 = P{S1 < S0}.\\n\\n(18.5)\\n\\nIn words, for two independent samples of the test statistic, one under the null hypothesis and the other under the\\nalternative, the area under the curve is the probability that the value under the alternative is smaller. We will see a\\nsimilar expression in an alternative approach to t procedures. This will leads to the Wilcoxon ranked sum test and an\\ninterpretation associated to the area under the empirical receiving operator characteristic.\\nExercise 18.13. For n = 16 observations, standard deviation σ = 3 and µ0 = 10 centimeters, determine the values\\nfor the area under the receiver operator characteristics in Figure 17.3.\\n\\nµ1\\n9\\n8\\n7\\n\\nAUC\\n0.8271\\n0.9703\\n0.9977\\n\\n334\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nHint: Use the integrate command for the integral in (18.5)\\n\\nNotice that, as expected, as the difference µ0 − µ1 increases, the mimic and the model butterﬂy are easier to\\n\\ndistinguish and the AUC increases.\\nExercise 18.14. Simulate P{S1 < S0} in the previous exercise and see how they match the values for the AUC.\\n\\n18.5 Multiple Hypothesis Testing\\nWe now consider testing multiple hypotheses. This is common in the world of “big data” with thousands of hypothesis\\non many issues in subjects including genomics, internet searches, or ﬁnancial transactions. For m hypotheses, let\\np1, . . . , pm be the p-values for m hypothesis tests.\\n\\n18.5.1 Familywise Error Rate\\nThe familywise error rate (FWER) is the probability of making even one type I error. If we set αB for the signiﬁcance\\nlevel for a single test, then the simplest strategy is to employ the Bonferroni correction. This uses the Bonferroni\\ninequality,\\n\\nP (A1 ∪ ··· ∪ Am) ≤ P (A1) + ··· + P (Am)\\n\\nfor events A1, . . . , Am.\\n\\nIf Ai is the event of rejecting the null hypothesis when it is true, then A1 ∪ ··· ∪ Am is the event that at least one\\nof the hypotheses is rejected when it is true. For each i, P (Ai) = αB and so α = P (A1 ∪ ··· ∪ Am) ≤ mαB. Thus,\\nthe Bonferroni correction is to reject if\\n\\npi ≤\\n\\nα\\nm\\n\\nfor all i.\\n\\nExercise 18.15. For m independent, αI level hypothesis tests, show that the familywise error α = 1 − (1 − αI )m.\\nThus, (1 − α)1/m = 1 − αI and αI = 1 − (1 − α)1/m is the level necessary to obtain an α familywise error rate.\\n\\nThis gives a cautionary take, if we take α = 0.05 and m = 20, then the probability of one or more false positive\\ntests, 1 − (1 − 0.05)20 ≈ 0.64, is well above 1/2. The Bonferroni correction, αB = 0.05/20 = 0.0025 and the\\nindependence correction, αI = 1 − (1 − 0.05)1/20 = 0.0256 will guarantee a familywise error rate α = 0.05\\n\\nNote that the second method allows for slightly higher values of α than the Bonferroni correction. However, it is\\nfar less general. For independent test statistics, Fisher’s method for testing multiple works directly with the p-values.\\nWe begin with the following exercise.\\nExercise 18.16. Let θ0 be the true state of nature. Assume that the distribution function for the text statistic S(X) is\\ncontinuous and strictly increasing for all possible values. Show that the p-value is uniformly distributed on the interval\\n[0, 1].\\n\\nIn this circumstance, if the null hypothesis is true for all m hypotheses, then\\n\\np1, . . . , pm are independent U (0, 1) random variables.\\n\\nRecall from the use of the probability transform that\\n\\n−2 ln p1, . . . ,−2 ln pm are independent Exp(1/2) random variables.\\n\\nSo their sum\\n\\n−2 ln p1 − ··· − 2 ln pm is a Γ(1/2, m) random variable.\\n\\nThus this Γ random variable can serve as a test statistic for the multiple hypothesis that all the null hypotheses are\\ntrue, rejecting if the sum above is sufﬁciency large. Traditionally, we use the fact that Γ(1/2, m) is also a member of\\n2m and then use this as the distribution of −2 ln p1 − ··· − 2 ln pm under the multiple\\nthe chi-square family, namely, χ2\\nhypothesis that all m null hypotheses hold.\\n\\n335\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nExample 18.17. For 10 independent test consider the p-values\\n\\n> p\\n\\n[1] 0.0086 0.0164 0.6891 0.7671 0.2967 0.5465 0.0247 0.8235 0.9603 0.0041\\n\\nThe test statistic for Fisher’s method\\n\\n> -2*sum(log(p))\\n[1] 41.5113\\n\\ngives a p-value of 0.3% for the multiple test for all 10 hypotheses.\\n\\n> 1-pchisq(-2*sum(log(p)),2*10)\\n[1] 0.003200711\\n\\n18.5.2 False Discovery Rate\\nWhen the number of tests becomes very large, then having all hypotheses true is an extremely strict criterion. A more\\nrelaxed and often more valuable criterion is the false discovery rate..\\n\\nThus, we can model question Is the null hypothesis hypothesis true? as a sequence of Bernoulli trials. Let π0 be the\\nsuccess parameter for the trials. Thus, with probability π0, the null hypothesis is true and the p-values follow FU , the\\nuniform distribution on the interval [0, 1]. With probability 1 − π0, the null hypothesis is false and the p-values follow\\nFR, the distribution of the receiver operating characteristic. Taken together, we say that the p-values are distributed\\naccording to the mixture\\n\\nF (x) = πFU (x) + (1 − π)FR(x) = π0x + (1 − π0)FR(x).\\n\\n(18.6)\\n\\nThus, if we reject whenever the p-value is below a chosen value α, then the type I error probability is α. From this\\n\\nwe determine the false discovery rate, here deﬁned as\\n\\nq = P{H0 is true|reject H0}.\\n\\nP{reject H0|H0 is true}P{H0 is true}\\n\\nP{reject H0}\\n\\n=\\n\\nαπ0\\nF (α)\\n\\n.\\n\\nUsing Bayes formula\\n\\nq =\\n\\nAn estimate of the false discovery rate can be\\ndetermined from an estimate of π0. This is deter-\\nmined by looking at the p-values and estimating\\nthe mixture in (18.6).\\nExample 18.18. Consider a simple hypothesis\\n\\nH0 : µ = 0 versus H1 : µ = 1.\\n\\nfor the mean µ based on 16 observations of nor-\\nmal random variable, variance 1. Thus, either the\\neffect is not present (µ = 0), or it is (µ = 1). If we\\ntake the signiﬁcance level α = 0.01, then based\\non n = 16 observations, the test statistic ¯X has\\nstandard deviation 1/√16 = 1/4,\\n\\n> alpha<-0.01\\n> (kalpha<-qnorm(1-alpha,0,1/4))\\n[1] 0.581587\\n\\n336\\n\\nFigure 18.7: False discovery rate versus π. Here the signiﬁcance level α =\\n0.01, the power, β = 0.953.\\n\\n0.000.050.100.150.200.250.00000.00050.00100.00150.00200.00250.00300.0035pifdr\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\nand, thus, we reject H0 if the sample mean ¯x >\\nkα = 0.581587. The power, i.e.., the probability\\nthat we reject H0 when H1 is true,\\n\\n> (p_1<-1-pnorm(xbar,1,1/4))\\n[1] 0.9529005\\n\\nIf we plot the false discovery rate versus π0, the probability H0 is true, then\\n\\n> pi<-seq(0,0.25,0.01)\\n> fdr<-alpha*pi0/(alpha*pi0+p_1*(1-pi))\\n> plot(pi0,fdr,type=\"l\")\\n\\nIn this case, for π = 0.10, we have a false discovery rate q = 0.00116, For 10,000 hypothesis, we have a mean of\\n\\n11.6 false discoveries.\\n\\n18.6 Answers to Selected Exercises\\n18.2. In this case the critical regions is C = {x; ¯x ≤ k(µ0)} for some value k(µ0). To ﬁnd this value, note that\\n\\nPµ0{Z ≤ −zα} = Pµ0{ ¯X ≤ −\\n\\nσ0√n\\n\\nzα + µ0}\\n\\nand k(µ0) = −(σ0/√n)zα + µ0. The power function\\n\\nσ0√n\\nπ(µ) = Pµ{ ¯X ≤ −\\n= Pµ(cid:26) ¯X − µ\\n\\nσ0√n\\nzα + µ0} = Pµ{ ¯X − µ ≤ −\\nσ0/√n(cid:27) = Φ(cid:18)−zα −\\n\\nµ − µ0\\n\\nσ0/√n ≤ −zα −\\n\\nzα − (µ − µ0)}\\nµ − µ0\\n\\nσ0/√n(cid:19) .\\n\\n18.4. The type II error rate β is 1 − π(1600) = P1600{R < rα}. This is the distribution function of a hypergeometric\\nrandom variable and thus these probabilities can be computed using the phyper command\\n\\n• For varying signiﬁcance, we have the R commands:\\n\\n> t<-200;N<-1600\\n> k<-400\\n> alpha<-c(0.05,0.02,0.01)\\n> ralpha<-c(49,51,53)\\n> beta<-1-phyper(ralpha-1,t,N-t,k)\\n> data.frame(alpha,beta)\\n\\nalpha\\n\\nbeta\\n0.05 0.5993010\\n0.02 0.4609237\\n0.01 0.3281095\\n\\n1\\n2\\n3\\n\\nNotice that the type II error probability is high for α = 0.05 and increases as α decreases.\\n\\n• For varying recapture size, we continue with the R commands:\\n\\n> k<-c(400,600,800)\\n> ralpha<-c(49,70,91)\\n> beta<-1-phyper(ralpha-1,t,N-t,k)\\n\\n337\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\n> data.frame(k,beta)\\n\\nk\\n\\nbeta\\n1 400 0.5993010\\n2 600 0.8043988\\n3 800 0.9246057\\n\\nNotice that increasing recapture size has a signiﬁcant impact on type II error probabilities.\\n18.6. The i-th observation satisﬁes\\n\\nP{Xi ≤ x} =(cid:90) x\\n\\n0\\n\\n1\\nθ\\n\\nd˜x =\\n\\nx\\nθ\\n\\nNow, X(n) ≤ x occurs precisely when all of the n-independent observations Xi satisfy Xi ≤ x. Because these\\nrandom variables are independent,\\n\\nFX(n) (x) = Pθ{X(n) ≤ x} = Pθ{X1 ≤ x, X1 ≤ x, . . . , Xn ≤ x}\\n\\n= Pθ{X1 ≤ x}P{X1 ≤ x},··· P{Xn ≤ x} =(cid:16) x\\n\\nθ(cid:17)(cid:16) x\\n\\nθ(cid:17)···(cid:16) x\\n\\nθ(cid:17) =(cid:16) x\\nθ(cid:17)n\\n\\n18.10. Yes, the p-value is below 0.05. No, the p-value is above 0.01.\\n18.11. The p-value is FS(x)(s|θ0)). By the deﬁnition of kα, FS(x)(kα|θ0)) = α. The distribution of p-values under\\nθ1,\\n\\nPθ1{FS(x)(S(X)|θ0) ≤ α} = Pθ1{FS(x)(S(X)|θ0) ≤ FS(x)(kα|θ0)}\\n\\n= Pθ1{S(X) ≤ kα} = 1 − β(α),\\n\\nthe power as a function of the signiﬁcance level α. This is the receiver operating characteristic.\\n18.12. To simplify notation denote the distributions functions FS(X)(·|θi) = Fi, i = 0, 1 and let fi denote their\\ncorresponding density functions. Then, for example, α = F0(kα). So,\\n\\nFR(α) = Pθ1{S(X) ≤ kα} = F1(kα) = F1(F\\n\\n−1\\n0\\n\\n(α))\\n\\nand\\n\\nα = F0(s0),\\n\\ns0 = F\\n\\n−1\\n0\\n\\n(α), dα = f0(s0)ds0\\n\\n(α)) dα,\\n\\n−1\\n0 F1(F\\n0\\n\\n−∞ F1(s0)f0(s0) ds0,\\n\\n(cid:82) 1\\n0 FR(α) dα =(cid:82) 1\\n=(cid:82) ∞\\n=(cid:82) ∞\\n−∞(cid:82) s0−∞ f1(s1)f0(s0) ds1ds0\\n=(cid:82)(cid:82){s1<s0} f1(s1)f0(s0) ds1ds0\\n\\n= P{S1 < S0}\\n\\n18.13. We use (18.5) and the integrate command. The interval [µ0 − 6mu0 + 6] goes 8 stande deviations (σ/√n)\\n\\nabove and below he mean.\\n\\n> sigma<-3;n<-16;mu0<-10\\n> mu1<-9;\\n\\nintegrand<-function(s) pnorm(s,mu1,sigma/sqrt(n))*dnorm(s,mu0,sigma/sqrt(n))\\n\\n> integrate(integrand,mu0-6,mu0+6)\\n0.8271107 with absolute error < 2.9e-06\\n\\n338\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nComposite Hypotheses\\n\\n> mu1<-8;\\n\\nintegrand<-function(s) pnorm(s,mu1,sigma/sqrt(n))*dnorm(s,mu0,sigma/sqrt(n))\\n\\n> integrate(integrand,mu0-6,mu0+6)\\n0.9703268 with absolute error < 2.9e-05\\n> mu1<-7;\\n\\nintegrand<-function(s) pnorm(s,mu1,sigma/sqrt(n))*dnorm(s,mu0,sigma/sqrt(n))\\n\\n> integrate(integrand,mu0-6,mu0+6)\\n0.9976611 with absolute error < 1.3e-05\\n18.14. We use (18.5) for the area under the curve and simulate P{S0 < S1}.\\n> mu0<-10;s0<-rnorm(100000,mu0,sigma/sqrt(n))\\n> mu1<-9;s1<-rnorm(100000,mu1,sigma/sqrt(n))\\n> length(s1[s1<s0])/length(s1)\\n[1] 0.82777\\n> mu1<-8;s1<-rnorm(100000,mu1,sigma/sqrt(n))\\n> length(s1[s1<s0])/length(s1)\\n[1] 0.97058\\n> mu1<-7;s1<-rnorm(100000,mu1,sigma/sqrt(n))\\n> length(s1[s1<s0])/length(s1)\\n[1] 0.99786\\n\\nTo compare:\\n\\nµ1\\n9\\n8\\n7\\n\\nAUC\\n0.8271\\n0.9703\\n0.9977\\n\\nsimulation\\nP{S0 < S1}\\n\\n0.8278\\n0.9706\\n0.9979\\n\\nand the simulated probabilities agree with the AUC to 3 decimal places.\\n18.15. By the complement rule, de Morgan’s law, and independence of the Ai, we have\\n\\nα = P (A1 ∪ ··· ∪ Am) = 1 − P ((A1 ∪ ··· ∪ Am)c) = 1 − P (Ac\\n= 1 − P (Ac\\n\\nm) = 1 − (1 − P (A1))··· (1 − P (Am)) = 1 − (1 − αI )m.\\n\\n1 ∩ ··· ∩ Ac\\nm)\\n\\n1)··· P (Ac\\n\\n18.16. Let FS(x)(s|θ0) be the distribution function for S(X) under θ0 and note that the conditions on function\\nFS(x)(s|θ0) insure that it is one to one and thus has an inverse. By 18.4, the p-value is FS(x)(S(X)|θ0). Choose\\nu in the interval [0, 1]. Then,\\n\\nPθ0{FS(x)(S(X)|θ0) ≤ u} = Pθ0{S(X) ≤ F\\n\\n−1\\nS(x)(u|θ0)}\\n−1\\nS(x)(u|θ0)|θ0) = u,\\n\\n= FS(x)(F\\n\\nshowing that FS(x)(S(X)|θ0) is uniformly distributed on the interval [0, 1].\\n\\n339\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\n340\\n\\n\\x0cTopic 19\\n\\nExtensions on the Likelihood Ratio\\n\\nWe begin with a composite hypothesis test\\n\\nH0 : θ ∈ Θ0\\n\\nversus H1 : θ ∈ Θ1\\n\\nwith Θ0 and Θ1 a partition of the parameter space Θ ( Θ0 ∩ Θ1 = ∅ and Θ0 ∪ Θ1 = Θ). Let C be the critical region\\nfor an α level test, i .e, we reject the null hypothesis whenever the data x fall in the critical region. Thus, the power\\nfunction\\n\\nhas the property that\\n\\nπ(θ) = Pθ{X ∈ C}\\n\\nπ(θ) ≤ α for all θ ∈ Θ0\\n\\nand that α is the maximum value of the power function on Θ0, the parameter values associated to the null hypothesis.\\nWe have seen several critical regions that were deﬁned by taking a statistic T (x) and deﬁning the critical region\\n\\nby have this statistic either be more or less that a critical value. For a one-sided test, we have seen critical regions\\n\\nFor a two-sided test, we saw\\n\\n{T (x) ≥ ˜kα}\\n\\nor {T (x) ≤ ˜kα}.\\n\\n{|T (x)| ≥ ˜kα}.\\n\\nwhere ˜kα is determined by the level α. We thus use the commands qnorm, qbinom, or qhyper when the test\\nstatistic has, respectively, a normal, binomial, or hypergeometric distribution under a appropriated choice of θ ∈ Θ0.\\nHere we will examine extensions of the likelihood ratio test for simple hypotheses that have desirable properties for a\\ncritical region.\\n\\n19.1 One-Sided Tests\\nLet’s collect a simple random sample of independent normal observations with unknown mean and known variance\\n0. We noticed, in the case of a simple hypothesis test\\nσ2\\n\\nH0 : µ = µ0\\n\\nversus H1 : µ = µ1\\n\\nthat the critical region as determined by the Neyman-Pearson lemma depended only on whether or not µ1 was greater\\nthan µ0. For example, if µ1 > µ0, then the critical region C = {x; ¯x ≥ ˜kα} shows that we reject H0 whenever the\\nsample mean is higher than some threshold value ˜kα irrespective of the difference between µ0 and µ1. An analogous\\nsituation occurs in the case that µ1 < µ0\\n\\nWe will examine the idea that if a test is most powerful against each possible alternative in a simple hypothesis\\ntest, when we can say that this test is in some sense best overall for a composite hypothesis. Stated in terms of the\\n\\n341\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\npower function, we are asking if a test has the property that its power function π is greater for every value of θ ∈ Θ1\\nthan the power function of any other test. Such a test is called uniformly most powerful. In general, a hypothesis\\nwill not have a uniformly most powerful test. However, we can hope for such a test for procedures involving simple\\nhypotheses in which the test statistic that emerged from the likelihood test did not depend on the speciﬁc value of\\nthe alternative. This was seen in the example above using independent normal data. In this case, the power function\\nπ(µ) = Pµ{ ¯X ≥ kα} increases as µ increases and so the test has the intuitive property of becoming more powerful\\nwith increasing µ.\\n\\nIn general, we look for a test statistic T (x) (like ¯x in the example above). Next, we check that the likelihood ratio,\\n\\nL(θ2|x)\\nL(θ1|x)\\n\\n,\\n\\nθ1 < θ2.\\n\\n(19.1)\\n\\ndepends on the data x only through the value of statistic T (x) and, in addition, this ratio is a monotone increasing\\nfunction of T (x). The Karlin-Rubin theorem states:\\n\\nIf these conditions hold, then for an appropriate value of ˜kα, C = {x; T (x) ≥ ˜kα} is the critical region for a\\n\\nuniformly most powerful α level test for the one-sided alternative hypothesis\\n\\nA corresponding criterion holds for the one sided test with the inequalities reversed:\\n\\nH0 : θ ≤ θ0\\n\\nversus H1 : θ > θ0.\\n\\nExercise 19.1 (mark and recapture). Use the hypothesis\\n\\nH0 : θ ≥ θ0\\n\\nversus H1 : θ < θ0.\\n\\nH0 : N ≥ N0\\n\\nversus H1 : N < N0\\n\\non the population size N and let the data be r, the number in the second capture that are tagged. Give the correspond-\\ning criterion to (19.1) and verify that it holds for the likelihood function L(N|r) and the test statistic T (r) = r.\\n\\nThese conditions are satisﬁed for the case above as well as the tests for p, the probability of success in Bernoulli\\n\\ntrials.\\n\\nExercise 19.2 (One sample one proportion z-test). For X = (X1, . . . , Xn) is a sequence of Bernoulli trials with\\nunknown success probability p, we can have and the one-sided tests with the alternative is greater\\n\\nH0 : p ≤ p0\\n\\nversus H1 : p > p0\\n\\nor less\\n\\nversus H1 : p < p0.\\nShow that (19.1) holds with T (x) = ¯x when the alternative is greater than.\\n\\nH0 : p ≥ p0\\n\\nExample 19.3. We return to the example of the survivability of bee hives over a given winter. The probability of\\nsurvival is p0 = 0.7. The one-sided alternative for a mild winter is that this survival probability has increased. This\\nleads us to consider the hypotheses\\n\\nH0 : p ≤ 0.7\\n\\nversus H1 : p > 0.7.\\n\\nfor a test of the probability that a feral bee hive survives a winter. If the expected number of successes, np, and failures,\\nn(1 − p), are both above 10, we can employ a test statistic\\n\\nz =\\n\\nˆp − p0\\n\\n(cid:112)p0(1 − p0)/n\\n\\n342\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nderived from the central limit theorem. For an α level test, the critical value is zα where α is the probability that a\\nstandard normal is at least zα\\n\\nFor this study, 112 colonies have been selected with 88 surviving. Thus ˆp = 0.7875 and z = 1.979.\\n\\nIf the\\nsigniﬁcance level is α = 0.05, then we will reject H0 because z = 1.979 > 1.645 = zα. Previously, we performed\\nthis test in R and found a p-value of 0.0303.\\n\\nA direct appeal to the central limit theorem gives a slightly different p-value, namely\\n\\n> 1-pnorm(1.979)\\n[1] 0.02390800\\n\\nThis is expressed in the R output as a continuity correction. In the topic Central Limit Theorem, we learned that\\nwe are approximating probability for the outcome P{X ≥ x} for X the number of successes in Bernoulli trials by\\nP{Y ≥ x + 1/2} where Y is a normal random variable. This correction can be seen by looking at the area associated\\nto a histogram for the mass function for X and the density function for Y . (See Figure 11.5.)\\nBecause P{X ≥ x} = P{X > x − 1} = 1 − P{X ≤ x − 1}, the R command for P{X ≥ x} is\\n\\n1-pbinom(x-1,n,p). The table below compares computing the P -value using the binomial directly binompvalue,\\nthe normal approximation normpvalue, and the normal approximation with the continuity correction normpvaluecc.\\nThe number for the test above are shown on line 9.\\n\\n> n<-112\\n> p<-0.7\\n> x<- 80:92\\n> binompvalue<-round(1-pbinom(x-1,n,p),4)\\n> normpvalue<-round(1-pnorm(x,n*p,sqrt(n*p*(1-p))),4)\\n> normpvaluecc<-round(1-pnorm(x-0.5,n*p,sqrt(n*p*(1-p))),4)\\n> data.frame(x,binompvalue,normpvalue,normpvaluecc)\\n\\nx binompvalue normpvalue normpvaluecc\\n0.4103\\n0.3325\\n0.2613\\n0.1989\\n0.1465\\n0.1042\\n0.0716\\n0.0474\\n0.0303\\n0.0186\\n0.0110\\n0.0063\\n0.0035\\n\\n0.3707\\n0.2959\\n0.2290\\n0.1714\\n0.1241\\n0.0868\\n0.0585\\n0.0381\\n0.0239\\n0.0144\\n0.0084\\n0.0047\\n0.0025\\n\\n0.4155\\n0.3367\\n0.2641\\n0.2001\\n0.1461\\n0.1026\\n0.0691\\n0.0446\\n0.0275\\n0.0162\\n0.0091\\n0.0048\\n0.0024\\n\\n80\\n1\\n81\\n2\\n82\\n3\\n83\\n4\\n84\\n5\\n85\\n6\\n86\\n7\\n87\\n8\\n9\\n88\\n10 89\\n11 90\\n12 91\\n13 92\\n\\nExercise 19.4. Use the central limit theorem to show that the power function, π, for a one-sided level α test with a\\n“greater than” alternative. Using the critical region,\\n\\nshow that the power is\\n\\nwhere Φ is the distribution function for a standard normal and zα is the critical value for an upper critical probability\\nα for the standard normal. Give the corresponding expression for π(p) for the ”less than” alternative.\\n\\n(19.2)\\n\\nC =(cid:40)x;\\n\\nˆp − p0\\n\\n(cid:112)p0(1 − p0)/n ≥ zα(cid:41) ,\\n\\nπ(p) = 1 − Φ(cid:32)zα(cid:115) p0(1 − p0)\\n\\np(1 − p)\\n\\n+\\n\\np0 − p\\n\\n(cid:112)p(1 − p)/n(cid:33)\\n\\n343\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nTo explore the properties of the power function in the case of overwintering of bee hives, we ﬁrst keep the number\\nof hives at n = 112 and considering increasing values p = 0.75, 0.80, 0.85, 0.90 for the alternative to see the power\\nincrease from about 30% to nearly 100%.\\n\\n> n<-112\\n> p0<-0.7\\n> zalpha<-qnorm(0.95)\\n> p<-c(0.75,0.80,0.85,0.90)\\n> power<-1-pnorm(zalpha*sqrt(p0*(1-p0)/(p*(1-p))) + (p0-p)/sqrt(p*(1-p)/n))\\n> data.frame(p,power)\\n\\np\\n\\npower\\n1 0.75 0.3019748\\n2 0.80 0.7767714\\n3 0.85 0.9902226\\n4 0.90 0.9999972\\n\\nPower increases with increasing sample size. Here we ﬁx the alternative at p = 0.8 and choose n from 40 to 240.\\n\\nThe power for these values increases from 38% to more than 97%.\\n\\n> n<-c(1:6)*40\\n> p0<-0.7\\n> zalpha<-qnorm(0.95)\\n> p<-0.8\\n> power<-1-pnorm(zalpha*sqrt(p0*(1-p0)/(p*(1-p))) + (p0-p)/sqrt(p*(1-p)/n))\\n> data.frame(n,power)\\n\\nn\\n\\npower\\n40 0.3808391\\n1\\n2\\n80 0.6374501\\n3 120 0.8035019\\n4 160 0.8993508\\n5 200 0.9506427\\n6 240 0.9766255\\n\\nExercise 19.5. Repeat the determination of power in the example above using the binomial distribution directly.\\nNotice that the values are closer for larger values of n. This is due to the increasing applicability of the central limit\\ntheorem and the decreasing importance of the continuity correction.\\n\\nExample 19.6. For a test of hive survivability over a harsh winter, we have\\n\\nH0 : p ≥ 0.7\\n\\nversus H1 : p < 0.7.\\n\\nIf we have 26 observations, then we are reluctant to use the central limit theorem and appeal directly to the binomial\\ndistribution. If 16 hives survive, then we use the binomial test as follows.\\n\\n> binom.test(16,26,0.7,alternative=c(\"less\"))\\n\\nExact binomial test\\n\\n16 and 26\\n\\ndata:\\nnumber of successes = 16, number of trials = 26, p-value = 0.2295\\nalternative hypothesis: true probability of success is less than 0.7\\n95 percent confidence interval:\\n\\n0.0000000 0.7743001\\n\\n344\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nsample estimates:\\nprobability of success\\n0.6153846\\n\\nand we do not reject for any signiﬁcance level α below 0.2295.\\n\\nExample 19.7. The p-value for the data above is 0.2295. Let’s use the pbinom command to see how the p-value\\ndecreases as the number of surviving hive x decreases from 16 to 10. We can use the command pvalue < alpha\\nto give the outcome for the test. If the p-value is below α, then we reject H0 and R returns true.\\n\\n> x<-16:10\\n> pvalue<- round(pbinom(x,26,0.7),4)\\n> data.frame(x,pvalue,pvalue<0.10,pvalue<0.05,pvalue<0.01)\\n\\nx pvalue pvalue...0.1 pvalue...0.05 pvalue...0.01\\nFALSE\\nFALSE\\nFALSE\\nFALSE\\nTRUE\\nTRUE\\nTRUE\\n\\n1 16 0.2295\\n2 15 0.1253\\n3 14 0.0603\\n4 13 0.0255\\n5 12 0.0094\\n6 11 0.0030\\n7 10 0.0009\\n\\nFALSE\\nFALSE\\nFALSE\\nTRUE\\nTRUE\\nTRUE\\nTRUE\\n\\nFALSE\\nFALSE\\nTRUE\\nTRUE\\nTRUE\\nTRUE\\nTRUE\\n\\nThus, we reject H0 when α = 0.10 for 14 or fewer surviving hives, α = 0.05 for 13 or fewer surviving hives, and\\nα = 0.01 for 12 or fewer surviving hives. Thus, as α decreases, the null hypothesis needs more evidence to reject and\\nthe critical region becomes smaller.\\n\\nExercise 19.8. Use the R command qbinom to compute the critical value for an α = 0.10, 0.05, 0.01 test. Does it\\nmatch the results in the example above.\\n\\n19.2 Likelihood Ratio Tests\\nThe likelihood ratio test is a popular choice for composite hypothesis when Θ0 is a subspace of the whole parameter\\nspace. The rationale for this approach is that the null hypothesis is unlikely to be true if the maximum likelihood on\\nΘ0 is sufﬁciently smaller that the likelihood maximized over Θ, the entire parameter space. In symbols, let ˆθ0 be the\\nparameter value that maximizes the likelihood for θ ∈ Θ0 and ˆθ be the parameter value that maximizes the likelihood\\nfor θ ∈ Θ. Then the likelihood ratio\\n\\n(19.3)\\n\\nΛ(x) =\\n\\nL(ˆθ0|x)\\nL(ˆθ|x)\\n\\n.\\n\\nNote that this ratio is the reciprocal from the version given by the Neyman-Pearson lemma. Thus, the critical region\\nconsists of those values that are below a critical value.\\n\\nThe critical region for an α-level likelihood ratio test is\\n\\n{Λ(x) ≤ λα}\\n\\n(19.4)\\n\\nAs with any α level test, λα is chosen so that\\n\\nPθ{Λ(X) ≤ λα} ≤ α for all θ ∈ Θ0.\\n\\nThis in the end may result in a procedure that many take several steps to develop. First, we must determine the\\nlikelihood L(θ|x) for the parameter θ based on the data x. We have two optimization problems - maximize L(θ|x)\\non the parameter space Θ and on the null hypothesis space Θ0. We evaluate the likelihood at these values and form\\nthe ratio. This generally give us a complex test statistic which we then simplify. We show this in some detail for a\\ntwo-sided test for the mean based on normal data.\\n\\n345\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nExample 19.9. Let Θ = R and consider the two-sided hypothesis\\n\\nH0 : µ = µ0\\n\\nversus H1 : µ (cid:54)= µ0.\\n\\nHere the data are n independent N (µ, σ0) random variables X1 . . . , Xn with known variance σ2\\n\\n0. The parameter\\nspace Θ is one dimensional giving the value µ for the mean. As we have seen before ˆµ = ¯x. Θ0 is the single point\\n{µ0} and so ˆµ0 = µ0. Using the information, we ﬁnd that\\n\\n0(cid:33)n\\nL(ˆµ0|x) =(cid:32) 1\\n(cid:112)2πσ2\\n\\nexp−\\n\\nand\\n\\nΛ(x) = exp−\\n\\nNow notice that\\n\\nThen, critical region (19.4),\\n\\n1\\n2σ2\\n\\n1\\n2σ2\\n0\\n\\n0(cid:33)n\\n(xi − µ0)2, L(ˆµ|x) =(cid:32) 1\\nn(cid:88)i=1\\n(cid:112)2πσ2\\n((xi − µ0)2 − (xi − ¯x)2)(cid:33) = exp−\\n0(cid:32) n(cid:88)i=1\\nσ0/√n(cid:19)2\\n(¯x − µ0)2 =(cid:18) ¯x − µ0\\n≥ −2 ln λα(cid:41)\\n\\n{Λ(x) ≤ λα} =(cid:40)(cid:18) ¯x − µ0\\nσ0/√n(cid:19)2\\n\\n−2 ln Λ(x) =\\n\\nn\\nσ2\\n0\\n\\nn\\n2σ2\\n0\\n\\n.\\n\\nexp−\\n\\n1\\n2σ2\\n0\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2\\n\\n(¯x − µ0)2.\\n\\nBecause ( ¯X − µ0)/(σ0/√n) is a standard normal random variable, −2 ln Λ(X) is the square of a single standard\\n\\nnormal. This is the deﬁning property of a χ-square random variable with 1 degree of freedom.\\n\\nNaturally we can use both\\n\\n(cid:18) ¯x − µ0\\nσ0/√n(cid:19)2\\n\\nand\\n\\n(cid:12)(cid:12)(cid:12)(cid:12)\\n\\n¯x − µ0\\n\\nσ0/√n(cid:12)(cid:12)(cid:12)(cid:12) .\\n\\nas a test statistic. For the ﬁrst, the critical value is just the square of the critical value for the second choice. We have\\nseen the second choice in the section on Composite Hypotheses using the example of a possible invasion of a model\\nbutterﬂy by a mimic.\\n\\nExercise 19.10 (Bernoulli trials). Consider the two-sided hypothesis\\n\\nUse the linear approximation of the logarithm to show that\\n\\nH0 : p = p0\\n\\nversus H1 : p (cid:54)= p0.\\n\\nAgain, this is approximately the square of a standard normal. Thus, we can either use\\n\\n− ln Λ(x) ≈\\n\\n(ˆp − p0)2\\np0(1 − p0)/n\\n\\n.\\n\\n(ˆp − p0)2\\np0(1 − p0)/n\\n\\nor\\n\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\\n\\nˆp − p0\\n\\n(cid:112)p0(1 − p0)/n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\\n\\nfor the test statistic. Under the null hypothesis, this statistic has approximately, respectively, the square and the absolute\\nvalue of a standard normal distribution.\\n\\nReturning to the example on the proportion of hives that survive the winter, for a two-sided test and a 98% conﬁ-\\n\\ndence interval, notice that the output give the χ2 statistic.\\n\\n346\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\n> prop.test(88,112,0.7,alternative=c(\"two.sided\"),conf.level = 0.98)\\n\\n1-sample proportions test with continuity correction\\n\\n88 out of 112, null probability 0.7\\n\\ndata:\\nX-squared = 3.5208, df = 1, p-value = 0.0606\\nalternative hypothesis: true p is not equal to 0.7\\n98 percent confidence interval:\\n\\n0.6785906 0.8652397\\n\\nsample estimates:\\n\\np\\n0.7857143\\n\\nto obtain the interval (0.676, 0.8652).\\n\\nExercise 19.11. Why is 0.0606 the p-value for the two-sided test equal to twice the value of the p-value for the\\ncorresponding one-sided test? Is the test signiﬁcant at the 10& level? 5% level? Explain.\\n\\nExercise 19.12. For the two-sided two-sample α-level proportion test\\n\\nH0 : p1 = p2\\n\\nversus H1 : p1 (cid:54)= p2,\\n\\nbased on n1 Bernoulli trials, x1,1, xi,2, . . . , x1,n1 from the ﬁrst population and, independently, n2 Bernoulli trials,\\nx2,1, xi,2, . . . , x2,n2 from the second, the likelihood ratio test is equivalent to the critical region\\n\\n|z| ≥ zα/2\\n\\n(19.5)\\n\\nwhere\\n\\nz =\\n\\nˆp1 − ˆp2\\n\\n(cid:114)ˆp0(1 − ˆp0)(cid:16) 1\\n\\nn1\\n\\n+ 1\\n\\nn2(cid:17)\\n\\nwith ˆpi, the sample proportion of successes from the observations from population i and ˆp0, the pooled proportion\\n\\nˆp0 =\\n\\n1\\n\\nn1 + n2\\n\\n((x1,1 + ··· + x1,n1) + (x2,1 + ··· + x2,n2)) =\\n\\nn1 ˆp1 + n2 ˆp2\\n\\nn1 + n2\\n\\n.\\n\\n(19.6)\\n\\nBy a variant of the central limit theorem, z has, under the null hypoth-\\n\\nesis, approximately a standard normal random variable.\\n\\nA one-sided two-sample proportion test\\n\\nH0 : p1 ≤ p2\\n\\nversus H1 : p1 ≥ p2,\\n\\n(19.7)\\n\\nalso uses the z-statistic (19.5) provided that the central limit theorem is ap-\\nplicable. One standard rule of thumb is that the both the expected number\\nof success and the expected number of failures in both sets of trails each\\nexceeds 10 for both sets of observations.\\n\\nExercise 19.13. The next winter was consider harsher than the one with\\n88 out of 112 hive surviving. In this more severe winter, we ﬁnd that only\\n64 out of 99 random selecting hives surviving. State an appropriate hy-\\npothesis test and carry out the test, stating the evidence against the null\\nhypothesis. Note that the rule of thumb on sample sizes necessary for the\\nz test is satisﬁed.\\n\\n347\\n\\nFigure 19.1: For the two-sided two-sample α-\\nlevel likelihood ratio test for proportions p1 and\\np2, we maximize the likelihood over Θ0 =\\n{(p1, p2); p1 = p2} (shown as the blue line) and\\nover Θ = [0, 1]×[0, 1], the entire parameter space,\\nshown as the square, and then take the ratio (19.3).\\n\\n00.5100.51p1p2\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nR handles this easily. Because R employs a continuity correction, the p-value is slightly different from the one in\\n\\nthe exercise.\\n\\n> prop.test(c(88,64),c(112,99),alternative=\"greater\")\\n\\n2-sample test for equality of proportions with continuity correction\\n\\nc(88, 64) out of c(112, 99)\\n\\ndata:\\nX-squared = 4.3909, df = 1, p-value = 0.01807\\nalternative hypothesis: greater\\n95 percent confidence interval:\\n\\n0.02818133 1.00000000\\n\\nsample estimates:\\n\\nprop 1\\n\\nprop 2\\n0.7857143 0.6464646\\n\\nPower analyses for two sample tests for proportions can be executed in R using the power.prop.test com-\\nmand. For example, if we want to be able to detect a difference between two proportions p1 = 0.7 and p2 = 0.6 in a\\none-sided test with a signiﬁcance level of α = 0.05 and power 1 − β = 0.8, then we will need a sample of n = 281\\nfrom each group.\\n\\n> power.prop.test(p1=0.70,p2=0.6,sig.level=0.05,power=0.8,\\n\\nalternative = c(\"one.sided\"))\\n\\nTwo-sample comparison of proportions power calculation\\n\\nn = 280.2581\\n\\np1 = 0.7\\np2 = 0.6\\n\\nsig.level = 0.05\\n\\npower = 0.8\\n\\nalternative = one.sided\\n\\nNOTE: n is number in *each* group\\nExercise 19.14. What is the power for 100 observations in a test with signiﬁcance level α = 0.10.\\nExercise 19.15. For the Salk vaccine trial, in the treatment group 56 out of 20000 contracted polio. From the control\\ngroup, 142 out of 20000 contracted polio. Give an appropriate hypothesis test, ﬁnd a p-value for the test, and assess\\nthe evidence against your null hypothesis.\\n\\n19.3 Chi-square Tests\\nThis exact computation for normal data for a two-sided test of the mean shows that the test statistic has a χ2 distribution\\nwith 1 degree of freedom. For the two-sided sample proportion test, we used the central limit theorem to assert that\\nour test statistic is approximately the square of a standard normal random variable, and hence is a χ2 random variable\\nwith one degree of freedom.\\n\\nThese ideas can be extended to the case in which Θ is a d-dimensional parameter space and k of these parameters\\n\\nare, under the null hypothesis, assume to have ﬁxed values. Thus, Θ0 is d − k-dimensional.\\nTheorem 19.16. Whenever the maximum likelihood estimator has an asymptotically normal distribution, let Λ(x) be\\nthe likelihood ratio (19.3) for an d-dimensional parameter space:\\n\\nH0 : θi = ci for all i = 1, . . . , k versus H1 : θ1 (cid:54)= c1 for some i = 1, . . . , k\\n\\n348\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nFigure 19.2: Thomas Morgan’s 1916 drawings of a (left) single and (right) double crossing over event. The places where homologous non-\\nsister chromatids (newly replicated chromosomes) exchange genetic material during chromosomal crossover during meiosis are called chaismata\\n(singular: chiasma)\\n\\nThen under H0, the distributions of\\n\\nconverge to a χ2\\n\\nk distribution as the sample size n → ∞.\\n\\n−2 ln Λn(X)\\n\\nMore generally, if the test is based on a d-dimensional parameter space and Θ0 is deﬁned has by k linear con-\\n\\nstraints, then the we can obtain the test above by a linear change of variables. Thus,\\n\\ndegrees of freedom = dim(Θ) − dim(Θ0).\\n\\nTypically we can ascertain the degrees of freedom by counting free parameters in both Θ and Θ0 and subtracting.\\nExercise 19.17. Use a second order Taylor series for ln L(θ|x) and the asymptotic normality of maximum likelihood\\nestimators to outline the argument for the case d = k = 1,\\n\\nThe basic approach taken for this case extends to the general case. If we expand ln L(c|x) in a Taylor series about\\nthe parameters θ1 = ˆθ1, θ2 = ˆθ2,··· , θk = ˆθk the maximum likelihood estimators, then the ﬁrst order terms in the\\nexpansion of ln L(c|x) vanish. The second order derivatives are the entries of the Fisher information matrix evaluated\\nat the maximum likelihood estimator. These terms converge by the law of large numbers. A multidimensional central\\nlimit theorem applies to the vector of terms √n(ˆθ1 − c1, . . . , ˆθk − ck). The result is the Fisher information matrix\\n\\nand its inverse multiplying to give the identity matrix and resulting the sum of the squares of k approximately normal\\nrandom variables. This is the deﬁnition of a χ2\\nExample 19.18. During meiosis, paired chromosomes experience crossing over events in the formation of gametes.\\nDuring prophase I, the four available chromatids (two from each parent) are in tightly aligned allowing breaks and\\nreattachments of homologous sites (called chiasmata) on two chromatids. (See Figure 19.1.)\\n\\nk distribution.\\n\\nRecombination can occur with a small probability at any location along chromosome. As described in Topic 9 in\\nthe discussion that moved us from Bernoulli trials to a Poisson random variable, the number of crossing over events\\ncan be modeled as Poisson random variable. The mean number of cross overs for a given chromosomal segment is\\n\\n349\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\ncalled its genetic length with Morgans as the unit of measurement. This name is in honor of Thomas Morgan who won\\nthe Nobel Prize in Physiology or Medicine in 1933 for discoveries relating the role the chromosome plays in heredity.\\nWe are now collecting whole genome sequences for trios - an individual along with both parents. Consequently, we\\ncan determine on both the father’s and the mother’s chromosome the number of crossing over events and address the\\nquestion: Are these processes different in the production of sperm and eggs? One simple question is: Are the number\\nof crossing over events different in sperm and in eggs? Using the subscript m for male and f for female, this leads to\\nthe hypothesis\\n\\nH0 : λm = λf\\n\\nversus H1 : λm (cid:54)= λf\\n\\nwhere λm and λf is the parameter in the Poisson random variable that gives the number of crossing over events in the\\nhuman chromosome across all 22 autosomes. (We will not look at the sex chromosomes X and Y in this circumstance.)\\nThe data are nm, nf the number of crossing over events for each parent’s chromosome. Thus, assuming that the\\n\\nrecombination sites are independent on the two parents, the likelihood function is\\n\\nL(λm, λf|nm, nf ) =\\n\\nλnm\\nm\\nnm!\\n\\ne\\n\\n−λm ·\\n\\nnf\\nλ\\nf\\nnf !\\n\\n−λf .\\n\\ne\\n\\nExercise 19.19. Show that the maximum likelihood estimates for the likelihood function above is\\n\\nThus,\\n\\nˆλm = nm and\\n\\nˆλf = nf .\\n\\nL(ˆλm, ˆλf|nm, nf ) =\\n\\nnnm\\nm\\nnm! ·\\n\\nnf\\nn\\nf\\nnf !\\n\\n−(nm+nf ).\\n\\ne\\n\\nUnder the null hypothesis, λm and λf have a common value. Let’s denote this by λ0. Then the likelihood function is\\n\\nL(λ0|nm, nf ) =\\n\\nλnm\\n0\\nnm!\\n\\ne\\n\\n−λ0 ·\\n\\nnf\\nλ\\n0\\nnf !\\n\\n−λ0 =\\n\\ne\\n\\nnm+nf\\nλ\\n0\\nnm!nf !\\n\\n−2λ0.\\n\\ne\\n\\nExercise 19.20. Show that the maximum likelihood estimate for the likelihood function above is\\n\\nˆλ0 =\\n\\nnm + nf\\n\\n2\\n\\n.\\n\\nThus,\\n\\nL(ˆλ0|nm, nf ) =\\n\\n((nm + nf )/2)nm+nf\\n\\nnm!nf !\\n\\n−(nm+nf ).\\ne\\n\\nThe likelihood ratio, after canceling the factorial and exponential factors, is\\n\\nΛ(nm, nf ) =\\n\\nL(ˆλ0|nm, nf )\\n\\nL(ˆλm, ˆλf|nm + nf )\\n\\n=\\n\\n(nm + nf )nm+nf\\nnf\\n2nm+nf nnm\\nf\\n\\nm n\\n\\n.\\n\\nFor the parameter space, d = 2 and k = 1. Our data for two individuals sharing the same parents are nm = 56\\n\\nand nf = 107. Thus,\\n\\n−2 ln Λ(nm, nf ) = −2((nm + nf )(ln(nm + nf ) − ln 2) − nf ln nf − nm ln nm) = 16.228.\\n\\nTo compute the p-value\\n\\n> nm<-56; nf<-107; n<-nm+nf\\n> 1-pchisq(-2*(n*(log(n)-log(2))-nf*log(nf)-nm*log(nm)),1)\\n[1] 5.615274e-05\\n\\nThis very low p-value, 0.0056%, allow us to reject the null hypothesis.\\n\\n350\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nExercise 19.21. A similar set up for gibbon, a primate species whose habitat is much of southeast Asia, has nm = 51\\nand nf = 120. Give the p-value for a likelihood ratio test. In addition, test if the proportion of recombination events\\nis the same in humans and gibbons.\\n\\nExercise 19.22. Consider the two sided hypotheses\\n\\nH0 : λ = λ0\\n\\nversus H1 : λ (cid:54)= λ0\\n\\nbased on n independent observations from an exponential random variable parameter λ. Show that\\n\\n−2 ln Λ(x) = −2n(ln(λ0 ¯x) − (λ0 ¯x − 1)).\\n\\n(19.8)\\n\\nSimulate 20 random variables with λ = 15 and perform the χ2 test with λ0 = 10 and report the p-value.\\n\\nFor λ = 11, 12, 13, 14, and 15, use simulations to estimate the power based on n = 20, 40 and 80 observations\\nand a signiﬁcance level α = 0.05. Use that fact that that Sn, Γ(n, λ) is the sum of n independent Exp(λ) random\\nvariables to compute power. Comment on what you see.\\n\\nFor the sample mean of n independent Exp(λ0) random variables,\\n\\nThus, by the law of large numbers,\\n\\nEλ0\\n\\n¯X =\\n\\n1\\nλ0\\n\\nand Varλ0 ( ¯X) =\\n\\n1\\nλ2\\n0n\\n\\n.\\n\\n¯x ≈\\n\\n1\\nλ0\\n\\nor λ0 ¯x ≈ 1.\\n\\nA second order Taylor series expansion of the logarithm about the point y = 1 yields\\n\\nln y ≈ (y − 1) −\\n\\n1\\n2\\n\\n(y − 1)2\\n\\nand\\n\\nln y − (y − 1) ≈ −\\n\\n1\\n2\\n\\n(y − 1)2.\\n\\nSubstitiuting y = λ0 ¯x into this approximation and using (19.8), we ﬁnd that\\n\\n−2 ln Λ(x) ≈ n(λ0 ¯x − 1)2 =(cid:18) ¯x − 1/λ0\\n1/(λ0√n)(cid:19)2\\n\\n.\\n\\n(19.9)\\n\\nConsequently, −2 ln Λ(x) is approximately equal to the square of the standardized score. By the central limit theorem,\\nthe standardized score is approximately normally distributed. In this way, we also see that −2 ln Λ(x) is approximately\\nthe square of a standard normal random variable, i.e., is approximately χ2\\n\\n1 as promised by Theorem 19.16.\\n\\nExercise 19.23. Use the normal approximation in (19.9) to compute the power for the values of n and λ and λ0 in the\\nprevious exercise.\\n\\n19.4 Answers to Selected Exercises\\n19.1. The critical region takes the form C{r; r ≤ rα} for an appropriate value rα for an α-level test.\\n\\nThe likelihood function for the population, N. is the hypergeometric distribution.\\n\\nRecall that\\n\\n• t be the number captured and tagged,\\n• k be the number in the second capture,\\n\\nL(N|r) = (cid:0)t\\n\\nr(cid:1)(cid:0)N−t\\nk−r(cid:1)\\n(cid:0)N\\nk(cid:1)\\n\\n351\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nIn this case, we show that, for N2 < N1, L(N2|r)/L(N1|r) increases with r.\\nk(cid:1) = (cid:0)N2−t\\n= (cid:0)t\\nr(cid:1)(cid:0)N2−t\\nk−r(cid:1)/(cid:0)N2\\nk(cid:1)\\nk−r(cid:1)(cid:0)N1\\nk(cid:1)\\nk−r(cid:1)(cid:0)N2\\n(cid:0)N1−t\\nr(cid:1)(cid:0)N1−t\\n(cid:0)t\\nk−r(cid:1)/(cid:0)N1\\nk(cid:1)\\n(N2 − t)k−r\\n(N1 − t)k−r ·\\n\\nL(N2|r)\\nL(N1|r)\\n\\n(N1)k\\n(N2)k\\n\\n=\\n\\nIncrease r by 1 to obtain\\n\\nTo see if this increases with r, we look at the ratio of ratios,\\n\\nL(N2|r + 1)\\nL(N1|r + 1)\\n\\n=\\n\\nL(N2|r + 1)\\n\\nL(N1|r + 1)(cid:46) L(N2|r)\\n\\nL(N1|r)\\n\\n(N1)k\\n(N2)k\\n\\n(N2 − t)k−r−1\\n(N1 − t)k−r−1 ·\\n(N2 − t)k−r (cid:46) (N1 − t)k−r−1\\n(N2 − t)k−r−1\\n(N1 − t)k−r\\n> 1\\n\\nN1 − t − k + r − 1\\nN2 − t − k + r − 1\\n\\n=\\n\\n=\\n\\nbecause the denominator is smaller than the numerator and thus, L(N2|r)/L(N1|r) increases with r showing that, by\\nthe Karlin-Rubin theorem, the level test is uniformly most powerful.\\n\\n19.2. The likelihood\\n\\nThus,\\n\\nIf p2 > p1, then\\n\\nL(p|x) = px1+···+xn (1 − p)n−(x1+···+xn)\\n= pn(cid:18) p\\np1(cid:19)n(cid:18) p2(1 − p1)\\np1(1 − p2)(cid:19)n¯x\\n\\n1 − p(cid:19)x1+···+xn\\n= pn(cid:18) p\\n=(cid:18) p2\\n\\n.\\n\\n1 − p(cid:19)n¯x\\n\\n.\\n\\nL(p2|x)\\nL(p1|x)\\np2\\np1\\n\\n> 1\\n\\nand\\n\\n1 − p1\\n1 − p2\\n\\n> 1.\\n\\nand so the product is greater than 1. Consequently, L(p2|x)/L(p1|x) is a monotone increasing function of ¯x.\\n19.4. To ﬁnd π(p), we need to rewrite this expression so that we can create an expression\\n\\nthat is approximately a standard normal under the parameter value p. Beginning with the expression deﬁning the\\ncritical region, we have that\\n\\nz =\\n\\nˆp − p\\n\\n(cid:112)p(1 − p)/n\\n\\nˆp − p0\\n\\n(cid:112)p0(1 − p0)/n ≥ zα\\nˆp − p0 ≥ zα(cid:112)p0(1 − p0)/n\\nˆp − p ≥ zα(cid:112)p0(1 − p0)/n + p0 − p\\n(cid:112)p(1 − p)/n ≥ zα(cid:112)p0(1 − p0)/n\\n(cid:112)p(1 − p)/n\\n(cid:112)p(1 − p)/n ≥ zα(cid:115) p0(1 − p0)\\n\\np(1 − p)\\n\\nˆp − p\\n\\nˆp − p\\n\\n(cid:112)p(1 − p)/n\\n(cid:112)p(1 − p)/n\\n\\np0 − p\\n\\np0 − p\\n\\n+\\n\\n+\\n\\nz =\\n\\n352\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nTake the probability to see that π(p) has the expression in (19.2).\\n\\nFor the “less than” alternative, the critical regions is\\n\\nUsing similar calculations, we have\\n\\nC =(cid:40)x;\\n\\nˆp − p0\\n\\n(cid:112)p0(1 − p0)/n ≤ −zα(cid:41) ,\\n\\nand\\n\\nz =\\n\\nˆp − p0\\n\\nˆp − p\\n\\n(cid:112)p0(1 − p0)/n ≤ −zα\\n(cid:112)p(1 − p)/n ≤ −zα(cid:115) p0(1 − p0)\\nπ(p) = Φ(cid:32)−zα(cid:115) p0(1 − p0)\\n\\np(1 − p)\\n\\n+\\n\\np(1 − p)\\n\\np0 − p\\n\\n+\\n\\n(cid:112)p(1 − p)/n\\n(cid:112)p(1 − p)/n(cid:33) .\\n\\np0 − p\\n\\n19.6. Here is the R output for the two examples. First with a ﬁxed number of observations n and varying probability\\nof success p.\\n\\n> n<-112;p0<-0.7;alpha<-0.05\\n> p<- c(0.75,0.80,0.85,0.90)\\n> qbinom(1-alpha,n,p0) #This gives the critical value for rejection of the\\n\\nnull hypothesis.\\n\\n[1] 86\\n> power<- 1-pbinom(qbinom(1-alpha,n,p0),n,p)\\n> data.frame(p,power)\\n\\np\\n\\npower\\n1 0.75 0.2972519\\n2 0.80 0.7713371\\n3 0.85 0.9859646\\n4 0.90 0.9999641\\n\\nNow with a varying number of observations n and a ﬁxed probability of success p for the alternative.\\n\\n> n<-c(1:6)*40;p<-0.8\\n> power<- 1-pbinom(qbinom(1-alpha,n,p0),n,p)\\n> data.frame(n,power)\\n\\nn\\n\\npower\\n1\\n40 0.2858914\\n2\\n80 0.5663745\\n3 120 0.7902112\\n4 160 0.8986082\\n5 200 0.9309691\\n6 240 0.9656983\\n\\n19.7. For the three signiﬁcance levels,\\n\\n> alpha<-c(0.10,0.05,0.01)\\n\\nwe have the critical values\\n\\n353\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\n> data.frame(alpha,qbinom(alpha,26,0.7))\\n\\nalpha qbinom.alpha..26..0.7.\\n15\\n14\\n13\\n\\n0.10\\n0.05\\n0.01\\n\\n1\\n2\\n3\\n\\n19.10. The likelihood is\\n\\nL(p|x) = (1 − p)n−(x1+···+xn)px1+···+xn .\\n\\nUsing the deﬁnition of the likelihood ratio, we ﬁnd that, under the null hypothesis, ˆp0 = p0 and ˆp, the sample\\nproportion, is the maximum likelihood estimator. Thus,\\n\\nΛ(x) =\\n\\n(1 − p0)n(1− ˆp)pn ˆp\\n(1 − ˆp)n(1− ˆp) ˆpn ˆp .\\n\\n0\\n\\nLet’s repeat the strategy that we used for normal data in the previous example:\\n\\n− ln Λ(x) = n((1 − ˆp)(ln(1 − ˆp) − ln(1 − p0)) + ˆp(ln ˆp − ln p0)).\\n\\nNext, let’s replace the logarithms with their linear approximation:\\n\\nln(1 − ˆp) − ln(1 − p0)) ≈ −\\n\\nˆp − p0\\n1 − p0\\n\\nln ˆp − ln p0 ≈\\n\\nˆp − p0\\np0\\n\\n.\\n\\nThen,\\n\\n− ln Λ(x) = n((cid:18)(1 − ˆp)(−\\n\\nˆp − p0\\n1 − p0\\n\\n) + ˆp(\\n\\n= n(ˆp − p0)(cid:18) ˆp − p0\\n\\np0(1 − p0)(cid:19) =\\n\\n)(cid:19) = n(ˆp − p0)(cid:18)−\\n\\nˆp − p0\\np0\\n(ˆp − p0)2\\np0(1 − p0)/n\\n\\n1 − ˆp\\n1 − p0\\n\\n+\\n\\nˆp\\n\\np0(cid:19)\\n\\n19.11. For a one-side test with alternative H1 : p > p0, the p-value\\nis the area under the standard normal density above the z-score of the\\ndata. For the two-sided test, the p-value is the area under the standard\\nnormal density farther from zero than the z-score of the data. This\\nis the sum of the area under the curve above this z-score and the area\\nunder the curve below the negative of the z-score. Because the standard\\nnormal density is symmetric about 0, these two areas are equal. FIgure\\n19.2 demonstrates the p-value as the area of the two regions under the\\ndensity curves outside the black vertical lines.\\n\\nThe test is signiﬁcant at the 10% level because the p-value is be-\\nlow 0.10. Correspondingly, the test is not signiﬁcant at the 5% level\\nbecause the p-value is above 0.05. These values are indicated by the\\narea under the density outside the inner (for10%) or outer (for 5%) red\\nvertical lines.\\n\\n19.12. For ni Bernoulli trials, xi = (xi,1, xi,2, . . . , xi,ni), i = 1, 2, we\\nhave the likelihood\\n\\nFigure 19.3: The critical values and z-test statistic\\nvalue used in Exercise 19.9.\\n\\nL(p1, p2|x1, x2) = p\\n= p\\n1\\n\\n(1 − p1)1−x1,1 ··· p\\n\\nx1,1\\n1\\n(x1,1+···+x1,n1 )\\n\\nx1,n1\\n1\\n\\n(1 − p1)1−x1,n1 · p\\n\\nx2,1\\n2\\n\\n(1 − p2)1−x2,1 ··· p\\n\\n(x2,1+···+x2,n2 )\\n\\nx2,n2\\n2\\n\\n(1 − p2)1−x2,n2\\n\\n(1 − p1)n1−(x1,1+···x1,n1 )p\\n\\n2\\n\\n(1 − p2)n2−(x2,1+···x2,n2 )\\n\\n354\\n\\n-3-2-101230.00.10.20.30.4xdnorm(x)-3-2-101230.00.10.20.30.4x-3-2-101230.00.10.20.30.4x-3-2-101230.00.10.20.30.4x-3-2-101230.00.10.20.30.4x-3-2-101230.00.10.20.30.4x-3-2-101230.00.10.20.30.4x-3-2-101230.00.10.20.30.4x\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nTo ﬁnd the maximum likelihood estimator, take logarithms and derivatives with respect to p1 and p2 to obtain\\n\\nˆp1 =\\n\\n1\\nn1\\n\\n(x1,1 + ··· + x1,n1)\\n\\nand\\n\\nˆp2 =\\n\\n1\\nn2\\n\\n(x2,1 + ··· + x2,n2).\\n\\nThen,\\n\\nL(ˆp1, ˆp2|x1, x2) = ˆpn1 ˆp1\\n\\n1\\n\\n(1 − ˆp1)n1(1− ˆp1) ˆpn2 ˆp2\\n\\n2\\n\\n(1 − ˆp2)n2(1− ˆp2)\\n\\nUnder the null hypothesis, p1 = p2. We set this equal to p0 to write the likelihood\\n(x1,1+···+x1,n1 )\\n(1 − p0)n1−(x1,1+···x1,n1 )\\nL(p0|x1, x2) = p\\n(x2,1+···+x2,n2 )\\n(1 − p0)n2−(x2,1+···x2,n2 )\\n·p\\n(x1,1+···+x1,n1 )+(x2,1+···+x2,n2 )\\n= p\\n0\\n·(1 − p0)n1−(x1,1+···x1,n1 )+n2−(x2,1+···x2,n2 )\\n= pn1 ˆp1+n2 ˆp2\\n\\n(1 − p0)n(1− ˆp1)+n2−(1− ˆp2)\\n\\n0\\n\\n0\\n\\n0\\n\\nAgain, take logarithms and derivatives with respect to p0 to obtain ˆp0 in equation (19.6), the proportion obtained\\n\\nby pooling the data. Here,\\n\\nL(ˆp0|x1, x2) = ˆpn1 ˆp1\\n\\n0\\n\\n(1 − ˆp0)n1(1− ˆp1) ˆpn2 ˆp2\\n\\n0\\n\\n(1 − ˆp0)n2(1− ˆp2)\\n\\nThus, the likelihood ratio,\\n\\nΛ(x1, x2) =\\n\\nˆpn1 ˆp1\\n0\\nˆpn1 ˆp1\\n1\\n\\n(1 − ˆp0)n1(1− ˆp1) ˆpn2 ˆp2\\n(1 − ˆp1)n1(1− ˆp1) ˆpn2 ˆp2\\n\\n0\\n\\n2\\n\\n(1 − ˆp0)n2(1− ˆp2)\\n(1 − ˆp2)n2(1− ˆp2)\\n\\n.\\n\\nAgain, replace the logarithms with their linear approximation:\\n\\nln(1 − ˆpi) − ln(1 − ˆp0)) ≈ −\\n\\nˆpi − ˆp0\\n1 − ˆp0\\n\\nln ˆpi − ln ˆp0 ≈\\n\\nˆpi − ˆp0\\n\\nˆp0\\n\\n.\\n\\n− ln Λ(x1, x2) = n1 ˆp1(ln ˆp1 − ln ˆp0) + n1(1 − ˆp1)(ln(1 − ˆp1) − ln(1 − ˆp0))\\n+n2 ˆp2(ln ˆp2 − ln ˆp0) + n2(1 − ˆp2)(ln(1 − ˆp2) − ln(1 − ˆp0))\\n\\n≈ n1 ˆp1(cid:18) ˆp1 − ˆp0\\n= n1(ˆp1 − ˆp0)(cid:18) ˆp1\\n= n1(ˆp1 − ˆp0)(cid:18) ˆp1 − ˆp0\\n\\nˆp0 (cid:19) − n2(1 − ˆp2)(cid:18) ˆp2 − ˆp0\\n1 − ˆp0 (cid:19) + n2 ˆp2(cid:18) ˆp2 − ˆp0\\n1 − ˆp0 (cid:19)\\nˆp0 (cid:19) − n1(1 − ˆp1)(cid:18) ˆp1 − ˆp0\\n1 − ˆp0(cid:19)\\nˆp0(1 − ˆp0)(cid:19) + n2(ˆp2 − ˆp0)(cid:18) ˆp2 − ˆp0\\nˆp0(1 − ˆp0)(cid:19)\\n\\n1 − ˆp0(cid:19) + n1(ˆp2 − ˆp0)(cid:18) ˆp2\\n\\n1 − ˆp2\\n\\n1 − ˆp1\\n\\nˆp0 −\\n\\nˆp0 −\\n\\nn1(ˆp1 − ˆp0)2 + n2(ˆp2 − ˆp0)2\\n\\nˆp0(1 − ˆp0)\\n\\n=\\n\\nNow note that\\n\\nn1(ˆp1 − ˆp0)2 = n1(cid:18) (n1 + n2)ˆp1 − (n1 ˆp1 + n2 ˆp2)\\n\\nn1 + n2\\n\\n(cid:19)2\\n\\n= n1n2\\n\\n2(cid:18) ˆp1 − ˆp2\\nn1 + n2(cid:19)2\\n\\n.\\n\\n355\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nPerform a similar computation for the second term\\n\\n− ln Λ(x1, x2) ≈\\n\\n=\\n\\n(n1n2\\n\\n2 + n2n2\\n\\n1)((ˆp1 − ˆp2)/(n1 + n2))2\\nˆp0(1 − ˆp0)\\n\\nn1n2(ˆp1 − ˆp2)2/(n1 + n2)\\n\\n=\\n\\nˆp0(1 − ˆp0)\\n\\n2\\n\\n(ˆp1 − ˆp2)2\\n\\nˆp0(1 − ˆp0)(cid:16) 1\\n\\nn1\\n\\n+ 1\\n\\nˆp1 − ˆp2\\n\\n(cid:114)ˆp0(1 − ˆp0)(cid:16) 1\\n\\nn1\\n\\n+ 1\\n\\nn2(cid:17)\\n\\nn2(cid:17) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\\n\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\\n\\n19.13. Beginning with the population parameters,\\n\\n• p1 is proportion of all hives that would have survived the ﬁrst, less harsh, winter, and\\n• p2 is proportion of all hives that would have survived the second, harsher, winter.\\n\\nThen we can write the hypothesis as (19.7). The sample proportions for each group and the pooled proportion are,\\nrespectively,\\n\\nˆp1 =\\n\\n= 0.7867,\\n\\nˆp2 =\\n\\n= 0.6598,\\n\\nˆp0 =\\n\\n= 0.7204.\\n\\n88\\n112\\n\\n64\\n99\\n\\n88 + 64\\n112 + 99\\n\\nThus,\\n\\nz =\\n\\nThe p-value\\n\\n> 1-pnorm(2.249)\\n[1] 0.01225625\\n\\nˆp1 − ˆp2\\n\\n(cid:114)ˆp0(1 − ˆp0)(cid:16) 1\\n\\nn1\\n\\n+ 1\\n\\nn2(cid:17) =\\n\\n0.7867 − 0.6598\\n\\n(cid:113)0.7204(1 − 0.7204)(cid:0) 1\\n\\n112 + 1\\n\\n99(cid:1) = 2.249\\n\\nia sufﬁciently law to say that we have moderate evidence against the null hypothesis and say that the harsher winter\\nreduced the proportion of surviving bee hives.\\n19.14. We see that the power 1 − β = 0.580.\\n> power.prop.test(n=100,p1=0.70,p2=0.6,sig.level=0.10,alternative = c(\"one.sided\"))\\n\\nTwo-sample comparison of proportions power calculation\\n\\nn = 100\\np1 = 0.7\\np2 = 0.6\\nsig.level = 0.1\\n\\npower = 0.5800652\\nalternative = one.sided\\n\\nNOTE: n is number in *each* group\\n\\n19.14. If pc is the proportion in the population that have the control and contract polio and pt is the proportion in the\\npopulation that have the treatment and contract polio. Then, we have the hypothesis test, We want to show that the\\nvaccine reduces the rate of polio infection. thus,\\n\\nH0 : pt ≥ pc\\n\\nversus H1 : pt < pc.\\n\\n356\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\n> prop.test(c(56,142),c(200000,200000),alternative=c(\"less\"))\\n\\n2-sample test for equality of proportions with continuity\\ncorrection\\n\\nc(56, 142) out of c(2e+05, 2e+05)\\n\\ndata:\\nX-squared = 36.508, df = 1, p-value = 7.602e-10\\nalternative hypothesis: less\\n95 percent confidence interval:\\n\\n-1.0000000000 -0.0003093083\\n\\nsample estimates:\\n\\nprop 1 prop 2\\n0.00028 0.00071\\n\\nWith the very low p-value of 7.6 × 10−10, we can reject H0 and say that the vaccine reduces the rate of polio,\\n\\n19.17. For the case in which d = k = 1. Then, for n observations, and maximum likelihood estimator ˆθ,\\n\\n√n(c − ˆθ)\\n\\nconverges in distribution to a normal random variable with variance 1/I(c), the reciprocal of the Fisher information.\\nThus,\\n\\n=\\n\\nc − ˆθ\\n1/(cid:112)nI(c))\\n\\n√n(c − ˆθ)\\n1/(cid:112)I(c))\\n\\nhas approximately a standard normal distribution and its square\\nn(c − ˆθ)2\\n1/I(c)\\n\\n(19.10)\\n\\nhas approximately a χ2\\n\\n1 distribution.\\n\\nWe next apply Taylor’s theorem to obtain the quadratic approximation\\n\\n−2 ln Λ1(X) = −2 ln L(c|X) + 2 ln L(ˆθ|X) ≈ −2(c − ˆθ)\\n\\nd\\ndθ\\n\\nln L(ˆθ1|X) − (c − ˆθ)2 d2\\n\\ndθ2 ln L(ˆθ|X)\\n\\n= −n(c − ˆθ)2 d2\\n\\ndθ2 ln L(X|c)\\n\\nThe linear term vanishes because d ln L(ˆθ|X)/dθ = 0 at ˆθ, the maximum likelihood estimator. Recall that the\\n\\nlikelihood\\n\\nand using the properties of the logarithm and the derivative, we see that\\n\\nL(X|θ) = fX (x1|θ)··· fX (xn|θ)\\n\\nd2\\ndθ2 ln L(X|θ) =\\n\\nd2\\ndθ2 ln fX (x1|θ) + ··· +\\n\\nd2\\ndθ2 ln fX (xn|θ)\\n\\nWe now apply the law of large numbers to see that for large n\\n\\nThus,\\n\\n1\\nn\\n\\nd2\\ndθ2 ln L(c|X) =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\nd2\\n\\ndθ2 fX (Xi|c) ≈ Eθ(cid:20) d2\\n\\ndθ2 fX (X1|c)(cid:21) = −I(c).\\n\\n−2 ln Λ1(X) ≈ −n(c − ˆθ)2 × (−I(c)) =\\n\\nn(c − ˆθ)2\\n1/I(c)\\n\\n357\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nwhich in (19.10) we have noted has approximately a χ2\\n19.19. Taking logarithms, we ﬁnd\\n\\n1 distribution.\\n\\nln L(λm, λf|nm, nf ) = nm ln λ − ln(nm!) − λm + nf ln λ − ln(nf !) − λf .\\n\\nThe derivative with respect to λm is\\n\\n∂\\n\\n∂λm\\n\\nln L(λ|nm, nf ) =\\n\\nnm\\nλm − 1.\\n\\nNow set this equal to 0 and solve for λm. Because the second derivative with respect to λm is negative, this is a\\nmaximum. A nearly identical computation can be used to ﬁind ˆλf .\\n19.20. Taking logarithms, we ﬁnd the score function,\\n\\nThe derivative with respect to λ is\\n\\nln L(λ|nm, nf ) = (nm + nf ) ln λ − ln(nm!nf !) − 2λ.\\n\\n∂\\n∂λ\\n\\nln L(λ|nm, nf ) =\\n\\nnm + nf\\n\\nλ\\n\\n− 2.\\n\\nNow set this equal to 0 and solve for λ. Because the second derivative with respect to λ is negative, this is a maximum.\\n\\n19.21. For the hypothesis on the number of crossing over events we have the test statistic and p-value\\n> nm<-51; nf<-120; n<-nm+nf\\n> 1-pchisq(-2*(n*(log(n)-log(2))-nf*log(nf)-nm*log(nm)),1)\\n[1] 8.664093e-08\\n\\nThis is also a very low p-value and we can reject the hypothesis of equal rtes of crossing over events.\\n\\nThe second question is a two-sample two-sided proportion test. Here is the R output.\\n\\n> prop.test(c(56,51),c(163,171))\\n\\n2-sample test for equality of proportions with continuity correction\\n\\nc(56, 51) out of c(163, 171)\\n\\ndata:\\nX-squared = 0.5926, df = 1, p-value = 0.4414\\nalternative hypothesis: two.sided\\n95 percent confidence interval:\\n\\n-0.06076261 0.15138795\\n\\nsample estimates:\\n\\nprop 1\\n\\nprop 2\\n0.3435583 0.2982456\\n\\nThis gives a p-value of 44%, much too high to reject a hypothesis of equal proportion of crossing over events derived\\nfrom the females in the two species, human and gibbon.\\n19.22. The likelihood function for observations x = (x1. . . . , xn) is\\n\\nL(λ|x) = λn exp(−λ\\n\\nxi) = λn exp(−λn¯x).\\n\\nWe have seem that the maximum likelihood estimate ˆλ = 1/¯x. Thus the likelihood ratio,\\n\\nΛ(x) =\\n\\nL(λ0|x)\\nL(ˆλ|x)\\n\\n=\\n\\nλn\\n0 exp(−λ0n¯x)\\n\\n(1/¯xn) exp(−(1/¯x)n¯x)\\n\\n= (λ0 ¯x)n exp(−λ0n¯x)\\nexp(−n)\\n\\n= (λ0 ¯x)n exp(−n(λ0 ¯x − 1)).\\n\\nn(cid:88)i=1\\n\\n358\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\nand\\n\\n−2 ln Λ(x) = −2n(ln λ0 ¯x − (λ0 ¯x − 1)).\\n\\n> x<-rexp(20,15)\\n> (xbar<-mean(x))\\n[1] 0.06126583\\n> -2*20*(log(10*xbar)-(10*xbar-1))\\n[1] 4.509281\\n> 1-pchisq(4.509281,1)\\n[1] 0.03371141\\n\\n> x<-rexp(20,15)\\n> (xbar<-mean(x))\\n[1] 0.07144714\\n> -2*20*(log(20*xbar)-(20*xbar-1))\\n[1] 2.880317\\n> 1-pchisq(2.880317,1)\\n[1] 0.08966837\\n\\nFor the two simulations, the p-values are 0.0337 and 0.0897.\\n\\nFor the simulation, we also take advantage of the fact that Sn, Γ(n, λ) is the sum of n independent Exp(λ) random\\nvariables. So, we simulation Sn/n 10,000 times for each value of λ and n and ﬁnd the proportion of times that we\\nreject λ0 = 10 using the χ2 statistic.\\n\\n> lambda0<-10;N<-10000; lambda<-11:15\\n> n<-20;power20<-rep(0,5)\\n> for (i in 1:5){xbar<-rgamma(N,n,lambda[i])/n;\\n\\nchisqstat<--2*n*(log(lambda0*xbar)-(lambda0*xbar-1));\\npvalue<-1-pchisq(chisqstat,1);power20[i]<-length(pvalue[pvalue<0.05])/N}\\n\\n> n<-40;power40<-rep(0,5)\\n> for (i in 1:5){xbar<-rgamma(N,n,lambda[i])/n;\\n\\nchisqstat<--2*n*(log(lambda0*xbar)-(lambda0*xbar-1));\\npvalue<-1-pchisq(chisqstat,1);power40[i]<-length(pvalue[pvalue<0.05])/N}\\n\\n> n<-80;power80<-rep(0,5)\\n> for (i in 1:5){xbar<-rgamma(N,n,lambda[i])/n;\\n\\nchisqstat<--2*n*(log(lambda0*xbar)-(lambda0*xbar-1));\\npvalue<-1-pchisq(chisqstat,1);power80[i]<-length(pvalue[pvalue<0.05])/N}\\n\\n> data.frame(lambda,power20,power40,power80)\\n\\nlambda power20 power40 power80\\n11 0.0692 0.0932 0.1358\\n12 0.1187 0.2017 0.3646\\n13 0.2021 0.3601 0.6385\\n14 0.3100 0.5378 0.8508\\n15 0.4093 0.7187 0.9507\\n\\n1\\n2\\n3\\n4\\n5\\n\\nWe can use the Γ(n, λ) distribution as a test statistic to compute power directly.\\n\\n> n<-20;upper<-qgamma(0.975,n,lambda0); lower<-qgamma(0.025,n,lambda0)\\n> power20<-1-pgamma(upper,n,lambda)+pgamma(lower,n,lambda)\\n> n<-40;upper<-qgamma(0.975,n,lambda0); lower<-qgamma(0.025,n,lambda0)\\n> power40<-1-pgamma(upper,n,lambda)+pgamma(lower,n,lambda)\\n> n<-80;upper<-qgamma(0.975,n,lambda0); lower<-qgamma(0.025,n,lambda0)\\n\\n359\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nExtensions on the Likelihood Ratio\\n\\n> power80<-1-pgamma(upper,n,lambda)+pgamma(lower,n,lambda)\\n> data.frame(lambda,power20,power40,power80)\\n\\nlambda\\n\\npower40\\n\\npower20\\n\\npower80\\n11 0.06283839 0.0832438 0.1254101\\n12 0.10841061 0.1856479 0.3434286\\n13 0.17993890 0.3413916 0.6217434\\n14 0.27219452 0.5214876 0.8383838\\n15 0.37812771 0.6897081 0.9489048\\n\\n1\\n2\\n3\\n4\\n5\\n\\nNotice that the power increases with n and with distance !λ − λ0| from the value under the null hypothesis.\\n\\n19.23, We ﬁrst ﬁnd the upper and lower rejection regions for ¯X, here approximated as a N (1/λ0, 1/√λ0n) and then\\ndetermine the probability of rejection when ¯X is N (1/λ, 1/√λn)\\n\\n> n<-20\\n> upper<-qnorm(0.975,1/lambda0,1/(lambda0*sqrt(n)))\\n> lower<-qnorm(0.025,1/lambda0,1/(lambda0*sqrt(n)))\\n> power20<-1-pnorm(upper,1/lambda,1/(lambda*sqrt(n)))\\n\\n+pnorm(lower,1/lambda,1/(lambda*sqrt(n)))\\n\\n> n<-40\\n> upper<-qnorm(0.975,1/lambda0,1/(lambda0*sqrt(n)))\\n> lower<-qnorm(0.025,1/lambda0,1/(lambda0*sqrt(n)))\\n> power40<-1-pnorm(upper,1/lambda,1/(lambda*sqrt(n)))\\n\\n+pnorm(lower,1/lambda,1/(lambda*sqrt(n)))\\n\\n> n<-80\\n> upper<-qnorm(0.975,1/lambda0,1/(lambda0*sqrt(n)))\\n> lower<-qnorm(0.025,1/lambda0,1/(lambda0*sqrt(n)))\\n> power80<-1-pnorm(upper,1/lambda,1/(lambda*sqrt(n)))\\n\\n+pnorm(lower,1/lambda,1/(lambda*sqrt(n)))\\n> data.frame(lambda,power20,power40,power80)\\n\\nlambda\\n\\npower40\\n\\npower20\\n\\npower80\\n11 0.04836719 0.06646455 0.1047011\\n12 0.07306953 0.13865740 0.2866999\\n13 0.11389874 0.25766108 0.5538240\\n14 0.16976769 0.41522390 0.7977917\\n15 0.24075449 0.58797216 0.9372622\\n\\n1\\n2\\n3\\n4\\n5\\n\\nThese values are lower than that found in the simulation. The normal approximation removes the skewness found\\nin the exponential distribution. This results in rejection occurring less frequently using the normal approximation and\\nthus lower power. Notice that as n increases, ¯X becomes less skewed and so power computations more closely agree.\\n\\n360\\n\\n\\x0cTopic 20\\n\\nt Procedures\\n\\nA curve has been found representing the frequency distribution of values of the means of such samples,\\nwhen these values are measured from the mean of the population in terms of the standard deviation of the\\nsample. . . . - William Sealy Gosset. 1908, The Probable Error of a Mean, Biometrika\\n\\nThe z-score is\\n\\nz =\\n\\n¯x − µ\\nσ/√n\\n\\n.\\n\\ntaken under the assumption that the population standard deviation is known.\\n\\nIf we are forced to replace the unknown σ2 with its unbiased estimator s2, then the statistic is known as t:\\n\\nt =\\n\\n¯x − µ\\ns/√n\\n\\n.\\n\\nThe term s/√n which estimates the standard deviation of the sample mean is called the standard error.\\n\\nWe have previously noted that for independent normal random variables the distribution of the t statistic can\\nbe determined exactly. Because we approximate σ with s, the t-statistic has a higher level of uncertainty than the\\ncorresponding z-statistic. This uncertainty decreases with n, the number of observations. Thus, when using the t\\ndistribution to construct a conﬁdence interval for the population mean µ, we saw that the margin of error decreased\\nas the number of observations increased. Typically, we do not use the number of observations n to describe this but\\nrather degrees of freedom n − 1 to match the division by n − 1 in the computation of the sample variance, s2.\\nWe now turn to using the t-statistic as a test statistic for hypothesis tests of the population mean. As with several\\nother procedures we have seen, the two-sided t test is a likelihood ratio test. We will save showing this result into the\\nlast section and instead focus on the applications of this widely used set of procedures.\\n\\n20.1 Guidelines for Using the t Procedures\\n\\n• Except in the case of small samples, the assumption that the data are a simple random sample from the population\\n\\nof interest is more important that the population distribution is normal.\\n\\n• For sample sizes less than 15, use t procedures if the data are close to normal.\\n• For sample sizes at least 15 use t procedures except in the presence of outliers or strong skewness.\\n• The t procedures can be used even for clearly skewed distributions when the sample size is large, typically over\\n\\n40 observations.\\n\\nThese criteria are designed to ensure that ¯x is a sample from a nearly normal distribution. When these guidelines\\nfail to be satisﬁed, then we can turn to alternatives that are not based on the central limit theorem, but rather use the\\nrankings of the data. These alternatives, the Mann-Whitney or Wilcoxon rank sum test and the Wilcoxon signed-ranked\\ntest, are discussed at the end of this topic.\\n\\n361\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n20.2 One Sample t Tests\\nWe will later explain that the likelihood ratio test for the two sided hypothesis\\nversus H1 : µ (cid:54)= µ0,\\n\\nH0 : µ = µ0\\n\\nbased on independent normal observations X1. . . . , Xn with unknown mean µ and unknown variance σ2 is a t-test.\\n\\nSo, compute the t statistic T (x) from the data x. Then, the critical region\\n\\nC = {|T (x)| > tn−1,α/2}.\\n\\nwhere tn−1,α/2 is the upper α/2 tail probability of the t distribution with n − 1 degrees of freedom.\\nExample 20.1. Radon is a radioactive, colorless, odorless, tasteless noble gas, occurring naturally as the decay\\nproduct of uranium. It is one of the densest substances that remains a gas under normal conditions.\\n\\nRadon is responsible for the majority of the public exposure to ionizing radiation and is the most variable from\\nlocation to location. Radon gas from natural sources can accumulate in buildings, especially in conﬁned areas such as\\nattics, and basements. Epidemiological evidence shows a clear link between breathing high concentrations of radon\\nand incidence of lung cancer. According to the United States Environmental Protection Agency, radon is the second\\nmost frequent cause of lung cancer, after cigarette smoking, causing 21,000 lung cancer deaths per year in the United\\nStates.\\n\\nTo check the reliability of radon detector, a university placed 12 detectors in a chamber having 105 picocuries of\\n\\nradon. (1 picocurie is 3.7 × 10−2 decays per second. This is roughly the activity of 1 picogram of the radium 226.)\\n\\nThe two-sided hypothesis\\n\\nwhere µ is the actual amount of radon radiation. In other words, we are checking to see if the detector is biased either\\nupward or downward.\\n\\nH0 : µ = 105 versus H1 : µ (cid:54)= 105,\\n\\nThe detector readings were:\\n\\n91.9\\n\\n95.0\\nUsing R, we ﬁnd for an α = 0.05 level signiﬁcance test:\\n\\n97.8 111.4 122.3\\n\\n105.4\\n\\n103.8\\n\\n99.6\\n\\n96.6\\n\\n119.3\\n\\n104.8\\n\\n101.7\\n\\n> radon<-c(91.9,97.8,111.4,122.3,105.4,95.0,103.8,99.6,96.6,119.3,104.8,101.7)\\n> hist(radon)\\n> mean(radon)\\n[1] 104.1333\\n> sd(radon)\\n[1] 9.39742\\n> length(radon)\\n[1] 12\\n> (tstar<-qt(0.975,11))\\n[1] 2.200985\\n\\nThus, the t-statistic is\\n\\nt =\\n\\n105 − 104.1333\\n9.39742/√12\\n\\n= −0.3195.\\n\\nThus, for a 5% signiﬁcance test, |t| < 2.200985, the critical value and we fail to reject H0. R handles this procedure\\neasily.\\n\\n> t.test(radon,alternative=c(\"two.sided\"),mu=105)\\n\\nOne Sample t-test\\n\\n362\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nradon\\n\\ndata:\\nt = -0.3195, df = 11, p-value = 0.7554\\nalternative hypothesis: true mean is not equal to 105\\n95 percent confidence interval:\\n\\n98.1625 110.1042\\n\\nsample estimates:\\nmean of x\\n104.1333\\n\\nThe output also gives the 95% conﬁdence interval\\n\\nThe power is the probability of rejecting when the parameter value is µ\\n\\ns\\n√n\\n\\n¯x ±\\n\\nt0.025,11.\\n\\nTo determine the power curve, we begin with the following exercise.\\n\\nπ(µ) = Pµ(cid:26)(cid:12)(cid:12)(cid:12)(cid:12)\\n\\n¯X − µ0\\n\\ns/√n (cid:12)(cid:12)(cid:12)(cid:12) ≥ tn−1,α/2(cid:27)\\n\\nExercise 20.2. If the observations X1, . . . , Xn are independent normal random variables with mean µ then\\n\\n˜T =\\n\\n¯X − µ0\\ns/√n\\n\\nhas a t distribution with n − 1 degrees of freedom and non-centrality parameter\\n\\na =\\n\\nµ − µ0\\nσ/√n\\n\\n.\\n\\nThus, the power function\\n\\nπ(µ) = Pµ(cid:110)| ˜T| ≥ tn−1,α/2(cid:111)\\n= 1 − Pµ(cid:110)| ˜T| < tn−1,α/2(cid:111)\\n= 1 − Pµ(cid:110)−tn−1,α/2 < ˜T < tn−1,α/2(cid:111)\\n\\nThus, we use the qt command to set the critical values\\ntn−1,α/2 and the pt command to ﬁnd the power using the ap-\\npropriate non-centrality parameter. This same function can be\\nused to construct the receiver operating characteristic, deter-\\nmine sample size to achieve disired type I and type II errors,\\nand to look at power as a function of the number of samples.\\nIn this regards, note that the non-centrality parameter increases\\nwith the number of observations and consequently power in-\\ncreases.\\n\\nReturning to the example of the radon detector, by design,\\nπ(µ0) = α, the signiﬁcance level. We estimate a by replacing\\nσ with the sample standard deviation s and, as an example,\\nestimate the power against an alternative of a change in ∆ =\\nµ − µ0 = 5 picocuries is\\n\\n363\\n\\nFigure 20.1: I black, t density with 11 degrees of freedom. Red\\nvertical lines at tn−1,α/2 = 2.2010 are the critical values for a\\ntwo-sided test with α = 0.05, In blue, t density with 11 degrees of\\nfreedom and non-centrality parameter a = 1.8431. The power is\\nthe area outside the red lines under the blue density function. The\\npower.t.test command considers only the larger area on the\\nright side.\\n\\n-4-202460.00.10.20.30.4tdensity-4-202460.00.10.20.30.4t-4-202460.00.10.20.30.4c(qt(0.025, 11), qt(0.025, 11))-4-202460.00.10.20.30.4c(qt(0.975, 11), qt(0.975, 11))\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n> delta<-5\\n> (a<-delta/(sd(radon)/sqrt(length(radon))))\\n[1] 1.843113\\n> 1-(pt(tstar,11,a)-pt(-tstar,11,a))\\n[1] 0.390913\\n\\nExercise 20.3. Draw the power function for this circumstance with signiﬁcance level α = 0.05, µ0 = 105 and n = 12.\\nUse the standard deviation obtained from the data.\\n\\nR makes this computation using the command power.t.test.\\n\\n> power.t.test(n=12,delta=5,sd=sd(radon),type=c(\"one.sample\"))\\n\\nOne-sample t test power calculation\\n\\nn = 12\\n\\ndelta = 5\\n\\nsd = 9.39742\\n\\nsig.level = 0.05\\n\\npower = 0.3907862\\nalternative = two.sided\\n\\nNotice that this command give a different value for the power. This is due to the fact that this R command accounts\\n\\nonly for the larger area in FIgure 20.1.\\n\\n> 1-(pt(qt(0.975,11),11,a))\\n[1] 0.3907862\\n\\nThe power.t.test command consider both one and two sample t procedures. It will also handle both one-\\nsided and two-sided tests. The command considers ﬁve issues - sample size n, the difference between the null and a\\nﬁxed value of the alternative delta, the standard deviation s, the signiﬁcance level α, and the power. We can use\\npower.t.test to drop out any one of these ﬁve and use the remaining four to determine the remaining value. For\\nexample, if we want to assure an 80% power against an alternative of 110, then we need to make 30 measurements.\\n\\n> power.t.test(power=0.80,delta=5,sd=sd(radon),type=c(\"one.sample\"))\\n\\nOne-sample t test power calculation\\n\\nn = 29.70383\\n\\ndelta = 5\\n\\nsd = 9.39742\\n\\nsig.level = 0.05\\n\\npower = 0.8\\n\\nalternative = two.sided\\n\\nIn these types of application, we often use the terms speciﬁcity and sensitivity. Recall that setting the signiﬁcance\\nlevel α is the same as setting the false positive rate or type I error probability. The speciﬁcity of the test is equal to\\n1 − α, the probability that the test is not rejected when the null hypothesis is true. The sensitivity is the same as the\\npower, one minus the type II error rate, 1 − β.\\nExercise 20.4. Plot the receiver operating characteristic for n = 6, 12 and 24 observations, using the standard\\ndeviation obtained from the data and an alternative µ = 110.\\n\\n364\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n20.3 Correspondence between Two-Sided Tests and Conﬁdence Intervals\\nFor a two-sided t-test, we have the following list of equivalent conditions:\\n\\nfail to reject with signiﬁcance level α.\\n\\n|t| < tn−1,α/2\\n\\nµ0 − ¯x\\n\\ns/√n(cid:12)(cid:12)(cid:12)(cid:12) < tn−1,α/2\\n\\nµ0 − ¯x\\ns/√n\\n\\n(cid:12)(cid:12)(cid:12)(cid:12)\\n\\n−tn−1,α/2 <\\n\\n< tn−1,α/2\\n\\n−tn−1,α/2\\n\\ns\\n√n\\ns\\n√n\\n\\n< µ0 − ¯x < tn−1,α/2\\n\\ns\\n√n\\ns\\n¯x − tn−1,α/2\\n√n\\nµ0 is in the γ = 1 − α conﬁdence interval\\n\\n< µ0 < ¯x + tn−1,α/2\\n\\nThis is displayed in Figure 20.1 with the green ¯x and the horizontal green line indicating the γ-level conﬁdence\\ninterval containing µ0. In addition, reject the hypothesis with signiﬁcance level α is equivalent to µ0 is not in the\\nconﬁdence interval. This is displayed in Figure 1 with the red ¯x and the horizontal line indicating the γ-level conﬁdence\\ninterval that fails to contain µ0.\\n\\nFigure 20.2: γ-level conﬁdence intervals and α level hypothesis tests. γ = 1− α. The blue density curve is density of the sampling distribution\\nunder the null hypothesis. The red vertical dashes show the critical values for a two-sided test. γ is the area under the density curve between the\\nvertical critical value lines. α is the area under the density curve outside the vertical critical value lines. The green ¯x shows the case of fails to reject\\nis equivalent to the conﬁdence interval contains µ0. The red ¯x show the case reject is equivalent to the conﬁdence interval fails to contain µ0.\\n\\n365\\n\\n000x−barµ0s/\\\\sqrt{n}area  γareaα/2areaα/2x−barcritical regioncritical regionconfidence intervalhypothesis testing\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n20.4 Matched Pairs Procedures\\nA matched pair procedure is called for when a pair of quantitative measurements from a simple random sample\\n\\nX1, X2, . . . , Xn,\\n\\nand Y1, Y2, . . . , Yn\\n\\nare made on the same subjects. The alternative can be either one-sided or two sided. Underlying this assumption is\\nthat the populations are the same under the null hypothesis.\\n\\nThus, when H0 holds and if in addition, if the data are normal, then ¯X − ¯Y is also normal and so\\n\\nT =\\n\\n¯X − ¯Y\\nSX−Y /√n\\n\\nhas a t distribution with n − 1 degrees of freedom.\\n\\nThe γ-level conﬁdence interval for the difference in the population means is\\n\\n¯x − ¯y ±\\n\\nsX−Y\\n\\nn\\n\\ntn−1,(1−γ)/2,.\\n\\nExample 20.5. Researchers are concerned about the impact of vitamin C content reduction due to storage and ship-\\nment. To test this, researchers randomly chose a collection of bags of wheat soy blend bound for Haiti, marked them,\\nand measured vitamin C from a sample of the contents. Five months later, the bags were opened and a second sample\\nwas measured for vitamin C content. The units are milligrams of vitamin C per 100g of wheat soy blend.\\n\\nFactory Haiti\\n40\\n37\\n39\\n35\\n35\\n41\\n37\\n\\n44\\n50\\n48\\n44\\n42\\n47\\n49\\n\\nFactory Haiti\\n38\\n40\\n35\\n38\\n34\\n35\\n34\\n\\n45\\n32\\n47\\n40\\n38\\n41\\n40\\n\\nFactory Haiti\\n43\\n38\\n38\\n38\\n41\\n40\\n35\\n\\n39\\n52\\n45\\n37\\n38\\n44\\n43\\n\\nFactory Haiti\\n37\\n34\\n38\\n34\\n40\\n36\\n\\n50\\n40\\n39\\n39\\n37\\n44\\n\\nHere is the R output with the 95% conﬁdence interval for µF − µH where\\n• µF is the mean vitamin C content of the wheat soy blend at the factory and\\n• µH is the mean vitamin C content of the wheat soy blend in Haiti.\\n\\n> factory<-c(44,50,48,44,42,47,49,45,32,47,40,38,41,40,39,52,45,37,38,44,43,\\n+50,40,39,39,37,44)\\n> haiti<-c(40,37,39,35,35,41,37,38,40,35,38,34,35,34,43,38,38,38,41,40,35,37,\\n+34,38,34,40,36)\\n> boxplot(factory,haiti)\\n> t.test(factory, haiti, alternative = c(\"two.sided\"),mu = 0, paired = TRUE)\\n\\nPaired t-test\\n\\nfactory and haiti\\n\\ndata:\\nt = 4.9589, df = 26, p-value = 3.745e-05\\nalternative hypothesis: true difference in means is not equal to 0\\n95 percent confidence interval:\\n\\n3.122616 7.544050\\n\\nsample estimates:\\nmean of the differences\\n5.333333\\n\\n366\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nFigure 20.3: Vitamin C content in milligrams per 100 grams, measured at the factory and measured 5 month later in Haiti.\\n\\nThe input\\n\\n> t.test(factory - haiti, alternative = c(\"two.sided\"),mu = 0)\\n\\ngives essentially the same output.\\n\\nIn addition, the output\\n\\n> t.test(haiti, alternative = c(\"less\"),mu = 40)\\n\\nOne Sample t-test\\n\\nhaiti\\n\\ndata:\\nt = -5.3232, df = 26, p-value = 7.175e-06\\nalternative hypothesis: true mean is less than 40\\n95 percent confidence interval:\\n\\n-Inf 38.23811\\n\\nsample estimates:\\nmean of x\\n37.40741\\n\\nshows that we would reject the one sided test\\n\\nH0 : µ ≥ 40 versus H1 : µ < 40,\\n\\nbased on a goal of having 40mg/100g vitamin C in the wheat soy blend consumed by the Haitians.\\n\\nWe have used R primarily to compute a conﬁdence interval. If the goal of the program is to have reduction in\\n\\nvitamin C be less that a given amount c, then we have the hypothesis\\n\\nWe can test this using R by replacing mu=0 with mu=c.\\n\\nH0 : µF − µH ≥ c versus H1 : µF − µH < c.\\n\\n367\\n\\n1235404550\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n20.5 Two Sample Procedures\\nNow we consider the situation in which the two samples\\n\\nX1, X2, . . . , XnX ,\\n\\nand Y1, Y2, . . . , YnY\\n\\nare independent but are not paired. In particular, the number of observations nX and nY in the two samples could be\\ndifferent. If the ﬁrst sample has common mean µX and variance σ2\\nX and the second sample has common mean µY\\nand variance σ2\\n\\nY , then\\n\\nE[ ¯X − ¯Y ] = µX − µY\\n\\nand Var( ¯X − ¯Y ) =\\n\\nσ2\\nX\\nnX\\n\\n+\\n\\nσ2\\nY\\nnY\\n\\n.\\n\\nFor the two sided hypothesis test\\n\\nH0 : µX = µY\\n\\nversus H1 : µX (cid:54)= µY ,\\n\\nThe corresponding t-statistic is\\n\\nt =\\n\\n¯x − ¯y\\n+\\n\\n(cid:113) s2\\n\\nX\\nnX\\n\\ns2\\nY\\nnY\\n\\n(20.1)\\n\\nX and s2\\n\\nY the unbiased sample variances. Unlike the match pairs procedures, the test statistic (20.1) does not\\nwith s2\\nhave a t distribution under the null hypothesis. Indeed, the density and the distribution of this statistic are difﬁcult to\\ncompute.\\n\\nIn this circumstance, we now make what is commonly known in statistics as a conservative approximation. We\\nreplace the actual distribution of the t statistic in (20.1) with one which has slightly bigger tails. Thus, the computed\\np-value which are just integrals of the density function will be slightly larger. In this way, a conservative procedures is\\none that does not decrease the type I error probability.\\n\\nThis goal can be accomplished by approximating an ordinary Student’s t distribution with the effective degrees of\\n\\nfreedom ν calculated using the Welch-Satterthwaite equation:\\n\\nν =\\n\\n(s2\\n\\nX /nX + s2\\n(s2\\nX /nX )2/(nX − 1) + (s2\\n\\nY /nY )2\\nY /nY )2/(nY − 1)\\n\\n.\\n\\n(20.2)\\n\\nAs we saw in our discussion on Interval Estimation, this also gives a γ-level conﬁdence interval for the difference\\n\\nin the means µx and µY .\\n\\n¯x − ¯y ± t(1−γ)/2,ν(cid:115) s2\\n\\nX\\nnX\\n\\n+\\n\\ns2\\nY\\nnY\\n\\n.\\n\\nWe also learned that the effective degrees of freedom are largest when the two sample variances are nearly equal. In\\nthis case the number of degrees of freedom is 2 fewer than the sum of the two sets of observations.\\n\\nExample 20.6. To investigate the effect on blood pressure of added calcium in the diet, a researchers conducts a\\ndouble blind randomized experiment.\\nIn\\nthe control group, the individual takes a placebo. The response variable is the decrease in systolic blood pressure,\\nmeasured in millimeters of mercury, after 12 weeks. The test subjects are all male.\\n\\nIn the treatment group, each individual receives a calcium supplement.\\n\\n> calcium<-c(7,-4,18,17,-3,-5,1,10,11,-2)\\n> mean(calcium)\\n[1] 5\\n> sd(calcium)\\n[1] 8.743251\\n> placebo<-c(-1,12,-1,-3,3,-5,5,2,-11,-1,-3)\\n> mean(placebo)\\n\\n368\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n[1] -0.2727273\\n> sd(placebo)\\n[1] 5.900693\\n> boxplot(placebo,calcium)\\n\\nThe null hypothesis is that the treatment did not reduce µt the mean blood pressure of the treatment any more\\nthan it did the mean µc for the control group. The alternative is that it did reduce blood pressure more. Formally the\\nhypothesis test is\\n\\nH0 : µc ≤ µt\\n\\nversus H1 : µc > µt.\\n\\nThe t-statistic is\\n\\nt =\\n\\n5.000 + 0.273\\n10 + 5.9012\\n\\n(cid:113) 8.7432\\n\\n11\\n\\n= 1.604.\\n\\n> t.test(calcium,placebo,alternative = c(\"greater\"))\\n\\nWelch Two Sample t-test\\n\\ncalcium and placebo\\n\\ndata:\\nt = 1.6037, df = 15.591, p-value = 0.06442\\nalternative hypothesis: true difference in means is greater than 0\\n95 percent confidence interval:\\n\\n-0.476678\\n\\nInf\\n\\nsample estimates:\\n\\nmean of x mean of y\\n5.0000000 -0.2727273\\n\\n369\\n\\n!12!10!5051015\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nThus. the evidence against the null hypothesis is modest with a p-value of about 6%. Notice that the effective\\n\\ndegrees of freedom is ν = 15.591. The maximum possible is value for degrees of freedom is 19.\\n\\nTo see a 90% conﬁdence interval remove the “greater than” alternative” and set the conﬁdence level.\\n\\n> t.test(calcium,placebo,conf.level = 0.9)\\n\\nWelch Two Sample t-test\\n\\ncalcium and placebo\\n\\ndata:\\nt = 1.6037, df = 15.591, p-value = 0.1288\\nalternative hypothesis: true difference in means is not equal to 0\\n90 percent confidence interval:\\n\\n-0.476678 11.022133\\n\\nsample estimates:\\n\\nmean of x mean of y\\n5.0000000 -0.2727273\\n\\nExample 20.7. The life span in days of 88 wildtype and 99 transgenic mosquitoes is given in the following data set.\\n\\n> mosquitoes<-read.delim(\"http://math.arizona.edu/˜jwatkins/mosquitoes.txt\")\\n> boxplot(mosquitoes)\\n\\nThe goal is to see if overstimulation of the insulin signaling cascade in the mosquito midgut reduces the µt, the\\n\\nmean life span of these transgenic mosquitoes from that of the wild type µwt.\\n\\nH0 : µwt ≤ µt\\n\\nversus H1 : µwt > µt.\\n\\n> wildtype<-mosquitoes[1:88,1]\\n> transgenic<-mosquitoes[,2]\\n> t.test(transgenic,wildtype,alternative = c(\"less\"))\\n\\nWelch Two Sample t-test\\n\\n370\\n\\n!!!!!!!!!!!wildtypetransgenic01020304050\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\ntransgenic and wildtype\\n\\ndata:\\nt = -2.4106, df = 169.665, p-value = 0.008497\\nalternative hypothesis: true difference in means is less than 0\\n95 percent confidence interval:\\n\\n-Inf -1.330591\\n\\nsample estimates:\\nmean of x mean of y\\n16.54545 20.78409\\n\\nTo determine a 98% conﬁdence interval, we again remove the alternative command.\\n\\n> t.test(transgenic,wildtype,conf.level=0.98)\\n\\nWelch Two Sample t-test\\n\\ntransgenic and wildtype\\n\\ndata:\\nt = -2.4106, df = 169.665, p-value = 0.01699\\nalternative hypothesis: true difference in means is not equal to 0\\n98 percent confidence interval:\\n\\n-8.3680812 -0.1091915\\n\\nsample estimates:\\nmean of x mean of y\\n16.54545 20.78409\\nExercise 20.8. Notice that the 98% conﬁdence interval, (−8.3680812,−0.1091915) does not contain 0. What can be\\nsaid about a two-sided test at the 2% signiﬁcance level? What can be said about the p-value for a one-sided test?\\n\\nThe two-sample procedure assumes that the two sets of observations are independent and so it is not an appropriate\\nprocedure when the the observations are paired. Moreover, the two sample procedure is yields less powerful test when\\nthe two sets of observations are positively correlated.\\nExample 20.9. Previously we investigated the relationship of age of parents to the de novo mutations in the offspring\\nfor the 78 Icelandic trios. Now, we address the simpler question: are fathers older than mothers at the time of the\\nchild’s birth? State as a hypothesis, we have\\n\\nH0 : µf ≤ µm versus H1 : µf > µm.\\n\\nwhere µm is the mean age of Icelandic fathers at the time of birth and µm is the mean age of Icelandic mothers. We\\nanticipate that the two parents’ ages are positively correlcate - older fathers with older mothers and younger fathers\\nwith younger mothers.\\n\\nThe appropriate matched-pair test has R commands.\\n\\n> t.test(father,mother,alternative=c(\"greater\"),paired=TRUE)\\n\\nPaired t-test\\n\\nfather and mother\\n\\ndata:\\nt = 6.6209, df = 77, p-value = 2.151e-09\\nalternative hypothesis: true difference in means is greater than 0\\n95 percent confidence interval:\\n\\n1.838564\\n\\nInf\\n\\nsample estimates:\\nmean of the differences\\n2.456197\\n\\n371\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nNotice that t = 6.6209. If we had performed (inappropriately) the two-sample t test, we ﬁnd\\n\\n> t.test(father,mother,alternative=c(\"greater\"))\\n\\nWelch Two Sample t-test\\n\\nfather and mother\\n\\ndata:\\nt = 2.7834, df = 153.252, p-value = 0.003028\\nalternative hypothesis: true difference in means is greater than 0\\n95 percent confidence interval:\\n\\n0.9958999\\n\\nInf\\n\\nsample estimates:\\nmean of x mean of y\\n29.65919 27.20299\\n\\nand t has the much lower value 2.7834.\\n\\nTo make the comparison, we rewrite the two t statistics as\\n\\nt =\\n\\n¯xm − ¯xf\\nxm−xf /n\\n\\n(cid:113)s2\\n\\n¯xm − ¯xf\\nxm + s2\\n\\nxf )/n\\n\\n(cid:113)(s2\\n\\nand\\n\\nt =\\n\\n.\\n\\n(20.3)\\n\\nNotice that the numerators are the same. Thus, the change in the values for the t statistics must arise from a\\ndifference in the values in the denominator. Recall that the law of cosines for variance requires the variances in the\\nexpressions above as well as their correlation r to obtain\\n\\ns2\\nxm−xf = s2\\n\\nxm + s2\\n\\nxf − 2rsxm sxf .\\n\\nThus, if r > 0,\\n\\ns2\\nxm−xf < s2\\n\\nxm + s2\\nxf\\n\\nand the denominator for the expression (20.3) on the left is smaller than the expression on the right. Consequently, the\\nt statistic is larger and the test is more powerful\\nExercise 20.10. Use the R output above and the values\\n> cor(father,mother)\\n[1] 0.8252784\\n> sd(father);sd(mother)\\n[1] 5.700042\\n[1] 5.314777\\n\\nto compute the two t statistics in (20.3).\\n\\nExample 20.11 (pooled two-sample t-test). Sometimes, the two-sample procedure is based on the assumption of a\\ncommon value σ2 for the variance of the two-samples. In this case, we pool the data to compute an unbiased estimate\\nfor the variance:\\n\\nThus, we weight the variance from each of the two samples by the number of degrees of freedom. If we modify the\\nt-statistics above to\\n\\ns2\\np =\\n\\nY(cid:1) .\\nX + (n2 − 1)s2\\n\\n1\\n\\nn1 + n2 − 2(cid:0)(n1 − 1)s2\\nsp(cid:113) 1\\n\\n(cid:113) s2\\n\\n¯x − ¯y\\n+\\n\\ns2\\np\\nnY\\n\\np\\nnX\\n\\nt =\\n\\n=\\n\\n¯x − ¯y\\n\\n+ 1\\nnY\\n\\nnX\\n\\n372\\n\\nThis indeed has the t-distribution with nX + nY − 2 degrees of freedom. This is accomplished in R by adding\\nvar.equal=TRUE command.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n> t.test(calcium,placebo,alternative = c(\"greater\"),var.equal=TRUE)\\n\\nTwo Sample t-test\\n\\ncalcium and placebo\\n\\ndata:\\nt = 1.6341, df = 19, p-value = 0.05935\\nalternative hypothesis: true difference in means is greater than 0\\n95 percent confidence interval:\\n\\n-0.3066129\\n\\nInf\\n\\nsample estimates:\\n\\nmean of x mean of y\\n5.0000000 -0.2727273\\n\\nNote that this increases the number of degrees of freedom and lowers the P -value from 6.4% to 5.9%. A natural\\nquestion to ask at this point is How do we compare the mean of more that two groups? As we shall soon learn, this\\nleads us to a test procedure, analysis of variance, that is a generalization of the pooled two-sample procedure?\\n\\n20.6 Summary of Tests of Signiﬁcance\\nThe procedures we have used to perform hypothesis tests are based on some quantity θ generally expressed as a value\\nin a parameter space Θ. In setting the hypothesis, we partition the parameter space into two parts Θ0 for the null\\nhypothesis, H0, and Θ1 for the alternative, H1. Our strategy is to look for generalizations of the Neyman-Pearson\\nparadigm. For example, the Karlin-Rubin criterion provides a condition for one-sided tests that allows us to say the\\nwe have a uniformly most powerful test.\\n\\nFor two-sided tests, we look to the likelihood ratio approach. For this approach, we ﬁrst maximize the likelihood\\nL(θ|x) both over Θ0 and over Θ and then compute the ratio Λ(x). If the data, x, lead to a ratio that is sufﬁciently\\nsmall, then likelihood for all values of θ ∈ Θ0 are less likely then some values in Θ1. This leads us to reject the\\nnull hypothesis in favor of the alternative. If the number of observations is large, then we can approximate, under the\\nnull hypothesis, the distribution of the test-statistic −2 ln Λ(x) with a χ2 distribution. This leads to a critical region\\nC = {−2 ln Λ(x) ≥ ˜kα} for an α-level test.\\nIn practice, much of our inference is for population proportions and the population means. In these cases, we often\\nreserve the test for those cases in which the central limit theorem applies and thus the estimates, the sample proportions\\nand the sample means, have approximately a normal distribution. We summarize these procedures below.\\n\\n20.6.1 General Guidelines\\n\\n• Hypotheses are stated in terms of a population parameter.\\n• The null hypothesis H0 is a statement that no effect is present.\\n• The alternative hypothesis H1 is a statement that a parameter differs from its null value in a speciﬁc direction\\n\\n(one-sided alternative) or in either direction (two-sided alternative).\\n\\n• A test statistic is designed to assess the strength of evidence against H0.\\n• If a decision must be made, specify the signiﬁcance level α.\\n• Assuming H0 is true, the p-value is the probability that the test statistic would take a value as extreme or more\\n\\nextreme than the value observed.\\n\\n• If the p-value is smaller than the signiﬁcance level α, then H0 is rejected and the data are said to be statistically\\n\\nsigniﬁcant at level α.\\n\\n373\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n20.6.2 Test for Population Proportions\\nThe design is based on Bernoulli trials . For this we have\\n\\n• A ﬁxed number of trials n.\\n• The outcome of each trial is independent of the other trials.\\n• Each trial has one of two outcomes success and failure.\\n• The probability of success p is the same for each trial.\\nThis test statistic is the z-score. and thus is based the applicability of the central limit theorem on using the\\nstandard normal distribution. This procedure is considered valid is the sample is small (< 10%) compared to the total\\npopulation and both np0 and n(1 − p0) is at least 10. Otherwise, use the binomial distribution directly for the test\\nstatistic.\\nThe statistics ˆp for a one-proportion procedure and ˆp1, ˆp2 for a two-sample procedure, is the appropriate propor-\\n\\ntions of success.\\n\\nnull hypothesis\\n\\nsample proportions\\ntwo-sided\\nsingle proportion H0 : p ≥ p0 H0 : p = p0\\n\\none-sided\\n\\ntwo proportions\\n\\nH0 : p ≤ p0\\nH0 : p1 ≥ p2 H0 : p1 = p2\\nH0 : p1 ≤ p2\\n\\nz =\\n\\nˆp−p0\\n\\ntest statistic\\n√p0(1−p0)/n\\n(cid:113)\\nˆp1−ˆp2\\nˆp(1−ˆp)( 1\\nn1\\n\\n+ 1\\nn2\\n\\n)\\n\\nz =\\n\\nThe pooled sample proportion ˆp = (x1 + x2)/(n1 + n2) where xi is the number of successes in the ni Bernoulli\\n\\ntrials from group i.\\n\\n20.6.3 Test for Population Means\\n\\n• Use the z-statistic when the standard deviations are known.\\n• Use the t-statistic when the standard deviations are computed from the data.\\n\\nt or z-procedure\\nsingle sample\\n\\ntwo samples\\n\\nnull hypothesis\\n\\none-sided\\ntwo-sided\\nH0 : µ ≤ µ0 H0 : µ = µ0\\nH0 : µ ≥ µ0\\nH0 : µ1 ≤ µ2 H0 : µ1 = µ2\\nH0 : µ1 ≥ µ2\\n\\nThe test statistic\\n\\nt =\\n\\nestimate − parameter\\n\\nstandard error\\n\\n.\\n\\nThe p-value is determined by the distribution of a random variable having a t distribution with the appropriate number\\nof degrees of freedom. For one-sample and two-sample z procedures, replace the values s with σ and s1 and s2 with\\nσ1 and σ2, respectively. Use the normal distribution for these tests.\\n\\n374\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nt-procedure\\none sample\\n\\ntwo sample\\n\\npooled two sample\\n\\nparameter\\n\\nestimate\\n\\nstandard error\\n\\ndegrees of freedom\\n\\nµ\\n\\nµ1 − µ2\\nµ1 − µ2\\n\\n¯x\\n\\n¯x1 − ¯x2 (cid:113) s2\\nsp(cid:113) 1\\n\\n¯x1 − ¯x2\\n\\n1\\nn1\\n\\ns√n\\n+ s2\\n2\\nn2\\n+ 1\\nn2\\n\\nn1\\n\\nn − 1\\n\\nν in equation (20.2)\\n\\nn1 + n2 − 2\\n\\n20.7 A Note on the Delta Method\\nFor a one sample test hypothesizing a value for g(µ), we use the t statistic\\n\\nFor a test that compare a function of the mean of a two samples g(µX ) and g(µY ) we can use the test statistic\\n\\ng(¯x) − g(µ0)\\n|g(cid:48)(¯x)|s/√n\\nand base the test on the t distribution with n − 1 degrees of freedom.\\ng(cid:48)(¯x) − g(¯y)\\n\\nt =\\n\\nt =\\n\\n(cid:113) (g(cid:48)(¯x)sX )2\\n\\nnX\\n\\n+ (g(cid:48)(¯y)sY )2\\n\\nnY\\n\\nThe degrees of freedom ν can be computed from the Welch-Satterthwaite equation specialized to this circumstance.\\n\\nν =\\n\\n(g(¯x)sX )2/nX + (g(cid:48)(¯y)sY )2/nY )2\\n\\n((g(cid:48)(¯x)sX )2/nX )2/(nX − 1) + ((g(cid:48)(¯y)sY )2/nY )2/(nY − 1)\\n\\n.\\n\\n20.8 The t Test as a Likelihood Ratio Test\\nAgain, we begin with independent normal observations X1. . . . , Xn with unknown mean µ and unknown variance σ2.\\nWe show that the critical region C = {x;|T (x)| > tn−1,α/2} is a consequence of the criterion given by a likelihood\\nratio test with signiﬁcance level α.\\n\\nThe likelihood function\\n\\nL(µ, σ2|x) =\\n\\n1\\n\\n(2πσ2)n/2 exp−\\n\\n1\\n2σ2\\n\\n(xi − µ)2\\n\\nln L(µ, σ2|x) = −\\n\\nn\\n2\\n\\n(ln 2π + ln σ2) −\\n\\n∂\\n∂µ\\n\\nln L(µ, σ2|x) = −\\n\\n1\\nσ2\\n\\nn(cid:88)i=1\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\n\\n1\\n2σ2\\n\\n(xi − µ)\\n\\n(xi − µ)2\\n\\n∂\\n∂σ2 ln L(µ, σ2|x) = −\\n\\nn\\n2σ2 +\\n\\n1\\n\\n2(σ2)2\\n\\nn(cid:88)i=1\\n\\n(xi − µ)2.\\n\\nˆσ2 =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2.\\n\\nH0 : µ = µ0\\n\\nversus H1 : µ (cid:54)= µ0,\\n375\\n\\nThus, ˆµ = ¯x.\\n\\nThus,\\n\\nFor the hypothesis\\n\\n\\x0ct Procedures\\n\\n2\\nn\\n\\n,\\n\\nIntroduction to the Science of Statistics\\n\\nthe likelihood ratio test\\n\\nwhere the value\\n\\nΛ(x) =\\n\\nL(µ0, ˆσ2\\n0|x)\\nL(ˆµ, ˆσ2|x)\\n\\nˆσ2\\n0 =\\n\\n1\\nn\\n\\nn(cid:88)i=1\\n\\n(xi − µ0)2\\n\\ngives the maximum likelihood on the set µ = µ0.\\n\\nL(µ0, ˆσ2\\n\\n0|x) =\\n\\n1\\n0)n/2 exp−\\n(2πˆσ2\\n\\nL(ˆµ, ˆσ2|x)) =\\n\\n1\\n\\n(2πˆσ2)n/2 exp−\\n\\nand the likelihood ratio is\\n\\nΛ(x) =(cid:18) ˆσ2\\n\\n0(cid:19)n/2\\n\\nˆσ2\\n\\n1\\n0)n/2 exp−\\n(2πˆσ2\\n\\n(2πˆσ2)n/2 , exp−\\n\\n2\\nn\\n\\n,\\n\\n1\\n\\n1\\n2ˆσ2\\n0\\n\\n1\\n2ˆσ2\\n\\n(xi − ¯x)2 =\\n\\n(xi − µ0)2 =\\n\\nn(cid:88)i=1\\nn(cid:88)i=1\\ni=1(xi − ¯x)2 (cid:19)−n/2\\n=(cid:18)(cid:80)n\\n(cid:80)n\\n\\ni=1(xi − µ0)2\\n\\nThe critical region λ(x) ≤ λ0 is equivalent to the fraction in parenthesis above being sufﬁciently large. In other\\n\\nwords for some value c,\\n\\ni=1(xi − µ0)2\\n\\ni=1(xi − ¯x)2 = (cid:80)n\\nc ≤ (cid:80)n\\ni=1 ((xi − ¯x) + (¯x − µ0))2\\n(cid:80)n\\n(cid:80)n\\n= (cid:80)n\\ni=1(xi − ¯x)2 + 2(cid:80)n\\ni=1(xi − ¯x)(¯x − µ0) +(cid:80)n\\n(cid:80)n\\n\\ni=1(xi − ¯x)2\\n\\ni=1(xi − ¯x)2\\n\\ni=1(¯x − µ0)2\\n\\n= 1 +\\n\\nn(¯x − µ0)2\\ni=1(xi − ¯x)2\\n\\n(cid:80)n\\n\\nIn a now familiar strategy, we have added and subtracted ¯x to decompose the variation. Continuing we ﬁnd that\\n\\nor\\n\\n(c − 1)(n − 1) ≤\\n\\nn(¯x − µ0)2\\n\\ni=1(xi − ¯x)2/(n − 1)\\n\\n(cid:80)n\\n(c − 1)(n − 1) ≤ T (x)2\\n\\n=\\n\\n(¯x − µ0)2\\n\\ns2/n\\n\\n(20.4)\\n\\nwhere\\n\\n¯x − µ0\\ns/√n\\nand s is the square root of the unbiased estimator of the variance\\n\\nT (x) =\\n\\nn − 1\\nTaking square roots in (20.4), we have the critical region\\n\\n1\\n\\ns2 =\\n\\nn(cid:88)i=1\\n\\n(xi − ¯x)2.\\n\\nC =(cid:110)x;(cid:112)(c − 1)(n − 1) ≤ |T (x)|(cid:111)\\n\\nNow take(cid:112)(c − 1)(n − 1) = tn−1,α/2.\\n\\n376\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n20.9 Non-parametric alternatives\\nThe strategy for hypothesis testing is to choose a test statistic with high power and to determine its distribution under\\nthe null hypothesis. We then compute the value of the test statistic for our data and make an assessment. For the t test,\\nthus assessment is based on the t distribution. This, in turn, relies on the ability to say that the distributions of sample\\nmeans are nearly normally distributed. This is typically assured upon appeal to the central limit theorem. In some\\ncircumstances, we then choose a signiﬁcance level α and then the decision to reject the null hypotheses is based on\\nrelating the test statistics to this critical value, rejecting if the test statistic is more extreme than this standard. In other\\ncircumstances, we compute the p-value and use that to indicate the strength of the evidence against the null hypothesis.\\nWe then follow up by deciding to reject the null hypothesis if the p-value is below the signiﬁcance level α.\\n\\n20.9.1 Permutation Test\\nIn the example of a test for the value of added calcium in the diet to lower blood pressure, the boxplot showed an\\noutlier in the blood pressure of one individual in the group that received a calcium supplement. As a consequence, the\\nassumption that the central limit theorem holds for the distribution of the sample means is in doubt. These data have a\\ndifference in sample means:\\n\\n> (diffdata<-mean(calcium)-mean(placebo))\\n[1] 5.272727\\n\\nWhen the central limit theorem applies, we can assess this difference by dividing by the standard error of the sample\\nmeans and use the value of a t statistic with the appropriate number of degrees of freedom.\\n\\nPermutation tests present an alternative to ﬁnding the sampling distribution for any test statistic, here the differ-\\nence in sample means. The procedures begin by asking, If the null hypothesis is true, then what shufﬂes of the data\\nare consistent with this statement?\\n\\nIn this case, we can think of the null hypothesis as stating that all of the observations for blood pressure are actually\\nderived from a single group and thus the assignment of an individual to a group, either placebo or calcium, should\\nhave no effect on the outcome. For the two groups, sizes n1 and n2, we have\\n\\n(cid:18)n1 + n2\\nn1 (cid:19)\\n\\nways to divide the entire sample into two groups, retaining the sizes found in the observations that constitute our data.\\nEach of these choices provides an additional value for the difference, under the null hypothesis, of sample means.\\nThese can be used to create an empirical sampling distribution. This methodology is feasible if n1 and n2 are not too\\nlarge. For larger sample sizes, we can also use a resampling technique, similar to the bootstrap, to randomly create\\nthe two groups and compute the difference in the means of these randomly chosen resampled groups.\\n\\n> bp<-c(placebo,calcium)\\n> diff<-rep(0,10000)\\n> groups<-c(rep(1,length(calcium)),\\n\\nrep(2,length(placebo)))\\n\\n> for(i in 1:10000){groupperm<-sample(groups);\\n\\ndiff[i]<-mean(bp[groupperm==1]\\n-mean(bp[groupperm==2]))}\\n\\nBecause the alternative is greater than, the p-value (which will vary a bit\\nfrom one simulation to the next) is the fraction of the simulations greater than\\nwhat is found in the data. (See Figure 20.3.)\\n\\n> (pvalue<-length(diff[diff>diffdata])/length(diff))\\n[1] 0.0556\\n\\n377\\n\\nFigure 20.4: Histogram of mean differ-\\nences for 10,000 simulated samples. The p-\\nvalue, 0.0556, is the area to the right of the\\nvertical red line.\\n\\nHistogram of diffdiffDensity-10-505100.000.020.040.060.080.100.12-10-505100.000.020.040.060.080.100.12\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nThis is slightly lower than the p-value 0.06442 found using the two-sample t-test.\\n\\nExercise 20.12. Create a permutation test for a matched pair procedure and apply it to the data set vitamin C in the\\nwheat soy blend, (Hint; Under the null hypothesis the difference in the measurements, in either order have the same\\ndistribution that is symmetric about 0. The procedure uses independent Bernoulli random variables to choose the\\norder of subtraction.)\\n\\nWhen the assumption of the normal distribution cannot be said to hold in a two-sample t-procedure, another option\\nis to use a test that does not depend on the numerical values of the observations but rather on the ranks of the data.\\nBecause this test is based on the ranks of the data, we cannot base the hypothesis test on the means of the data, but\\nrather on a parameter that uses only the ranks of the data. For these non-parametric tests the hypotheses are often\\nstated in terms of medians.\\n\\n20.9.2 Mann-Whitney or Wilcoxon Rank Sum Test\\nWe will explain this procedure more carefully in the case of the data on the lifetime of wildtype and transgenic\\nmosquitoes.\\nIn this case our data are x1, x2, . . . , xnx are the lifetimes in days for the wildtype mosquitoes and\\ny1, y2, . . . , yny are the lifetimes in days for the transgenic mosquitoes.\\n\\nFor the Wilcoxon rank sum test, the hypothesis is based a question that can be addressed by the rankings of\\nthe data. For this discussion, we will assume that the data are two independent samples from populations having\\ncontinuous distributions FX and FY for random variables X and Y . Because the distributions are continuous, we\\nknow that P{X = Y } = 0. (We will discuss the case P{X = Y } (cid:54)= 0 below.) The null hypothesis\\n\\nH0 : P{X > Y } = P{Y > X} =\\n\\n1\\n2\\n\\n.\\n\\nIn words, independent random observations, one from FX and one from FY are equally likely to be larger. The\\nalternative hypothesis may be two-sided,\\n\\nH1 : P{X > Y } (cid:54)= P{Y > X},\\n\\nor one-sided,\\n\\nor\\nThe following identity will be useful in our discussion.\\n\\nH1 : P{X > Y } > P{Y > X}\\n\\nH1 : P{X > Y } < P{Y > X}\\n\\n(20.5)\\n\\nExercise 20.13. The sums of the ﬁrst m integers\\n\\nj = 1 + 2 + ··· + n =\\n\\nn(n + 1)\\n\\n2\\n\\nn(cid:88)j=1\\n\\nLet’s look at a small pilot data set to get a sense of the proce-\\n\\ndure. The values are the lifespan in days.\\n\\n> wildtype\\n[1] 31 36 18 11 33 9 34 47\\n> transgenic\\n[1]\\n\\n8 21 24 25 38\\n\\n3\\n\\nFrom these date, we see that the ranks\\n• wildtype - 3 4 5 9 10 11 12 14\\n• transgenic - 1 2 6 7 8 13\\n\\nFigure 20.5: Life span in days for 6 transgenic (in red) and 8\\nwildtype (in black) mosquitoes.\\n\\n378\\n\\n01020304050days01020304050days\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nThe strategy for the test to see if there is a signiﬁcant differ-\\nence in the ranks of the data. The basic statistic is the sum of the\\nranks of one of the samples. For the transgenic mosquitoes, this sum is\\n\\nRy =\\n\\nny(cid:88)i=1\\n\\nRy,i = 1 + 2 + 6 + 7 + 8 + 13 = 37\\n\\nfor ny = 6 observations\\n\\nWe can now compare this sum of ranking to all(cid:0)14\\n\\nusing the wilcox.test command in R.\\n> wilcox.test(transgenic,wildtype,alternative=c(\"less\"))\\n\\n6(cid:1) = 3003 possible rankings of the data. This is accomplished\\n\\nWilcoxon rank sum test\\n\\ntransgenic and wildtype\\n\\ndata:\\nW = 16, p-value = 0.1725\\nalternative hypothesis: true location shift is less than 0\\n\\nThus, the 17.25% of the ranks below the given value of 37 give us the p-value. This small amount of data gives\\na hint that the transgenic mosquito may have a shorter lifespan. The U statistic is related to the sum of the ranks by\\nsubtracting the minimum possible value as shown in Exercise 20.6.\\n\\nUy =\\n\\n(Ry,j − j) = Ry −\\n\\nny(ny + 1)\\n\\n2\\n\\n.\\n\\nny(cid:88)j=1\\n\\nR uses another variant W of the Ry statistic.\\n\\nExercise 20.14. Deﬁne Ry to be the sum of the ranks for the ny observations in the second sample and set Ux =\\nRx − nx(nx+1)\\n\\n. Then,\\n\\n2\\n\\nUx + Uy = nxny\\n\\n(20.6)\\n\\nWe previously saw the expressions for the probabilities (20.5) in the discussion of the distribution of p-values and\\nthe receiving operator characteristic. We will now show how this connection reappears in the ranked sum test by giving\\na pictorial explanation for the identity (20.6).\\n\\nFirst, write the differences in the rank Rx,j and the mini-\\nmum possible rank j for the ordered entries for the wildtype\\nmosquitos.\\n\\nUx =\\n\\n(Rx,j − j)\\n\\nnx(cid:88)j=1\\n\\n= (3 − 1) + (4 − 2) + (5 − 3) + (9 − 4)\\n\\n+(10 − 5) + (11 − 6) + (12 − 7) + (14 − 8)\\n\\n= 2 + 2 + 2 + 5 + 5 + 5 + 5 + 6\\n\\n(20.7)\\n\\nand the same differences for the transgenic mosquitos.\\n\\nFigure 20.6: Pictoral representation of (20.6). The terms in (20.7)\\nfor Ux are the heights of vertical rectangles in each row, left to right\\nbelow the solid line. The terms in (20.8) for Uy are the lengths of\\nhorizontal rectangles below in each row, bottom to top right of the\\nsolid line.Their total is the total number of boxes - nxny.\\n\\n379\\n\\nUy =\\n\\n(Ry,j − j)\\n\\nny(cid:88)j=1\\n\\n= (1 − 1) + (2 − 2) + (6 − 3)\\n\\n+(7 − 4) + (8 − 5) + (13 − 6)\\n\\n= 0 + 0 + 3 + 3 + 3 + 7.\\n\\n(20.8)\\n\\n0.00.20.40.60.81.00.00.20.40.60.81.0false positive ratetrue positive rate0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\ncaption to the left.\\n\\nThe terms in the sums can be seen as the areas of boxes that ﬁll a grid of nx × ny squares as described in the ﬁgure\\nFor a second method to draw the solid line in the ﬁgure, write x’s and y’s in the order from smallest to largest. In\\nthe mosquito example, this is y y x x x y y y x x x x y x. Following the letters in order, moving up for each y and\\nright for each x will produce the solid line in Figure 20.6. Because the y transgenic lifespan is shorter, then we will\\nsee more movement upwards early and the area under the solid line will be considerably above one-half. In this case,\\nthe area is 2/3. The alternative hypotheses can now be stated as the area under the solid line differs from one-half (for\\na two-sided alternative) or is above or below one-half (for a one-sided alternative).\\n\\nIf the entire data set is used, then we cannot carry out all of the comparisons without a long computational time.\\nHowever, we have a version of the central limit theorem that gives the mean and standard deviation of the Ry, Uy or\\nW -statistic. Thus, we can use the normal distribution to determine a p-value. As with the binomial distribution, R\\nuses a continuity correction to deal with the fact that the test statistic W is a discrete random variable.\\n\\n> wilcox.test(transgenic,wildtype,alternative=c(\"less\"))\\n\\nWilcoxon rank sum test with continuity correction\\n\\ntransgenic and wildtype\\n\\ndata:\\nW = 3549.5, p-value = 0.0143\\nalternative hypothesis: true location shift is less than 0\\n\\nNotice the value W = 3549.5 is not an integer. This is a result of the fact that ties are resolved by giving fractional\\nvalues to ties. For example, if the third and fourth values are equal, they are both given the rank 3 1/2. If the seventh,\\neighth, and ninth values are equal. they are all given the rank (7+8+9)/3 = 8.\\n\\nWe now re-draw the above ﬁgure using all of the\\ndata with the goal of interpreting the graph as an em-\\npirical receiving operator characteristic. The test\\nstatistic associated to the graph would be to identify\\nthe mosquito genotype solely on its lifespan. Thus\\nwe will ﬁx a number of days d0. If the mosquito life\\nis shorter, we will classify it as “transgenic” . If it is\\nlonger, we will classify it as “wildtype.” Any choice\\nof d0 will result on a point on the curve. Looking to\\nthe horizontal axis gives the false positive rate, the\\nprobability that a mosquito that has a shorter lifespan\\nhas been incorrectly identiﬁed as transgenic. Look-\\ning to the vertical axis gives the true positive rate,\\nthe probability that a mosquito that has a shorter\\nlifespan is correctly classiﬁed as transgenic. Ideally,\\nwe would like to make choice for d0 so that the false\\npositive rate is low and the true positive rate is high.\\nFigure 20.7: Pictorial representation of (20.6). The terms in (20.7) for Ux\\nSo, the desired graph quickly increases for small val-\\nare number of vertical rectangles in each row, left to right below the solid line.\\nues of the false positive rate. The metric associated\\nThe terms in (20.8) for Uy are number of horizontal rectangles below in each\\nto this is the area under the curve or AUC for the\\nrow, bottom to top right of the solid line.\\nreceiving operator characteristic. Thus, the desired graph has area under the curve nearly equal to 1. This area is equal\\nto Ux/(nxny) and the signiﬁcance of its difference from one-half is the essence of the Mann-Whitney rank sum test.\\nIn this example, the areas is 0.567 and so does not make for a very powerful test.\\n\\nThis circumstance is common in medical testing. A patient is administered a test and based on the outcome of the\\ntest, a physician will make a diagnosis and prescribe a treatment. The quality of the test can be obtained by calculating\\nthe AUC, or correspondingly, the Mann-Whitney U statistic. One standard for an improvement is a statistically and\\nmedically signiﬁcant increase in AUC.\\n\\n380\\n\\n0.00.20.40.60.81.00.00.20.40.60.81.0false positive ratetrue positive rate\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\n20.9.3 Wilcoxon Signed-Rank Test\\nFor the Wilcoxon signed-rank test, the hypothesis is based on the median values mx and my for an experimental\\nprocedure in which pairs are matched. This gives an alternative to the measurement of the amount of vitamin C in\\nwheat soy blend at the factory and 5 months later in Haiti. Our data are\\n\\nare the measurements of vitamin C content of the wheat soy blend at the factory and\\n\\nx1, x2, . . . , xn\\n\\ny1, y2, . . . , yn\\n\\nare the measurements of vitamin C content of the wheat soy blend in Haiti.\\n\\nFor the hypothesis test to see if vitamin C content decreases due to shipping and shelf time, set\\n• mF is the median vitamin C content of the wheat soy blend at the factory and\\n• mH is the median vitamin C content of the wheat soy blend in Haiti.\\nTo perform the test\\n\\nH0 : mF ≤ mH versus H1 : mF > mH , ,\\n\\nwe use a test statistic based on both the sign of the difference yi − xi in the paired observations and in the ranks of\\n|yi − xi| Here is the R command and output. Note the choice paired=TRUE for the signed-rank test.\\n> wilcox.test(factory, haiti, alternative = c(\"greater\"),paired=TRUE)\\n\\nWilcoxon signed rank test with continuity correction\\n\\nfactory and haiti\\n\\ndata:\\nV = 341, p-value = 0.0001341\\nalternative hypothesis: true location shift is greater than 0\\n\\nWarning message:\\nIn wilcox.test.default(factory, haiti, alternative = c(\"greater\"),\\n\\n:\\n\\ncannot compute exact p-value with ties\\n\\n20.10 Answers to Selected Exercises\\n20.2. Recall that for Z1, Z2, . . . Zn, independent standard normal random variables with standard deviation sZ,\\n\\n˜T =\\n\\n√n ¯Z − a\\n\\nsZ\\n\\nhas a t distribution with n − 1 degrees of freedom and non-centrality parameter a.\\ndenominator, we add and subtract the mean µ to obtain\\n\\nIf we reproduce the calculation we made in determining power for a one-sample two-sided z-test, then, in the\\n\\nIf the Xi have common standard deviation σ, we standardize the variables, writing\\n\\n¯X − µ\\ns/√n\\n\\n=\\n\\n( ¯X − µ) + (µ − µ0)\\n\\ns/√n\\n\\nZi =\\n\\nXi − µ\\n\\nσ\\n\\n.\\n\\n381\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nThus, sZ = s/σ is the standard deviation of the Zi and upon dividing each term by σ,\\n\\n( ¯X − µ) + (µ − µ0)\\n\\ns/√n\\n\\n=\\n\\n=\\n\\n√n(( ¯X − µ)/σ + (µ − µ0)/σ)\\n√n ¯Z − √n(µ0 − µ)/σ\\n\\ns/σ\\n\\nsZ\\n\\nwhich has t distribution with non-centrality parameter a = √n(µ − µ0)/σ.\\n\\n20.3. To plot the power function, π, we ﬁrst enter the data.\\n\\n> radon<-c(91.9,97.8,111.4,122.3,105.4,95.0,\\n\\n103.8,99.6,96.6,119.3,104.8,101.7)\\n\\n> mu0<-105\\n> mu<-seq(95,115,0.01)\\n> a<-(mu0-mu)/(sd(radon)/sqrt(length(radon)))\\n> tstar<-qt(0.975,11)\\n> pi<-1-(pt(tstar,11,a)-pt(-tstar,11,a))\\n> plot(mu,pi,type=\"l\")\\n\\n20.4. Recall that the receiver operating characteristic is a plot of α the\\nsigniﬁcance level versus the power.\\n\\n> alpha<-seq(0,1,0.01) delta<-5\\n> n<-6; talpha<-qt(1-alpha/2,n-1); a<-delta/(sd(radon)/sqrt(n-1))\\n> power6<-1-(pt(talpha,n-1,a)-pt(-talpha,n-1,a))\\n> plot(alpha,power6,type=\"l\",xlim=c(0,1),ylim=c(0,1),\\n\\nFigure 20.8: Power curve for radon detector.\\n\\nxlab=c(\"significance\"),ylab=c(\"power\"))\\n\\n> par(new=TRUE)\\n> n<-12; talpha<-qt(1-alpha/2,n-1); a<-delta/(sd(radon)/sqrt(n-1))\\n> power12<-1-(pt(talpha,n-1,a)-pt(-talpha,n-1,a))\\n> par(new=TRUE)\\n> plot(alpha,power12,type=\"l\",xlim=c(0,1),ylim=c(0,1),xlab=c(\"\"),ylab=c(\"\"),col=\"red\")\\n> n<-24; talpha<-qt(1-alpha/2,n-1); a<-delta/(sd(radon)/sqrt(n-1))\\n> power24<-1-(pt(talpha,n-1,a)-pt(-talpha,n-1,a))\\n> par(new=TRUE)\\n> plot(alpha,power24,type=\"l\",xlim=c(0,1),ylim=c(0,1),xlab=c(\"\"),ylab=c(\"\"),col=\"blue\")\\n\\nWe compare the ROCs for low signiﬁcance levels by using the head\\ncommand. Notice how the power increases with sample size.\\n\\n> head(data.frame(alpha,power6,power12,power24))\\n\\nalpha\\n\\npower6\\n\\npower12\\n\\npower24\\n0.00 0.00000000 0.0000000 0.0000000\\n0.01 0.04304033 0.1435099 0.4179953\\n0.02 0.07812389 0.2199330 0.5299071\\n0.03 0.10924263 0.2773645 0.5988122\\n0.04 0.13759358 0.3241511 0.6480282\\n0.05 0.16380961 0.3638693 0.6858441\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n\\n20.8. This means that we can reject the null hypothesis that transgenic\\nand wildtype mosquitoes have the seem mean lifetime. For a one-\\nsided test, the p-value is 0.01699/2 = 0.008549.\\n20.9. By the correspondence between two-sided hypothesis tests and\\nconﬁdence intervals, the fact that 0 is not in the conﬁdence interval,\\n\\nFigure 20.9: Receiver operating characteristic for the\\nradon detector with 6 (black), 12 (red) and 24 (blue)\\nobservations and a null µ0 = 105 and an alternative\\nµ = 110.\\n\\n382\\n\\n951001051101150.20.40.60.8mupi0.00.20.40.60.81.00.00.20.40.60.81.0significancepower0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0\\x0cIntroduction to the Science of Statistics\\n\\nt Procedures\\n\\nindicates that the test is signiﬁcant at the 2% level and thus the p-value < 0.02. Notice that the output shows a p-value\\nof 0.01699. For a one-sided test, we know that the p-value is half that of a two-sided test and thus below 0.01. Notice\\nthat the R give a p-value of 0.008497 for a one-sided test.\\n20.11. The numerator in the test statistics\\n\\nFor the unpaired two-sample test.\\n\\n¯xm − ¯xf = 29.65919 − 27.20299 = 2.4562.\\n\\nxm + s2\\ns2\\n\\nxf = 5.7000422 + 5.3147772 = 60.73733\\n\\nand\\n\\nt =\\n\\n¯xm − ¯xf\\nxm−xf /n\\n\\n=\\n\\n(cid:113)s2\\n\\n2.4562\\n\\n(cid:112)60.73733/78\\n\\n= 2.783448,\\n\\nmatching the R output. For the paired two-sample test\\ns2\\nxm−xf = s2\\n\\nxm + s2\\n\\nxf − 2rsxm sxf = 5.7000422 + 5.3147772 − 2(0.3571538)(5.700042)(5.314777) = 10.73462.\\n\\nand\\n\\nt =\\n\\n¯xm − ¯xf\\nxm−xf /n\\n\\n=\\n\\n(cid:113)s2\\n\\n2.4562\\n\\n(cid:112)10.7346278/78\\n\\n= 6.62091,\\n\\nagain matching the output.\\n20.12. Under the null hypothesis, the two sets of observations, X1, . . . , Xn and Y1, . . . , Yn have the same distribution.\\nThus, ∆k = Yk − Xk, k = 1, . . . , n are independent and their distribution is symmetric about zero. Thus, we can\\nrandomly take the sign ±∆k as the basis for the permutation test. Here is the R code. We use one million simulations\\nto be able to determine a small p-value\\n> delta<-factory-haiti\\n> deltamean<-mean(delta)\\n> deltasim<-numeric(1000000)\\n> for (i in 1:1000000){deltasim[i]<-mean(rbinom(length(delta),1,0.5)*delta)}\\n> length(deltasim[deltasim>deltamean])/1000000\\n[1] 4e-05\\n\\nThis yields a permutation test p-value of 4 × 10−5. The t-test procedure gave a very similar p-value, 3.745 × 10−5\\n20.13. We prove this using mathematical induction. For the case m = 1, we have 1 = 1(1+1)\\nand the identity holds\\ntrue. Now assume that the identity holds for m = k. We then check that it also holds for m = k + 1\\n\\n2\\n\\n1 + 2 + ··· + k + (k + 1) =\\n\\nk(k + 1)\\n\\n2\\n\\n+ (k + 1) =(cid:18) k\\n\\n2\\n\\n+ 1(cid:19) (k + 1) =\\n\\nk + 2\\n\\n2\\n\\n(k + 1) =\\n\\n(k + 1)(k + 2)\\n\\n2\\n\\n.\\n\\nSo, by the principle of mathematical induction, we have the identity for all non-negative integers.\\n20.14. By Exercise 20.10, the sum of the ranks\\n\\nRx + Ry =\\n\\n(nx + ny)(nx + ny + 1)\\n\\n2\\n\\nThus,\\n\\nUy + Ux =(cid:18)Ry −\\n\\nny(ny + 1)\\n\\n2\\n\\n(cid:19) +(cid:18)Rx −\\n\\n(ny + nx)(ny + nx + 1)\\n\\n(cid:19)\\n\\nnx(nx + 1)\\n\\n2\\nny(ny + 1)\\n\\n−\\n\\n2\\n\\n−\\n\\n2\\n\\nnx(nx + 1)\\n\\n2\\n\\n=\\n\\n=\\n\\n1\\n2\\n\\n(ny(ny + 1) + nynx + nx(nx + 1) + nynx − ny(ny + 1) − nx(nx + 1)) = nynx.\\n\\n383\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\n384\\n\\n\\x0cTopic 21\\n\\nGoodness of Fit\\n\\nThe object of this paper is to investigate a criterion of the probability on any theory of an observed system\\nof errors, and to apply it to the determination of goodness of ﬁt. - Karl Pearson. 1900, On the Criterion\\nthat a Given System of Deviations from the Probable in the Case of a Correlated System of Variables is\\nsuch that it can be Reasonably Supposed to have Arisen from Random Sampling, Philosophical Magazine\\n\\n21.1 Fit of a Distribution\\nGoodness of ﬁt tests examine the case of a sequence of independent observations each of which can have 1 of k\\npossible categories. For example, each of us has one of 4 possible of blood types, O, A, B, and AB. The local blood\\nbank has good information from a national database of the fraction of individuals having each blood type,\\n\\nπO, πA, πB, and πAB.\\n\\nThe actual fraction pO, pA, pB, and pAB of these blood types in the community for a given blood bank may be different\\nthan what is seen in the national database. As a consequence, the local blood bank may choose to alter its distribution\\nof blood supply to more accurately reﬂect local conditions.\\n\\nTo place this assessment strategy in terms of formal hypothesis testing, let π = (π1, . . . , πk) be postulated values\\n\\nof the probability\\n\\nand let p = (p1, . . . , pk) denote the possible states of nature. Then, the parameter space is\\n\\nPπ{individual is a member of i-th category} = πi\\n\\nΘ = {p = (p1, . . . , pk); pi ≥ 0 for all i = 1, . . . , k,\\n\\nk(cid:88)i=1\\n\\npi = 1}.\\n\\nThis parameter space has k − 1 free parameters. Once these are chosen, the remaining parameter value is determined\\nby the requirement that the sum of the pi equals 1. Thus, dim(Θ) = k − 1.\\n\\nThe hypothesis is\\n\\nH0 : pi = πi, for all i = 1, . . . , k versus H1 : pi (cid:54)= πi, for some i = 1, . . . , k.\\n\\n(21.1)\\n\\nThe parameter space for the null hypothesis is a single point π = (π1, . . . , πk). Thus, dim(Θ0) = 0. Consequently,\\nthe likelihood ratio test will have a chi-square test statistic with dim(Θ) − dim(Θ0) = k − 1 degrees of freedom. The\\ndata x = (x1, . . . , xn) are the categories for each of the n observations.\\n\\nLet’s use the likelihood ratio criterion to create a test for the distribution of human blood types in a given popula-\\n\\ntion. For the data\\n\\nx = {O, B, O, A, A, A, A, A, O, AB}\\n\\n385\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nfor the blood types of tested individuals, then, in the case of independent observations, the likelihood is\\n\\nL(p|x) = pO · pB · pO · pA · pA · pA · pA · pA · pO · pAB = p3\\n\\nOp5\\n\\nApBpAB.\\n\\nNotice that the likelihood has a factor of pi whenever an observation take on the value i. In other words, if we\\n\\nsummarize the data using\\n\\nni = #{observations from category i}\\n\\nto create n = (n1, n2,··· , nk), a vector that records the number of observations in each category, then, the likelihood\\nfunction\\n(21.2)\\nThe likelihood ratio is the ratio of the maximum value of the likelihood under the null hypothesis and the maxi-\\n\\nL(p|n) = pn1\\n\\n1 ··· pnk\\nk .\\n\\nmum likelihood for any parameter value. In this case, the numerator is the likelihood evaluated at π.\\n\\nΛ(n) =\\n\\nL(π|n)\\nL(ˆp|n)\\n\\n=\\n\\n1 πn2\\nπn1\\nˆpn1\\n1 ˆpn2\\n\\n2 ··· πnk\\n2 ··· ˆpnk\\n\\nk\\n\\nk\\n\\n=(cid:18) π1\\nˆp1(cid:19)n1\\n\\n···(cid:18) πk\\nˆpk(cid:19)nk\\n\\n.\\n\\n(21.3)\\n\\nTo ﬁnd the maximum likelihood estimator ˆp, we, as usual, begin by taking the logarithm in (21.2),\\n\\nln L(p|n) =\\n\\nni ln pi.\\n\\nk(cid:88)i=1\\n\\nk(cid:88)i=1\\n\\ns(p) =\\n\\npi = 1.\\n\\nBecause not every set of values for pi is admissible, we cannot just take derivatives, set them equal to 0 and solve.\\n\\nIndeed, we must ﬁnd a maximum under the constraint\\n\\nThe maximization problem is now stated in terms of the method of Lagrange multipliers. This method tells us\\nthat at the maximum likelihood estimator (ˆp1, . . . , ˆpk), the gradient of ln L(p|n) is proportional to the gradient of the\\nconstraint s(p). To explain this brieﬂy, recall that the gradient of a function is a vector that is perpendicular to a level\\nset of that function. In this case,\\n\\n∇ˆps(p)\\n\\nis perpendicular to the level set {p; s(p) = 1}.\\n\\nNow imagine walking along the set of parameter values of p given by the constraint s(p) = 1, keeping track of\\nthe values of the function ln L(p|n). If the walk takes us from a value of this function below (cid:96)0 to values above (cid:96)0\\nthen (See Figure 21.1.), the level surfaces\\n\\n{p; s(p) = 1}\\n\\nand\\n\\n{ln L(p|n = (cid:96)0}\\n\\n∇ˆps(p)\\n\\nand ∇ˆp ln L(p|n)\\n\\nintersect. Consequently, the gradients\\n\\npoint in different directions on the intersection of these two surfaces. At a local maximum or minimum of the log-\\nlikelihood function, the level surfaces are tangent and the two gradients are parallel. In other words, the these two\\ngradients vectors are related by a constant of proportionality, λ, known as the Lagrange multiplier. Consequently, at\\nextreme values,\\n\\n(cid:18) ∂\\n\\n∂p1\\n\\nln L(ˆp|n), . . . ,\\n\\n∂\\n∂pk\\n\\n∇p ln L(ˆp|n) = λ∇ˆps(p).\\n\\nln L(ˆp|n)(cid:19) = λ(cid:18) ∂\\n(cid:18) n1\\n\\nˆpk(cid:19) = λ(1, . . . , 1)\\n\\n, . . . ,\\n\\n∂p1\\n\\nnk\\n\\nˆp1\\n\\ns(p), . . . ,\\n\\n∂\\n∂pk\\n\\ns(p)(cid:19)\\n\\n386\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nFigure 21.1: Lagrange multipliers Level sets of the log-likelihood function.shown in dashed blue. The level set {s(p) = 1} shown in black. The\\ngradients for the log-likelihood function and the constraint are indicated by dashed blue and black arrows, respectively. At the maximum, these two\\narrows are parallel. Their ratio λ is called the Lagrange multiplier. If we view the blue dashed lines as elevation contour lines and the black line as\\na trail, crossing contour line indicates walking either up or down hill. When the trail reaches its highest elevation, the trail is tangent to a contour\\nline and the gradient for the hill is perpendicular to the trail.\\n\\nEach of the components of the two vectors must be equal. In other words,\\n\\nni\\nˆpi\\n\\n= λ, ni = λˆpi\\n\\nfor all i = 1, . . . , k.\\n\\n(21.4)\\n\\nNow sum this equality for all values of i and use the constraint s(p) = 1 to obtain\\n\\nReturning to (21.4), we have that\\n\\nn =\\n\\nk(cid:88)i=1\\n\\nn1\\nˆpi\\n\\nni = λ\\n\\nk(cid:88)i=1\\n\\nˆpi = λs(ˆp) = λ.\\n\\n= n and\\n\\nˆpi =\\n\\nni\\nn\\n\\n.\\n\\n(21.5)\\n\\nThis is the answer we would guess - the estimate for pi is the fraction of observations in category i. Thus, for the\\n\\nintroductory example,\\n\\nˆpO =\\n\\n3\\n10\\n\\n,\\n\\nˆpA =\\n\\n5\\n10\\n\\n,\\n\\nˆpB =\\n\\n1\\n10\\n\\n,\\n\\nand\\n\\nˆpO =\\n\\n1\\n10\\n\\n.\\n\\nNext, we substitute the maximum likelihood estimates ˆpi = ni/n into the likelihood ratio (21.3) to obtain\\n\\nΛ(n) =\\n\\nL(π|n)\\nL(ˆp|n)\\n\\n=(cid:18) π1\\n\\nn1/n(cid:19)n1\\n\\n···(cid:18) πk\\n\\nnk/n(cid:19)nk\\n\\n=(cid:18) nπ1\\nn1 (cid:19)n1\\n\\n···(cid:18) nπk\\nnk (cid:19)nk\\n\\n.\\n\\n(21.6)\\n\\nRecall that we reject the null hypothesis if this ratio is too low, i.e, the maximum likelihood under the null hypoth-\\n\\nesis is sufﬁciently smaller than the maximum likelihood under the alternative hypothesis.\\n\\nLet’s review the process. the random variables X1, X2, . . . , Xn are independent, taking values in one of k cate-\\ngories each having distribution π. In the example, we have 4 categories, namely the common blood types O, A, B,\\nand AB. Next, we organize the data into\\n\\nNi = #{j; Xj = i},\\n\\n387\\n\\ns(p) = 1     level sets for the log-likelihood, values increasingmoving downward   −−\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nthe number of observations in category i. Next, create the vector N = (N1, . . . , Nk) to be the vector of observed\\nnumber of occurrences for each category i. In the example we have the vector (3,5,1,1) for the number of occurrences\\nof the 4 blood types.\\n\\nWhen the null hypothesis holds true, −2 ln Λ(N) has approximately a χ2\\n\\nthe the likelihood ratio test statistic\\n\\nk−1 distribution. Using (21.6) we obtain\\n\\n−2 ln Λ(N) = −2\\n\\nk(cid:88)i=1\\n\\nNi ln\\n\\nnπi\\nNi\\n\\n= 2\\n\\nNi ln\\n\\nNi\\nnπi\\n\\nk(cid:88)i=1\\n\\nThe last equality uses the identity ln(1/x) = − ln x for the logarithm of reciprocals.\\nThe test statistic −2 ln Λn(n) is generally rewritten using the notation Oi = ni for the number of observed\\noccurrences of i and Ei = nπi for the number of expected occurrences of i as given by H0. Then, we can write the\\ntest statistic as\\n\\n−2 ln Λn(O) = 2\\n\\nOi ln\\n\\nOi\\nEi\\n\\n(21.7)\\n\\nk(cid:88)i=1\\n\\nThis is called the G2 test statistic. Thus, we can perform our inference on the hypothesis (21.1) by evaluating G2.\\nThe p-value will be the probability that the a χ2\\n\\nk−1 random variable takes a value greater than −2 ln Λn(O)\\n\\nThe traditional method for a test of goodness of ﬁt, we use, instead of the G2 statistic, the chi-square statistic\\n\\nχ2 =\\n\\nk(cid:88)i=1\\n\\n(Ei − Oi)2\\n\\nEi\\n\\n.\\n\\n(21.8)\\n\\nThus, large values for χ2 result from large differences between the observed values Oi and the expected values Ei.\\nConsequently. large values of χ2 is evidence against the null hypothesis.\\n\\nThe χ2 test statistic was introduced between 1895 and 1900 by Karl Pearson and consequently has been in use for\\nlonger that the concept of likelihood ratio tests. Indeed, R output call the test Pearson’s Chi-squared test\\nin the case of contingency tables, our next topic.\\n\\nWe establish the relation between (21.7) and (21.8), through the following two exercises.\\n\\nExercise 21.1. Deﬁne\\n\\nShow that\\n\\nδi =\\n\\nOi − Ei\\n\\nEi\\n\\n=\\n\\nOi\\nEi − 1.\\n\\nEiδi = 0 and Ei(1 + δi) = Oi.\\n\\nk(cid:88)i=1\\n\\nExercise 21.2. Show the relationship between the G2 and χ2 statistics in (21.7) and (21.8) by applying the quadratic\\nTaylor polynomial approximation for the natural logarithm,\\n\\nln(1 + δi) ≈ δi −\\n\\n1\\n2\\n\\nδ2\\ni\\n\\nand keeping terms up to the square of δi\\n\\nTo compute either the G2 or χ2 statistic, we begin by creating a table.\\n\\ni\\n\\n1\\n\\n2\\nobserved O1 O2\\nexpected E1 E2\\n\\n···\\nk\\n··· Ok\\n··· Ek\\n\\nWe show this procedure using a larger data set on blood types.\\n\\n388\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nExample 21.3. The Red Cross recommends that a blood bank maintains 44% blood type O, 42% blood type A, 10%\\nblood type B, 4% blood type AB. You suspect that the distribution of blood types in Tucson is not the same as the\\nrecommendation. In this case, the hypothesis is\\n\\nH0 : pO = 0.44, pA = 0.42, pB = 0.10, pAB = 0.04 versus H1 : at least one pi is unequal to the given values\\n\\nBased on 400 observations, we observe 228 for type O, 124 for type A, 40 for type B and 8 for type AB by\\n\\ncomputing 400 × pi using the values in H0. This gives the table\\nA\\n124\\n168\\n\\nobserved\\nexpected\\n\\nO\\n228\\n176\\n\\ntype\\n\\nB AB\\n8\\n40\\n40\\n16\\n\\nUsing this table, we can compute the value of either (21.7) and (21.8). The chisq.test command in R uses\\n\\n(21.8). The program computes the expected number of observations.\\n\\n> chisq.test(c(228,124,40,8),p=c(0.44,0.42,0.10,0.04))\\n\\nChi-squared test for given probabilities\\n\\ndata:\\nX-squared = 30.8874, df = 3, p-value = 8.977e-07\\n\\nc(228, 124, 40, 8)\\n\\nThe number of degrees of freedom is 4 − 1 = 3. Note that the p-value is very low and so the distribution of blood\\ntypes in Tucson is very unlikely to be the same as the national distribution. We can also perform the test using the\\nG2-statistic in (21.7):\\n\\n> O<-c(228,124,40,8)\\n> E<-sum(O)*c(0.44,0.42,0.10,0.04)\\n> G2stat<-2*sum(O*log(O/E))\\n> G2stat\\n[1] 31.63731\\n> 1-pchisq(G2stat,3)\\n[1] 6.240417e-07\\n\\nOne way to visualize the discrepancies from from the null hypothesis is to display them with a hanging chi-gram.\\n\\nThis plots category i with a bar of height of the standardized residuals (also known as Pearson residuals).\\n\\nOi − Ei√Ei\\n\\n.\\n\\n(21.9)\\n\\nNote that these values can be either positive or negative.\\n\\n> resid<-(O-E)/sqrt(E)\\n> barplot(resid, names.arg=c(\"O\",\"A\",\"B\",\"AB\"),\\n\\nxlab=\"chigram for blood donation data\")\\n\\nExample 21.4. Is sudden infant death syndrome seasonal (SIDS)? Here we are hypothesizing that 1/4 of each of the\\noccurrences of sudden infant death syndrome take place in the spring, summer, fall, and winter. Let p1, p2, p3, and p4\\nbe the respective probabilities for these events. Then the hypothesis takes the form\\n\\nH0 : p1 = p2 = p3 = p4 =\\n\\n1\\n4\\n\\n,\\n\\nversus H1 : at least one pi is unequal to 1\\n4\\n\\n.\\n\\nTo test this hypothesis, public health ofﬁcials from King County, Washington, collect data on n = 322 cases,\\n\\nﬁnding\\n\\nn1 = 78, n2 = 71, n3 = 87, n4 = 86\\n\\n389\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nFigure 21.2: The heights of the bars for each category are the standardized residuals (21.9). Thus, blood type O is overrepresented and types A\\nand AB are underrepresented compare to the expectations under the null hypothesis.s\\n\\nfor deaths in the spring, summer, fall, and winter, respectively. Thus, we ﬁnd more occurrences of SIDS in the fall and\\nwinter. Is this difference statistical signiﬁcant or are these difference better explained by chance ﬂuctuations?\\n\\nWe carry out the chi square test. In this case, each of the 4 categories is equally probable. Because this is the\\n\\ndefault value in R, we need not include this in the command.\\n\\n> chisq.test(c(78,71,87,86))\\n\\nChi-squared test for given probabilities\\n\\ndata:\\nX-squared = 2.0994, df = 3, p-value = 0.552\\n\\nc(78, 71, 87, 86)\\n\\nThis p-value is much to high to reject the null hypothesis.\\n\\nExample 21.5 (Hardy-Weinberg equilibrium). As we saw with Gregor Mendel’s pea experiments, the two-allele\\nHardy-Weinberg principle states that after two generations of random mating the genotypic frequencies can be repre-\\nsented by a binomial distribution. So, if a population is segregating for two alleles A1 and A2 at an autosomal locus\\nwith frequencies p1 and p2, then random mating would give a proportion\\n\\np11 = p2\\n\\n1 for the A1A1 genotype, p12 = 2p1p2 for the A1A2 genotype, and p22 = p2\\n\\n2 for the A2A2 genotype.\\n\\nThen, with both genes in the homozygous genotype and half the genes in the heterozygous genotype, we ﬁnd that\\n\\np1 = p11 +\\n\\n1\\n2\\n\\np12\\n\\np2 = p22 +\\n\\n1\\n2\\n\\np12.\\n\\n(21.11)\\n\\nOur parameter space Θ = {(p11, p12, p22); p11 + p12 + p22 = 1} is 2 dimensional. Θ0, the parameter space\\nfor the null hypothesis, are those values p1, p2 that satisfy (21.11). With the choice of p1, the value p2 is determined\\nbecause p1 + p2 = 1. Thus, dim(Θ0) = 1. Consequently, the chi-square test statistic will have 2-1=1 degree of\\nfreedom. Another way to see this is the following.\\n\\nMcDonald et al. (1996) examined variation at the CVJ5 locus in the American oyster, Crassostrea virginica. There\\n\\nwere two alleles, L and S, and the genotype frequencies in Panacea, Florida were 14 LL, 21 LS, and 25 SS. So,\\n\\nˆp11 =\\n\\n14\\n60\\n\\n,\\n\\nˆp12 =\\n\\n21\\n60\\n\\n,\\n\\nˆp22 =\\n\\n25\\n60\\n\\n.\\n\\n390\\n\\n(21.10)\\n\\nOABABchigram for blood donation data-3-2-10123\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nFigure 21.3: Plot of the chi-square density function with 3 degrees of freedom. The black vertical bar indicates the value of the test statistic in\\nExample 21.3. The area 0.552 under the curve to the right of the vertical line is the p-value for this test. This is much to high to reject the null\\nhypothesis. The red vertical lines show the critical values for a test with signiﬁcance α = 0.05 (to the left) and α = 0.01 (to the right). Thus, the\\narea under the curve to the right of these vertical lines is 0.05 and 0.01, respectively. These values can be found using qchisq(1-α,3). We can\\nalso see that the test statistic value of 30.8874 in Example 21.3 has a very low p-value.\\n\\nSo, the estimate of p1 and p2 are\\n\\n1\\n2\\nSo, the expected number of observations is\\n\\nˆp1 = ˆp11 +\\n\\nˆp12 =\\n\\n49\\n120\\n\\n,\\n\\nˆp2 = ˆp22 +\\n\\n1\\n2\\n\\nˆp12 =\\n\\n71\\n120\\n\\n.\\n\\nE11 = 60ˆp2\\n\\n1 = 10.00417, E12 = 60 × 2ˆp1 ˆp2 = 28.99167, E22 = 60ˆp2\\n\\n2 = 21.00417.\\n\\nThe chi-square statistic\\n\\nχ2 =\\n\\n(14 − 10)2\\n\\n10\\n\\n+\\n\\n(21 − 29)2\\n\\n29\\n\\n+\\n\\n(25 − 21)2\\n\\n21\\n\\n= 1.600 + 2.207 + 0.762 = 4.569\\n\\nThe p-value\\n\\n> 1-pchisq(4.569,1)\\n[1] 0.03255556\\n\\nThus, we have moderate evidence against the null hypothesis of a Hardy-Weinberg equilibrium. Many forces may\\n\\nbe the cause of this - non-random mating, selection, or migration to name a few possibilities.\\nExercise 21.6. Perform the chi-squared test using the G2 statistic for the example above.\\n\\n21.2 Contingency tables\\nContingency tables, also known as two-way tables or cross tabulations are a convenient way to display the frequency\\ndistribution from the observations of two categorical variables. For an r× c contingency table, we consider two factors\\nA and B for an experiment. This gives r categories\\n\\nA1, . . . Ar\\n\\n391\\n\\n0246810120.000.050.100.150.200.25xchi square density0246810120.000.050.100.150.200.250246810120.000.050.100.150.200.250246810120.000.050.100.150.200.250246810120.000.050.100.150.200.25\\x0cIntroduction to the Science of Statistics\\n\\nfor factor A and c categories\\n\\nB1, . . . Bc\\n\\nGoodness of Fit\\n\\nfor factor B.\\n\\nHere, we write Oij to denote the number of occurrences for which an individual falls into both category Ai and\\n\\ncategory Bj. The results is then organized into a two-way table.\\n\\nB1\\nB2\\nO11 O12\\nA1\\nO21 O22\\nA2\\n...\\n...\\n...\\nOr1 Or2\\nAr\\ntotal O·1 O·2\\n\\nBc\\n\\ntotal\\n···\\n··· O1c O1·\\n··· O2c O2·\\n...\\n...\\n··· Orc Or·\\n··· O·c\\nn\\n\\n...\\n\\nExample 21.7. Returning to the study of the smoking habits of 5375 high school children in Tucson in 1967, here is a\\ntwo-way table summarizing some of the results.\\n\\nstudent\\nsmokes\\n\\nstudent\\n\\ndoes not smoke\\n\\n2 parents smoke\\n1 parent smokes\\n0 parents smoke\\ntotal\\n\\n400\\n416\\n188\\n1004\\n\\n1380\\n1823\\n1168\\n4371\\n\\ntotal\\n1780\\n2239\\n1356\\n5375\\n\\nFor a contingency table, the null hypothesis we shall consider is that the factors A and B are independent. For\\nthe experimental design, we assume that the number of observations n is ﬁxed but the marginal distributions (row and\\ncolumn totals) are not.\\n\\nTo set the parameters for this model, we deﬁne\\n\\npij = P{an individual is simultaneously a member of category Ai and category Bj}.\\n\\nThen, we have the parameter space\\n\\nΘ = {p = (pij, 1 ≤ i ≤ r, 1 ≤ j ≤ c); pij ≥ 0 for all i, j = 1,\\n\\nr(cid:88)i=1\\n\\nc(cid:88)j=1\\n\\npij = 1}.\\n\\nWrite the marginal distribution\\n\\nand\\n\\npi· =\\n\\np·j =\\n\\npij = P{an individual is a member of category Ai}\\n\\npij = P{an individual is a member of category Bj}.\\n\\nThe null hypothesis of independence of the categories A and B can be written\\n\\nc(cid:88)j=1\\nr(cid:88)i=1\\n\\nWrite\\n\\nwhere\\n\\nH0 : pij = pi·p·j, for all i, j\\n\\nversus H1 : pij (cid:54)= pi·p·j, for some i, j.\\n\\nn = {nij, 1 ≤ i ≤ r, 1 ≤ j ≤ c}\\n\\nnij = #{observations simultaneously in category Ai and category Bj}.\\n\\n392\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAs with 21.2, the likelihood function\\n\\nL(p|n) =\\n\\nr(cid:89)i=1\\n\\nc(cid:89)j=1\\n\\nnij\\nij .\\n\\np\\n\\nGoodness of Fit\\n\\n(21.12)\\n\\nFollow the procedure as before for the goodness of ﬁt test to end with a G2 and its corresponding χ2 test statistic.\\nThe G2 statistic follows from the likelihood ratio test criterion. The χ2 statistics is a second order Taylor series\\napproximation to G2.\\n\\n−2\\n\\nr(cid:88)i=1\\n\\nc(cid:88)j=1\\n\\nOij ln\\n\\nEij\\nOij ≈\\n\\nr(cid:88)i=1\\n\\nc(cid:88)j=1\\n\\n(Oij − Eij)2\\n\\nEij\\n\\n.\\n\\n(21.13)\\n\\nThe null hypothesis pij = pi·p·j can be written in terms of observed and expected observations as\\n\\nor\\n\\nEij\\nn\\n\\n=\\n\\nOi·\\nn\\n\\nO·j\\nn\\n\\n.\\n\\nEij =\\n\\nOi·O·j\\n\\nn\\n\\n.\\n\\nThe test statistic, under the null hypothesis, has a χ2 distribution. To determine the number of degrees of freedom,\\n\\nconsider the following. Start with a contingency table with no entries but with the prescribed marginal values.\\n\\nB1 B2\\n\\nA1\\nA2\\n...\\nAr\\ntotal O·1 O·2\\n\\n··· Bc\\n\\n··· O·c\\n\\ntotal\\nO1·\\nO2·\\n...\\nOr·\\nn\\n\\nThe number of degrees of freedom is the number of values that we can place in the contingency table before all the\\nremaining values are determined. To begin, ﬁll in the ﬁrst row with values E11, E12, . . . , E1,c−1. The ﬁnal value E1,c\\nin this determined by the other values in the row and the constraint that the row sum must be O1·. Continue ﬁlling the\\nrows, noting that the value in column c is determined by the constraint on the row sum. Finally, when the time comes\\nto ﬁll in the bottom row r, notice that all the values are determined by the constraint on the row sums O·j. Thus, we\\ncan ﬁll c− 1 values in each of the r− 1 rows before the remaining values are determined. Thus, the number of degrees\\nof freedom is (r − 1) × (c − 1).\\nExercise 21.8. Verify that G2 statistic in (21.13) is the likelihood ratio test statistic.\\n\\nExercise 21.9. Give dim(Θ) and dim(Θ0), the dimensions, respectively, of the parameter space and the null hypoth-\\nesis space. Show that the difference is (r − 1) × (c − 1),\\nExample 21.10. Returning to the data set on smoking habits in Tucson, we ﬁnd that the expected table is\\n\\nFor example,\\n\\n2 parents smoke\\n1 parent smokes\\n0 parents smoke\\ntotal\\n\\nstudent\\nsmokes\\n332.49\\n418.22\\n253.29\\n1004\\n\\nstudent\\n\\ndoes not smoke\\n\\n1447.51\\n1820.78\\n1102.71\\n\\n4371\\n\\ntotal\\n1780\\n2239\\n1356\\n5375\\n\\nE11 =\\n\\nO1·O·1\\n\\nn\\n\\n=\\n\\n1780 · 1004\\n\\n5375\\n\\n= 332.49.\\n\\n393\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nTo compute the chi-square statistic\\n\\n(400−332.49)2\\n\\n332.49\\n\\n+ (416−418.22)2\\n+ (188−253.29)2\\n\\n418.22\\n\\n253.29\\n\\n1447.51\\n\\n+ (1380−1447.51)2\\n+ (1823−1820.78)2\\n+ (1168−1102.71)2\\n\\n1820.78\\n\\n1102.71\\n\\n= 13.71\\n\\n+ 3.15\\n\\n+ 0.012\\n\\n+ 0.003\\n\\n+ 16.83\\n\\n+ 3.866\\n\\n= 37.57\\n\\nThe number of degrees of freedom is (r − 1) × (c − 1) = (3 − 1) × (2 − 1) = 2. This can be seen by noting that\\n\\none the ﬁrst two entries in the ”student smokes” column is ﬁlled, the rest are determined. Thus, the p-value\\n\\n> 1-pchisq(37.57,2)\\n[1] 6.946694e-09\\n\\nis very small and leads us to reject the null hypothesis. Thus, we conclude that children smoking habits are not\\nindependent of their parents smoking habits. An examination of the individual cells shows that the children of parents\\nwho do not smoke are less likely to smoke and children who have two parents that smoke are more likely to smoke.\\nUnder the null hypothesis, each cell has a mean approximately 1 and so values much greater than 1 show contribution\\nthat leads to the rejection of H0.\\n\\nR does the computation for us using the chisq.test command\\n\\n> smoking<-matrix(c(400,416,188,1380,1823,1168),nrow=3)\\n> smoking\\n\\n[,1] [,2]\\n400 1380\\n416 1823\\n188 1168\\n\\n[1,]\\n[2,]\\n[3,]\\n> chisq.test(smoking)\\n\\nPearson’s Chi-squared test\\n\\ndata:\\nX-squared = 37.5663, df = 2, p-value = 6.959e-09\\n\\nsmoking\\n\\nWe can look at the residuals (Oij − Eij)/(cid:112)Eij for the entries in the χ2 test as follows.\\n\\n> smokingtest<-chisq.test(smoking)\\n> residuals(smokingtest)\\n\\n[,1]\\n\\n[,2]\\n[1,]\\n3.7025160 -1.77448934\\n[2,] -0.1087684 0.05212898\\n[3,] -4.1022973 1.96609088\\n\\nNotice that if we square these values, we obtain the entries found in computing the test statistic.\\n\\n394\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\n> residuals(smokingtest)ˆ2\\n\\n[,1]\\n\\n[,2]\\n[1,] 13.70862455 3.14881241\\n[2,]\\n0.01183057 0.00271743\\n[3,] 16.82884348 3.86551335\\n\\nExercise 21.11. Make three horizontally placed chigrams that summarize the residuals for this χ2 test in the example\\nabove.\\n\\nExercise 21.12 (two-by-two tables). Here is the contingency table can be thought of as two sets of Bernoulli trials as\\nshown.\\n\\ngroup 1\\n\\ngroup 2\\n\\nsuccesses\\n\\nx1\\n\\nx2\\n\\ntotal\\n\\nx1 + x2\\n\\nfailures\\n\\ntotal\\n\\nn1 − x1\\n\\nn1\\n\\nn2 − x2\\n\\nn2\\n\\n(n1 + n2) − (x1 + x2)\\n\\nn1 + n2\\n\\nShow that the chi-square test is equivalent to the two-sided two sample proportion test.\\n\\n21.3 Applicability and Alternatives to Chi-squared Tests\\nThe chi-square test uses the central limit theorem and so is based on the ability to use a normal approximation. One\\ncriterion, the Cochran conditions requires no cell has count zero, and more than 80% of the cells have counts at least\\n5. If this does not hold, then Fisher’s exact test uses the hypergeometric distribution (or its generalization) directly\\nrather than normal approximation.\\nFor example, for the 2 × 2 table,\\n\\nB2\\n\\ntotal\\nB1\\nO11 O12 O1·\\nO21 O22 O2·\\nn\\n\\nA1\\nA2\\ntotal O·1 O·2\\n\\nThe idea behind Fisher’s exact test is to begin with an empty table:\\n\\nB1 B2\\n\\nA1\\nA2\\ntotal O·1 O·2\\n\\ntotal\\nO1·\\nO2·\\nn\\n\\nand a null hypothesis that uses equally likely outcomes to ﬁll in the table. We will use as an analogy the model of\\nmark and recapture. Normally the goal is to ﬁnd n, the total population. In this case, we assume that this population\\nsize is known and will consider the case that the individuals in the two captures are independent. This is assumed in\\nthe mark and recapture protocol. Here we test this independence.\\n\\nIn this regard,\\n• A1 - an individual in the ﬁrst capture and thus tagged.\\n• A2 - an individual not in the ﬁrst capture and thus not tagged.\\n• B1 - an individual in the second capture.\\n• B2 - an individual not in the second capture\\nThen, from the point of view of the A classiﬁcation:\\n\\n395\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\n• We have O1· from a population n with the A1 classiﬁcation (tagged individuals). This can be accomplished in\\n\\n(cid:18) n\\nO1·(cid:19) =\\n\\nn!\\n\\nO1·!O2·!\\n\\nways. The remaining O2· = n − O1· have the A2 classiﬁcation (untagged individuals). Next, we ﬁll in the\\nvalues for the B classiﬁcation\\n\\n• From the O·1 belonging to category B1 (individuals in the second capture), O11 also belong to A1 (have a tag).\\n\\nThis outcome can be accomplished in\\n\\nways.\\n\\n(cid:18)O·1\\nO11(cid:19) =\\n\\nO·1!\\n\\nO11!O21!\\n\\n• From the O·2 belonging to category B2 (individuals not in the second capture), O12 also belong to A1 (have a\\n\\ntag). This outcome can be accomplished in\\n\\nways.\\n\\n(cid:18)O·2\\nO21(cid:19) =\\n\\nO·2!\\n\\nO12!O22!\\n\\nUnder the null hypothesis that every individual can be placed in any group, provided we have the given marginal\\n\\ninformation. In this case, the probability of the table above has the formula from the hypergeometric distribution\\n\\n(cid:0)O1·\\nO11(cid:1)(cid:0)O2··\\nO21(cid:1)\\n(cid:0) n\\nO·1(cid:1)\\n\\n=\\n\\nO·1!/(O11!O21!) · O·2!/(O12!O22!)\\n\\nn!/(O1·!O2·!)\\n\\n=\\n\\nO·1!O·2!O1·!O2·!\\n\\nO11!O12!O21!O22!n!\\n\\n.\\n\\n(21.14)\\n\\nNotice that the formula is symmetric in the column and row variables. Thus, if we had derived the hypergeometric\\n\\nformula from the point of view of the B classiﬁcation we would have obtained exactly the same formula (21.14).\\n\\nTo complete the exact test, we rely on statistical software to do the following:\\n• compute the hypergeometric probabilities over all possible choices for entries in the cells that result in the given\\n\\nmarginal values, and\\n\\n• rank these probabilities from most likely to least likely.\\n• Find the ranking of the actual data.\\n• For a one-sided test of too rare, the p-value is the sum of probabilities of the ranking lower than that of the data.\\nA similar procedure applies to provide the Fisher exact test for r × c tables.\\n\\nExample 21.13. As a test of the assumptions for mark and recapture. We examine a small population of 120 ﬁsh. The\\nassumption are that each group of ﬁsh are equally likely to be capture in the ﬁrst and second capture and that the two\\ncaptures are independent. This could be violated, for example, if the tagged ﬁsh are not uniformly dispersed in the\\npond.\\n\\nTwenty-ﬁve are tagged and returned to the pond. For the second capture of 30, seven are tagged. With this\\n\\ninformation, given in red in the table below, we can complete the remaining entries.\\n\\nin 2nd capture\\n\\nnot in 2nd capture\\n\\nin 1st capture\\n\\nnot in 1st capture\\n\\ntotal\\n\\n7\\n23\\n30\\n\\n18\\n72\\n90\\n\\ntotal\\n25\\n95\\n120\\n\\nFisher’s exact test show a much too high p-value to reject the null hypothesis.\\n\\n396\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\n> fish<-matrix(c(7,23,18,72),ncol=2)\\n> fisher.test(fish)\\n\\nFisher’s Exact Test for Count Data\\n\\nfish\\n\\ndata:\\np-value = 0.7958\\nalternative hypothesis: true odds ratio is not equal to 1\\n95 percent confidence interval:\\n\\n0.3798574 3.5489546\\n\\nsample estimates:\\nodds ratio\\n1.215303\\n\\nExercise 21.14. Perform the χ2 test on the data set above and report the ﬁndings.\\n\\nExample 21.15. We now return to a table on hemoglobin genotypes on two Indonesian islands. Recall that heterozy-\\ngotes are protected against malaria.\\n\\ngenotype AA AE EE\\nFlores\\n0\\nSumba\\n4\\n\\n128\\n119\\n\\n6\\n78\\n\\nWe noted that heterozygotes are rare on Flores and that it appears that malaria is less prevalent there since the\\n\\nheterozygote does not provide an adaptive advantage. Here are both the chi-square test and the Fisher exact test.\\n\\n> genotype<-matrix(c(128,119,6,78,0,4),nrow=2)\\n> genotype\\n\\n[,1] [,2] [,3]\\n0\\n4\\n\\n[1,]\\n[2,]\\n> chisq.test(genotype)\\n\\n128\\n119\\n\\n6\\n78\\n\\nPearson’s Chi-squared test\\n\\ndata:\\nX-squared = 54.8356, df = 2, p-value = 1.238e-12\\n\\ngenotype\\n\\nWarning message:\\nIn chisq.test(genotype) : Chi-squared approximation may be incorrect\\n\\nand\\n\\n> fisher.test(genotype)\\n\\nFisher’s Exact Test for Count Data\\n\\ngenotype\\n\\ndata:\\np-value = 3.907e-15\\nalternative hypothesis: two.sided\\n\\nNote that R cautions against the use of the chi-square test with these data.\\n\\n397\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\n21.4 Answer to Selected Exercise\\n21.1. For the ﬁrst identity, using δi = (Oi − Ei)/Ei.\\n\\nand for the second\\n\\nOi − Ei\\n\\nEi\\n\\nEi\\n\\nEiδi =\\n\\nk(cid:88)i=1\\n\\nk(cid:88)i=1\\nEi(1 + δi) = Ei(cid:18) Ei\\n\\nEi\\n\\n+\\n\\n=\\n\\nk(cid:88)i=1\\nEi (cid:19) = Ei\\n\\nOi − Ei\\n\\nOi\\nEi\\n\\n= Oi.\\n\\n(Oi − Ei) = n − n = 0\\n\\n21.2. We apply the quadratic Taylor polynomial approximation for the natural logarithm,\\n\\nand use the identities in the previous exercise. Keeping terms up to the square of δi, we ﬁnd that\\n\\nln(1 + δi) ≈ δi −\\n\\n1\\n2\\n\\nδ2\\ni ,\\n\\nOi ln\\n\\nEi(1 + δi) ln(1 + δi)\\n\\nOi\\nEi\\n\\n= 2\\n\\nk(cid:88)i=1\\n\\nEi(1 + δi)(δi −\\n\\n1\\n2\\n\\nδ2\\ni ) ≈ 2\\n\\nEi(δi +\\n\\n1\\n2\\n\\nδ2\\ni )\\n\\nk(cid:88)i=1\\n\\n−2 ln Λn(O) = 2\\n\\n≈ 2\\n\\n= 2\\n\\nk(cid:88)i=1\\nk(cid:88)i=1\\nk(cid:88)i=1\\nk(cid:88)i=1\\n\\nEiδi +\\n\\nEiδ2\\ni\\n\\nk(cid:88)i=1\\n\\n= 0 +\\n\\n(Ei − Oi)2\\n\\n.\\n\\nEi\\n\\n21.6. Here is the R output.\\n\\n> O<-c(14,21,25)\\n> phat<-c(O[1]+O[2]/2,O[3]+O[2]/2)/sum(O)\\n> phat\\n[1] 0.4083333 0.5916667\\n> E<-sum(O)*c(phat[1]ˆ2,2*phat[1]*phat[2],phat[2]ˆ2)\\n> E\\n[1] 10.00417 28.99167 21.00417\\n> sum(E)\\n[1] 60\\n> G2stat<-2*sum(O*log(O/E))\\n> G2stat\\n[1] 4.572896\\n> 1-pchisq(G2stat,1)\\n[1] 0.03248160\\n\\n21.8. First we maximize the likelihood L(p|n) in (21.12). As with (21.5), we ﬁnd that the maximum likelihood\\nestimate\\n\\nˆpij =\\n\\nnij\\nn\\n\\n398\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nis simply the fraction of observations that are simultaneously in categories Ai and Bj. Here n is the total number of\\nobservations. Thus,\\n\\nlog L(ˆp|n) =\\n\\nnij log\\n\\nnij\\nn\\n\\n.\\n\\nr(cid:88)i=1\\nr(cid:89)i=1\\n\\nc(cid:88)j=1\\nc(cid:89)j=1\\n\\nTo maximize under the null hypothesis note that p0,ij = pi· · p·j and therefore\\n\\nL(p0|n) =\\n\\nr(cid:89)i=1\\n\\nc(cid:89)j=1\\n\\n(pi·p·j)nij =\\n\\nnij\\ni· p\\np\\n\\nnij·j =\\n\\nr(cid:89)i=1\\n\\npni·\\ni·\\n\\n·\\n\\nc(cid:89)j=1\\n\\nn·j·j .\\np\\n\\nWe now have two maximization problems, for pi· and p·j. Again, we return to the strategy to determine the maximum\\nlikelihood estimate (21.5) to see that\\n\\nˆpi· =\\n\\nand\\n\\nˆp·j =\\n\\nni·\\nn\\n\\nn·j\\nn\\n\\n.\\n\\nThus, the maximum likelihood estimate under the null hypothesis\\nni·\\nn ·\\n\\nˆp0,ij = ˆpi· · ˆp·j =\\n\\nn·j\\nn\\n\\nand\\n\\nNext we subtract to ﬁnd the logarithm of the likelihood ratio.\\n\\nlog L(ˆp0|n) =\\n\\nn·j\\n\\nn ·\\n\\nr(cid:88)i=1\\n\\nnij log(cid:16) ni·\\nc(cid:88)j=1\\nc(cid:88)j=1\\nr(cid:88)i=1\\nn − log nij(cid:17) =\\n\\nn (cid:17) .\\nnij(cid:16)log(cid:16) ni·\\nc(cid:88)j=1\\nr(cid:88)i=1\\n\\nn ·\\n\\nOij log\\n\\nn·j\\n\\nn (cid:17) − log\\n\\nnij\\n\\nn (cid:17) .\\n\\nEij\\nOij\\n\\nlog Λ(n) = log L(ˆp0|n) − log L(ˆp|n) =\\n\\n=\\n\\nr(cid:88)i=1\\n\\nc(cid:88)j=1\\n\\nnij(cid:16)log\\n\\nni·n·j\\n\\nMultiply by −2 to obtain the desired expression for G2 as the likelihood ratio test statistic.\\n21.9. For the parameter space Θ, we have r × c probabilities pij with the single constraint that their sum is 1. Thus,\\ndim(Θ) = rc − 1. For the null hypothesis space Θ0, we have r row probabilities pi· with the constraint that the sum\\nis 1 and c column probabilities p·j with the constraint that the sum is 1. Thus, dim(Θ0) = (r − 1) + (c − 1). Finally,\\n\\ndim(Θ) − dim(Θ0) = rc − 1 − (r − 1) − (c − 1) = rc − r − c + 1 = (r − 1)(c − 1).\\n\\n21.11. Here is the R output\\n> resid<-residuals(smokingtest)\\n> colnames(resid)<-c(\"smokes\",\"does not smoke\")\\n> par(mfrow=c(1,3))\\n> barplot(resid[1,],main=\"2 parents\",ylim=c(-4.5,4.5))\\n> barplot(resid[2,],main=\"1 parent\",ylim=c(-4.5,4.5))\\n> barplot(resid[3,],main=\"0 parents\",ylim=c(-4.5,4.5))\\n\\n21.12. The table of expected observations\\n\\nsuccesses\\n\\nfailures\\n\\ntotal\\n\\ngroup 1\\nn1(x1+x2)\\n\\nn1+n2\\n\\ngroup 2\\nn2(x1+x2)\\n\\nn1+n2\\n\\ntotal\\n\\nx1 + x2\\n\\nn1((n1+n2)−(x1+x2))\\n\\nn2((n1+n2)−(x1+x2))\\n\\nn1+n2\\n\\nn1\\n\\nn1+n2\\n\\nn2\\n\\n399\\n\\n(n1 + n2) − (x1 + x2)\\n\\nn1 + n2\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nGoodness of Fit\\n\\nFigure 21.4: Chigram for the data on teen smoking in Tucson, 1967. R commands found in Exercise 21.9.\\n\\nNow, write ˆpi = xi/ni for the sample proportions from each group, and\\n\\nˆp0 =\\n\\nx1 + x2\\nn1 + n2\\n\\nfor the pooled sample proportion. Then we have the table of observed and expected observations\\n\\nobserved\\nsuccesses\\nfailures\\n\\ntotal\\n\\nexpected\\nsuccesses\\nfailures\\n\\ntotal\\n\\ngroup 1\\nn1 ˆp1\\n\\ngroup 2\\nn2 ˆp2\\n\\nn1(1 − ˆp1) n2(1 − ˆp2)\\n\\nn1\\n\\nn2\\n\\ngroup 1\\nn1 ˆp0\\n\\ngroup 2\\nn2 ˆp0\\n\\nn1(1 − ˆp0) n2(1 − ˆp0)\\n\\nn1\\n\\nn2\\n\\ntotal\\n\\n(n1 + n2)ˆp0\\n\\n(n1 + n2)(1 − ˆp0)\\n\\nn1 + n2\\n\\ntotal\\n\\n(n1 + n2)ˆp0\\n\\n(n1 + n2)(1 − ˆp0)\\n\\nn1 + n2\\n\\nThe chi-squared test statistic\\n\\n(n1( ˆp1− ˆp0))2\\n\\nn1 ˆp0\\n\\n+ (n1((1− ˆp1)−(1− ˆp0)))2\\n\\nn1(1− ˆp0)\\n\\n=\\n\\nn1\\n+ n1\\n\\n( ˆp1− ˆp0)2\\n( ˆp1− ˆp0)2\\n(1− ˆp0)\\n\\nˆp0\\n\\nn2 ˆp0\\n\\n+ (n2( ˆp2− ˆp0))2\\n+ (n2((1− ˆp2)+(1− ˆp0)))2\\n+ n2\\n+ n2\\n\\n( ˆp2− ˆp0)2\\n( ˆp2− ˆp0)2\\n(1− ˆp0)\\n\\nn2(1− ˆp0)\\n\\nˆp0\\n\\n=\\n\\nˆp0(1−p0) + n2(ˆp2 − ˆp0)2\\n= − ln Λ(x1, x2)\\nfrom the likelihood ratio computation for the two-sided two sample proportion test.\\n21.14. The R commands follow:\\n\\nn1(ˆp1 − ˆp0)2\\nn1( ˆp1− ˆp0)2+n2( ˆp2− ˆp0)2\\nˆp0(1−p0)\\n\\n=\\n\\n1\\n\\n1\\n\\nˆp0(1−p0)\\n\\n400\\n\\nsmokesdoes not smoke2 parents-4-2024smokesdoes not smoke1 parent-4-2024smokesdoes not smoke0 parents-4-2024\\x0cIntroduction to the Science of Statistics\\n\\n> chisq.test(fish)\\n\\nGoodness of Fit\\n\\nPearson’s Chi-squared test with Yates’ continuity correction\\n\\ndata:\\nX-squared = 0.0168, df = 1, p-value = 0.8967\\n\\nfish\\n\\nThe p-value is notably higher for the χ2 test.\\n\\n401\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\n402\\n\\n\\x0cTopic 22\\n\\nAnalysis of Variance\\n\\nThe above property of the variance, by which each independent cause makes its own contribution to the\\ntotal, enables us to analyze the total, and to assign, with more or less of accuracy, the several portions\\nto their appropriate causes, or groups of causes. In Table II is shown the analysis of the total variance\\nfor each plot, divided as it may be ascribed . . . . - Ronald Fisher. 1921, Studies in Crop Variation. I. An\\nexamination of the yield of dressed grain from Broadbalk, Journal of Agricultural Science\\n\\n22.1 Overview\\nTwo-sample t procedures are designed to compare the means of two populations. Our next step is to compare the\\nmeans of several populations. We shall explain the methodology through an example. Consider the data set gathered\\nfrom the forests in Borneo.\\nExample 22.1 (Rain forest logging). The data on 30 forest plots in Borneo are the number of trees per plot.\\n\\nnever logged\\n\\nlogged 1 year ago\\n\\nlogged 8 years ago\\n\\nni\\n¯yi\\nsi\\n\\n12\\n\\n23.750\\n5.065\\n\\n12\\n\\n14.083\\n4.981\\n\\n9\\n\\n15.778\\n5.761\\n\\nWe compute these statistics from the data y11, . . . y1n1,\\n\\ny21, . . . y2n2 and y31, . . . y2n2\\n\\n¯yj =\\n\\n1\\nnj\\n\\nnj(cid:88)i=1\\n\\nyij\\n\\nand s2\\n\\nj =\\n\\n1\\n\\nnj − 1\\n\\nnj(cid:88)i=1\\n\\n(yij − ¯yi)2\\n\\nOne way analysis of variance (ANOVA) is a statistical procedure that allows us to test for the differences in\\nmeans for two or more independent groups. In the situation above, we have set our design so that the data in each of\\nthe three groups is a random sample from within the groups. The basic question is: Are these means the same (the null\\nhypothesis) or not (the alternative hypothesis)?\\n\\nAs the case with the t procedures, the appropriateness of one way analysis of variance is based on the applicability\\nof the central limit theorem. As with t procedures, ANOVA has an alternative, the Kruskal-Wallis test, based on the\\nranks of the data for circumstances in which the central limit theorem does not apply.\\n\\nThe basic idea of the test is to examine the ratio of s2\\n\\nbetween, the variance between the groups 1, 2, and 3. and\\nresidual, a statistic that measures the variances within the groups. If the resulting ratio test statistic is sufﬁciently\\ns2\\nlarge, then we say, based on the data, that the means of these groups are distinct and we are able to reject the null\\nhypothesis. Even though the boxplots use different measures of center (median vs. mean) and spread (quartiles vs.\\nstandard deviation), this idea can be expressed by examining in Figure 22.1 the ﬂuctuation in the centers of boxes\\ncompared to the width of the boxes.\\n\\n403\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nAs we have seen before, this decision to reject H0 will\\nbe the consequence a sufﬁciently high value of a test statis-\\ntic - in this case the F statistic. The distribution of this\\ntest statistic will depend on the number of groups (3 in the\\nexample above) and the number of total observations (33\\nin the example above). Consequently, variances between\\ngroups that are not statistically signiﬁcant for small sample\\nsizes can become signiﬁcant as the sample sizes and, with\\nit, the power increase.\\n\\n22.2 One Way Analysis of Variance\\n\\nFor one way analysis of variance, we expand to more than\\nthe two groups seen for t procedures and ask whether or not\\nthe means of all the groups are the same. The hypothesis\\nin this case is\\n\\nFigure 22.1: Side-by-side boxplots of the number of trees per plot.\\nThe groups will be considered different if the differences between the\\ngroups (indicated by the variation in the center lines of the boxes) is\\nlarge compared to the width of the boxes in the boxplot.\\n\\nH0 : µj = µk for all j, k and H1 : µj (cid:54)= µk for some j, k.\\n\\nThe data {yij, 1 ≤ i ≤ nj, 1 ≤ j ≤ q} represents that we have ni observation for the i-th group and that we have\\n\\nq groups. The total number of observations is denoted by n = n1 + ··· + nq. The model is\\n\\nyij = µj + \\x01ij.\\n\\nwhere \\x01ij are independent N (0, σ) random variables with σ2 unknown. This allows us to deﬁne the likelihood and\\nto use that to determine the analysis of variance F test as a likelihood ratio test. Notice that the model for analysis\\nrequires a common value σ for all of the observations.\\n\\nIn order to develop the F statistic at the test statistic, we will need to introduce two types of sample means:\\n\\n• The within group means is simply the sample mean of the observations inside each of the groups,\\n\\nyj =\\n\\n1\\nnj\\n\\nnj(cid:88)i=1\\n\\nyij,\\n\\nj = 1. . . . , q.\\n\\nThese are given in the table in Example 22.1 for the Borneo rains forest. The within group mean yj is the\\nmaximum likelihood estimate of µj under H0.\\n\\n• The mean of the data taken as a whole, known as the grand mean,\\n\\ny =\\n\\n1\\nn\\n\\nq(cid:88)j=1\\n\\nnj(cid:88)i=1\\n\\nyij =\\n\\n1\\nn\\n\\nq(cid:88)j=1\\n\\nnj ¯yj.\\n\\nThis is the weighted average of the ¯yi with weights ni, the sample size in each group. The Borneo rain forest\\nexample has an overall mean\\n\\ny =\\n\\n1\\nn\\n\\n3(cid:88)j=1\\n\\nnj ¯yj =\\n\\n1\\n\\n12 + 12 + 9\\n\\n(12 · 23.750 + 12 · 14.083 + 9 · 15.778) = 18.06055.\\n\\nThe grand mean y is the maximum likelihood estimate of the common values for the µj under H1.\\n\\n404\\n\\nnever loggedlogged 1 year agologged 8 years ago5101520253035\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nsource of\\nvariation\\nbetween groups\\n\\nresiduals\\n\\ntotal\\n\\nmean\\nsquare\\n\\nsums of\\nsquares\\nSSbetween\\n\\ndegrees of\\nfreedom\\nq − 1\\nn − q\\nn − 1\\nTable I: Table for one way analysis of variance\\n\\nSSresidual\\n\\nSStotal\\n\\ns2\\nbetween = SSbetween/(q − 1)\\ns2\\nresidual = SSresidual/(n − q)\\n\\nAnalysis of variance uses the total sum of squares\\n\\nSStotal =\\n\\nq(cid:88)j=1\\n\\nnj(cid:88)i=1\\n\\n(yij − y)2,\\n\\n(22.1)\\n\\nthe total square variation of individual observations from their grand mean. SStotal appears because SStotal/n is the\\nthe maximum likelihood estimate for σ2 under H1.\\n\\nHowever, the test statistic is determined by decomposing SStotal. We start with a bit of algebra to rewrite the\\n\\ninterior sum in (22.1) as\\n\\nnj(cid:88)i=1\\n\\n(yij − y)2 =\\n\\nnj(cid:88)i=1\\n\\n(yij − yj)2 + nj(yj − y)2 = (nj − 1)s2\\n\\nj + nj(yj − y)2.\\n\\n(22.2)\\n\\nHere, s2\\n\\nj is the unbiased estimator of the variance based on the observations in the j-th group.\\nExercise 22.2. Show the ﬁrst equality in (22.2). (Hint: Begin with the difference in the two sums.)\\n\\nTogether (22.1) and (22.2) yields the decomposition of the variation\\n\\nSStotal = SSresidual + SSbetween\\n\\nwith\\n\\nSSresidual =\\n\\nq(cid:88)j=1\\n\\nnj(cid:88)i=1\\n\\n(yij − yj)2 =\\n\\nq(cid:88)j=1\\n\\n(nj − 1)s2\\n\\nj\\n\\nand SSbetween =\\n\\nq(cid:88)j=1\\n\\nnj(yj − y)2.\\n\\nSSresidual/n is the the maximum likelihood estimate for σ2 under H0.\\n\\nFor the rain forest example, we ﬁnd that\\n\\nSSbetween =\\n\\n3(cid:88)j=1\\n\\nand\\n\\nnj(yj − y)2 = 12 · (23.750 − y)2 + 12 · (14.083 − y)2 + 9 · (15.778 − y)2) = 625.1793\\n\\nSSresidual =\\n\\n3(cid:88)j=1\\n(nj − 1)s2\\n\\nj = (12 − 1) · 5.0652 + (12 − 1) · 4.9812 + (9 − 1) · 5.7612 = 820.6234\\n\\nFrom this, we obtain the general form for one-way analysis of variance as shown in Table I.\\n\\n• The q − 1 degrees of freedom between groups is derived from the q groups minus one degree of freedom used\\n\\nto compute y.\\n\\n405\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\n• The n − q degrees of freedom within the groups is derived from the nj − 1 degree of freedom used to compute\\n\\nthe variances s2\\n\\nj. Add these q values for the degrees of freedom to obtain n − q.\\n\\nThe test statistic\\n\\nF =\\n\\ns2\\nbetween\\ns2\\nresidual\\n\\n=\\n\\nSSbetween/(q − 1)\\nSSresidual/(n − q)\\n\\n.\\n\\nis, under the null hypothesis, a constant multiple of the ratio of two independent χ2 random variables with parameter\\nq− 1 for the numerator and n− q for the denominator. This ratio is called an F random variable with q− 1 numerator\\ndegrees of freedom and n − q denominator degrees of freedom.\\n\\nUsing Table II, we ﬁnd the value of the test statistic for the rain forest data is\\n\\nF =\\n\\ns2\\nbetween\\ns2\\nresidual\\n\\n=\\n\\n312.6\\n27.4\\n\\n= 11.43.\\n\\nand the p-value (calculated below) is 0.0002. The critical value for an α = 0.01 level test is 5.390. So, we do reject\\nthe null hypothesis that mean number of trees does not depend on the history of logging.\\n> 1-pf(11.43,2,30)\\n[1] 0.0002041322\\n> qf(0.99,2,30)\\n[1] 5.390346\\n\\nConﬁdence intervals are determined using the\\ndata from all of the groups as an unbiased estimate\\nfor the variance, σ2. Using all of the data allows\\nus to increase the number of degrees of freedom in\\nthe t distribution and thus reduce the upper critical\\nvalue for the t statistics and with it the margin of\\nerror.\\n\\nThe variance s2\\n\\nresiduals is given by the expres-\\nsion SSresiduals/(n − q), shown in the table in\\nthe “mean square” column and the “residuals” row.\\nThe standard deviation sresidual is the square root\\nof this number. For example, the γ-level conﬁ-\\ndence interval for µj is\\n\\n¯yj ± t(1−γ)/2,n−q\\n\\nsresidual\\n√nj\\n\\n.\\n\\nThe conﬁdence for the difference in µj−µk is sim-\\nilar to that for a pooled two-sample t conﬁdence\\ninterval and is given by\\n¯yj − ¯yk ± t(1−γ)/2,n−qsresidual(cid:115) 1\\n\\n1\\nnk\\n\\nnj\\n\\n+\\n\\n.\\n\\nFigure 22.2: Upper tail critical values. The density for an F random variable\\nwith numerator degrees of freedom, 2, and denominator degrees of freedom, 30.\\nThe indicated values 3.316, 4.470, and 5.390 are critical values for signiﬁcance\\nlevels α = 0.05, 0.02, and 0.01, respectively.\\n\\nsource of\\nvariation\\nbetween groups\\nresiduals\\ntotal\\n\\ndegrees of\\n\\nsums of\\nfreedom squares\\n625.2\\n820.6\\n1445.8\\n\\n2\\n30\\n32\\n\\nmean\\nsquare\\n312.6\\n27.4\\n\\nTable II: Analysis of variance information for the Borneo rain forest data\\n\\n406\\n\\n012345670.00.20.40.60.81.0xdf(x, 2, 30)012345670.00.20.40.60.81.0012345670.00.20.40.60.81.0012345670.00.20.40.60.81.0012345670.00.20.40.60.81.0012345670.00.20.40.60.81.0\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nFigure 22.3: Side-by-side boxplot of queen development times. The time is measured in days. the plots show cool (1) medium (2) and warm (3)\\nhive temperatures.\\n\\nIn this case, the 95% conﬁdence interval for the mean number of trees on a lot “logged 1 year ago” has n − q =\\n\\n33 − 3, t0.025,30 = 2.042, sresidual = √27.4 = 5.234 and the conﬁdence interval is\\n\\n14.083 ± 2.042\\n\\n= 14.083 ± 4.714 = (9.369, 18.979).\\n\\n√27.4\\n√12\\n\\nExercise 22.3. Give the 95% conﬁdence intervals for the difference in trees between plots never logged and plots\\nlogged 8 years ago.\\nExample 22.4. The development time for a European queen in a honey bee hive is suspected to depend on the tem-\\nperature of the hive. To examine this, queens are reared in a low temperature hive (31.1◦ C), a medium temperature\\nhive (32.8◦ C) and a high temperature hive (34.4◦ C). The hypothesis is that higher temperatures increase metabolism\\nrate and thus reduce the time needed from the time the egg is laid until an adult queen honey bee emerges from the\\ncell. The hypothesis is\\n\\nH0 : µlow = µmed = µhigh\\n\\nversus H1 : µlow, µmed, µhigh differ\\n\\nwhere µlow, µmed, and µhigh are, respectively, the mean development time in days for queen eggs reared in a low, a\\nmedium, and a high temperature hive. Here are the data and a boxplot:\\n\\n> ehblow<-c(16.2,14.6,15.8,15.8,15.8,15.8,16.2,16.7,15.8,16.7,15.3,14.6,\\n\\n15.3,15.8)\\n\\n> ehbmed<-c(14.5,14.7,15.9,15.5,14.7,14.7,14.7,15.5,14.7,15.2,15.2,15.9,\\n\\n14.7,14.7)\\n\\n> ehbhigh<-c(13.9,15.1,14.8,15.1,14.5,14.5,14.5,14.5,13.9,14.5,14.8,14.8,\\n\\n13.9,14.8,14.5,14.5,14.8,14.5,14.8)\\n\\n> boxplot(ehblow,ehbmed,ehbhigh)\\n\\nThe commands in R to perform analysis and the output are shown below. The ﬁrst command puts all of the data\\nin a single vector, ehb. Next, we label the groups with the variable or factor name temp. Expressed in this way, this\\nvariable is considered by R as a numerical vector. We then need to inform R to convert these numbers into factors\\nand list the factors in the vector ftemp. Without this, the command anova(lm(ehb∼temp)) would attempt to do\\nlinear regression with temp as the explanatory variable.\\n\\n407\\n\\n12314.014.515.015.516.016.5\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\n> ehb<-c(ehblow,ehbmed,ehbhigh)\\n> temp<-c(rep(1,length(ehblow)),rep(2,length(ehbmed)),rep(3,length(ehbhigh)))\\n> ftemp<-factor(temp,c(1:3))\\n> anova(lm(ehb˜ftemp))\\nAnalysis of Variance Table\\n\\nResponse: ehb\\n\\nDf Sum Sq Mean Sq F value\\n2 11.222 5.6111 23.307 1.252e-07 ***\\n\\nftemp\\nResiduals 44 10.593 0.2407\\n---\\nSignif. codes: 0 *** 0.001 ** 0.01 * 0.05 . 0.1\\n\\nPr(>F)\\n\\n1\\n\\nthe table can be computed directly from the formulas above.\\n\\nThe anova output shows strong evidence against the null hypothesis. The p-value is 1.252 × 10−7. The values in\\nFor the sums of square between groups, SSbetween,\\n\\n> length(ehblow)*(mean(ehblow)-mean(ehb))ˆ2\\n\\n+ length(ehbmed)*(mean(ehbmed)-mean(ehb))ˆ2\\n+ length(ehbhigh)*(mean(ehbhigh)-mean(ehb))ˆ2\\n\\n[1] 11.22211\\n\\nand within groups, SSresidual,\\n\\n> sum((ehblow-mean(ehblow))ˆ2)+sum((ehbmed-mean(ehbmed))ˆ2)\\n\\n+ sum((ehbhigh-mean(ehbhigh))ˆ2)\\n\\n[1] 10.59278\\n\\nFor conﬁdence intervals we use s2\\n\\nresid = 0.2407, sresid = 0.4906 and the t-distribution with 44 degrees of\\n\\nfreedom. For the medium temperature hive, the 95% conﬁdence interval for µmed can be computed\\n\\n> mean(ehblow)\\n[1] 15.74286\\n> qt(0.975,44)\\n[1] 2.015368\\n> length(ehblow)\\n[1] 14\\n\\nThus, the intverval is\\n\\n¯ymed ± t0.025,44\\n\\nsresid\\n√nmed\\n\\n= 15.742 ± 2.0154\\n\\n0.4906\\n√14\\n\\n= (15.478, 16.006).\\n\\n22.3 Contrasts\\nAfter completing a one way analysis of variance, resulting in rejecting the null hypotheses, a typical follow-up proce-\\ndure is the use of contrasts. Contrasts use as a null hypothesis that some linear combination of the means equals to\\nzero.\\nExample 22.5. If we want to see if the rain forest has seen recovery in logged areas over the past 8 years. This can\\nbe written as\\n\\nor\\n\\nH0 : µ2 = µ3\\n\\nversus H1 : µ2 (cid:54)= µ3.\\n\\nH0 : µ2 − µ3 = 0 versus H1 : µ2 − µ3 (cid:54)= 0\\n\\n408\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nUnder the null hypothesis, the test statistic\\n\\nt =\\n\\n¯y2 − ¯y3\\n\\nsresidual(cid:113) 1\\n\\nn2\\n\\n,\\n\\n+ 1\\nn3\\n\\nhas a t-distribution with n − q degrees of freedom. Here\\n5.234(cid:113) 1\\n\\n14.083 − 15.778\\n12 + 1\\n\\nt =\\n\\n9\\n\\nwith n − q = 33 − 3 degrees of freedom, the p-value for this 2-sided test is\\n> 2*pt(-0.7344094,30)\\n[1] 0.4684011\\n\\n= −0.7344,\\n\\nis considerably too high to reject the null hypothesis.\\n\\nExample 22.6. To see if the mean queen development medium hive temperature is midway between the time for the\\nhigh and low temperature hives, we have the contrast,\\n\\nH0 :\\n\\n1\\n2\\n\\n(µlow + µhigh) = µmed\\n\\nversus H1 :\\n\\n1\\nµlow − µmed +\\n2\\nNotice that, under the null hypothesis\\n\\nH0 :\\n\\n1\\n2\\n\\nµhigh = 0 versus H1 :\\n\\n1\\n2\\n\\n1\\n2\\n\\n(µlow + µhigh) (cid:54)= µmed\\n\\nµlow − µmed +\\n\\n1\\n2\\n\\nµhigh (cid:54)= 0\\n\\nE(cid:20) 1\\n\\n2\\n\\n¯Ylow − ¯Ymed +\\n\\n1\\n2\\n\\n¯Yhigh(cid:21) =\\n\\n1\\n2\\n\\nµlow − µmed +\\n\\n1\\n2\\n\\nµhigh = 0\\n\\nor\\n\\nand\\n\\nVar(cid:18) 1\\n\\n¯Ylow − ¯Ymed +\\nThis leads to the test statistic\\n\\n2\\n\\n1\\n2\\n\\n¯Yhigh(cid:19) =\\n\\n1\\n4\\n\\nσ2\\nnlow\\n\\n+\\n\\nσ2\\nnmed\\n\\n+\\n\\n1\\n4\\n\\nσ2\\n\\nnhigh\\n\\n= σ2(cid:18) 1\\n\\n4nlow\\n\\n+\\n\\n1\\n\\nnmed\\n\\n+\\n\\n1\\n\\n4nhigh(cid:19) .\\n\\nt =\\n\\nThe p-value,\\n\\n1\\n\\n2 ¯ylow − ¯ymed + 1\\n+ 1\\n\\nsresidual(cid:113) 1\\n\\n4nlow\\n\\nnmed\\n\\n2 ¯yhigh\\n\\n+ 1\\n\\n4nhigh\\n\\n=\\n\\n2 14.563\\n\\n1\\n\\n2 15.743 − 15.043 + 1\\n0.4906(cid:113) 1\\n\\n4·14 + 1\\n\\n14 + 1\\n4·19\\n\\n= 0.7005.\\n\\n> 2*(1-pt(0.7005,44))\\n[1] 0.487303\\n\\nagain, is considerably too high to reject the null hypothesis.\\n\\nExercise 22.7. Under the null hypothesis appropriate for one way analysis of variance, with ni observations in group\\n\\ni = 1, . . . , q and ¯Yi =(cid:80)ni\\n\\nj=1 Yij/ni,\\n\\nE[c1 ¯Y1 + ··· + Yqµq] = c1µ1 + ··· + cqµq, Var(c1 ¯Y1 + ··· + cqYq) =\\n\\n409\\n\\nc2\\n1σ2\\nn1\\n\\n+ ··· +\\n\\nc2\\nqσ2\\nnq\\n\\n.\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nIn general, a contrast begins with a linear combination of the means\\n\\nψ = c1µ1 + ··· + cqµq.\\n\\nThe hypothesis is\\n\\nFor sample means, ¯y1, . . . , ¯yq, the test statistic is\\n\\nH0 : ψ = 0 versus H1 : ψ (cid:54)= 0\\n\\nt =\\n\\nc1 ¯y1 + ··· + cq ¯yq\\n+ ··· +\\n\\nsresidual(cid:113) c2\\n\\n1\\nn1\\n\\n.\\n\\nc2\\nq\\nnq\\n\\nUnder the null hypothesis the t statistic has a t distribution with n − q degrees of freedom.\\n\\n22.4 Two Sample Procedures\\nWe now show that the t-sample procedure results from a likelihood ratio test. We keep to two groups in the devel-\\nopment of the F test. The essential features can be found in this example without the extra notation necessary for an\\narbitrary number of groups.\\n\\nOur hypothesis test is based on two independent samples of normal random variables. The data are\\n\\nyij = µj + \\x01ij.\\n\\nwhere \\x01ij are independent N (0, σ) random variables with σ unknown. Thus, we have nj independent N (µj, σ)\\nrandom variables Y1j . . . , Ynj j with unknown common variance σ2, j = 1 and 2. The assumption of a common\\nvariance is critical to the ability to compute the test statistics.\\n\\nConsider the two-sided hypothesis\\n\\nThus, the parameter space is\\n\\nFor the null hypothesis, the possible parameter values are\\n\\nH0 : µ1 = µ2\\n\\nversus H1 : µ1 (cid:54)= µ2.\\nΘ = {(µ1, µ2, σ2); µ1, µ2 ∈ R, σ2 > 0}.\\n\\nStep 1. Determine the log-likelihood. To ﬁnd the test statistic derived from a likelihood ratio test, we ﬁrst write\\n\\nthe likelihood and its logarithm based on observations y = (y11, . . . , yn11, y12, . . . , yn22).\\n\\nΘ0 = {(µ1, µ2, σ2); µ1 = µ2, σ2 > 0}\\n\\nL(µ1.µ2, σ2|y) =(cid:18) 1\\n\\n√2πσ2(cid:19)n1+n2\\n\\nexp−\\n\\n(yi1 − µ1)2 +\\n\\n1\\n\\n2σ2(cid:32) n1(cid:88)i=1\\n2σ2(cid:32) n1(cid:88)i=1\\n\\n1\\n\\n(yi2 − µ2)2(cid:33)\\nn2(cid:88)i=1\\n(yi2 − µ2)2(cid:33)\\nn2(cid:88)i=1\\n\\n(22.3)\\n\\n(22.4)\\n\\nln L(µ1.µ2, σ2|y) = −\\n\\n(n1 + n2)\\n\\n2\\n\\n(ln 2π + ln σ2) −\\n\\n(yi1 − µ1)2 +\\n\\nStep 2. Find the maximum likelihood estimates and the maximum value of the likelihood. By taking partial\\nderivatives with respect to µ1 and µ2 we see that with two independent samples, the maximum likelihood estimate for\\nthe mean µj for each of the samples is the sample mean ¯yj.\\n\\nˆµ1 = ¯y1 =\\n\\n1\\nn1\\n\\nn1(cid:88)i=1\\n\\nyi1,\\n\\nˆµ2 = ¯y2 =\\n\\n1\\nn2\\n\\nn2(cid:88)i=1\\n\\nyi2.\\n\\n410\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nThus, the maximum likelihood estimate of the variance is the weighted average, weighted according to the sample\\nsize, of the maximum likelihood estimator of the variance for each of the respective samples.\\n\\nNow differentiate (22.4) with respect to σ2\\n\\n∂\\n∂σ2 ln L(µ1, µ2, σ2|x) = −\\n\\nn1 + n2\\n\\n2σ2 +\\n\\n1\\n\\n2(σ2)2(cid:32) n1(cid:88)i=1\\n\\n(yi1 − µ1)2 +\\n\\n(yi2 − µ2)2(cid:33) .\\nn2(cid:88)i=1\\n(yi2 − ¯y2)2(cid:33) .\\n\\n(yi1 − ¯y1)2 +\\n\\n(yi2 − ¯y2)2(cid:33)\\n\\nn2(cid:88)i=1\\n\\nn2(cid:88)i=1\\n2ˆσ2(cid:32) n1(cid:88)i=1\\n\\nn1 + n2\\n\\n1\\n\\n2\\n\\nˆσ2 =\\n\\n1\\n\\nn1 + n2(cid:32) n1(cid:88)i=1\\n\\n(yi1 − ¯y1)2 +\\n\\nL(ˆµ1, ˆµ2, ˆσ2|x) =\\n\\n1\\n\\n(2πˆσ2)(n1+n2)/2 exp−\\n\\n=\\n\\n1\\n\\n(2πˆσ2)(n1+n2)/2 exp−\\n\\nNow, substitute these values into the likelihood (22.3) to see that the maximum value for the likelihood is\\n\\nStep 3. Find the parameters that maximize the likelihood under the null hypothesis and then ﬁnd the\\nmaximum value of the likelihood on Θ0. Next, for the likelihood ratio test, we ﬁnd the maximum likelihood under\\nthe null hypothesis. In this case the two means have a common value which we shall denote by µ.\\n\\n√2πσ2(cid:19)n1+n2\\n\\nexp−\\n\\nL(µ, σ2|y) =(cid:18) 1\\nln L(µ, σ2|x) = −\\nThe µ derivative of (22.6) is\\n\\n2\\n\\n(n1 + n2)\\n\\n(ln 2π + ln σ2) −\\n\\n1\\n\\n1\\n\\n(yi1 − µ)2 +\\n\\n2σ2(cid:32) n1(cid:88)i=1\\n2σ2(cid:32) n1(cid:88)i=1\\nn2(cid:88)i=1\\n\\n(yi1 − µ) +\\n\\n(yi2 − µ)2(cid:33)\\nn2(cid:88)i=1\\n(yi2 − µ)2(cid:33)\\nn2(cid:88)i=1\\n(yi1 − µ)2 +\\n(yi2 − µ)(cid:33) .\\n\\n(22.5)\\n\\n(22.6)\\n\\nSet this to 0 and solve to realize that the maximum likelihood estimator under the null hypothesis is the grand sample\\nmean y obtained by considering all of the data being derived from one large sample\\n\\n∂\\n∂µ\\n\\nln L(µ, σ2|x) =\\n\\n1\\n\\nσ2(cid:32) n1(cid:88)i=1\\nn1 + n2(cid:32) n1(cid:88)i=1\\n\\n1\\n\\nyi1 +\\n\\nˆµ0 = y =\\n\\nyi2(cid:33) =\\n\\nn2(cid:88)i=1\\n\\nn1 ¯y1 + n2 ¯y2\\n\\nn1 + n2\\n\\n.\\n\\nIntuitively, if the null hypothesis is true, then all of our observations are independent and have the same distribution\\nand thus, we should use all of the data to estimate the common mean of this distribution.\\n\\nThe value for σ2 that maximizes (22.5) on Θ0, is also the maximum likelihood estimator for the variance obtained\\n\\nby considering all of the data being derived from one large sample:\\n\\n(yi1 − y)2 +\\nWe can ﬁnd that the maximum value on Θ0 for the likelihood is\\n\\nˆσ2\\n0 =\\n\\n1\\n\\nn1 + n2(cid:32) n1(cid:88)i=1\\n\\n(yi2 − y)2(cid:33) .\\n\\nn2(cid:88)i=1\\n0(cid:32) n1(cid:88)i=1\\n\\n1\\n\\n(2πˆσ2\\n\\n0)(n1+n2)/2 exp−\\n\\n1\\n2ˆσ2\\n\\n(yi1 − y)2 +\\n\\n(yi2 − y)2(cid:33)\\n\\nn2(cid:88)i=1\\n\\nL(ˆµ0, ˆσ2\\n\\n0|x) =\\n\\n=\\n\\n(2πˆσ2\\n\\n1\\n\\n0)(n1+n2)/2 exp−\\n411\\n\\nn1 + n2\\n\\n2\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nStep 4. Find the likelihood statistic Λ(y). From steps 2 and 3, we ﬁnd a likelihood ratio of\\n\\nΛ(y) =\\n\\nL(ˆµ0, ˆσ2\\n0|x)\\nL(ˆµ, ˆσ2|x)\\n\\nˆσ2(cid:19)−(n1+n2)/2\\n=(cid:18) ˆσ2\\n\\n0\\n\\n=(cid:18) (cid:80)n1\\ni=1(yi1 − y)2 +(cid:80)n2\\ni=1(yi1 − ¯y1)2 +(cid:80)n2\\n(cid:80)n1\\n\\ni=1(yi2 − y)2\\n\\ni=1(yi2 − ¯y2)2(cid:19)−(n1+n2)/2\\n\\n.\\n\\n(22.7)\\n\\nThis is the ratio, SStotal, of the variation of individuals observations from the grand mean and SSresiduals.\\nvariation of these observations from the mean of its own groups.\\n\\nthe\\n\\nStep 5. Simplify the likelihood statistic to determine the test statistic F . Traditionally, the likelihood ratio is\\n\\nsimpliﬁed by looking at the differences of these two types of variation, the numerator in (22.7)\\n\\nSStotal =\\n\\nand the denominator in (22.7)\\n\\n(yi1 − y)2 +\\n\\n(yi2 − y)2\\n\\nn1(cid:88)i=1\\nn1(cid:88)i=1\\n\\nn2(cid:88)i=1\\nn2(cid:88)i=1\\n\\nSSresiduals =\\n\\n(yi1 − ¯y1)2 +\\n\\n(yi2 − ¯y2)2\\n\\nExercise 22.8. Show that SStotal − SSresiduals = n1(¯y1 − y)2 + n2(¯y2 − y)2.\\n\\nIn words, SStotal the sums of squares of the differences of an individual observation from the overall mean y, is\\nthe sum of two sources. The ﬁrst is the sums of squares of the difference of the average of each group mean and the\\noverall mean,\\n\\nThe second is the sums of squares of the difference of the individual observations with its own group mean, SSresiduals.\\nThus, we can write\\n\\nSSbetween = n1(y − ¯y1)2 + n2(y − ¯y2)2.\\n\\nSStotal = SSresidual + SSbetween\\n\\nNow, the likelihood ratio (22.7) reads\\n\\nΛ(y) =(cid:18) SSresidual + SSbetween\\n\\nSSresiduals\\n\\n(cid:19) =(cid:18)1 +\\n\\nSSbetween\\n\\nSSresiduals(cid:19)−(n1+n2)/2\\n\\nDue to the negative power in the exponent, the critical region Λ(y) ≤ λ0 is equivalent to\\n\\nSSbetween\\nSSresiduals\\n\\n=\\n\\nn1(y − ¯y1)2 + n2(y − ¯y2)2\\n(n1 − 1)s2\\n1 + (n2 − 1)s2\\n\\n2 ≥ c\\n\\n(22.8)\\n\\nfor an appropriate value c. The ratio in (22.8) is, under the null hypothesis, a multiple of an F -distribution. The last\\nstep to divide both the numerator and denominator by the degrees of freedom. Thus, we see, as promised, we reject if\\nthe F -statistics is too large, i.e., the variation between the groups is sufﬁciently large compared to the variation within\\nthe groups.\\nExercise 22.9 (pooled two-sample t-test). For an α level test, show that the test above is equivalent to\\n\\n|T (y)| > tα/2,n1+n+2.\\n\\nwhere\\n\\n¯y1 − ¯y2\\n+ 1\\nn2\\nand sp is the standard deviation of the data pooled into one sample.\\n\\nT (y) =\\n\\n.\\n\\nn1\\n\\nsp(cid:113) 1\\nn1 + n2 − 2(cid:0)(n1 − 1)s2\\n\\n1\\n\\n412\\n\\ns2\\np =\\n\\n2(cid:1)\\n1 + (n2 − 1)s2\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nExercise 22.10. Generalize the formulae for y, SSbetween and SSresiduals from the case q = 2 to an arbitrary number\\nof groups.\\n\\nThus, we can use the two-sample procedure to compare any two of the three groups. For example, to compared\\n\\nthe never logged forest plots to those logged 8 years ago., we ﬁnd the pooled variance\\n\\ns2\\np =\\n\\n1\\n\\nn1 + n2 − 2\\n\\n((n1 − 1)s2\\n\\n1 + (n2 − 1)s2\\n\\n2) =\\n\\n1\\n19\\n\\n(11 · 5.0652 + 8 · 5.7612) = 28.827\\n\\nand sp = 5.37. Thus, the t-statistic\\n\\nt =\\n\\n23.750 − 15.778\\n12 + 1\\n\\n5.37(cid:113) 1\\n\\n9\\n\\n= 7.644.\\n\\n> 1-pt(7.644,19)\\n[1] 1.636569e-07\\nThus, the p-value at 1.64 × 107 is strong evidence against the null hypothesis.\\n\\n22.5 Kruskal-Wallis Rank-Sum Test\\nThe Kruskal-Wallis test is an alternative to one-way analysis of variance in much the same way that the Wilcoxon\\nrank-sum test is a alternative to two-sample t procedures. Like the Wilcoxon test, we replace the actual data with their\\nranks. This non-parametric alternative obviates the need to use the normal distribution arising from an application of\\nthe central limit theorem. The H test statistic has several analogies with the F statistic. To compute this statistic:\\n\\n• Replace the data {yij, 1 ≤ i ≤ nj, 1 ≤ j ≤ q} for ni observations for the i-th group from each of the q groups\\nwith {rij, 1 ≤ i ≤ nj, 1 ≤ j ≤ q}, the ranks of the data taking all of the groups together. For ties, average the\\nranks.\\n\\n• The total number of observations n = n1 + ··· + nq.\\n• The average rank within the groups\\n\\n¯ri =\\n\\n1\\nni\\n\\nni(cid:88)j=1\\n\\nrij,\\n\\ni = 1, . . . , q.\\n\\nr =\\n\\n1\\nn\\n\\n(1 + ··· + n) =\\n\\n1\\nn\\n\\nn(n + 1) =\\n\\nn + 1\\n\\n2\\n\\n.\\n\\n• The grand average of the ranks\\n\\n(See Exercise 20.6.)\\n\\n• The Kruskal-Wallis test statistic looks at the sums of squares of ranks between groups and the total sum of\\n\\nsquares of ranks\\n\\nH =\\n\\nSSRbetween\\n\\nSSRtotal/(n − 1)\\n\\n=\\n\\n(cid:80)g\\ni=1(cid:80)ni\\n(cid:80)q\\n\\ni=1 ni(¯ri − r)2\\nj=1(rij − r)2/(n − 1)\\n\\n,\\n\\n• For larger data sets (each ni ≥ 5), the p-value is approximately the probability that a χ2\\n\\nexceeds the value of the H statistic.\\n\\nq−1 random variable\\n\\n• For smaller data sets, more sophisticated procedures are necessary.\\n• The test can be followed by using a procedure analogous to contrasts based on the Wilcoxon rank-sum test.\\n\\n413\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nExercise 22.11. For the case of no ties, show that\\n\\nIn this case,\\n\\nH =\\n\\n12\\n\\nn(n + 1)\\n\\nSSRtotal =\\n\\n(n − 1)n(n + 1)\\n\\n12\\n\\nni(cid:18)¯ri −\\n\\nn + 1\\n\\n2 (cid:19)2\\n\\n=\\n\\n12\\n\\nn(n + 1)\\n\\nq(cid:88)i=1\\n\\nq(cid:88)i=1\\n\\nni¯r2\\n\\ni − 3(n + 1).\\n\\nThe Kruskal-Wallis test also gives a very small p-value to the queen development times for Africanized honey\\n\\nbees. Begin with the R commands in Example 22.4 to enter the data and create the temperature factors ftemp.\\n\\n> kruskal.test(ehb˜ftemp)\\n\\nKruskal-Wallis rank sum test\\n\\ndata:\\nKruskal-Wallis chi-squared = 20.4946, df = 2, p-value = 3.545e-05\\n\\nehb by ftemp\\n\\n22.6 Answer to Selected Exercises\\n22.2. Let’s look at this difference for each of the groups.\\n\\n(yij − y)2 −\\n\\nni(cid:88)j=1\\n\\n(yij − ¯yi)2 =\\n\\nni(cid:88)j=1(cid:0)(yij − y)2 − (yij − ¯yi)2(cid:1)\\n\\n=\\n\\n(2yij − y − ¯yi)(−y + ¯yi) = ni(2¯yi − y − ¯yi)(−y + ¯yi) = ni(¯yi − y)2\\n\\nni(cid:88)j=1\\nni(cid:88)j=1\\n\\nNow the numerator in (22.7) can be written to show the decomposition of the variation into two sources - the within\\ngroup variation and the between group variation.\\n\\nn1(cid:88)i=1\\n\\n(yi1 − y)2 +\\n\\nn2(cid:88)i=1\\n\\n(yi2 − y)2 =\\n\\n(y1j − ¯y1)2 +\\n\\nn1(cid:88)i=1\\n\\nn2(cid:88)i=1\\n\\n(y2j − ¯y2)2 + n1(y − ¯y1)2 + n2(y − ¯y2)2.\\n2 + n1(y − ¯y1)2 + n2(y − ¯y2)2.\\n\\n= (n1 − 1)s2\\n\\n1 + (n2 − 1)s2\\n\\n22.3. Here, we are looking for a conﬁdence interval for µ1 − µ3. From the summaries, we need\\n\\nn1 = 12,\\n\\n¯y1 = 23.750, n3 = 9,\\n\\n¯y3 = 17.778.\\n\\nFrom the computation for the test, we have sresidual = √27.4 = 5.234 and using the qt(0.975,30) command we\\nﬁnd t0.025,30 = 2.042. Thus,\\n\\n(¯y1 − ¯y3)\\n\\n= (23.750 − 17.778) ±2.042 · 5.234(cid:114) 1\\n\\n=\\n\\n5.972\\n\\n±t(0.975,30)sresidual(cid:114) 1\\n\\nn1\\n1\\n9\\n±2.079 = (3.893, 8.051)\\n\\n12\\n\\n+\\n\\n+\\n\\n1\\nn3\\n\\n414\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\n22.7. This follows from the fact that expectation is a linear functional and the generalized Pythagorean identity for the\\nvariance of a linear combination of independent random variables.\\n\\n22.8. Look at the solution to Exercise 22.2.\\n\\n22.9. We will multiply the numerator in (22.8) by (n1 + n2)2 and note that (n1 + n2)y = n1 ¯y1 + n2 ¯y2. Then,\\n\\n(n1 + n2)2(n1(y − ¯y1)2 + n2(y − ¯y2)2 = n1((n1 + n2)y − (n1 + n2)¯y1)2 + n2((n1 + n2)y − (n1 + n2)¯y2)2\\n\\n= n1(n1 ¯y1 + n2 ¯y2 − (n1 + n2)¯y1)2 + n2(n1 ¯y1 + n2 ¯y2 − (n1 + n2)¯y2)2\\n= n1(n2(¯y2 − ¯y1))2 + n2(n1(¯y1 − ¯y2))2\\n= (n1n2\\n\\n1)(¯y1 − ¯y2)2 = n1n2(n1 + n2)(¯y1 − ¯y2)2\\n\\n2 + n2n2\\n\\nConsequently\\n\\nThe denominator\\n\\nThe ratio\\n\\n(n1(y − ¯y1)2 + n2(y − ¯y2)2 =\\n\\nn1n2\\n\\nn1 + n2\\n\\n(¯y1 − ¯y2)2 = (¯y1 − ¯y2)2/(cid:18) 1\\n\\nn1\\n\\n+\\n\\n1\\n\\nn2(cid:19) .\\n\\nn1(cid:88)j=1\\n\\n(yi1 − ¯y1)2 +\\n\\nn2(cid:88)j=1\\n\\n(yi2 − ¯y2)2 = (n1 + n2 − 2)s2\\np.\\n\\nSSbetween\\nSSresiduals\\n\\n=\\n\\n(¯y1 − ¯y2)2\\n\\n(n1 + n2 − 2)s2\\n\\np(cid:16) 1\\n\\nn1\\n\\n+ 1\\n\\nn2(cid:17) =\\n\\nT (y)2\\n\\nn1 + n2 − 2\\n\\n.\\n\\nThus, the test is a constant multiple of the square of the t-statistic. Take the square root of both sides to create a test\\nusing a threshold value for |T (y)| for the critical region.\\n22.10. For observations, yi1, . . . yini in group i = 1, . . . , q, let n = n1 + ··· + nq be the total number of observations,\\nthen the grand mean\\n\\nwhere ¯yi is the sample mean of the observations in group i. The sums of squares are\\n\\ny =\\n\\n1\\nn\\n\\n(n1 ¯y1 + ··· + nq ¯yq)\\n\\nSSbetween =\\n\\nq(cid:88)i=1\\n\\nni(¯yi − y)2\\n\\nand SSresiduals =\\n\\nq(cid:88)i=1\\n\\n(ni − 1)s2\\n\\ni\\n\\nwhere s2\\n\\ni is the sample variance of the observations in group i.\\n\\n22.11. In anticipation of its need, let’s begin by showing that\\n\\nNotice that the formula holds for the case n = 1 with\\n\\nj2 =\\n\\nn(cid:88)j=1\\n\\nn(n + 1)(2n + 1)\\n\\n6\\n\\n.\\n\\n12 =\\n\\n1(1 + 1)(2 · 1 + 1)\\n\\n6\\n\\n=\\n\\n6\\n6\\n\\n= 1.\\n\\n415\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nAnalysis of Variance\\n\\nNow assume that the identity holds for n = k. We then check that it also holds for n = k + 1\\n\\n12 + 22 + ··· + k2 + (k + 1)2 =\\n=\\n\\nk(k + 1)(2k + 1)\\n\\n6\\n\\n+ (k + 1)2\\n\\nk + 1\\n\\n6\\n\\n(k(2k + 1) + 6(k + 1)) =\\n\\nk + 1\\n\\n6\\n\\n(2k2 + 7k + 6)\\n\\n=\\n\\n(k + 1)(k + 2)(2k + 3)\\n\\n6\\n\\nThis is the formula for n = k + 1 and so by the mathematical induction, we have the identity for all non-negative\\nintegers.\\n\\nWith no ties, each rank appears once and\\n\\nSSRtotal =\\n\\n=\\n\\n=\\n\\n=\\n\\nn(cid:88)j=1(cid:18)j −\\n\\nn + 1\\n\\n2 (cid:19)2\\n\\nn(n + 1)(2n + 1)\\n\\n6\\n\\n=\\n\\nn(cid:88)j=1\\n\\nj2 − 2\\n\\nj\\n\\nn(cid:88)j=1\\n\\nn(n + 1)\\n\\nn + 1\\n\\n2\\n\\n2\\n\\n− 2\\n\\n2\\n\\nn + 1\\n\\n+\\n\\n2 (cid:19)2\\nn(cid:88)j=1(cid:18) n + 1\\n+ n(cid:18) n + 1\\n2 (cid:19)2\\n\\n(n − 1)n(n + 1)\\n\\n12\\n\\n.\\n\\nn(n + 1)(2n + 1)\\n\\nn(n + 1)2\\n\\n6\\nn(n + 1)\\n(2(2n + 1) − 3(n + 1)) =\\n\\n12\\n\\n4\\n\\n−\\n\\n416\\n\\n\\x0cAppendix A: A Sample R Session\\n\\nThe purpose of this appendix is to become accustomed to the R and the way if responds to line commands. R can be\\ndownloaded from\\n\\nhttp://cran.r-project.org/\\n\\nBe sure to download the version of R corresponding to your operating system - linux, MacOS, or windows.\\n\\nAs you progress, you will learn the statistical ideas behind the commands that ask for graphs or computation. Note\\nthat R only prints output when it is requested. On a couple of occasions, you will see a plus (+) sign at the beginning\\nof the line. This is supplied by R and you will not type this on your R console.\\n\\n• Learn how to access help. Type\\n\\n> help.start()\\n\\nto access on-line manuals, references, and other material.\\n\\n• Find fundamental constants\\n\\n> pi\\n> exp(1)\\n> round(exp(1),4)\\n\\n• The <- is used to indicate an assignment. Type\\n\\n> x<-rnorm(50)\\n> length(x)\\n> hist(x)\\n> mean(x)\\n> sd(x)\\n> summary(x)\\n\\n• To see what the sort command does type\\n\\n> ?sort\\n\\nNext, sort the values in x, ﬁrst in increasing and then decreasing order.\\n\\n> sort(x)\\n> sort(x,decreasing=TRUE)\\n\\nThe ﬁrst command gives 50 independent standard normal random variables and stores them as a vector x. It\\nthen gives the number of entries in x, creates a histogram, computes the mean and standard deviation, and gives\\na summary of the data. The last two commands give the values of x sorted from bottom to top and then from\\ntop to bottom\\n\\n417\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nA Sample R Session\\n\\n• To prepare for a scatterplot, enter\\n\\n> (y<-rnorm(x))\\n\\nThis gives 50 additional independent standard normal random variables and stores them as a vector y. When\\nthe command is placed in parentheses, R prints out the value of the variable.\\n\\n• To make a scatterplot of these data, type\\n\\n> plot(x,y)\\n\\nA graphics window will appear automatically.\\n\\n• To ﬁnd the correlation between x and y.\\n\\n> cor(x,y)\\n\\n• To preform a t-test, type\\n\\n> t.test(x,y)\\n> t.test(x,y,alternative=\"greater\")\\n\\nNotice the difference in p-value.\\n\\n• To check to see what is in your workspace, type\\n\\n> ls()\\n\\n• To remove a variable x\\n\\n> rm(x)\\n\\nNow type ls() again to see that x has been removed.\\n\\n• To make a variety of graphs of sin(θ)\\n\\n> theta<-seq(0,2*pi,length=100)\\n> plot(theta,sin(theta))\\n> par(new=TRUE)\\n> plot(theta,sin(theta),type=\"h\")\\n> plot(theta,sin(theta),type=\"l\")\\n> plot(theta,sin(theta),type=\"s\")\\n> theta<-seq(0,2*pi,length=10)\\n> plot(theta,sin(theta),type=\"l\")\\n> plot(theta,sin(theta),type=\"b\")\\n\\nTo see what these commands mean, type\\n\\n> help(plot)\\n\\n• To make some simple arithmetic and repeating sequences, type\\n\\n418\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nA Sample R Session\\n\\n> 1:25\\n> seq(1,25)\\n> seq(25,1,-1)\\n> seq(1,25,2)\\n> seq(1,25,length=6)\\n> seq(0,2,0.1)\\n> rep(0,25)\\n> rep(1,25)\\n\\n• Make a vector of integers from 1 to 25\\n\\n> n<-1:25\\n\\n• Randomly shufﬂe these 25 numbers\\n\\n>\\n\\nsample(n)\\n\\n• Choose 10 without replacement.\\n\\n> sample(n,10)\\n\\n• Choose 30 with replacement.\\n\\n> samp<-sample(n,30,replace=TRUE)\\n> samp\\n\\n• Turn this into a 3 × 10 matrix and view it.\\n\\n>(A<-matrix(samp,ncol=10))\\n>(B<-matrix(samp,nrow=3))\\n\\nNotice that these give the same matrix. The entries are ﬁlled by moving down the columns from left to right.\\n\\n• Check the dimension.\\n\\n> dim(A)\\n\\n• View it as a spreadsheet.\\n\\n> fix(A)\\n\\nYou will need to close the window before entering the next command into R.\\n\\n• Find the transpose.\\n\\n> t(A)\\n\\n• View the ﬁrst row.\\n\\n> A[1,]\\n\\nthe second, third and fourth column,\\n\\n419\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nA Sample R Session\\n\\n> A[,2:4]\\n\\nall but the second, third and fourth column,\\n\\n> A[,-(2:4)]\\n\\nand the 1,4 entry\\n\\n> A[1,4]\\n\\n• Turn this into a 10 × 3 matrix.\\n> matrix(samp,ncol=3)\\n\\n• Make a segmented bar plot of these numbers.\\n\\n> data<-matrix(samp,nrow=3)\\n> barplot(data)\\n\\n• Perform a chi-squared test..\\n> chisq.test(data)\\n\\n• Make a column of weight vectors equal to the square root of n.\\n\\n> w<-sqrt(n)\\n\\n• Simulate some response variables, and display them in a table.\\n\\n> r<- n + rnorm(n)*w\\n> data.frame(n,r)\\n\\n• Create a regression line, display the results, create a scatterplot, and draw the regression line on the plot in red.\\n\\n> regress.rn<-lm(r˜n)\\n> summary(regress.rn)\\n> plot(n,r)\\n> abline(regress.rn,col=\"red\")\\n\\nNote that the order of r and n for the regression line is reversed from the order in the plot.\\n\\n• Plot the residuals and put labels on the axes.\\n\\n> plot(fitted(regress.rn), resid(regress.rn),xlab=\"Fitted values\",\\n+ ylab=\"Residuals\",main=\"Residuals vs Fitted\")\\n\\n• Simulate 100 tosses of a fair coin and view the results\\n\\n> x<-rbinom(100,1,0.5)\\n> x\\n\\nNext, keep a running total of the number of heads, plot the result with steps (type = \"s\")\\n\\n420\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nA Sample R Session\\n\\n> c<-cumsum(x)\\n> plot(c,type=\"s\")\\n\\n• Roll a fair dice 1000 times, look at a summary, and make a table.\\n\\n> fair<-sample(c(1:6),1000,replace=TRUE)\\n> summary(fair)\\n> table(fair)\\n\\n• Roll a biased dice 1000 times, look at a summary, and make a table.\\n\\n> biased<-sample(c(1:6),1000,replace=TRUE,prob=c(1/12,1/12,1/12,1/4,1/4,1/4))\\n> summary(biased)\\n> table(biased)\\n\\n• The next data set arise from the famous Michaelson-Morley experiment. To see the data set, type\\n\\n> morley\\n\\nThere are ﬁve experiments (column Expt) and each has 20 runs (column Run) and Speed is the recorded\\nspeed of light minus 290,000 km/sec.\\n\\n• The data in the ﬁrst two columns are labels, type\\n\\n> morley$Expt <- factor(morley$Expt)\\n\\nso that the experiment number will be a factor\\n\\n• Now make a labeled boxplot of the speed in column 3\\n\\n> boxplot(morley[,3]˜morley$Expt,main=\"Speed of Light Data\", xlab=\"Experiment\",\\n+ ylab=\"Speed\")\\n\\n• Perform an analysis of variance to see if the speed are measured speeds are signiﬁcantly different between\\n\\nexperiments. .\\n\\n> anova.mm<-aov(Speed˜Expt,data=morley)\\n> summary(anova.mm)\\n\\n• Draw a cubic.\\n\\n> x<-seq(-2,2,0.01)\\n> plot(x,xˆ3-3*x,type=\"l\")\\n\\n• Draw a bell curve.\\n\\n> curve(dnorm(x),-3,3)\\n\\n• Look at the probability mass function for a binomial distribution.\\n\\n> x<-c(0:100)\\n> prob<-dbinom(x,100,0.5)\\n> plot(x,prob,type=\"h\")\\n\\n421\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nA Sample R Session\\n\\n• To plot a parameterized curve, start with a sequence and give the x and y values.\\n\\n> angle<-seq(-pi,pi,0.01)\\n> x<-sin(3*angle)\\n> y<-cos(4*angle)\\n> plot(x,y,type=\"l\")\\n\\nThe type =\"l\" (the letter ell, not the number one) command connects the values in the sequence with lines.\\n• Now we will plot contour lines and a surface. First, we give a sequence of values. This time we specify the\\n\\nnumber of terms.\\n\\n> x<-seq(-pi, pi, len=150)\\n> y<-x\\n\\nThen, we deﬁne a function for these x and y values and draw a contour map. Then, choose the number of levels.\\n\\n> f<-outer(x,y,function(x,y) cos(y)/(1+xˆ2))\\n> contour(x,y,f)\\n> contour(x,y,f,nlevels=20)\\n\\n• For a color coded “heat map”,\\n\\n> image(x,y,f)\\n\\n• To draw a surface plot,\\n\\n> persp(x,y,f,col=\"orange\")\\n\\nand change the viewing angle\\n\\n> persp(x,y,f,col=\"orange\",theta=-30, phi=45)\\n\\n422\\n\\n\\x0cIndex\\n\\nR command\\nD, 119\\nabline, 40, 59, 420\\nabs, 181, 210\\nanova, 407\\naov, 421\\narray, 7, 70\\natan, 295\\nbarplot, 7, 389, 399\\nbeta, 165, 171\\nbinom.test, 344\\nbinom, 157, 170\\nboxplot, 24, 30, 67, 368, 407\\nchisq.test, 389, 390\\nchisq, 167, 171\\nchol2inv, 62\\nchoose, 89, 90, 94, 128, 152\\ncolnames, 9, 67\\ncolors, 6\\ncor, 36\\ncumsum, 128, 179, 181, 188, 306\\ncurve, 116, 164, 421\\ndata.frame, 40, 87, 94, 118, 134, 198\\ndbinom, 170\\ndet, 62\\ndgamma, 273\\ndgeom, 118\\ndim, 419\\ndpois, 210\\neval, 115\\nexpression, 115\\nexp, 163, 171\\nfactor, 407\\nfisher.test, 396\\nfix, 419\\nfor, 87, 134, 184, 186, 238\\nfunction, 183, 188, 422\\nf, 168, 171\\ngamma, 164, 171\\ngeom, 158, 170\\ngnls, 51\\nh=0, 45\\n\\nhead, 28\\nhelp.start, 417\\nhelp, 418\\nhist, 10, 150, 233, 362\\nhyper, 161, 170, 264, 327\\nintegrate, 183, 188\\nkruskal.test, 414\\nlegend, 6\\nlength, 22, 135, 150\\nlines, 126, 131\\nlm, 40, 41, 46, 49, 55, 58, 266, 407, 420\\nlnorm, 171\\nlog, 45, 55\\nmatrix, 397, 419\\nmean, 22, 150, 183, 184, 186\\nmfrow, 58, 188\\nnbinom, 158\\nnmle, 51\\nnorm, 165, 171, 179, 282\\nnumeric, 91, 184\\norder, 306\\nouter, 189, 422\\npar, 126, 135, 150, 188\\npbinim, 343\\npbinom, 198\\npchisq, 350, 389, 391\\npersp, 422\\npexp, 116\\npf, 406\\npgeom, 118\\nphyper, 337\\npie, 6\\npi, 417\\nplot, 12, 14, 55, 126, 128, 135, 179, 181, 188, 264\\npnorm, 198, 210, 310, 325, 343\\npoints, 29\\npois, 159\\npower.prop.test, 348, 356\\npower.t.test, 364\\nppois, 210\\npredict, 40, 59\\nprop.test, 333, 346, 348\\n\\n423\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nIndex\\n\\nqbeta, 297\\nqbinom, 353\\nqf, 406\\nqhyper, 327\\nqnorm, 209, 309, 325, 344\\nqqnorm, 150\\nqqplot, 30\\nqt, 284, 297, 362, 408\\nquantile, 28, 29, 209, 295\\nrbeta, 187\\nrbinom, 241\\nrcauchy, 181, 188\\nreplicate, 91, 185, 235\\nrep, 87, 134, 184, 186, 209, 233\\nreside, 59, 420\\nresiduals, 394\\nresid, 40, 46, 56\\nrm, 418\\nrnorm, 36, 165\\nround, 6\\nrownames, 9, 67\\nrunif, 126, 163, 183, 186, 188, 189, 209, 233\\nsample, 69, 124, 134, 160, 171, 235, 305, 419\\nscale, 28\\nsd, 25, 150, 184, 186\\nseq, 199\\nsort, 12, 21, 30, 135\\nsqrt, 126, 183\\nsummary, 24, 46, 58, 186, 266\\nsum, 22, 94, 124, 134, 152, 235\\nt.test, 362, 366, 369\\ntable, 124, 421\\nt, 62, 168, 171\\nunif, 163, 171\\nv=0, 45\\nvar, 25, 184\\nweighted.mean, 23\\nwilcox.test, 379\\n\\naddition principle, 83\\nallele, 74\\n\\ndominant, 74\\nrecessive, 74\\n\\nalternative hypothesis, 303\\nanalysis of variance, 403\\nArchaeopteryx, 266\\nArcheopteryx, 13, 36\\narcsine distribution, 165\\narea under the curve, 307, 334, 380\\nasymptotic normality, 276\\n\\nmaximum likelihood estimator, 270\\n\\nAUC, 307, 334\\naxioms of probability, 82\\n\\nbalancing selection, 10\\nbar graph, 7\\n\\nsegmented, 9\\n\\nBayes factor, 102, 317\\nBayes formula, 100, 219, 336\\nBayesian sequential updating, 222\\nBayesian statistics, 218\\n\\nhypothesis testing, 316\\ninterval estimation, 296\\n\\nBernoulli random variable, 157, 170, 252\\nBernoulli trials, 141, 199, 206, 216, 221, 263, 311, 374\\nbest region, 304\\nbeta function, 165\\nbeta random variable, 165, 171, 218, 220\\nbias, 234, 241\\nbias-variance decomposition, 242\\nbinomial random variable, 142, 157, 170, 195\\nbinomial test, 344\\nbinomial theorem, 89, 142\\nbird fecundity, 203\\nbirthday problem, 87\\nblood type, 385, 389\\nBonferroni correction, 335\\nBonferroni inequality, 85, 335\\nbootstrap, 294\\nBorneo forests, 403\\nboxplot, 24, 404\\nBradley effect, 68\\n\\ncategorical data, 4\\ncategorical variable, 3\\nCauchy random variable, 181\\nCauchy-Schwarz inequality, 34\\ncausality, 65\\ncensus, 65\\ncentral limit theorem, 194, 205, 245\\ncentral moments, 146\\nchain rule, 98\\ncharacteristic function, 147\\nchi-square random variable, 167\\nchi-squared random variable, 171, 248\\nchi-squared test, 348\\n\\ncontingency table, 391\\nlikelihood ratio, 348\\ntwo-way table, 391\\n\\nchiasma, 349\\nchoose function, 88\\nchromotid, 349\\n\\n424\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nIndex\\n\\nCochran conditions, 395\\ncoefﬁcient of determination, 43\\ncoefﬁcient of linear expansion, 200, 202\\ncoefﬁcient of variation, 26\\ncoefﬁcient of volume expansion, 203\\ncombination, 88\\ncomplement, 84, 96\\ncomplement rule, 84\\ncomposite hypothesis, 323\\n\\none-sided test, 323, 324, 326\\ntwo-sided test, 323\\n\\nconditional distribution, 66\\nconditional mass function, 122\\nconditional probability, 97\\nconﬁdence interval, 218, 281, 365\\n\\nanalysis of variance, 406\\ndelta method, 292\\ninterpretation, 290\\nlinear regression slope, 288\\nmatched pair t, 286\\nmaximum likelihood estimation, 294\\nsample proportion, 289\\nsummary, 290\\nt, 283\\ntwo sample proportions, 289\\ntwo sample t, 286\\ntwo sample z, 286\\ntwo sided test, 365\\nz, 282\\n\\nconﬁdence level, 281\\nconfounding variable, 65\\nconjugate pair, 227\\nconservative procedure, 368\\nconsistency, 249, 276\\n\\nmaximum likelihood estimator, 269\\n\\ncontingency table, 7, 391\\nchi-squared test, 391\\nCochran conditions, 395\\n\\ncontinuity correction, 198\\nproportion test, 343\\n\\ncontinuity property, 85\\ncontinuous random variable, 115, 119, 142, 146\\ncontrasts, 408\\ncontrolled experiment, 67\\nconvex function, 245\\ncorrelation, 34\\n\\ndistributional, 149\\nsimulated, 36\\n\\ncovariance, 33\\n\\ndistributional, 148\\ncovariance matrix, 149\\n\\ncovariate, 37\\nCram´er-Rao bound, 250, 251\\ncredible interval, 296\\ncritical region, 304\\n\\nbest, 304\\nmost powerful, 304\\ncross tabulation, 66, 391\\ncross tabulation table, 7\\ncross tabulation table , 7\\ncumulative distribution function, 112\\nCurrent Population Survey, 71\\n\\nde Morgan’s law, 86\\nde Morgan, August, 137\\ndegrees of freedom, 283, 361\\n\\ncontingency table, 394\\ndenominator for F statistic, 405\\ngoodness of ﬁt, 389\\nnumerator for F statistic, 405\\n\\ndelta method, 202, 206, 234, 236, 292, 375\\n\\nconﬁdence interval, 292\\nmultidimensional, 207\\n\\ndensity\\n\\nposterior, 219\\nprior, 218\\n\\ndensity function, 119, 217\\n\\njoint, 122, 123\\nmarginal, 123\\ndesign matrix, 52\\ndifference rule, 84\\ndifferentiation, 119\\ndigamma function, 273\\ndiscrete random variable, 114, 137, 146\\ndiscrete uniform random variable, 160, 170\\ndiscriptor variable, see explanatory variable\\ndistribution\\n\\narcsine, 165\\nconditional, 66\\njoint, 121\\nMaxwell-Boltzmann, 168\\n\\ndistribution function, 112\\ndistribution of ﬁtness effects, 236, 273\\ndominant allele, 74\\nDoob, Joseph, 111\\ndouble blind procedure, 68\\ndouble reciprocal plot, 49\\nDoyle, Arthur Conan, 179\\nDravet sydome, 29\\n\\neffective degrees of freedom, 287, 368\\nefﬁciency, 250, 276\\n\\n425\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nIndex\\n\\nmaximum likelihood estimator, 270\\n\\nelement, 82, 96\\nempirical cumulative distribution function, 11\\n\\nsurvival function, 12\\n\\nequally likely outcomes, 82, 97, 161\\nestimate, 216\\nestimator, 216\\n\\ninterval, 218\\nmaximum likelihood, 244, 261\\nmethod of moments, 231\\npoint, 218\\nsummary of maximum likelihood, 269\\nunbiased, 200, 242\\n\\nethical guidelines, 66\\nevent, 81, 96\\nexpected value, 137, 139\\n\\ncontinuous random variable, 143\\ndiscrete random variable, 140\\n\\nexperiment, 67\\n\\ncontrolled, 67\\nHeshey-Chase, 75\\nMichelson-Morley, 73\\nnatural, 70\\n\\nexplanatory variable, 13, 37, 65\\nexploratory data analysis, 65\\nexponential family, 254, 275\\nexponential random variable, 115, 126, 143, 148, 163,\\n\\n171, 196, 197, 253, 351\\n\\nextrapolation, 41\\n\\nF random variable, 168, 171, 406\\nF test, 405\\nfactor, 3, 68, 391\\nfactorial moment, 146\\nfalling factorial, 87\\nfalling function, 87\\nfalse discovery rate, 336\\nfalse negative, 103, 304\\nfalse positive, 103, 303\\nfalse positive fraction, 306\\nfamilywise error rate, 335\\nﬁrst quartile, 24, 31\\nFisher information, 251\\n\\nmatrix, 273\\n\\nFisher’s exact test, 395\\nFisher’s method, 335\\nFisher, Ronald, 403\\nﬁt, 38\\nﬁtness, 236\\nﬁve number summary, 24\\nﬂoor function, 10, 112\\n\\nFourier transform, 147\\nfundamental principle of counting, 81, 86\\n\\nG square test statistic, 388\\nGalton, Sir Francis, 33\\ngamma function, 164\\ngamma random variable, 164, 171, 218, 236, 272\\nGarﬁeld, James A., 65\\nGenBank, 72\\ngene ontology, 5\\ngenerating function\\nprobability, 147\\n\\ngenetic recombination, 349\\ngeometric random variable, 117, 158, 170\\ngeometric sequence, 117\\nGlobal Health Observatory, 71\\ngoodness of ﬁt, 385\\ngoogle, 111\\nGossett, William Sealy, 283\\ngradient descent, 51\\ngrand mean, 404, 411\\nGusset, William Sealy, 361\\n\\nhanging chi-gram, 389, 395, 399\\nHardy-Weinberg principle, 390\\nhat matrix, 53\\nHershy-Chase experiment, 75\\nheteroscedasticity, 40\\nheterozygosity, 245\\nhistogram, 10\\nhomoscedasticity, 40\\nhoney bees, 68, 69, 333, 344, 407\\nhybridization, 307\\nhypergeometric random variable, 99, 161, 170, 235, 247,\\n\\n351\\n\\nhypothesis, 303\\n\\nalternative, 303\\nBayesian approach, 316\\ncomposite, 323\\nnull, 303\\nsimple, 303, 334\\n\\nideal gas, 168\\nimportance sampling, 186\\n\\nfunction, 186\\nweights, 186\\n\\nINCAP, 77\\ninclusion-exclusion rule, 85\\nindependence, 105\\n\\nrandom variable, 148\\n\\nindependent random variables, 123\\nindividual, 3\\n\\n426\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nIndex\\n\\ninformation, 225\\n\\nobserved, 270\\n\\ninformation inequality, 251\\ninput, 37\\ninput variable, see also explanatory variable\\nIntergovernmental Panel on Climate Change, 17\\ninterquartile range, 24, 31\\nintersection, 96\\ninterval estimation, 281\\n\\nBayesian, 296\\n\\ninterval estimator, 218\\ninvariance property, 261, 275\\n\\nJensen’s inequality, 245\\njoint density, 217\\njoint density function, 122, 123\\njoint distribution, 121\\njoint mass function, 122, 148\\njoint probability density function, 122\\n\\nK¨olreute, Josef Gottlieb, 74\\nKarlin-Rubin theorem, 342, 373\\nKolmogorov, Andrey, 81\\nKruskal-Wallis test, 413\\nkurtosis, 147\\n\\nLagrange multipliers, 386\\nLaplace transform, 147\\nlaw of blending, 74\\nlaw of cosines, 34\\nlaw of independent assortment, 106\\nlaw of large numbers, 179, 231, 250\\nlaw of mass action, 47\\nlaw of segregation, 106\\nlaw of total probability, 99, 120\\nleast squares regression, see linear regression\\nLeibnitz integral rule, 257\\nleptokurtic, 147\\nlevel, 3, 68\\nlikelihood function, 217\\nlikelihood ratio, 386\\nlikelihood ratio test, 306, 345, 404\\n\\nLincoln-Peterson method, 234, 247, 263\\nlinear regression, 37, 216, 264, 265\\n\\nchi-squared test, 348\\nt test, 375\\ntwo sided test, 346\\n\\nﬁt, 38\\nheteroscedasticity, 40\\nhomoscedasticity, 40\\nmultiple, 51\\nnonlinear regression, 51\\n\\nresidual, 38\\nslope conﬁdence interval, 288\\nweighted least squares, 266\\n\\nLineweaver-Burke plot, 49\\nlink function, 45\\nlinkage disequilibrium, 107\\nlog likelihood surface, 270\\nlog-normal random variable, 167\\nlog-odds, 255\\nlogarithm, 16, 45\\nlurking variable, 65\\n\\nmalaria, 397\\nmargin of error, 199, 281, 285\\nmarginal density, 217\\nmarginal density function, 122, 123\\nmarginal distribution, 9, 392\\n\\ntwo-way table, 9\\n\\nmarginal mass function, 122, 123\\nmark and recapture, 234, 247, 263, 326, 396\\n\\nLincoln Peterson method, 327\\npower function, 327\\ntwo-sided test, 327\\n\\nmass function, 116\\njoint, 122, 148\\nmarginal, 122, 123\\nmatched pair procedures\\n\\nt interval, 286\\nt test, 366\\n\\nmatrix, 51\\n\\ndeterminant, 52\\ninverse, 52\\ntranspose, 52\\n\\nmaximum likelihood estimation\\n\\nconﬁdence interval, 294\\n\\nmaximum likelihood estimator, 244, 261\\n\\nasymptotic properties, 269\\nefﬁciency, 270\\nsummary, 269\\n\\nMaxwell-Boltzmann distribution, 168\\nmean, 137\\n\\ndistributional, 146\\ngrand, 404\\nposterior, 220\\nsample, 21, 31, 195, 206\\ntrimmed, 23\\ntruncated, 23\\nweighted, 22\\nwithin group, 404\\nmean square error, 242\\nmedian, 21, 31\\n\\n427\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nIndex\\n\\nmemorylessness, 117, 164\\nMendel, Gregor, 74, 106, 289, 390\\nmethod of moments, 181, 231, 275\\nmethod of steepest descent, 51\\nMichaelis constant, 48\\nMichaelis-Menten kinetics, 47\\nMichaelis-Metens kinetic equation, 48\\nMichelson-Morley experiment, 73\\nMillennium Development Goals, 71\\nmimic, 307\\nmixture, 121, 169, 336\\n\\ncontinuous, 219\\n\\nmodel, 307\\n\\nparametric, 66\\nprobability, 81\\n\\nmoment generating function, 147\\nmoments, 146\\n\\nmethod of, 231\\nmonotonicity rule, 85\\nMonte Carlo integration, 182\\nMorgan, Thomas Hunt, 75\\n\\ngenetic recombination, 349\\n\\nmosquitoes, 370\\nmultidimensional\\n\\nFisher information, 271\\n\\nmultidimensional delta method, 207\\nmultiple regression, 51\\nmultiplication principle, 98\\nmuon, 234\\nmutually exclusive, 83\\n\\nNational Foundation for Infantile Paralysis, 76\\nnatural experiment, 70\\nnatural parameter, 254, 275\\nnegative binomial random variable, 158, 170\\nneutrino, 234\\nNeyman, Jerzy, 281, 303\\nNeyman-Pearson lemma, 304, 314\\nnon-parametric tests, 378\\nnonlinear regression, 51\\nnormal equations, 53\\nnormal probability plot, 150\\nnormal random variable, 145, 165, 171, 195, 218, 225,\\n\\n252, 263\\n\\nnormal score, see standardized version\\nnull hypothesis, 303\\n\\nObama, Barack, 65\\nobservational study, 66\\nobserved information, 270\\nodds, 86, 292\\n\\nposterior, 316\\nprior, 316\\n\\noil production, 15, 45\\none-sided test, 323, 324\\npower function, 326\\nuniformly most powerful, 342\\n\\norder statistic, 21, 165, 267\\noutcome, 81, 96\\noutpit, 37\\noyster, 390\\n\\np-value, 331, 343\\npangenesis, 74\\nparallel axis theorem, 26\\nparameter space, 216\\nparametric model, 66\\nPareto random variable, 167, 232, 245, 247, 253\\npartition, 99\\nPascal’s triangle, 90\\nPearson residual, 389\\nPearson, Egon Sharpe, 303\\nPearson, Karl, 385, 388\\npermutation, 87\\npermutation test, 377\\npie chart, 4\\npion, 234\\nplacebo effect, 68\\nplatykurtic, 147\\nplot\\n\\nresidual, 40\\n\\nPochhammer symbol, 87\\npoint estimation, 281\\npoint estimator, 218\\nPoisson random variable, 159, 170, 199, 210, 254\\nPolya, George, 193\\npolynomial regression, 54\\npooled two-sample t procedure, 412\\npooled two-sample t procedures, 372\\npopulation, 3, 65\\npopulation means\\n\\ntests, 374\\n\\npopulation proportions\\n\\ntests, 374\\n\\nposterior density, 219\\nposterior mean, 220\\nposterior probability, 102, 317\\npower, 69, 304\\nt test, 364\\n\\npower function, 323\\n\\nlilelihood ratio, 351\\nLincoln Peterson method, 327\\n\\n428\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nIndex\\n\\nmark and recapture, 327\\none-sided test, 326\\nproportion test, 344\\nsimulated, 351\\nt test, 363\\ntwo-sided test, 327\\n\\npower law distribution, 167\\npredictor variable, see also explanatory variable\\nprior density, 218\\nprior probability, 102, 316\\nprobability, 81\\naxioms, 82\\nconditional, 97\\n\\nprobability density function, 119\\n\\njoint , 122\\n\\nprobability generating function, 147\\nprobability mass function, 116\\nprobability model, 81\\nprobability transform, 125, 126, 233\\npropagation of error, 200\\n\\nmultidimensional, 203\\n\\npropagation of uncertainty, 200\\nproportion, 197\\n\\nsample, 197\\n\\nproportion test, 343, 374\\n\\nbinomial test, 344\\ncontinuity correction, 343\\npower function, 344\\n\\nproposal density, 186\\nPublic Health Service, 76\\nPunnett square, 106\\nPunnett, Reginald, 74\\nPythagorean identity, 34, 179\\n\\ndistributional, 149\\n\\nPythagoreanf identity, 193\\n\\nquadratic identity for variance, 148, 179\\nquantile, 28\\nquantile function, 125\\nquantile plot, 150\\nquantile-quantile plot, 28\\nquantitative variable, 3\\nquartile, 24\\n\\nﬁrst, 24, 31\\nthird, 24, 31\\n\\nradon, 362\\nrandom sample, 69\\n\\nsimple, 69\\nstratiﬁed, 70\\n\\nrandom variable, 111, 170\\n\\nBernoulli, 157, 252\\nbeta, 165, 218, 220\\nbinomial, 142, 157, 195\\nCauchy, 181\\nchi-square, 167\\nchi-squared, 248\\ncontinuous, 115, 119, 142, 146\\ndartboard, 115, 126, 143\\ndiscrete, 114, 137, 146\\ndiscrete uniform, 160\\nexponential, 115, 126, 143, 148, 163, 196, 197, 253,\\n\\n351\\n\\nF, 168, 406\\ngamma, 164, 218, 236, 272\\ngeometric, 117, 158\\nhypergeometric, 99, 161, 235, 247, 351\\nlog-normal, 167\\nnegative binomial, 158\\nnormal, 145, 165, 195, 218, 225, 252, 263\\nPareto, 167, 232, 245, 247, 253\\nPoisson, 159, 199, 210, 254\\nt, 168, 283\\nuniform, 125, 163, 217, 267\\n\\nrandom variables\\n\\nindependent, 123\\n\\nreceiver operating characteristic, 307, 311, 334, 336\\nreceiving operator characteristic, 380\\nrecessive allele, 74\\nregression\\n\\npolynomial, 54\\n\\nreliability engineering, 223\\nresidual, 38\\n\\nPearson, 389\\nstandardized, 389\\n\\nresidual plot, 40, 46\\nresidual sum of squares, 405\\nresistant measure, 23\\nresponse variable, 13, 37, 65\\nrising factorial, 87\\nrisk, 317\\n\\nSalk vaccine ﬁeld trials, 76\\nsample, 65\\nsample mean, 21, 31, 195, 206\\nsample proportion, 197, 206, 216\\nsample proportion interval, 289\\nsample size determination, 310\\nsample space, 81, 96\\nsample standard deviation, 25, 31\\nsample sum, 194, 205\\nsample variance, 25, 31\\n\\n429\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nIndex\\n\\nunbiased estimator, 242\\n\\nsampling with replacement, 124, 162\\nsampling without replacement, 99, 124, 161, 235\\nscatterplot, 13\\nscore function, 251, 261, 276\\nsegmented bar graph, 9\\nsensitivity, 103, 304, 364\\nsequential updating\\nBayesian, 222\\n\\nset theory, 96\\nsigniﬁcance level, 303, 324\\nSilver, Nate, 97\\nsimple hypothesis, 303, 334\\nsimple random sample, 69, 217\\nsimulating discrete random variables, 124\\nskewness, 147, 196\\nsmoking, 392\\nsocial desirability bias, 68\\nspeciﬁcity, 103, 304, 364\\nspeed of light, 73\\nspin, 234\\nstandard deviation, 25, 146\\n\\nsample, 25, 31\\n\\nstandard error, 283, 361\\nstandard score, see standardized version\\nstandardized residual, 389\\nstandardized score, 205, 286\\nstandardized version, 28, 146\\nstate of nature, 81, 217\\nstate space, 111\\nstatistic, 215\\nsteepest descent, 51\\nStirling approximation, 249\\nstratiﬁcation, 66\\nstratiﬁed random sample, 70\\nstudentized score, 286\\nSturges’ formula, 10\\nsubject, 67\\nsubset, 82, 96\\nsudden infant death syndrome, 389\\nsufﬁcient statistic, 254, 275\\nsummary\\n\\nconﬁdence interval, 290\\nestimators, 269\\nnormal approximations, 205\\nproperties of random variables, 170\\nrandom variables and expectation, 146\\nsigniﬁcance tests, 373\\nsimple hypothesis, 313\\n\\nsupercedure, 69\\nsurvival function, 12, 23, 113, 143\\n\\nt interval, 283\\nt procedures\\n\\nguidelines, 361\\npooled two-sample, 372\\ntwo sample, 368\\n\\nt random variable, 168, 171, 283\\nt test, 361\\n\\nmatched pair procedures, 366\\none sample, 362\\npower, 364\\npower function, 363\\n\\ntarget variable, 13\\ntelescoping sum, 92\\nthermal expansion, 202\\nthird quartile, 24, 31\\ntime plot, 15\\ntotal sum of squares, 405\\ntransform\\n\\nprobability, 126\\n\\ntree diagram, 101\\ntrigamma function, 273\\ntrue negatiive probability, 303\\ntrue positive, 103\\ntrue positive fraction, 306\\ntruncated mean, 23\\nTufte, Edward, 3\\ntwo sample proportion interval, 289\\ntwo sample t interval, 286\\ntwo sample t procedures, 368, 410\\ntwo sample z interval, 286\\ntwo sided test\\n\\nconﬁdence interval, 365\\nlikelihood ratio test, 346\\n\\ntwo-sided test, 323\\ntwo-way table, 7, 391\\n\\nchi-squared test, 391\\n\\ntype I error, 303, 324\\ntype II error, 304, 324\\n\\nunbiased estimator, 200, 242\\n\\nlinear regression, 266\\n\\nuniform random variable, 125, 163, 171, 217, 267\\nuniformly most powerful test, 342\\nunion, 96\\nUnited States census, 71\\nuniversal set, 82, 96\\nupper tail probability, 282\\n\\nvariable, 3\\n\\ncategorical, 3\\nconfounding, 65\\n\\n430\\n\\n\\x0cIntroduction to the Science of Statistics\\n\\nIndex\\n\\ndescriptor, 13\\nexplanatory, 37, 65\\ninput, 13\\nlurking, 65\\npredictor, 13\\nquantitative, 3\\nresponse, 13, 37, 65\\ntarget, 13\\n\\nvariance, 25, 146\\n\\ndistributional, 146\\nquadratic identity, 27\\nsample, 25, 31\\n\\nvitamin C wheat soy blend, 366\\n\\nweighted least squares, 266\\nweighted mean, 22\\nWelch-Satterthwaite equation, 287, 368\\nWells, H.G., vii\\nWilcoxon rank sum test, 378\\nWilcoxon signed rank test, 334, 381\\nWilkes, Samuel, vii\\nwithin group mean, 404\\nWomen’s Health Initiative, 72\\n\\nz interval, 282\\nz-score, see standardized version\\nz-value, see standardized version\\n\\n431\\n\\n\\x0c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livro_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chapters_content(pdf_text_raw,start_end_sections_titles):\n",
    "    stop_words = stopwords.words('english')\n",
    "    nlp = spacy.load('en',disable=['parser','ner'])\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB']\n",
    "    \n",
    "    start_section = 0\n",
    "    section_text = []\n",
    "    for range_ in start_end_sections_titles:\n",
    "        section_text.append(pdf_text_raw[start_section:range_[0]])\n",
    "        start_section = range_[1]\n",
    "        \n",
    "    section_text.append(pdf_text_raw[start_section:])\n",
    "    section_text = section_text[1:]    \n",
    "    corpus = []\n",
    "    for sc_text in section_text:\n",
    "        documents = []\n",
    "        doc_tokened = sc_text.split(\".\")\n",
    "        for index,doc in enumerate(doc_tokened):\n",
    "            doc_text_no_punc = simple_preprocess(doc,deacc=True) \n",
    "            tokenized_text_non_stop_words = [ word for word in doc_text_no_punc \\\n",
    "                                             if word not in stop_words]\n",
    "            text_non_stop_words = ' '.join(tokenized_text_non_stop_words)\n",
    "            tokenized_lemmas = nlp(text_non_stop_words)\n",
    "            tokenized_lemmas = [token.lemma_ for token in tokenized_lemmas \\\n",
    "                                if token.pos_ in allowed_postags]\n",
    "            documents.append(tokenized_lemmas)\n",
    "        corpus.append(list(filter(lambda x: len(x) >= 2, documents)))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "livro_edited = livro_text.split('\\n\\n')\n",
    "#print(livro_edited)\n",
    "print(livro_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brief Contents \\n \\n1. Introduction…………………………………………………………11 ', '2. Probability…………………………………………………………...21 ', '3. Random Variables…………………………….…………………….37 ', '4. Expectation…………………………………………………………..69 ', '5. Equalities…………………………………………………………….85 ', '6. Convergence of Random Variables………………………………...89 ', '7. Models, Statistical Inference and Learning………………………105 ', '8. Estimating the CDF and Statistical Functionals…………………117 ', '9. The Bootstrap………………………………………………………129 ', '10. Parametric Inference……………………………………………..145 ', '11. Hypothesis Testing and p-values…………………………………179 ', '12. Bayesian Inference………………………………………………..205   ', '13. Statistical Decision Theory……………………………………….227 ', '\\x0c14. Linear Regression………………………………………………...245 ', '15. Multivariate Models……………………………………………...269 ', '16. Inference about Independence…………………………………..279   ', '17. Undirected Graphs and Conditional Independence……………297 ', '18. Loglinear Models…………………………………………………309 ', '19. Causal Inference………………………………………………….327 ', '20. Directed Graphs………………………………………………….343 ', '21. Nonparametric curve Estimation……………………………….359 ', '22. Smoothing Using Orthogonal Functions..………………………393 ', '23. Classification……………………………………………………..425 ', '24. Stochastic Processes………………………………………………473 ', '25. Simulation Methods………………………………………………505 ']\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "chapter_heads_menu = list(filter(lambda x: '………' in x, livro_edited))\n",
    "print(chapter_heads_menu)\n",
    "print(len(chapter_heads_menu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brief Contents  1. Introduction', '2. Probability...', '3. Random Variables..', '4. Expectation..', '5. Equalities.', '6. Convergence of Random Variables...', '7. Models, Statistical Inference and Learning', '8. Estimating the CDF and Statistical Functionals', '9. The Bootstrap', '10. Parametric Inference..', '11. Hypothesis Testing and p-values', '12. Bayesian Inference..205   ', '13. Statistical Decision Theory.', '\\x0c14. Linear Regression...', '15. Multivariate Models...', '16. Inference about Independence..279   ', '17. Undirected Graphs and Conditional Independence', '18. Loglinear Models', '19. Causal Inference.', '20. Directed Graphs.', '21. Nonparametric curve Estimation.', '22. Smoothing Using Orthogonal Functions..', '23. Classification..', '24. Stochastic Processes', '25. Simulation Methods']\n"
     ]
    }
   ],
   "source": [
    "processed_livro = [re.sub('\\\\n','',ch) for ch in chapter_heads_menu]\n",
    "processed_livro = [re.sub('…','',ch) for ch in processed_livro]\n",
    "processed_livro = [re.sub('\\d{1,}\\s$','',ch) for ch in processed_livro]\n",
    "#processed_livro = [re.findall(('(?<!\\S)[A-Za-z]+(?!\\S)|(?<!\\S)[A-Za-z]+(?=:(?!\\S))'),ch) for ch in processed_livro]\n",
    "print(processed_livro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chapters_titles_(pdf_text_raw):\n",
    "    p = re.compile(r'([\\n]{2,2}(\\d\\.)+\\d* [A-Za-z0-9? ]+[\\n]{1,2})')\n",
    "    all_iter = p.finditer(pdf_text_raw)\n",
    "    titles = p.findall(pdf_text_raw)\n",
    "    titles_clean = titles#[re.sub(r'(\\d\\.)+\\d*', '', str(tl[0])) for tl in titles]#[re.sub('\\W+','', tl) for tl in titles]\n",
    "    titles_clean = [re.sub('\\\\n', '', str(tl[0])) for tl in titles_clean]\n",
    "    indexes = [m.span() for m in all_iter]\n",
    "    return titles_clean,indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['2.3 Quantiles and Standardized Variables', '7.8 Simulating Random Variables', '8.9 Quantile Plots and Probability Plots', '1.1 Types of Data', '1.2 Categorical Data', '1.2.2 Bar Charts', '1.4 Histograms and the Empirical Cumulative Distribution Function', '1.5 Scatterplots', '1.6 Time Plots', '1.7 Answers to Selected Exercises', '2.1 Measuring Center', '2.1.2 Means', '2.2 Measuring Spread', '2.2.2 Sample Variance and Standard Deviation', '2.3 Quantiles and Standardized Variables', '2.5 Answers to Selected Exercises', '3.1 Covariance and Correlation', '3.2 Linear Regression', '3.2.1 Transformed Variables', '3.3 Extensions', '3.3.1 Nonlinear Regression', '3.3.2 Multiple Linear Regression', '3.4 Answers to Selected Exercises', '3.6. First we rearrange terms', '4.1 Preliminary Steps', '4.2 Professional Ethics', '4.3 Formal Statistical Procedures', '4.3.1 Observational Studies', '4.3.2 Randomized Controlled Experiments', '2. Randomize the assignment of subjects to treatments to eliminate bias due to systematic differences among', '4.3.3 Natural experiments', '4.4 Case Studies', '2. Take a sample of unit frames consisting of housing units in census blocks that contain a very high proportion of', '4.4.2 Experiments', '5.1 Introduction', '5.2 Equally Likely Outcomes and the Axioms of Probability', '5.3 Consequences of the Axioms', '5.4 Counting', '5.4.1 Fundamental Principle of Counting', '5.4.2 Permutations', '5.4.3 Combinations', '5.5 Answers to Selected Exercises', '5.8. If', '6.2 The Multiplication Principle', '6.3 The Law of Total Probability', '6.4 Bayes formula', '6.5 Independence', '6.6 Answers to Selected Exercises', '7.1 Introduction', '7.2 Distribution Functions', '7.3 Properties of the Distribution Function', '7.3.1 Discrete Random Variables', '7.3.2 Continuous Random Variables', '7.4 Mass Functions', '7.5 Density Functions', '7.6 Mixtures', '7.7 Joint and Conditional Distributions', '7.7.2 Continuous Random Variables', '7.8 Simulating Random Variables', '7.8.1 Discrete Random Variables and the sample Command', '7.8.2 Continuous Random Variables and the Probability Transform', '7.9 Answers to Selected Exercises', '8.2 Discrete Random Variables', '8.3 Bernoulli Trials', '8.4 Continuous Random Variables', '8.5 Summary', '6. The third moment of the standardized random variable', '8.7 Independence', '8.8 Covariance and Correlation', '8.8.1 Equivalent Conditions for Independence', '8.9 Quantile Plots and Probability Plots', '8.10 Answers to Selected Exercises', '9.1 Examples of Discrete Random Variables', '9.2 Examples of Continuous Random Variables', '9.3 More on Mixtures', '9.4 R Commands', '9.5 Summary of Properties of Random Variables', '9.5.1 Discrete Random Variables', '9.5.2 Continuous Random Variables', '9.6 Answers to Selected Exercises', '9.4. We have mass functions', '1. For Bernoulli trials with a known number of trials n but unknown success probability parameter p has joint', '0. The parameter'], [(2059, 2102), (7687, 7722), (9052, 9095), (40682, 40702), (43243, 43266), (46820, 46839), (53009, 53077), (58046, 58065), (62294, 62312), (67274, 67310), (70613, 70636), (71158, 71172), (75856, 75879), (77616, 77663), (83443, 83486), (90710, 90746), (96460, 96493), (103606, 103630), (120197, 120227), (130546, 130563), (131284, 131313), (133763, 133798), (142637, 142674), (144048, 144081), (152863, 152887), (155564, 155591), (156858, 156895), (157503, 157533), (159450, 159492), (160673, 160784), (169419, 169447), (170584, 170603), (172400, 172519), (175094, 175114), (196209, 196228), (199239, 199299), (203446, 203479), (206804, 206819), (207460, 207502), (208791, 208812), (211062, 211083), (218213, 218249), (220569, 220580), (227843, 227878), (229448, 229483), (232806, 232826), (246072, 246091), (251585, 251621), (257727, 257746), (260304, 260333), (264652, 264698), (264998, 265032), (265807, 265843), (269251, 269272), (274197, 274221), (276917, 276932), (279209, 279251), (281342, 281378), (285151, 285185), (286055, 286112), (288238, 288304), (292133, 292169), (313053, 313085), (317782, 317805), (321135, 321169), (328652, 328667), (330861, 330920), (334452, 334471), (335525, 335558), (338496, 338543), (339462, 339505), (342462, 342499), (351563, 351607), (363638, 363684), (373952, 373975), (375980, 375997), (376647, 376695), (376773, 376808), (378817, 378853), (380985, 381021), (382089, 382120), (460946, 461059), (719364, 719383)])\n"
     ]
    }
   ],
   "source": [
    "chapters = get_chapters_titles_(livro_text)\n",
    "print(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\yaniv\\anaconda3\\lib\\site-packages (from python-docx) (4.4.1)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py): started\n",
      "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.10-cp37-none-any.whl size=184496 sha256=dd37db7406693883f68e9d69bf066a94d05fee35345c247bace763e0ee3da9a3\n",
      "  Stored in directory: C:\\Users\\yaniv\\AppData\\Local\\pip\\Cache\\wheels\\18\\0b\\a0\\1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.10\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_italic_bold_docx(doc_path):\n",
    "    document = Document(doc_path)\n",
    "    bolds=[]\n",
    "    italics=[]\n",
    "    full_text = []\n",
    "    font_sizes = []\n",
    "    for para in document.paragraphs:\n",
    "        #print(\"next paragraph:\")\n",
    "        #rint(para.text)\n",
    "        for run in para.runs:\n",
    "            #print('next run')\n",
    "            #print(run.text)\n",
    "            full_text.append(run.text)\n",
    "            font_sizes.append(run.font.size)\n",
    "            if run.italic :\n",
    "                italics.append(run.text)\n",
    "            if run.bold :\n",
    "                bolds.append(run.text)\n",
    "\n",
    "    return {'bold_phrases':bolds,'italic_phrases':italics,'full_text':full_text,'font_sizes':font_sizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vika = read_italic_bold_docx('../data/raw/pdf/7kLHJ-F33GI/livro.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vika['italic_phrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "viki = read_italic_bold_docx('../data/raw/pdf/7kLHJ-F33GI/statbook.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An Introduction to the Science of Statistics:',\n",
       " 'From Theory to Implementation',\n",
       " 'Preliminary Edition',\n",
       " 'c ',\n",
       " 'Joseph C. Watkins',\n",
       " 'Contents',\n",
       " 'i',\n",
       " '',\n",
       " 'ii',\n",
       " '',\n",
       " 'iii',\n",
       " '',\n",
       " 'iv',\n",
       " '',\n",
       " 'v',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '',\n",
       " 'vi',\n",
       " 'Preface',\n",
       " 'Statistical thinking will one day be as necessary a qualification for efficient citizenship as the ability to read and write. – Samuel Wilkes, 1951, paraphrasing H. G. Wells from Mankind in the Making',\n",
       " 'The value of statistical thinking is now accepted by researchers and practitioners from a broad range of endeavors. This viewpoint has become common wisdom in a world of big data. The challenge for statistics educators is to adapt their pedagogy to accommodate the circumstances associated to the information age. This choice of pedagogy should be attuned to the quantitative capabilities and scientific background of the students as well as the intended use of their newly acquired knowledge of statistics.',\n",
       " 'Many university students, presumed to be proficient in college algebra, are taught a variety of procedures and standard tests under a well-developed pedagogy. This approach is sufficiently refined so that students have a good intuitive understanding of the underlying principles presented in the course. However, if the statistical needs presented by a given scientific question fall outside the battery of methods presented in the standard curriculum, then students are typically at a loss to adjust the procedures to accommodate the additional demand.',\n",
       " 'On the other hand, undergraduate students majoring in mathematics frequently have a course on the theory of statistics as a part of their program of study. In this case, the standard curriculum repeatedly finds itself close to the very practically minded subject that statistics is. However, the demands of the syllabus provide very little time to explore these applications with any sustained attention.',\n",
       " 'Our goal is to find a middle ground.',\n",
       " 'Despite the fact that calculus is a routine tool in the development of statistics, the benefits to students who have learned calculus are infrequently employed in the statistics curriculum. The objective of this book is to meet this need with a one semester course in statistics that moves forward in recognition of the coherent body of knowledge provided by statistical theory having an eye consistently on the application of the subject. Such a course may not be able to achieve the same degree of completeness now presented by the two more standard courses described above. However, it ought to able to achieve some important goals:',\n",
       " 'leaving students capable of understanding what statistical thinking is and how to integrate this with scientific procedures and quantitative modeling and',\n",
       " 'learning how to ask statistics experts productive questions, and how to implement their ideas using statistical software and other computational tools.',\n",
       " 'Inevitably, many important topics are not included in this book. In addition, I have chosen to incorporate abbre-viated introductions of some more advanced topics. Such topics can be skipped in a first pass through the material. However, one value of a textbook is that it can serve as a reference in future years. The context for some parts of the exposition will become more clear as students continue their own education in statistics. In these cases, the more advanced pieces can serve as a bridge from this book to more well developed accounts. My goal is not to compose a stand alone treatise, but rather to build a foundation that allows those who have worked through this book to introduce themselves to many exciting topics both in statistics and in its areas of application.',\n",
       " 'vii',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '',\n",
       " 'Who Should Use this Book',\n",
       " 'The major prerequisites are comfort with calculus and a strong interest in questions that can benefit from statistical analysis. Willingness to engage in explorations utilizing statistical software is an important additional requirement. The original audience for the course associated to this book are undergraduate students minoring in mathematics. These student have typically completed a course in multivariate calculus. Many have been exposed to either linear algebra or differential equations. They enroll in this course because they want to obtain a better understanding of their own core subject. Even though we regularly rely on the mechanics of calculus and occasionally need to work with matrices, this is not a textbook for a mathematics course, but rather a textbook that is dedicated to a higher level of understanding of the concepts and practical applications of statistics. In this regard, it relies on a solid grasp of concepts and structures in calculus and algebra.',\n",
       " 'With the advance and adoption of the Common Core State Standards in mathematics, we can anticipate that primary and secondary school students will experience a broader exposure to statistics through their school years. As a consequence, we will need to develop a curriculum for teachers and future teachers so that they can take content in statistics and turn that into curriculum for their students. This book can serve as a source of that content.',\n",
       " 'In addition, those engaged both in industry and in scholarly research are experiencing a surge in the need to design more complex experiments and analyze more diverse data types. Universities and industry are responding with advanced educational opportunities to extend statistics education beyond the theory of probability and statistics, linear models and design of experiments to more modern approaches that include stochastic processes, machine learning and data mining, Bayesian statistics, and statistical computing. This book can serve as an entry point for these critical topics in statistics.',\n",
       " 'An Annotated Syllabus',\n",
       " 'The four parts of the course - organizing and collecting data, an introduction to probability, estimation procedures and hypothesis testing - are the building blocks of many statistics courses. We highlight some of the particular features in this book.',\n",
       " 'Organizing and Collecting Data',\n",
       " 'Much of this is standard and essential - organizing categorical and quantitative data, appropriately displayed as contin-gency tables, bar charts, histograms, boxplots, time plots, and scatterplots, and summarized using medians, quartiles, means, weighted means, trimmed means, standard deviations, correlations and regression lines. We use this as an opportunity to introduce to the statistical software package R and to add additional summaries like the empirical cu-mulative distribution function and the empirical survival function. One example incorporating the use of this is the comparison of the lifetimes of wildtype and transgenic mosquitoes and a discussion of the best strategy to display and summarize data if the goal is to examine the differences in these two genotypes of mosquitoes in their ability to carry and spread malaria. A bit later, we will do an integration by parts exercise to show that the mean of a non-negative continuous random variable is the area under its survival function.',\n",
       " 'Collecting data under a good design is introduced early in the text and discussion of the underlying principles of experimental design is an abiding issue throughout the text. With each new mathematical or statistical concept comes an enhanced understanding of what an experiment might uncover through a more sophisticated design than what was previously thought possible. The students are given readings on design of experiment and examples using R to create a sample under variety of protocols.',\n",
       " 'Introduction to Probability',\n",
       " 'Probability theory is the analysis of random phenomena. It is built on the axioms of probability and is explored, for example, through the introduction of random variables. The goal of probability theory is to uncover properties arising from the phenomena under study. Statistics is devoted to the analysis of data. One goal of statistical science is to',\n",
       " 'viii',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '',\n",
       " 'articulate as well as possible what model of random phenomena underlies the production of the data. The focus of this section of the course is to develop those probabilistic ideas that relate most directly to the needs of statistics.',\n",
       " 'Thus, we must study the axioms and basic properties of probability to the extent that the students understand conditional probability and independence. Conditional probability is necessary to develop Bayes formula which we will later use to give a taste of the Bayesian approach to statistics. Independence will be needed to describe the likelihood function in the case of an experimental design that is based on independent observations. Densities for continuous random variables and mass function for discrete random variables are necessary to write these likelihood functions explicitly. Expectation will be used to standardize a sample sum or sample mean and to perform method of moments estimates.',\n",
       " 'Random variables are developed for a variety of reasons. Some, like the binomial, negative binomial, Poisson or the gamma random variable, arise from considerations based on Bernoulli trials or exponential waiting. The hyperge-ometric random variable helps us understand the difference between sampling with and without replacement. The F , t and chi-square random variables will later become test statistics. Uniform random variables are the ones simulated by random number generators. Because of the central limit theorem, the normal family is the most important among the list of parametric families of random variables.',\n",
       " 'The flavor of the text returns to becoming more authentically statistical with the law of large numbers and the central limit theorem. These are largely developed using simulation explorations and first applied to simple Monte Carlo techniques and importance sampling to estimate the value of an definite integrals. One cautionary tale is an example of the failure of these simulation techniques when applied without careful analysis. If one uses, for example, Cauchy random variables in the evaluation of some quantity, then the simulated sample means can appear to be converging only to experience an abrupt and unpredictable jump. The lack of convergence of an improper integral reveals the difficulty. The central object of study is, of course, the central limit theorem. It is developed both in terms of sample sums and sample means and proportions and used in relatively standard ways to estimate probabilities. However, in this book, we can introduce the delta method which adds ideas associated to the central limit theorem to the context of propagation of error.',\n",
       " 'Estimation',\n",
       " 'In the simplest possible terms, the goal of estimation theory is to answer the question: What is that number? An estimate is a statistic, i. e., a function of the data. We look to two types of estimation techniques - method of moments and maximum likelihood and several criteria for an estimator using, for example, variance and bias. Several examples including mark and recapture and the distribution of fitness effects from genetic data are developed for both types of estimators. The variance of an estimator is approximated using the delta method for method of moments estimators and using Fisher information for maximum likelihood estimators. An analysis of bias is based on quadratic Taylor series approximations and the properties of expectations. Both classes of estimators are often consistent. This implies that the bias decreases towards zero with an increasing number of observations. R is routinely used in simulations to gain insight into the quality of estimators.',\n",
       " 'The point estimation techniques are followed by interval estimation and, notably, by confidence intervals. This brings us to the familiar one and two sample t-intervals for population means and one and two sample z-intervals for population proportions. In addition, we can return to the delta method and the observed Fisher information to construct confidence intervals associated respectively to method of moment estimators and and maximum likelihood estimators. We also add a brief introduction on bootstrap confidence intervals and Bayesian credible intervals in order to provide a broader introduction to strategies for parameter estimation.',\n",
       " 'Hypothesis Testing',\n",
       " 'For hypothesis testing, we first establish the central issues - null and alternative hypotheses, type I and type II errors, test statistics and critical regions, significance and power. We then present the ideas behind the use of likelihood ratio tests as best tests for a simple hypothesis. This is motivated by a game designed to explain the importance of the Neyman Pearson lemma. This approach leads us to well-known diagnostics of an experimental design, notably, the receiver operating characteristic and power curves.',\n",
       " 'ix',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '',\n",
       " 'Extensions of the Neyman Pearson lemma form the basis for the t test for means, the chi-square test for goodness of fit, and the F test for analysis of variance. These results follow from the application of optimization techniques from calculus, including the use of Lagrange multipliers to develop goodness of fit tests. The Bayesian approach to hypothesis testing is explored for the case of simple hypothesis using morphometric measurements, in this case a butterfly wingspan, to test whether a habitat has been invaded by a mimic species.',\n",
       " 'The desire of a powerful test is articulated in a variety of ways. In engineering terms, power is called sensitivity. We illustrate this with a radon detector. An insensitive instrument is a risky purchase. This can be either because the instrument is substandard in the detection of fluctuations or poor in the statistical basis for the algorithm used to determine a change in radon level. An insensitive detector has the undesirable property of not sounding its alarm when the radon level has indeed risen.',\n",
       " 'The course ends by looking at the logic of hypotheses testing and the results of different likelihood ratio analyses applied to a variety of experimental designs. The delta method allows us to extend the resulting test statistics to multivariate nonlinear transformations of the data. The textbook concludes with a practical view of the consequences of this analysis through case studies in a variety of disciplines including, for example, genetics, health, ecology, and bee biology. This will serve to introduce us to the well known t procedure for inference of the mean, both the likelihood-based G',\n",
       " '2',\n",
       " ' test and the traditional chi-square test for discrete distributions and contingency tables, and the',\n",
       " 'test for one-way analysis of variance. We add short descriptions for the corresponding non-parametric procedures, namely, permutation, ranked-sum and signed-rank tests for quantitative data, and exact tests for categorical data',\n",
       " 'Exercises and Problems',\n",
       " 'One obligatory statement in the preface of a book such as this is to note the necessity of working problems. The mate-rial can only be mastered by grappling with the issues through the application to engaging and substantive questions. In this book, we address this imperative through exercises and through problems. The exercises, integrated into the textbook narrative, are of two basic types. The first is largely mathematical or computational exercises that are meant to provide or extend the derivation of a useful identity or data analysis technique. These experiences will prepare the student to perform the calculations that routinely occur in investigations that use statistical thinking. The second type form a collection of questions that are meant to affirm the understanding of a particular concept.',\n",
       " 'Problems are collected at the end of each of the four parts of the book. While the ordering of the problems generally follows the flow of the text, they are designed to be more extensive and integrative. These problems often incorporate several concepts and will call on a variety of problem solving strategies combining handwritten work with the use of statistical software. Without question, the best problems are those that the students chose from their own interests.',\n",
       " 'Acknowledgements',\n",
       " 'The concept that let to this book grew out of a conversation with the late Michael Wells, Professor of Biochemistry at the University of Arizona. He felt that if we are asking future life scientist researchers to take the time to learn calculus and differential equations, we should also provide a statistics course that adds value to their abilities to design experiments and analyze data while reinforcing both the practical and conceptual sides of calculus. As a consequence, course development received initial funding from a Howard Hughes Medical Institute grant (52005889). Christopher Bergevin, an HHMI postdoctoral fellow, provided a valuable initial collaboration.',\n",
       " 'Since that time, I have had the great fortune to be the teacher of many bright and dedicated students whose future contribution to our general well-being is beyond dispute. Their cheerfulness and inquisitiveness has been a source of inspiration for me. More practically, their questions and their persistence led to a much clearer exposition and the addition of many dozens of figures to the text. Through their end of semester projects, I have been introduced to many interesting questions that are intriguing in their own right, but also have added to the range of applications presented throughout the text. Four of these students - Beryl Jones, Clayton Mosher, Laurel Watkins de Jong, and Taylor Corcoran - have gone on to become assistants in the course. I am particularly thankful to these four for their contributions to the dynamical atmosphere that characterizes the class experience.',\n",
       " 'x',\n",
       " 'Part I',\n",
       " 'Organizing and Producing Data',\n",
       " '1',\n",
       " 'Topic 1',\n",
       " 'Displaying Data',\n",
       " 'There are two goals when presenting data: convey your story and establish credibility. - Edward Tufte',\n",
       " 'Statistics is a mathematical science that is concerned with the collection, analysis, interpretation or explanation, and presentation of data. Properly used statistical principles are essential in guiding any inquiry informed by data and, especially in the phase of data exploration, is routinely a fundamental source for discovery and innovation. Insights from data may come from a well conceived visualization of the data, from modern methods of statistical learning and model selection as well as from time-honored formal statistical procedures.',\n",
       " 'The first encounters one has to data are through graphical displays and numerical summaries. The goal is to find an elegant method for this presentation that is at the same time both objective and informative - making clear with a few lines or a few numbers the salient features of the data. In this sense, data presentation is at the same time an art, a science, and an obligation to impartiality.',\n",
       " 'In the section, we will describe some of the standard presentations of data and at the same time, taking the opportu-nity to introduce some of the commands that the software package R provides to draw figures and compute summaries of the data.',\n",
       " '1.1',\n",
       " '\\t',\n",
       " 'Types of Data',\n",
       " 'A data set provides information about a group of individuals. These individuals are, typically, representatives chosen from a population under study. Data on the individuals are meant, either informally or formally, to allow us to make inferences about the population. We shall later discuss how to define a population, how to choose individuals in the population and how to collect data on these individuals.',\n",
       " 'Individuals are the objects described by the data.',\n",
       " 'Variables are characteristics of an individual. In order to present data, we must first recognize the types of data under consideration.',\n",
       " '– Categorical variables partition the individuals into classes. Other names for categorical variables are levels or factors. One special type of categorical variables are ordered categorical variables that suggest a ranking, say small. medium, large or mild, moderate, severe.',\n",
       " '– Quantitative variables are those for which arithmetic operations like addition and differences make sense.',\n",
       " 'Example 1.1 (individuals and variables). We consider two populations - the first is the nations of the world and the second is the people who live in those countries. Below is a collection of variables that might be used to study these populations.',\n",
       " '3',\n",
       " 'Exercise 1.2. Classify the variables as quantitative or categorical in the example above.',\n",
       " 'The naming of variables and their classification as categorical or quantitative may seem like a simple, even trite, exercise. However, the first steps in designing an experiment and deciding on which individuals to include and which information to collect are vital to the success of the experiment. For example, if your goal is to measure the time for an animal (insect, bird, mammal) to complete some task under different (genetic, environmental, learning) conditions, then, you may decide to have a single quantitative variable - the time to complete the task. However, an animal in your study may not attempt the task, may not complete the task, or may perform the task. As a consequence, your data analysis will run into difficulties if you do not add a categorical variable to include these possible outcomes of an experiment.',\n",
       " 'Exercise 1.3. Give examples of variables for the population of vertebrates, of proteins.',\n",
       " '1.2',\n",
       " '\\t',\n",
       " 'Categorical Data',\n",
       " '1.2.1',\n",
       " '\\t',\n",
       " 'Pie Chart',\n",
       " 'A pie chart is a circular chart divided into sectors, illustrating relative magnitudes in frequencies or percents. In a pie chart, the area is proportional to the quantity it represents.',\n",
       " 'Example 1.4. As the nation debates strategies for delivering health insurance, let’s look at the sources of funds and the types of expenditures.',\n",
       " '',\n",
       " 'Figure 1.1: 2008 United States health care (a) expenditures (b) income sources, Source: Centers for Medicare and Medicaid Services, Office of the Actuary, National Health Statistics Group',\n",
       " '4',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Exercise 1.5. How do you anticipate that this pie chart will evolve over the next decade? Which pie slices are likely to become larger? smaller? On what do you base your predictions?',\n",
       " 'Example 1.6. From UNICEF, we read “The proportion of children who reach their fifth birthday is one of the most fundamental indicators of a country’s concern for its people. Child survival statistics are a poignant indicator of the priority given to the services that help a child to flourish: adequate supplies of nutritious food, the availability of high-quality health care and easy access to safe water and sanitation facilities, as well as the family’s overall economic condition and the health and status of women in the community. ”',\n",
       " '',\n",
       " 'Example 1.7. Gene Ontology (GO) project is a bioinformatics initiative whose goal is to provide unified terminology',\n",
       " 'of genes and their products. The project began in 1998 as a collaboration between three model organism databases, Drosophila, yeast, and mouse. The GO Consortium presently includes many databases, spanning repositories for',\n",
       " 'plant, animal and microbial genomes. This project is supported by National Human Genome Research Institute. See',\n",
       " 'http://www.geneontology.org/',\n",
       " '',\n",
       " 'Figure 1.2: ',\n",
       " 'The 25 most frequent Biological Process Gene Ontology (GO) terms.',\n",
       " '5',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'To make a simple pie chart in R for the proportion of AIDS cases among US males by transmission category.',\n",
       " 'males<- c(58,18,16,7,1)',\n",
       " 'pie(males)',\n",
       " 'This many be sufficient for your own personal use. However, if we want to use a pie chart in a presentation, we will have to provide some essential details. For a more descriptive pie chart, one has to become accustomed to learning to interact with the software to settle on a graph that is satisfactory to the situation.',\n",
       " 'Define some colors ideal for black and white print.',\n",
       " 'colors <- c(\"white\",\"grey70\",\"grey90\",\"grey50\",\"black\")',\n",
       " 'Calculate the percentage for each category.',\n",
       " 'male_labels <- round(males/sum(males)',\n",
       " '*',\n",
       " '100, 1)',\n",
       " 'The number 1 indicates rounded to one decimal place.',\n",
       " '> male_labels <- paste(male_labels, \"\\\\%\", sep=\" \")',\n",
       " 'This adds a space and a percent sign.',\n",
       " 'Create a pie chart with defined heading and custom colors and labels and create a legend.',\n",
       " 'pie(males, main=\"Proportion of AIDS Cases among Males by Transmission Category + Diagnosed - USA, 2005\", col=colors, labels=male_labels, cex=0.8)',\n",
       " 'legend(\"topright\", c(\"Male-male contact\",\"Injection drug use (IDU)\",',\n",
       " '\"High-risk heterosexual contact\",\"Male-male contact and IDU\",\"Other\"),',\n",
       " 'cex=0.8,fill=colors)',\n",
       " 'The entry cex=0.8 indicates that the legend has a type set that is 80% of the font size of the main title.',\n",
       " 'Proportion of AIDS Cases among Males by Transmission Category Diagnosed − USA, 2005',\n",
       " '',\n",
       " '',\n",
       " ' Male−male contact',\n",
       " '',\n",
       " ' Injection drug use (IDU)',\n",
       " '58 %                             ',\n",
       " '',\n",
       " ' High−risk heterosexual contact',\n",
       " '',\n",
       " '',\n",
       " ' Male−male contact and IDU',\n",
       " '',\n",
       " ' Other',\n",
       " '',\n",
       " ' 1 %',\n",
       " '',\n",
       " ' 7 %',\n",
       " '18%',\n",
       " '\\t',\n",
       " '16%',\n",
       " '6',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " '1.2.2',\n",
       " '\\t',\n",
       " 'Bar Charts',\n",
       " 'Because the human eye is good at judging linear measures and poor at judging relative areas, a bar chart or bar graph is often preferable to pie charts as a way to display categorical data.',\n",
       " 'To make a simple bar graph in R,',\n",
       " '> barplot(males)',\n",
       " 'For a more descriptive bar chart with information on females:',\n",
       " 'Enter the data for females and create a 5  2 array.',\n",
       " 'females <- c(0,71,27,0,2)',\n",
       " 'hiv<-array(c(males,females), dim=c(5,2))',\n",
       " 'Generate side-by-side bar graphs and create a legend,',\n",
       " 'barplot(hiv, main=\"Proportion of AIDS Cases by Sex and Transmission Category + Diagnosed - USA, 2005\", ylab= \"percent\", beside=TRUE,',\n",
       " '+ names.arg = c(\"Males\", \"Females\"),col=colors)',\n",
       " 'legend(\"topright\", c(\"Male-male contact\",\"Injection drug use (IDU)\",',\n",
       " '\"High-risk heterosexual contact\",\"Male-male contact and IDU\",\"Other\"),',\n",
       " 'cex=0.8,fill=colors)',\n",
       " '',\n",
       " 'Example 1.8. Next we examine a segmented bar plot. This shows the ancestral sources of genes for 75 populations throughout Asia. the data are based on information gathered from 50,000 genetic markers. The designations for the groups were decided by the software package STRUCTURE.',\n",
       " '1.3',\n",
       " '\\t',\n",
       " 'Two-way Tables',\n",
       " 'Relationships between two categorical variables can be shown through a two-way table (also known as a contingency table , cross tabulation table or a cross classifying table ).',\n",
       " '7',\n",
       " '',\n",
       " ' REPORTS',\n",
       " '',\n",
       " '\\n',\n",
       " '',\n",
       " 'Fig. 1. Maximum-likelihood tree of 75 populations. A hypothetical most-recent common ancestor (MRCA) composed of ancestral alleles as inferred from the genotypes of one gorilla and 21 chimpanzees was used to root the tree. Branches with bootstrap values less than 50% were condensed. Population identification numbers (IDs), sample collection locations with latitudes and longitudes, ethnicities, language spoken, and size of pop-ulation samples are shown in the table adjacent to each branch in the tree. Linguistic groups are indicated with colors as shown in the legend. All',\n",
       " '\\n',\n",
       " 'population IDs except the four HapMap samples are denoted by four characters. The first two letters indicate the country where the samples were collected or (in the case of Affymetrix) genotyped, according to the following convention: AX, Affymetrix; CN, China; ID, Indonesia; IN, India; JP, Japan; KR, Korea; MY, Malaysia; PI, the Philippines; SG, Singapore; TH, Thailand; and TW, Taiwan. The last two letters are unique IDs for the population. To the right of the table, an averaged graph of results from STRUCTURE is shown for K = 14.',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " '',\n",
       " '\\n',\n",
       " 'does not smoke',\n",
       " 'smokes',\n",
       " '',\n",
       " '2 parents',\n",
       " '\\t',\n",
       " '1 parent',\n",
       " '\\t',\n",
       " '0 parents',\n",
       " 'Example 1.9. In 1964, Surgeon General Dr. Luther Leonidas Terry published a landmark report saying that smoking may be hazardous to health. This led to many influential reports on the topic, including the study of the smoking habits of 5375 high school children in Tucson in 1967. Here is a two-way table summarizing some of the results.',\n",
       " 'The row variable is the parents smoking habits.',\n",
       " 'The column variable is the student smoking habits.',\n",
       " 'The cells display the counts for each of the categories of row and column variables.',\n",
       " 'A two-way table with r rows and c columns is often called an r by c table (written r',\n",
       " '\\t',\n",
       " 'c).',\n",
       " 'The totals along each of the rows and columns give the marginal distributions. We can create a segmented bar graph as follows:',\n",
       " 'smoking<-matrix(c(400,1380,416,1823,188,1168),ncol=3)',\n",
       " 'colnames(smoking)<-c(\"2 parents\",\"1 parent\", \"0 parents\")',\n",
       " 'rownames(smoking)<-c(\"smokes\",\"does not smoke\")',\n",
       " 'smoking',\n",
       " '> barplot(smoking,legend=rownames(smoking))',\n",
       " '9',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Example 1.10. Hemoglobin E is a variant of hemoglobin with a mutation in the globin gene causing substitution of glutamic acid for lysine at position 26 of the globin chain. HbE (E is the one letter abbreviation for glutamic acid.) is the second most common abnormal hemoglobin after sickle cell hemoglobin (HbS). HbE is common from India to Southeast Asia. The chain of HbE is synthesized at a reduced rate compare to normal hemoglobin (HbA) as the HbE produces an alternate splicing site within an exon.',\n",
       " 'It has been suggested that Hemoglobin E provides some protection against malaria virulence when heterozygous,',\n",
       " 'but is causes anemia when homozygous. The circumstance in which the heterozygotes for the alleles under considera-tion have a higher adaptive value than the homozygote is called balancing selection.',\n",
       " 'The table below gives the counts of differing hemoglobin genotypes on two Indonesian islands.',\n",
       " 'Because the heterozygotes are rare on Flores, it appears malaria is less prevalent there since the heterozygote does not provide an adaptive advantage.',\n",
       " 'Exercise 1.11. Make a segmented barchart of the data on hemoglobin genotypes. Have each bar display the distribu-tion of genotypes on the two Indonesian islands.',\n",
       " '1.4',\n",
       " '\\t',\n",
       " 'Histograms and the Empirical Cumulative Distribution Function',\n",
       " 'Histograms are a common visual representation of a quantitative variable. Histograms summarize the data using rectangles to display either frequencies or proportions as normalized frequencies. In making a histogram, we',\n",
       " 'Divide the range of data into bins of equal width (usually, but not always). Count the number of observations in each class.',\n",
       " 'Draw the histogram rectangles representing frequencies or percents by area. Interpret the histogram by giving',\n",
       " 'the overall pattern',\n",
       " '– the center',\n",
       " '– the spread',\n",
       " '– the shape (symmetry, skewness, peaks)',\n",
       " 'and deviations from the pattern',\n",
       " '– outliers',\n",
       " '– gaps',\n",
       " 'The direction of the skewness is the direction of the longer of the two tails (left or right) of the distribution.',\n",
       " 'No one choice for the number of bins is considered best. One possible choice for larger data sets is Sturges’ formula to choose b1 + log',\n",
       " '2',\n",
       " ' nc bins. (b c, the floor function, is obtained by rounding down to the next integer.)',\n",
       " 'Exercise 1.12. The histograms in Figure 1.4 shows the distribution of lengths of a normal strain and mutant strain of Bacillus subtilis. Describe the distributions.',\n",
       " 'Example 1.13. Taking the age of the presidents of the United States at the time of their inauguration and creating its histogram, empirical cumulative distribution function and boxplot in R is accomplished as follows.',\n",
       " '10',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Figure 1.4: Histogram of lengths of Bacillus subtilis. Solid lines indicate wild type and dashed line mutant strain.',\n",
       " 'age<- c(57,61,57,57,58,57,61,54,68,51,49,64,50,48,65,52,56,46,54,49,51,47,55,55, 54,42,51,56,55,51,54,51,60,61,43,55,56,61,52,69,64,46,54,47,70)',\n",
       " 'par(mfrow=c(1,2))',\n",
       " 'hist(age)',\n",
       " 'plot(ecdf(age),xlab=\"age\",main=\"Age of Presidents at the Time of Inauguaration\", sub=\"Empriical Cumulative Distribution Function\")',\n",
       " 'Histogram of age',\n",
       " '\\t',\n",
       " 'Age of Presidents at Inauguaration',\n",
       " '',\n",
       " '\\n',\n",
       " '40',\n",
       " '\\t',\n",
       " '45',\n",
       " '\\t',\n",
       " '50',\n",
       " '\\t',\n",
       " '55',\n",
       " '\\t',\n",
       " '60',\n",
       " '\\t',\n",
       " '65',\n",
       " '\\t',\n",
       " '70',\n",
       " '',\n",
       " 'age',\n",
       " 'Empriical Cumulative Distribution Function',\n",
       " 'So the age of presidents at the time of inauguration range from the early forties to the late sixties with the frequency starting their tenure peaking in the early fifties. The histogram in generally symmetric about 55 years with spread from around 40 to 70 years.',\n",
       " 'The empirical cumulative distribution function F',\n",
       " 'n',\n",
       " '(x) gives, for each value x, the fraction of the data less than or equal to x. If the number of observations is n, then',\n",
       " 'F',\n",
       " 'n',\n",
       " '(x) = ',\n",
       " 'n',\n",
       " '1',\n",
       " ' #(observations less than or equal to x):',\n",
       " '',\n",
       " '11',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Thus, F',\n",
       " 'n',\n",
       " '(x) = 0 for any value of x less than all of the observed values and F',\n",
       " 'n',\n",
       " '(x) = 1 for any x greater than all of the observed values. In between, we will see jumps that are multiples of the 1=n. For example, in the empirical cumulative distribution function for the age of the presidents, we will see a jump of size 4=45 at x = 57 to indicate the fact that 4 of the 44 presidents were 57 at the time of their inauguration.',\n",
       " 'For an alternative method to create a graph of the empirical cumulative distribution function, first place the observations in order from smallest to largest. For the age of presidents data, we can accomplish this in R by writing sort(age). Next match these up with the integral multiples of the 1 over the number of observations. In R, we enter 1:length(age)/length(age). Finally, type=\"s\" to give us the steps described above.',\n",
       " 'plot(sort(age),1:length(age)/length(age),type=\"s\",ylim=c(0,1), main = c(\"Age of Presidents at the Time of Inauguration\"), sub=(\"Empiricial Cumulative Distribution Function\"), xlab=c(\"age\"),ylab=c(\"cumulative fraction\"))',\n",
       " 'Exercise 1.14. Give the fraction of presidents whose age at inauguration was under 60. What is the range for the age at inauguration of the youngest fifth of the presidents?',\n",
       " 'Exercise 1.15. The histogram for data on the length of three bacterial strains is shown below. Lengths are given in microns. Below the histograms (but not necessarily directly below) are empirical cumulative distribution functions corresponding to these three histograms.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Match the histograms to their respective empirical cumulative distribution functions.',\n",
       " 'In looking at life span data, the natural question is “What fraction of the individuals have survived a given length of time?” The survival function S',\n",
       " 'n',\n",
       " '(x) gives, for each value x, the fraction of the data greater than or equal to x. If the number of observations is n, then',\n",
       " '12',\n",
       " '',\n",
       " '20',\n",
       " '\\t',\n",
       " '25',\n",
       " '\\t',\n",
       " '30',\n",
       " '\\t',\n",
       " '35',\n",
       " '\\t',\n",
       " '40',\n",
       " 'average age of the parent',\n",
       " '1.5',\n",
       " '\\t',\n",
       " 'Scatterplots',\n",
       " 'We now consider two dimensional data. The values of the first variable x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ' are assumed known and in an experiment and are often set by the experimenter. This variable is called the explanatory, predictor, discriptor, or input variables and in a two dimensional scatterplot of the data display its values on the horizontal axis. The values y',\n",
       " '1',\n",
       " '; y',\n",
       " '2',\n",
       " ' : : : ; y',\n",
       " 'n',\n",
       " ', taken from observations with input x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ' are called the response or target variable and its values are displayed on the vertical axis. In describing a scatterplot, take into consideration',\n",
       " 'the form, for example,',\n",
       " '– linear',\n",
       " '– curved relationships',\n",
       " '– clusters',\n",
       " 'the direction,',\n",
       " '– a positive or negative association',\n",
       " 'and the strength of the aspects of the scatterplot.',\n",
       " 'Example 1.16. Genetic evolution is based on mutation. Consequently, one fundamental question in evolutionary biology is the rate of de novo mutations. To investigate this question in humans, Kong et al, sequenced the entire genomes of 78 Icelandic trios and recorded the age of the parents and the number of de novo mutations in the offspring.',\n",
       " 'The plot shows a moderate positive linear association, children of older parent have, on average, more mutations. The number of mutations range from 40 for children of younger parents to 100 for children of older parents. We will later learn that the father is the major source of this difference with age.',\n",
       " 'Example 1.17 (Fossils of the Archeopteryx). The name Archeopteryx derives from the ancient Greek meaning “ancient feather” or “ancient wing”. Archeopteryx is generally accepted by palaeontologists as being the oldest known bird. Archaeopteryx lived in the Late Jurassic Period around 150 million years ago, in what is now southern Germany',\n",
       " 'during a time when Europe was an archipelago of islands in a shallow warm tropical sea. The first complete specimen of Archaeopteryx was announced in 1861, only two years after Charles Darwin published On the Origin of Species,',\n",
       " '13',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'and thus became a key piece of evidence in the debate over evolution. Below are the lengths in centimeters of the femur and humerus for the five specimens of Archeopteryx that have preserved both bones.',\n",
       " 'femur<-c(38,56,59,64,74)',\n",
       " 'humerus<-c(41,63,70,72,84)',\n",
       " 'plot(femur, humerus,main=c(\"Bone Lengths for Archeopteryx\"))',\n",
       " 'Unless we have a specific scientific question, we have no real reason for a choice of the explanatory variable.',\n",
       " 'Bone Lengths for Archeopteryx',\n",
       " '',\n",
       " '\\n',\n",
       " '●',\n",
       " '●',\n",
       " '●',\n",
       " '●',\n",
       " '●',\n",
       " '40',\n",
       " '\\t',\n",
       " '45',\n",
       " '\\t',\n",
       " '50',\n",
       " '\\t',\n",
       " '55',\n",
       " '\\t',\n",
       " '60',\n",
       " '\\t',\n",
       " '65',\n",
       " '\\t',\n",
       " '70',\n",
       " '\\t',\n",
       " '75',\n",
       " 'femur',\n",
       " 'Describe the scatterplot.',\n",
       " 'Example 1.18. This historical data show the 20 largest banks in 1974. Values given in billions of dollars.',\n",
       " '14',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Income vs. Assets (in billions of dollars)',\n",
       " '\\n',\n",
       " 'assets',\n",
       " 'Describe the scatterplot.',\n",
       " 'In 1972, Michele Sindona, a banker with close ties to the Mafia, along with a purportedly bogus Freemasonic lodge, and the Nixon administration purchased controlling interest in Bank 19, Long Island’s Franklin National Bank. As a result of his acquisition of a controlling stake in Franklin, Sindona had a money laundering operation to aid his alleged ties to Vatican Bank and the Sicilian drug cartel. Sindona used the bank’s ability to transfer funds, produce letters of credit, and trade in foreign currencies to begin building a banking empire in the United States. In mid-1974, management revealed huge losses and depositors started taking out large withdrawals, causing the bank to have to borrow over $1 billion from the Federal Reserve Bank. On 8 October 1974, the bank was declared insolvent due to mismanagement and fraud, involving losses in foreign currency speculation and poor loan policies.',\n",
       " 'What would you expect to be a feature on this scatterplot of a failing bank? Does the Franklin Bank have this feature?',\n",
       " '1.6',\n",
       " '\\t',\n",
       " 'Time Plots',\n",
       " 'Some data sets come with an order of events, say ordered by time.',\n",
       " 'Example 1.19. The modern history of petroleum began in the 19th century with the refining of kerosene from crude oil. The world’s first commercial oil wells were drilled in the 1850s in Poland and in Romania.The first oil well in North America was in Oil Springs, Ontario, Canada in 1858. The US petroleum industry began with Edwin Drake’s drilling of a 69-foot deep oil well in 1859 on Oil Creek near Titusville, Pennsylvania for the Seneca Oil Company. The industry grew through the 1800s, driven by the demand for kerosene and oil lamps. The introduction of the internal combustion engine in the early part of the 20th century provided a demand that has largely sustained the industry to this day. Today, about 90% of vehicular fuel needs are met by oil. Petroleum also makes up 40% of total energy consumption in the United States, but is responsible for only 2% of electricity generation. Oil use increased exponentially until the world oil crises of the 1970s.',\n",
       " '15',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Worldwide Oil Production',\n",
       " 'With the data given in two columns oil and year, the time plot plot(year,oil,type=\"b\") is given on the left side of the figure below. This uses type=\"b\" that puts both lines and circles on the plot.',\n",
       " 'World Oil Production',\n",
       " '',\n",
       " '\\n',\n",
       " '',\n",
       " 'year',\n",
       " '\\t',\n",
       " 'year',\n",
       " 'Figure 1.5: Oil production (left) and the logarithm of oil production (right) from 1880 to 1988.',\n",
       " 'Sometimes a transformation of the data can reveal the structure of the time series. For example, if we wish to examine an exponential increase displayed in the oil production plot, then we can take the base 10 logarithm of the production and give its time series plot. This is shown in the plot on the right above. (In R, we write log(x) for the natural logarithm and log(x,10) for the base 10 logarithm.)',\n",
       " 'Exercise 1.20. What happened in the mid 1970s that resulted in the long term departure from exponential growth in the use of oil?',\n",
       " '16',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Example 1.21. The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body tasked with evaluating the risk of climate change caused by human activity. The panel was established in 1988 by the World Meteorological Organization and the United Nations Environment Programme, two organizations of the United Nations. The IPCC does not perform original research but rather uses three working groups who synthesize research and prepare a report. In addition, the IPCC prepares a summary report. The Fourth Assessment Report (AR4) was completed in early 2007. The fifth was released in 2014.',\n",
       " 'Below is the first graph from the 2007 Climate Change Synthesis Report: Summary for Policymakers.',\n",
       " 'The technique used to draw the curves on the graphs is called local regression. At the risk of discussing concepts that have not yet been introduced, let’s describe the technique behind local regression. Typically, at each point in the data set, the goal is to draw a linear or quadratic function. The function is determined using weighted least squares, giving most weight to nearby points and less weight to points further away. The graphs above show the approximating curves. The blue regions show areas within two standard deviations of the estimate (called a confidence interval). The goal of local regression is to provide a smooth approximation to the data and a sense of the uncertainty of the data. In practice, local regression requires a large data set to work well.',\n",
       " '',\n",
       " '17',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Example 1.22. The next figure give a time series plot of a single molecule experiment showing the movement of kinesin along a microtubule. In this case the kinesin has at its foot a glass bead and its heads are attached to a microtubule. The position of the glass bead is determined by using a laser beam and the optical properties of the bead to locate the bead and provide a force on the kinesin molecule. In this time plot, the load on the microtubule has a force of 3.5 pN and the concentration of ATP is 100 M. What is the source of fluctuations in this time series plot of bead position? How would you expect this time plot to change with changes in ATP concentration and with changes in force?',\n",
       " '',\n",
       " '1.7',\n",
       " '\\t',\n",
       " 'Answers to Selected Exercises',\n",
       " '1.11. Here are the R commands:',\n",
       " 'genotypes<-matrix(c(128,6,0,119,78,4),ncol=2)',\n",
       " 'colnames(genotypes)<-c(\"Flores\",\"Sumba\")',\n",
       " 'rownames(genotypes)<-c(\"AA\",\"AE\",\"EE\")',\n",
       " 'genotypes',\n",
       " 'Flores Sumba',\n",
       " 'AA',\n",
       " '\\t',\n",
       " '128',\n",
       " '\\t',\n",
       " '119',\n",
       " 'AE',\n",
       " '\\t',\n",
       " '6',\n",
       " '\\t',\n",
       " '78',\n",
       " 'EE',\n",
       " '\\t',\n",
       " '0',\n",
       " '\\t',\n",
       " '4',\n",
       " 'barplot(genotypes,legend=rownames(genotypes),args.legend=list(x=\"topleft\")) The legend was moved to the left side to avoid crowding with the taller bar for the data on Sumba.',\n",
       " '18',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " '\\n',\n",
       " 'EE',\n",
       " '',\n",
       " 'AE',\n",
       " 'AA',\n",
       " 'Flores',\n",
       " '\\t',\n",
       " 'Sumba',\n",
       " '1.12. The lengths of the normal strain has its center at 2.5 microns and range from 1.5 to 5 microns. It is somewhat skewed right with no outliers. The mutant strain has its center at 5 or 6 microns. Its range is from 2 to 14 microns and it is slightly skewed right. It has not outliers.',\n",
       " '1.14. Look at the graph to the point above the value 60 years. Look left from this point to note that it corresponds to a value of 0.80.',\n",
       " 'Look at the graph to the point right from the value 0.20. Look down to note that it corresponds to 49 years. .',\n",
       " '1.15. Match histogram wild1f to wilddaf. Note that both show the range is from 2 to 5 microns and that about half of the data lies between 2 and 3 microns. Match histogram wild2f with wildcf. The data is relatively uniform from 3.5 to 6.5 microns. Finally, match histogram wild3f with wildbf. The range is from 2 to 8 microns with most of the data between 3 and 6 microns. .',\n",
       " '1.22. The fluctuation are due to the many bombardments with other molecules in the cell, most frequently, water molecules.',\n",
       " 'As force increases, we expect the velocity to increase - to a point. If the force is too large, then the kinesin is ripped away from the microtubule. As ATP concentration increases, we expect the velocity to increase - again, to a point. If ATP concentration is sufficiently large, then the biochemical processes are saturated.',\n",
       " '19',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " '20',\n",
       " 'Topic 2',\n",
       " 'Describing Distributions with Numbers',\n",
       " 'There are three kinds of lies: lies, damned lies, and statistics. - Benjamin Disraeli',\n",
       " 'It is easy to lie with statistics. It is hard to tell the truth without it. - Andrejs Dunkels',\n",
       " 'We next look at quantitative data. Recall that in this case, these data can be subject to the operations of arithmetic.',\n",
       " 'In particular, we can add or subtract observation values, we can sort them and rank them from lowest to highest.',\n",
       " 'We will look at two fundamental properties of these observations. The first is a measure of the center value for the data, i.e., the median or the mean. Associated to this measure, we add a second value that describes how these observations are spread or dispersed about this given measure of center.',\n",
       " 'The median is the central observation of the data after it is sorted from the lowest to highest observations. In addition, to give a sense of the spread in the data, we often give the smallest and largest observations as well as the observed value that is 1/4 and 3/4 of the way up this list, known at the first and third quartiles. This difference, known as the interquartile range is a measure of the spread or the dispersion of the data. For the mean, we commonly use the standard deviation to describe the spread of the data.',\n",
       " 'These concepts are described in more detail in this section.',\n",
       " '2.1',\n",
       " '\\t',\n",
       " 'Measuring Center',\n",
       " '2.1.1',\n",
       " '\\t',\n",
       " 'Medians',\n",
       " 'The median take the middle value for x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ' after the data has been sorted from smallest to largest,',\n",
       " 'x',\n",
       " '(1)',\n",
       " '; x',\n",
       " '(2)',\n",
       " '; : : : ; x',\n",
       " '(n)',\n",
       " ':',\n",
       " '(x',\n",
       " '(k)',\n",
       " ' is called the k-th order statistic. Sorting can be accomplished in R by using the sort command.)',\n",
       " 'If n is odd, then this is just the value of the middle observation x',\n",
       " '((n+1)=2)',\n",
       " '. If n is even, then the two values closest to the center are averaged.',\n",
       " '1',\n",
       " '2',\n",
       " ' ',\n",
       " '(x',\n",
       " '(n=2) ',\n",
       " '+',\n",
       " ' ',\n",
       " 'x',\n",
       " '(n=2+1)',\n",
       " '):',\n",
       " '',\n",
       " 'If we store the data in R in a vector x, we can write median(x) to compute the median.',\n",
       " '2.1.2',\n",
       " '\\t',\n",
       " 'Means',\n",
       " 'For a collection of numeric data, x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ', the sample mean is the numerical average',\n",
       " '21',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'Alternatively, if the value x occurs n(x) times in the data, then use the distributive property to see that',\n",
       " 'So the mean x depends only on the proportion of observations p(x) for each value of x.',\n",
       " 'Example 2.1. For the data set f1; 2; 2; 2; 3; 3; 4; 4; 4; 5g; we have n = 10 and the sum',\n",
       " '1 + 2 + 2 + 2 + 3 + 3 + 4 + 4 + 4 + 5 = 1n(1) + 2n(2) + 3n(3) + 4n(4) + 5n(5)',\n",
       " '= 1(1) + 2(3) + 3(2) + 4(3) + 5(1) = 30',\n",
       " 'Thus, x = 30=10 = 3.',\n",
       " 'Example 2.2. For the data on the length in microns of wild type Bacillus subtilis data, we have',\n",
       " 'So the sample mean x = 2:49.',\n",
       " 'If we store the data in R in a vector x, we can write mean(x) which is equal to sum(x)/length(x) to compute the mean.',\n",
       " 'To extend this idea a bit, we can take a real-valued function h and instead consider the observations h(x',\n",
       " '1',\n",
       " '); h(x',\n",
       " '2',\n",
       " '); : : : ; h(x',\n",
       " 'n',\n",
       " '), then',\n",
       " 'Exercise 2.3. Let x',\n",
       " 'n',\n",
       " ' be the sample mean for the quantitative data x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " '. For an additional observation x',\n",
       " 'n+1',\n",
       " ', use x to give a formula for x',\n",
       " 'n+1',\n",
       " ', the mean of n + 1 observations. Generalize this formula for the case of k additional observations x',\n",
       " 'n+1',\n",
       " ' : : : ; x',\n",
       " 'n+k',\n",
       " 'Many times, we do not want to give the same weight to each observation. For example, in computing a student’s grade point average, we begin by setting values x',\n",
       " 'i',\n",
       " ' corresponding to grades ( A 7!4, B 7!3 and so on) and giving weights w',\n",
       " '1',\n",
       " '; w',\n",
       " '2',\n",
       " '; : : : ; w',\n",
       " 'n',\n",
       " ' equal to the number of units in a course. We then compute the grade point average as a weighted mean. To do this:',\n",
       " 'Multiply the value of each course by its weight x',\n",
       " 'i',\n",
       " 'w',\n",
       " 'i',\n",
       " '. This is called the number of quality points for the course. Add up the quality points:',\n",
       " 'X',\n",
       " 'n',\n",
       " 'x',\n",
       " '1',\n",
       " 'w',\n",
       " '1',\n",
       " ' + x',\n",
       " '2',\n",
       " 'w',\n",
       " '2',\n",
       " ' + : : : + x',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " ' =',\n",
       " '\\t',\n",
       " 'x',\n",
       " 'i',\n",
       " 'w',\n",
       " 'i',\n",
       " 'i=1',\n",
       " 'Add up the weights, i. e., the number of units attempted:',\n",
       " 'X',\n",
       " 'n',\n",
       " 'w',\n",
       " '1',\n",
       " ' + w',\n",
       " '2',\n",
       " ' + : : : + w',\n",
       " 'n',\n",
       " ' =',\n",
       " '\\t',\n",
       " 'w',\n",
       " 'i',\n",
       " 'i=1',\n",
       " '22',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'Figure 2.1: Empirical Survival Function for the Bacterial Data. This figure displays how the area under the survival function to the right of the y-axis and above the x-axis is the mean value x for non-negative data. For x = 1:5; 2:0; 2:5; 3:0; 3:5; 4:0, and 4.5. This area is the sum of the area of the retangles displayed. The width of each of the rectangles is x and the height is equal to p(x). Thus, the area is the product xp(x). The sum of these areas are presented in Example 2.2 to compute the sample mean.',\n",
       " 'Divide the total quality points by the number of units attempted:',\n",
       " 'X',\n",
       " 'n',\n",
       " 'p',\n",
       " 'j',\n",
       " ' = w',\n",
       " 'j',\n",
       " '=',\n",
       " '\\t',\n",
       " 'w',\n",
       " 'i',\n",
       " 'i=1',\n",
       " 'be the proportion or fraction of the weight given to the j-th observation, then we can rewrite (2.1) as X',\n",
       " 'n',\n",
       " 'x',\n",
       " 'i',\n",
       " 'p',\n",
       " 'i',\n",
       " ':',\n",
       " 'i=1',\n",
       " 'If we store the weights in a vector w, then we can compute the weighted mean using weighted.mean(x,w)',\n",
       " 'If an extremely high observation is changed to be even higher, then the mean follows this change while the median does not. For this reason, the mean is said to be sensitive to outliers while the median is not. To reduce the impact of extreme outliers on the mean as a measure of center, we can also consider a truncated mean or trimmed mean. The p trimmed mean is obtained by discarding both the lower and the upper p 100% of the data and taking the arithmetic mean of the remaining data.',\n",
       " 'In R, we write mean(x, trim = p) where p, a number between 0 and 0.5, is the fraction of observations to be trimmed from each end before the mean is computed.',\n",
       " 'Note that the median can be regarded as the 50% trimmed mean. The median does not change with a changes in the extreme observations. Such a property is called a resistant measure. On the other hand, the mean is not a resistant measure.',\n",
       " '23',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'Exercise 2.4. Give the relationship between the median and the mean for a (a) left skewed, (b) symmetric, or (c) right skewed distribution.',\n",
       " '2.2',\n",
       " '\\t',\n",
       " 'Measuring Spread',\n",
       " '2.2.1',\n",
       " '\\t',\n",
       " 'Five Number Summary',\n",
       " 'The first and third quartile, Q',\n",
       " '1',\n",
       " ' and Q',\n",
       " '3',\n",
       " ', are, respectively, the median of the lower half and the upper half of the data. The five number summary of the data are the values of the minimum, Q',\n",
       " '1',\n",
       " ', the median, Q',\n",
       " '3',\n",
       " ' and the maximum. These values, along with the mean, are given in R using summary(x). Returning to the data set on the age of presidents:',\n",
       " '> summary(age)',\n",
       " 'Min. 1st Qu. Median Mean 3rd Qu. Max. 42.00 51.00 55.00 54.98 58.00 70.00',\n",
       " 'We can display the five number summary using a boxplot.',\n",
       " '> boxplot(age, main = c(\"Age of Presidents at the Time of Inauguration\"))',\n",
       " 'Age of Presidents at the Time of Inauguration',\n",
       " '',\n",
       " 'The value Q',\n",
       " '3',\n",
       " '\\t',\n",
       " 'Q',\n",
       " '1',\n",
       " ' is called the interquartile range and is denoted by IQR. It is found in R with the command IQR.',\n",
       " 'Outliers are somewhat arbitrarily chosen to be those above Q',\n",
       " '3',\n",
       " ' + ',\n",
       " '3',\n",
       " '2',\n",
       " ' IQR and below Q',\n",
       " '1',\n",
       " ' ',\n",
       " '3',\n",
       " '2',\n",
       " ' IQR. With this criterion, the ages of Ronald Reagan and Donald Trump, considered outliers, are displayed by the two circles at the top of the boxplot. The boxplot command has the default value range = 1.5 in the choice of displaying outliers. This can be altered to loosen or tighten this criterion.',\n",
       " 'Exercise 2.5. Use the range command to create a boxplot for the age of the presidents at the time of their inaugu-ration using as outliers any value above Q',\n",
       " '3',\n",
       " ' + IQR and below Q',\n",
       " '1',\n",
       " ' IQR as the criterion for outliers. How many outliers does this boxplot have?',\n",
       " '24',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'Example 2.6. Consider a two column data set. Column 1 - MPH - gives car gas milage. Column 2 - origin - gives the country of origin for the car. We can create side by side boxplots with the command',\n",
       " '> boxplot(MPG,Origin)',\n",
       " 'to produce',\n",
       " '',\n",
       " '2.2.2',\n",
       " '\\t',\n",
       " 'Sample Variance and Standard Deviation',\n",
       " 'The sample variance averages the square of the differences from the mean',\n",
       " 'The sample standard deviation, s',\n",
       " 'x',\n",
       " ', is the square root of the sample variance. We shall soon learn the rationale for the decision to divide by n 1. However, we shall also encounter circumstances in which division by n is preferable. We will routinely drop the subscript x and write s to denote standard deviation if there is no ambiguity.',\n",
       " 'Example 2.7. For the data set on Bacillus subtilis data, we have x = 498=200 = 2:49',\n",
       " 'So the sample variance s',\n",
       " '2',\n",
       " 'x',\n",
       " ' = 90:48=199 = 0:4546734 and standard deviation s',\n",
       " 'x',\n",
       " ' = 0:6742947. To accomplish this in R',\n",
       " '25',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'bacteria<-c(rep(1.5,18),rep(2.0,71),rep(2.5,48),rep(3,37),rep(3.5,16),rep(4,6), + rep(4.5,4))',\n",
       " 'length(bacteria)',\n",
       " '[1] 200',\n",
       " 'mean(bacteria) [1] 2.49',\n",
       " 'var(bacteria) [1] 0.4546734',\n",
       " 'sd(bacteria) [1] 0.6742947',\n",
       " 'For quantitative variables that take on positive values, we can take the ratio of the standard deviation to the mean cv',\n",
       " 'x',\n",
       " ' = ',\n",
       " 's',\n",
       " 'x',\n",
       " 'x',\n",
       " ' ;',\n",
       " '',\n",
       " 'called the coefficient of variation as a measure of the relative variability of the observations. Note that cv',\n",
       " 'x',\n",
       " ' is a pure number and has no units.',\n",
       " 'For the data of bacteria lengths, the coefficient of variability is',\n",
       " 'introduce the next exercise, define the sum of squares about the value',\n",
       " '\\t',\n",
       " ',',\n",
       " 'X',\n",
       " 'n',\n",
       " 'SS( ) =   (x',\n",
       " 'i',\n",
       " '    )',\n",
       " '2',\n",
       " ':',\n",
       " 'i=1',\n",
       " 'Exercise 2.9. Flip a fair coin 16 times, recording the number of heads. Repeat this activity 20 times, giving x',\n",
       " '1',\n",
       " '; : : : ; x',\n",
       " '20',\n",
       " ' heads. Our instincts say that the mean should be 8. Compute SS(8). Next find x for the data you generated and compute SS(x). Notice that SS(8) > SS(x).',\n",
       " 'Note that in repeating the experiment of flipping a fair coin 16 times and recording the number of heads, we would like to compute the variation about 8, the value that our intuition tells us is the true mean. In many circumstances, we do not have such intuition. Thus, we doing the best we can by computing x, the mean from the data. In this case, the variation about the sample mean is smaller than the variation about what may be called a true mean. Thus, division of ',\n",
       " 'P',\n",
       " 'n',\n",
       " 'i=1',\n",
       " '(x',\n",
       " 'i',\n",
       " ' x)',\n",
       " '2',\n",
       " ' by n systematically underestimates the variance. The definition of sample variance is based on the fact that this can be compensated for this by dividing by something small than n. We will learn why the appropriate choice is n 1 when we investigate Unbiased Estimation in Topic 13.',\n",
       " 'To show that the phenomena in Exercise 2.9 is true more broadly, we next perform a little algebra. This is similar to the computation of the parallel axis theorem in physics. The parallel axis theorem is used to determine the moment of inertia of a rigid body about any axis, given the moment of inertia of the object about the parallel axis through the object’s center of mass (x) and the perpendicular distance between the axes. In this case, we a looking at the rigid motion of a finite number of equal point masses.',\n",
       " 'In the formula for SS( ), divide the difference in the value of each observation x',\n",
       " 'i',\n",
       " ' to the value into the difference to the sample mean x and then the distance from the sample mean to (i.e. x ).',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " 'SS( ) =',\n",
       " '\\t',\n",
       " '((x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x) + (x',\n",
       " '\\t',\n",
       " '))',\n",
       " '2',\n",
       " ' =\\t(x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x)',\n",
       " '2',\n",
       " ' + 2',\n",
       " '\\t',\n",
       " '(x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x)(x',\n",
       " '\\t',\n",
       " ') +\\t(x',\n",
       " '\\t',\n",
       " ')',\n",
       " '2',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " '=\\t(x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x)',\n",
       " '2',\n",
       " ' +',\n",
       " '\\t',\n",
       " '(x',\n",
       " '\\t',\n",
       " ')',\n",
       " '2',\n",
       " ' =\\t(x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x)',\n",
       " '2',\n",
       " ' + n(x',\n",
       " '\\t',\n",
       " ')',\n",
       " '2',\n",
       " ':',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " '26',\n",
       " 'the difference between x and the chosen value',\n",
       " '\\t',\n",
       " '. We shall see this idea of partitioning in other contexts.',\n",
       " 'Note that the minimum value of SS( ) can be obtained by minimizing the second term. This takes place at = x. Thus,',\n",
       " 'Exercise 2.10. The following formulas may be useful in aggregating data. Suppose you have data sets collected on two consecutive days with the following summary statistics.',\n",
       " 'Now combine the observations of the two days and use this to show that the combined mean',\n",
       " 'x',\n",
       " ' ',\n",
       " '=',\n",
       " ' ',\n",
       " 'n',\n",
       " '1',\n",
       " 'x',\n",
       " '1 ',\n",
       " '+',\n",
       " ' ',\n",
       " 'n',\n",
       " '2',\n",
       " 'x',\n",
       " '2',\n",
       " '',\n",
       " 'n',\n",
       " '1',\n",
       " ' + n',\n",
       " '2',\n",
       " 'and the combined variance',\n",
       " '',\n",
       " '(Hint: Use (2.2)).',\n",
       " 'Exercise 2.11. For the data set x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ', let',\n",
       " 'y',\n",
       " 'i',\n",
       " ' = a + bx',\n",
       " 'i',\n",
       " ':',\n",
       " 'Give the summary statistics for the y data set given the corresponding values of the x data set. (Consider carefully the consequences of the fact that a might be less than 0.)',\n",
       " 'Among these, the quadratic identity',\n",
       " 'var(x + bx) = b',\n",
       " '2',\n",
       " 'var(x)',\n",
       " 'is one of the most frequently used and useful in all of statistics.',\n",
       " '2.3',\n",
       " '\\t',\n",
       " 'Quantiles and Standardized Variables',\n",
       " 'A single observation, say 87 on a exam, gives little information about the performance on the exam. One way to include more about this observation would be to give the value of the empirical cumulative distribution function. Thus,',\n",
       " 'F',\n",
       " 'n',\n",
       " '(87) = 0:7223',\n",
       " '27',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'tells us that about 72% of the exam scores were below 87. This is sometimes reported by saying that 87 is the 0.7223 quantile for the exam scores.',\n",
       " 'We can determine this value using the R command quantile. For the ages of presidents at inauguration, we have that the 72% quantile is 57 year old.',\n",
       " 'quantile(age,0.72)',\n",
       " '72%',\n",
       " '57',\n",
       " 'Thus, for example, for the ages of the president, we have that IQR(age) can also be computed using the command quantile(age,3/4) - quantile(age,1/4). R returns the value 7. The quantile command on its own returns the five number summary.',\n",
       " '0%\\t25%\\t50%',\n",
       " '\\t',\n",
       " '75% 100%',\n",
       " '42',\n",
       " '\\t',\n",
       " '51',\n",
       " '\\t',\n",
       " '55',\n",
       " '\\t',\n",
       " '58',\n",
       " '\\t',\n",
       " '70',\n",
       " 'Another, and perhaps more common use of the term quantiles is a general term for partitioning ranked data into equal parts. For example, quartiles partitions the data into 4 equal parts. Percentiles partitions the data into 100 equal parts. Thus, the k-th q-tile is the value in the data for which k=q of the values are below the given value. This naturally leads to some rounding issues which leads to a large variety of small differences in the definition of quantiles.',\n",
       " 'Exercise 2.12. For the example above, describe the quintile, decile, and percentile of the observation 87.',\n",
       " 'A second way to evaluate a score of 87 is to related it to the mean. Thus, if the mean x = 76. Then, we might say that the exam score is 11 points above the mean. If the scores are quite spread out, then 11 points above the mean is just a little above average. If the scores are quite tightly spread, then 11 points is quite a bit above average. Thus, for comparisons, we will sometimes use the standardized version of x',\n",
       " 'i',\n",
       " ',',\n",
       " 'z',\n",
       " 'i',\n",
       " ' ',\n",
       " '=',\n",
       " ' ',\n",
       " 'x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x ',\n",
       " ':',\n",
       " '',\n",
       " 's',\n",
       " 'x',\n",
       " 'The observations z',\n",
       " 'i',\n",
       " ' have mean 0 and standard deviation 1. The value z',\n",
       " 'i',\n",
       " ' is also called the standard score , the z-value, the z-score, and the normal score. An individual z-score, z',\n",
       " 'i',\n",
       " ', gives the number of standard deviations an observation x',\n",
       " 'i',\n",
       " ' is above (or below) the mean.',\n",
       " 'The R command scale transforms the data to the standard score. For the ages of the presidents, we use the scale command to show the standardized ages. The head command show the first 6 rows of the output for presidents from George Washington to John Qunicy Adams.',\n",
       " 'head(data.frame(scale(age),(age-mean(age))/sd(age))) scale.age. X.age...mean.age...sd.age.',\n",
       " 'Exercise 2.13. What are the units of the standard score? What is the relationship of the standard score of an obser-vation x',\n",
       " 'i',\n",
       " ' and y',\n",
       " 'i',\n",
       " ' = ax',\n",
       " 'i',\n",
       " ' + b?',\n",
       " '2.4',\n",
       " '\\t',\n",
       " 'Quantile-Quantile Plots',\n",
       " 'In addition to side by side boxplots or histograms, we can also compare two cumulative distribution function directly with the quantile-quantile or Q-Q plot. If the quantitative data sets x and y have the same number of observations,',\n",
       " '28',\n",
       " '',\n",
       " 'Figure 2.2: age of first seizure (left) side-by-side boxplots, (center) empirical cumulative distribution functions, (right) Q-Q plot with Q',\n",
       " '1',\n",
       " ', the median, and Q',\n",
       " '3',\n",
       " ' indicated by the solid ',\n",
       " 'red',\n",
       " ' dots. The solid line on the plot has intercept 0 and slope 1. (missense age=nonsense age)',\n",
       " 'then this is simply plot(sort(x),sort(y)). In this case the Q-Q plot matches each of the quantiles for the two data sets. If the data sets have an unequal number of observations, then observations from the larger data are reduced by interpolation to create data sets of equal length and the Q-Q plot is plot(sort(xred),sort(yred)) for the reduced data sets xred and yred.',\n",
       " 'Example 2.14. Dravet syndrome, also known as Severe Myoclonic Epilepsy of Infancy (SMEI), is a rare and catas-trophic form of intractable epilepsy that begins in infancy. A recent study looks at de novo mutations in the DNA sequence SCN1A that codes for a sodium channel protein. An improperly functioning sodium channel can have severe consequences for brain function.',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viki['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[285750,\n",
       " 196850,\n",
       " 152400,\n",
       " 152400,\n",
       " 177800,\n",
       " 317500,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 95250,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 317500,\n",
       " 120650,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 184150,\n",
       " 120650,\n",
       " 114300,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 120650,\n",
       " 120650,\n",
       " 152400,\n",
       " 120650,\n",
       " 120650,\n",
       " 152400,\n",
       " 127000,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 114300,\n",
       " 76200,\n",
       " 114300,\n",
       " 114300,\n",
       " 184150,\n",
       " 107950,\n",
       " 107950,\n",
       " 184150,\n",
       " 114300,\n",
       " 114300,\n",
       " 95250,\n",
       " 260350,\n",
       " 304800,\n",
       " 88900,\n",
       " 260350,\n",
       " 317500,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 165100,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 171450,\n",
       " 152400,\n",
       " 127000,\n",
       " 146050,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 101600,\n",
       " 88900,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 114300,\n",
       " 114300,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 101600,\n",
       " 107950,\n",
       " 88900,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 114300,\n",
       " 222250,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 133350,\n",
       " 127000,\n",
       " 6350,\n",
       " 95250,\n",
       " 6350,\n",
       " 76200,\n",
       " 107950,\n",
       " 6350,\n",
       " 69850,\n",
       " 127000,\n",
       " 6350,\n",
       " 88900,\n",
       " 6350,\n",
       " 95250,\n",
       " 6350,\n",
       " 95250,\n",
       " 6350,\n",
       " 95250,\n",
       " 95250,\n",
       " 127000,\n",
       " 158750,\n",
       " 88900,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 152400,\n",
       " 127000,\n",
       " 146050,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 88900,\n",
       " 6350,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 82550,\n",
       " 127000,\n",
       " 88900,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 82550,\n",
       " 82550,\n",
       " 127000,\n",
       " 82550,\n",
       " 127000,\n",
       " 82550,\n",
       " 127000,\n",
       " 82550,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 88900,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 120650,\n",
       " 107950,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 177800,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 120650,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 101600,\n",
       " 101600,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 247650,\n",
       " 247650,\n",
       " 127000,\n",
       " 127000,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 107950,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 120650,\n",
       " 184150,\n",
       " 127000,\n",
       " 171450,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 107950,\n",
       " 127000,\n",
       " 114300,\n",
       " 107950,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 114300,\n",
       " 139700,\n",
       " 127000,\n",
       " 127000,\n",
       " 76200,\n",
       " 76200,\n",
       " 76200,\n",
       " 76200,\n",
       " 76200,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 120650,\n",
       " 120650,\n",
       " 127000,\n",
       " 114300,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 139700,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 120650,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 152400,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 101600,\n",
       " 101600,\n",
       " 127000,\n",
       " 127000,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 107950,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 165100,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 76200,\n",
       " 127000,\n",
       " 76200,\n",
       " 76200,\n",
       " 76200,\n",
       " 127000,\n",
       " 76200,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 107950,\n",
       " 260350,\n",
       " 317500,\n",
       " 127000,\n",
       " 127000,\n",
       " 107950,\n",
       " 114300,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 177800,\n",
       " 152400,\n",
       " 127000,\n",
       " 146050,\n",
       " 120650,\n",
       " 165100,\n",
       " 120650,\n",
       " 165100,\n",
       " 120650,\n",
       " 165100,\n",
       " 120650,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 107950,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 127000,\n",
       " 127000,\n",
       " 152400,\n",
       " 127000,\n",
       " 139700,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 107950,\n",
       " 139700,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 88900,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 107950,\n",
       " 139700,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 88900,\n",
       " 127000,\n",
       " 171450,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 171450,\n",
       " 152400,\n",
       " 127000,\n",
       " 146050,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 101600,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 152400,\n",
       " 127000,\n",
       " 146050,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 247650,\n",
       " 247650,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 114300,\n",
       " 222250,\n",
       " 158750,\n",
       " 158750,\n",
       " 114300,\n",
       " 158750,\n",
       " 114300,\n",
       " 158750,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 165100,\n",
       " 88900,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 88900,\n",
       " 114300,\n",
       " 152400,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 88900,\n",
       " 107950,\n",
       " 127000,\n",
       " 127000,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 114300,\n",
       " 152400,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 107950,\n",
       " 101600,\n",
       " 127000,\n",
       " 88900,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 120650,\n",
       " 114300,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 247650,\n",
       " 171450,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 247650,\n",
       " 88900,\n",
       " 127000,\n",
       " 177800,\n",
       " 171450,\n",
       " 127000,\n",
       " 114300,\n",
       " 152400,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 171450,\n",
       " 127000,\n",
       " 184150,\n",
       " 127000,\n",
       " 177800,\n",
       " 114300,\n",
       " 107950,\n",
       " 127000,\n",
       " 95250,\n",
       " 139700,\n",
       " 95250,\n",
       " 139700,\n",
       " 95250,\n",
       " 95250,\n",
       " 95250,\n",
       " 127000,\n",
       " 127000,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viki['font_sizes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'S',\n",
       " 'tat',\n",
       " 'i',\n",
       " 'stics:',\n",
       " ' ',\n",
       " 'A',\n",
       " ' ',\n",
       " 'Con',\n",
       " 'c',\n",
       " 'ise',\n",
       " ' ',\n",
       " 'Cou',\n",
       " 'r',\n",
       " 'se',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'S',\n",
       " 'ta',\n",
       " 't',\n",
       " 'istical',\n",
       " ' ',\n",
       " 'In',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'ence',\n",
       " 'Brief',\n",
       " ' ',\n",
       " 'Contents',\n",
       " '1.',\n",
       " ' ',\n",
       " 'Int',\n",
       " 'r',\n",
       " 'oduct',\n",
       " 'i',\n",
       " 'on…',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '1',\n",
       " '1',\n",
       " 'Pa',\n",
       " 'r',\n",
       " 't',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'P',\n",
       " 'r',\n",
       " 'obabi',\n",
       " 'l',\n",
       " 'ity',\n",
       " '2.',\n",
       " ' ',\n",
       " 'P',\n",
       " 'r',\n",
       " 'obability…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '..',\n",
       " '.',\n",
       " '21',\n",
       " '3.',\n",
       " ' ',\n",
       " 'Random',\n",
       " ' ',\n",
       " 'V',\n",
       " 'ariabl',\n",
       " 'e',\n",
       " 's…',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '.',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '.',\n",
       " '3',\n",
       " '7',\n",
       " '4.',\n",
       " ' ',\n",
       " 'Expec',\n",
       " 't',\n",
       " 'ation',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…….',\n",
       " '.',\n",
       " '69',\n",
       " '5.',\n",
       " ' ',\n",
       " 'Equalities……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '….',\n",
       " '8',\n",
       " '5',\n",
       " '6.',\n",
       " ' ',\n",
       " 'Con',\n",
       " 'v',\n",
       " 'erge',\n",
       " 'n',\n",
       " 'ce',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'R',\n",
       " 'and',\n",
       " 'o',\n",
       " 'm',\n",
       " ' ',\n",
       " 'V',\n",
       " 'ariabl',\n",
       " 'e',\n",
       " 's……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '…..',\n",
       " '.',\n",
       " '89',\n",
       " 'Part',\n",
       " ' ',\n",
       " 'II',\n",
       " ' ',\n",
       " 'S',\n",
       " 'tatisti',\n",
       " 'c',\n",
       " 'al',\n",
       " ' ',\n",
       " 'Infe',\n",
       " 'r',\n",
       " 'e',\n",
       " 'nce',\n",
       " '7.',\n",
       " ' ',\n",
       " 'Models,',\n",
       " ' ',\n",
       " 'S',\n",
       " 'tatistical',\n",
       " ' ',\n",
       " 'In',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'ence',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'Learning…',\n",
       " '…',\n",
       " '……………',\n",
       " '…',\n",
       " '…105',\n",
       " '8.',\n",
       " ' ',\n",
       " 'Est',\n",
       " 'i',\n",
       " 'mat',\n",
       " 'i',\n",
       " 'ng',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'C',\n",
       " 'D',\n",
       " 'F',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'S',\n",
       " 'tatis',\n",
       " 't',\n",
       " 'ical',\n",
       " ' ',\n",
       " 'F',\n",
       " 'unct',\n",
       " 'i',\n",
       " 'ona',\n",
       " 'l',\n",
       " 's……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '1',\n",
       " '17',\n",
       " '9.',\n",
       " ' ',\n",
       " 'The',\n",
       " ' ',\n",
       " 'B',\n",
       " 'o',\n",
       " 'otstr',\n",
       " 'ap',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '129',\n",
       " '10.',\n",
       " ' ',\n",
       " 'P',\n",
       " 'a',\n",
       " 'ram',\n",
       " 'e',\n",
       " 'tric',\n",
       " ' ',\n",
       " 'I',\n",
       " 'nfe',\n",
       " 'r',\n",
       " 'ence…',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…..',\n",
       " '1',\n",
       " '45',\n",
       " '1',\n",
       " '1.',\n",
       " ' ',\n",
       " 'Hy',\n",
       " 'p',\n",
       " 'ot',\n",
       " 'h',\n",
       " 'esis',\n",
       " ' ',\n",
       " 'T',\n",
       " 'e',\n",
       " 'sting',\n",
       " ' ',\n",
       " 'a',\n",
       " 'nd',\n",
       " ' ',\n",
       " 'p-',\n",
       " 'v',\n",
       " 'alu',\n",
       " 'e',\n",
       " 's……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '179',\n",
       " '12.',\n",
       " ' ',\n",
       " 'B',\n",
       " 'a',\n",
       " 'yes',\n",
       " 'i',\n",
       " 'an',\n",
       " ' ',\n",
       " 'In',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " 'nce……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '….',\n",
       " '.',\n",
       " '205',\n",
       " '13.',\n",
       " ' ',\n",
       " 'S',\n",
       " 'tatistical',\n",
       " ' ',\n",
       " 'Decis',\n",
       " 'i',\n",
       " 'on',\n",
       " ' ',\n",
       " 'The',\n",
       " 'o',\n",
       " 'ry',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '….',\n",
       " '2',\n",
       " '27',\n",
       " 'Part',\n",
       " ' ',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " ' ',\n",
       " 'S',\n",
       " 'tatis',\n",
       " 't',\n",
       " 'ical',\n",
       " ' ',\n",
       " 'M',\n",
       " 'o',\n",
       " 'dels',\n",
       " ' ',\n",
       " 'a',\n",
       " 'nd',\n",
       " ' ',\n",
       " 'Met',\n",
       " 'h',\n",
       " 'ods',\n",
       " '14.',\n",
       " ' ',\n",
       " 'Lin',\n",
       " 'e',\n",
       " 'ar',\n",
       " ' ',\n",
       " 'Reg',\n",
       " 'r',\n",
       " 'ession……',\n",
       " '…',\n",
       " '…………',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '…………',\n",
       " '…',\n",
       " '…..',\n",
       " '.24',\n",
       " '5',\n",
       " '15.',\n",
       " ' ',\n",
       " 'Mult',\n",
       " 'i',\n",
       " 'vari',\n",
       " 'a',\n",
       " 'te',\n",
       " ' ',\n",
       " 'Mode',\n",
       " 'l',\n",
       " 's………',\n",
       " '…',\n",
       " '…………',\n",
       " '…',\n",
       " '…………',\n",
       " '…',\n",
       " '……….',\n",
       " '.',\n",
       " '.',\n",
       " '2',\n",
       " '69',\n",
       " '16.',\n",
       " ' ',\n",
       " 'In',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'ence',\n",
       " ' ',\n",
       " 'abo',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 'Ind',\n",
       " 'e',\n",
       " 'pen',\n",
       " 'd',\n",
       " 'enc',\n",
       " 'e',\n",
       " '………',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '…..',\n",
       " '2',\n",
       " '79',\n",
       " '17.',\n",
       " ' ',\n",
       " 'Un',\n",
       " 'd',\n",
       " 'i',\n",
       " 'r',\n",
       " 'ected',\n",
       " ' ',\n",
       " 'G',\n",
       " 'r',\n",
       " 'aphs',\n",
       " ' ',\n",
       " 'a',\n",
       " 'nd',\n",
       " ' ',\n",
       " 'C',\n",
       " 'o',\n",
       " 'ndi',\n",
       " 't',\n",
       " 'ional',\n",
       " ' ',\n",
       " 'Indep',\n",
       " 'e',\n",
       " 'nde',\n",
       " 'n',\n",
       " 'ce…',\n",
       " '…',\n",
       " '………',\n",
       " '2',\n",
       " '97',\n",
       " '18.',\n",
       " ' ',\n",
       " 'L',\n",
       " 'o',\n",
       " 'gl',\n",
       " 'i',\n",
       " 'near',\n",
       " ' ',\n",
       " 'Model',\n",
       " 's',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '3',\n",
       " '09',\n",
       " '19.',\n",
       " ' ',\n",
       " 'Causal',\n",
       " ' ',\n",
       " 'Inf',\n",
       " 'e',\n",
       " 'r',\n",
       " 'enc',\n",
       " 'e',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '.',\n",
       " '3',\n",
       " '27',\n",
       " '20.',\n",
       " ' ',\n",
       " 'Di',\n",
       " 'r',\n",
       " 'ected',\n",
       " ' ',\n",
       " 'Gr',\n",
       " 'a',\n",
       " 'p',\n",
       " 'h',\n",
       " 's…',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '.',\n",
       " '3',\n",
       " '4',\n",
       " '3',\n",
       " '21.',\n",
       " ' ',\n",
       " 'Nonparametr',\n",
       " 'i',\n",
       " 'c',\n",
       " ' ',\n",
       " 'curve',\n",
       " ' ',\n",
       " 'E',\n",
       " 's',\n",
       " 'tim',\n",
       " 'a',\n",
       " 'tion',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '…………',\n",
       " '…',\n",
       " '…….',\n",
       " '35',\n",
       " '9',\n",
       " '22.',\n",
       " ' ',\n",
       " 'Smo',\n",
       " 'o',\n",
       " 'thi',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'Using',\n",
       " ' ',\n",
       " 'Or',\n",
       " 't',\n",
       " 'hogo',\n",
       " 'n',\n",
       " 'al',\n",
       " ' ',\n",
       " 'Fu',\n",
       " 'n',\n",
       " 'ct',\n",
       " 'i',\n",
       " 'on',\n",
       " 's',\n",
       " '..…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…3',\n",
       " '9',\n",
       " '3',\n",
       " '23.',\n",
       " ' ',\n",
       " 'C',\n",
       " 'l',\n",
       " 'as',\n",
       " 's',\n",
       " 'if',\n",
       " 'i',\n",
       " 'cat',\n",
       " 'i',\n",
       " 'on……………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '….',\n",
       " '.',\n",
       " '425',\n",
       " '24.',\n",
       " ' ',\n",
       " 'S',\n",
       " 'tochastic',\n",
       " ' ',\n",
       " 'P',\n",
       " 'r',\n",
       " 'oc',\n",
       " 'e',\n",
       " 'ss',\n",
       " 'e',\n",
       " 's…',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '…',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '473',\n",
       " '25.',\n",
       " ' ',\n",
       " 'S',\n",
       " 'i',\n",
       " 'mul',\n",
       " 'a',\n",
       " 'tion',\n",
       " ' ',\n",
       " 'M',\n",
       " 'eth',\n",
       " 'o',\n",
       " 'ds',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '…',\n",
       " '……',\n",
       " '…',\n",
       " '………',\n",
       " '5',\n",
       " '05',\n",
       " 'Appen',\n",
       " 'd',\n",
       " 'ix',\n",
       " ' ',\n",
       " 'Fund',\n",
       " 'a',\n",
       " 'ment',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'Conce',\n",
       " 'p',\n",
       " 'ts',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'Infe',\n",
       " 'r',\n",
       " 'e',\n",
       " 'nce',\n",
       " ' ',\n",
       " ' ',\n",
       " '!#%&%)*,.012!456*8:<2.,>?@#%AB%DE!:%5,:!BH.55JK%',\n",
       " '@*H*5@*5LMN!OP*,L#*.PDH:.@:.QLSL!,:LU:Q.QLWQ:XLQY%55BHD.[',\n",
       " '.>%8>%D.,\\\\5]%@.,E_B>H*,L>H*5LL%K@L.H*Q‘5:Q.5.QL).bX[!',\n",
       " 'L<!Q:KcdDfb5L?:%!%\\\\*%.Q?:B%fJ[,.]H6.#*@[,Q:!@:_28!',\n",
       " 'hi*QhH:.L%jPDH:.@:.QL)lm5L5Qn2L@?:!5Lb.5%0!%*%Q::56LP!',\n",
       " 'LP*.>H:.!oiX[%:@:*%,.m%MQY%@:qB]H:.!ji:!.Q:BH&H:r,:B%550@L.Q#H:L',\n",
       " '*0N!.5<sJOQ!:@LQ),m@]%QS.)%:@L>:tE<uL%.Q.,S%v0.q::.r5.[',\n",
       " ']%n%5!L,*,&nU:_[5!bE<b.Q!\\\\%x,:!BH.55Jh%?@*H*5@*5L5:LE5:Qo',\n",
       " '0*_2n.@5*%56N!b%,H%QL8Q:!:%Bi*Lm%8!:%BH:z@:L#:L',\n",
       " 'j<[\\x7f\\x81_\\x7f\\x81#',\n",
       " ' ',\n",
       " ',]4WW\\x87',\n",
       " 'E',\n",
       " '\\n',\n",
       " 'Bi\\x89\\x81\\x87\\x8cN2,H\\x89W\\x89',\n",
       " '%',\n",
       " ' ',\n",
       " 'H:n%.BQ!QL:,Lhbq*',\n",
       " 'L!5.QQ*54H%BH52L54BH*,B!t:!>:.%xPDH:.P*.Qd@L:L%:Dcr%ML%[',\n",
       " '_*L\\x8c5hPDH:.@:.QrQB%@:>Q#*rb5.nBH*t.5dHv>%D.,m5]%@.\\\\:\\x90',\n",
       " ':L%:D\\\\s%\\x89L%_*Lt.\\\\L!,:L\\x91@L.QL)QB%@:>Q#*L&2DH:.P*.QY%,j*%!#',\n",
       " '*BivL%>[*L>:Q.L#:.@:h\\x93L:@L5E%L#*5:KbQL\\x94\\x96r!>,,*Qh:Q.QE:.P*',\n",
       " '*%!#b*BHbPDH:.P*.L%j:L!P\\x8c5o\\x98n%qh*U*,L.,:!5LL',\n",
       " '.%U%:\\x8cDBH!.,2*H*5@:.L.%t<\\x99:QL!%.Qv*BHML!,:LO:Q.Q[',\n",
       " '*5@*r%:mhH25\\\\<!L1L!#::.,,*5!Sb.5mQ!>,,*Qr:Q.QE:.P*r,\\x7ff@LL%!.Q',\n",
       " '*0!QL:%.qJv%@*H*5@:.]H1*,L!P?%8>_*2!5!%%&r._!QmBiDO>5.,O%q',\n",
       " '!!@5*,>zH:U>%:M@]%.%.M:B%A@*H*5@*5L.%6_!Lz:!!AX!:@.5%UB!:>%',\n",
       " '@*H*5@*5]%#:L!Pd5x!:)XL@H%@5%r*BHML!,:L:L5L#*5@:\\x9aB%O@]%5.LQo\\x89\\x9c.',\n",
       " '%!@LU@:L#:\\\\b8L%xb5:*%BH52:56%\\x93BHD\\x8c@!5AXOrQ.\\x9a%:!,L',\n",
       " '.?B%@.z@!B%,..qJvH?hH:L>H:.]Hj@*H*5@:.LQSm@.OW%_v*2!5s.5%zQ[',\n",
       " '!',\n",
       " ' ',\n",
       " ' ',\n",
       " '!',\n",
       " '*H1_*Q12!P*.,M%\\x8c@X!@r!L_*!>%D5L)b5:!,,L@@*%5OB%@.',\n",
       " '@*H*5@:.L5.5%z!5O:%.?:,:!Q@?XQN!@z2,\\x7fb5<',\n",
       " ' ',\n",
       " '*@tOBHB%5o',\n",
       " 'r,>bL@]%uP*Q#*\\x8c.L%:',\n",
       " ' ',\n",
       " 'B%@.4@!B%,..qJc%',\n",
       " ' ',\n",
       " '@Di*.P*5LhE5DE5$',\n",
       " 'm<bQ:H\\\\s6.]H@]X*Bi6sH6d‘L!Q.@.!‘bL',\n",
       " ' ',\n",
       " 'd8L%>[*L\\x9c:L5LQOL!',\n",
       " '.L%!Q\\x91%Q,%:E.,n&',\n",
       " ' ',\n",
       " 'L@)L%01@Ltd6P*,L#*\\x87*!_s![20QP',\n",
       " '@*%58%r2L@PDH:.@:.Qz2,.DE5$*',\n",
       " ' ',\n",
       " 'UJ[,.]H&hH:L>H:.]H\\x9a@*H*5@*5',\n",
       " 'L%:@‘@1Qh:[',\n",
       " ' ',\n",
       " 'MD',\n",
       " ' ',\n",
       " '*5>?!c*Q.!,h%,',\n",
       " ' ',\n",
       " ',.@.@.*%.Q4NL!#:.',\n",
       " 'Q*,[QJr5>Q:5!B%1.#*Q!*%5Q:%',\n",
       " ' ',\n",
       " '>H:t\\x90[1Q:\\\\%\\x9aQ<!L@.[Q:',\n",
       " 'L%LQ,*>WX2%*P**H.,oL,@!UQ@*5hi*.%oj!:%5]%>2Q.\\x9c_*H',\n",
       " ' ',\n",
       " '\\x90U[?m@Q',\n",
       " '!,b:>@LQ:.%‘!,n,L@!*HBH:t%!:nL%:@t!8@!B%5.qJv%,K>H*\\x90',\n",
       " '>H*5]%\\x91PDH:.@:.QLn.m12!8%:%:tN@!',\n",
       " ' ',\n",
       " '*Bi6L%:@%nL:t.\\x9ch:>%@?%',\n",
       " '*,\\\\>%5vN]H::QnH:.X[%X',\n",
       " 'b\\x9c12!U.):qD%,.mN%\\x93%!:rQ:!:%BH:Ls5v>H*o2@*H*5@:.L\\x93%',\n",
       " 'Q!>,,*Q\\x93@L.QL6%\\x93\\x93L5j%\\x93!*HBH:\\x9c@:QE:.>L!,:Lr@L5LQ6%',\n",
       " '%:LEBHE:5*H*q!zBL5L',\n",
       " 'bvQ<!L4%,H%QL',\n",
       " ' ',\n",
       " ':!.Q8:BHK%:',\n",
       " ' ',\n",
       " '*:%q*.%B%.q',\n",
       " ' ',\n",
       " '%8D%%EK.',\n",
       " ' ',\n",
       " 'B@@',\n",
       " 'Q!:@%',\n",
       " ' ',\n",
       " 'B!>\\x90,%>,.%r!BH*%Q::.?@L!@L:@.!j12%*P*:%5r,L[',\n",
       " '@5JvQ@:.>H*5!?%8!*H.L%j2L5L',\n",
       " 'bOB]%4!5::L',\n",
       " ' ',\n",
       " '*!5L>.',\n",
       " ' ',\n",
       " ',:!BH.55J*Hv%v,Y]',\n",
       " ' ',\n",
       " 'LQ#**H\\x9c:%.',\n",
       " '5@*H*5@*5]%).[NL:QLHA',\n",
       " ' ',\n",
       " '%d_,%5%&L%#*.,>_*2MH:h2.P*BH.5',\n",
       " '%,:L#L',\n",
       " 'bJ',\n",
       " ' ',\n",
       " '!L,L*H\\x94)t*P*A]!!5',\n",
       " ' ',\n",
       " 'XL.%X!:54*Q.!,L%.QYi*.%M.cW]!!U%',\n",
       " 'Q>%:5L.,OL!QL,:L',\n",
       " 'b1L<!Qx!,B%*H>_*:5S.,NQ:L,L)1_N!:)B%:%>_*@.&.[NL:QLHx55\\x91*',\n",
       " '!,1!@5:&%,!PPDH:.P*.Q\\x9112!E\\x87,\\x89jXL5._!S5\\x875o*):5!#\\x91s]z*nb5L',\n",
       " '%*%Q::5\\\\2L5mH:d,:L%.5@*5\\\\%KXL%!!!5]%55hBH:*H\\x94\\\\0n<',\n",
       " '\\x93!5>\\x93\\x9cE<',\n",
       " ' ',\n",
       " '*nQ%L@E:.UH1![\\x93:m,.@::5,*5!_[LQ,sN!S!m!',\n",
       " 'J\\x93MB%:%Q*Q:1id&5#*:2QmPDH:.P*.L%BN_*.%B%.)%,v12%:@*:%5',\n",
       " '%L@?L%:q\\x8c%,‘P*Q#*bB?*52,5*0Bi*:%\\x94',\n",
       " 'bt%B%,!c*?@B%3:x5:@MQ:',\n",
       " ' ',\n",
       " '5&:%B%5.5J6H7:[LQ!',\n",
       " ' ',\n",
       " '\\x89L@',\n",
       " '2DH:.P*.Q88%,:#%Dj[!P*,L#*\\\\!qKDH%O*OB@@tB%qrHq',\n",
       " '\\x93!.\\\\XrbL@.)q[:Q0,.\\\\%\\x89:Qs%#0@*H*5@:.]H%*Q!@%\\x9aBP*Q:!:H',\n",
       " '@!B%5.qJ‘56!:OQ#%!5hbQP*,L#*\\\\]%',\n",
       " ' ',\n",
       " '@LU5z,,z*\\x8c\\x93!:45',\n",
       " '*,\\\\Q!#*\\x90[m%@*H*5@*5LQ',\n",
       " 'bQ!:@',\n",
       " ' ',\n",
       " '><!Qv!Q@uE5D2qu%,uL<!Q:8MD',\n",
       " ' ',\n",
       " 'hi*L@Y%\\x818',\n",
       " ' ',\n",
       " 'L!',\n",
       " '.L%!Q@!%U*Bi0\\x9cL<%LMH.\\x9a%\\x93@*H*5@:.Lz5A:.zQ!@:>%ALQ>:',\n",
       " '*q*5%\\x89sL%:@.\\x9aQhH.,6,&\\x87B]%sr!@%Q>BH:M*\\x9ch%Hr*,sh\\x7f',\n",
       " '*Q:.%#%\\x89.#*,5*q!S%\\x89X!:@.5&:m:BH\\x87*,\\x93>H*Q:.%%5\\x87%LPt,L@@*%B%,.',\n",
       " 'Q:q*z*zW%PnB%LHS\\x9c#Es]!X:5<',\n",
       " ' ',\n",
       " 'Q!:@Lm%:0X!:5',\n",
       " 'b\\x9cn5DB%@',\n",
       " ' ',\n",
       " 'BQ2>%',\n",
       " ' ',\n",
       " '1!5#*L',\n",
       " ' ',\n",
       " '!,Ls:5!!U%',\n",
       " ' ',\n",
       " 'LYH:5J%:8%>@2!##',\n",
       " '!L\\x9cn<%M*@.L‘*vP*:5%O>![24BHY%Q%6\\x89\\x8c]!%.!_:*5hX!!%L',\n",
       " '<bM5d5#*L@LP*.,m*QD5]%2Q*%.5L\\x7f>%#t@L@5:x%@rPDH:Ltbq*%,',\n",
       " '@[%',\n",
       " ' ',\n",
       " '‘\\x8c5.5!!*H.O@QNQ:L,LLUid*,vLHb]%DDBH,*QO1!5#',\n",
       " '*,\\\\P*,L#m:%@!:.H*z@!@LLQ',\n",
       " '6ddz\\x93L,:5:%@)B5L\\x87b5:L2Sb.DMP*,L#*\\x9aL%d,:\\x93N%%.m%.',\n",
       " '*,\\x9cL%>[*.,n\\x7f\\x93Q%LQ,*m12!O5),%\\x93:.Q>*',\n",
       " ' ',\n",
       " '%vHEQ!>,,*5',\n",
       " 'YH!B%%6]%8X\\\\,:Lj',\n",
       " 'mB@@\\x93B%Pr%1*n*\\x90[\\x93.SQ!LQ:Qvb5:>@!B%,..qJd:L!P!E*bN!@h%',\n",
       " 'Y%,!B%!zHx,LLPD%5#J\\x8cb.D8.s:0B%:5b%xPDH:.P*.L%o5,NL@LQ%S\\\\B%:5',\n",
       " ':%.Q*Hbr0@:,?58,:!BH.55J>.',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '\"#$&',\n",
       " ' ',\n",
       " '(*+-+023',\n",
       " ' ',\n",
       " '\"45',\n",
       " ' ',\n",
       " '(06(#7+8:5;0>',\n",
       " '-*4+#',\n",
       " '):LQ!\\\\B%P\\x87%[:S12!m.\\x87%X!,\\x87PDH:.@:.L%%5,NL@LQ\\x93Htq*\\x91Q.!@&L!@.Q',\n",
       " 'BH*0.,.zHUh%D,.5]%@.,bBH:.,:!5L',\n",
       " ' ',\n",
       " '%X@Di*.P*5]%[.,NQ:L,L',\n",
       " '.s:0.#!Q::\\\\%:%B%5.5J',\n",
       " ' ',\n",
       " 'A5B0C-*4++023-BD;EGI6*5',\n",
       " ' ',\n",
       " '(*+-5J6',\n",
       " ' ',\n",
       " 'K',\n",
       " 'C-L5A',\n",
       " 'L@O.L%6%@d5..,@*:H*Q‘54x5!@',\n",
       " ' ',\n",
       " '&:Q._*.%o1QY%@:qB]H:.!jXQ.',\n",
       " '*Q:.,M%hL@:.>H*5!>%:m%.',\n",
       " ' ',\n",
       " '@1QLYH1L%:Qs%\\x91PDH:.@:.L%',\n",
       " ' ',\n",
       " '.,NQ:L,L%NziDdHB%q',\n",
       " '2:.Q>%D.,\\x9c.L%:,.d%vBiDd.5t%:\\x9cH%@.!,rB%L\\x93!5%Lv:d*\\x9c*%\\x90',\n",
       " '*5LhH@*H*5@*5]%S5,NL@LQ%&Q1Q.,4!:hQ!#*_2L4v@LL%cB%@t%',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " '!',\n",
       " 'S@!B%,..qJ',\n",
       " '',\n",
       " '',\n",
       " 'zH*O!LQ*H:.U@[QL@',\n",
       " '6:Q@%LKHD',\n",
       " 'J,NL@LQdH',\n",
       " ' ',\n",
       " 'ziD',\n",
       " ' ',\n",
       " '‘5.',\n",
       " 'x5!:',\n",
       " ' ',\n",
       " '&@!B%5.qJv%,85,NL@LQ%',\n",
       " '*,KX[!Q!#D%5!K!@‘DBH,*Qh!,:!BH.55J:BHhL<!Q:vP*2DB%@:.',\n",
       " '@[QL:@Ln.Q.,.',\n",
       " ' ',\n",
       " 'KH:%<?DBH.Q',\n",
       " 'hB]%A,*]b',\n",
       " ' ',\n",
       " 'L][55',\n",
       " ' ',\n",
       " '!',\n",
       " ' ',\n",
       " '%*Q\\x8c12!E?.',\n",
       " ' ',\n",
       " '>%#.%LQL',\n",
       " ' ',\n",
       " '‘!P8DB%,:L:',\n",
       " 'L%E*%.',\n",
       " ' ',\n",
       " '@L_*.%fL%.5L',\n",
       " ' ',\n",
       " 'r5.5.!%*%,.',\n",
       " ' ',\n",
       " 'bL>%:Evb5Df@LP!L8X%*u*cH_',\n",
       " 'E<b.Q!8dcL[>:%:L>%,:!:>%c*1%.#:]HL@:%:LU@QN',\n",
       " ':_NL@LQLLutr!,.',\n",
       " ' ',\n",
       " 'L:XLQY%555.H?:L#:.!*,‘X[%2#',\n",
       " ' ',\n",
       " '6',\n",
       " ' ',\n",
       " ':2%%',\n",
       " '[DQ@2.@',\n",
       " ' ',\n",
       " 'm%,',\n",
       " '\\n',\n",
       " '@.>_:m%AE*.@]%HLU',\n",
       " ' ',\n",
       " 'bN:%',\n",
       " ' ',\n",
       " 'b,.Dn%B%[*L',\n",
       " '6',\n",
       " '>%#v_,%5Ln%8_[LQ:Q.:QL',\n",
       " '',\n",
       " '#7C#7-NL',\n",
       " ' ',\n",
       " '6',\n",
       " ' ',\n",
       " ' ',\n",
       " '+#7*',\n",
       " ' ',\n",
       " '(',\n",
       " '2DH:.P*.QY%,b%KQ!,*Qb:L5L#*5@:m%',\n",
       " ' ',\n",
       " ':L8:t1L@L#nY%,!B%!zN!:',\n",
       " '*%K:.,',\n",
       " ' ',\n",
       " 'mQ:45v5Q*5!B%P*Biv*K:]HLv>]cs%#\\x8c::_*@*',\n",
       " '*@!!,!,b*0Q!:@%',\n",
       " '#7C#7-',\n",
       " ' ',\n",
       " '*IC',\n",
       " ' ',\n",
       " '7-',\n",
       " ' ',\n",
       " '-6$',\n",
       " 'LP*.>H:.!',\n",
       " ' ',\n",
       " '.L%:5',\n",
       " ' ',\n",
       " '@.UBiDM*ULP*5hH:',\n",
       " '%?,2,\\x7fb‘EB%#*qJ',\n",
       " 'L.%:@5BLH*5!',\n",
       " ' ',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vika['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vikush = read_italic_bold_docx('../data/raw/pdf/7kLHJ-F33GI/statistics_firstfive.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bold_phrases': [],\n",
       " 'italic_phrases': [],\n",
       " 'full_text': ['An Introduction to',\n",
       "  'Statistics',\n",
       "  'Keone Hon',\n",
       "  '<',\n",
       "  '>',\n",
       "  'Contents',\n",
       "  '1',\n",
       "  '2',\n",
       "  'Chapter 1',\n",
       "  'Descriptive Statistics',\n",
       "  '1.1\\tDescriptive vs. Inferential',\n",
       "  'There are two main branches of statistics: descriptive and inferential. Descrip-tive statistics is used to say something about a set of information that has been collected only. Inferential statistics is used to make predictions or comparisons about a larger group (a population) using information gathered about a small part of that population. Thus, inferential statistics involves generalizing beyond the data, something that descriptive statistics does not do.',\n",
       "  'Other distinctions are sometimes made between data types.',\n",
       "  'Discrete data are whole numbers, and are usually a count of objects. (For instance, one study might count how many pets diﬀerent families own; it wouldn’t make sense to have half a goldfish, would it?)',\n",
       "  'Measured data, in contrast to discrete data, are continuous, and thus may take on any real value. (For example, the amount of time a group of chil-dren spent watching TV would be measured data, since they could watch any number of hours, even though their watching habits will probably be some multiple of 30 minutes.)',\n",
       "  'Numerical data are numbers.',\n",
       "  'Categorical data have labels (i.e. words). (For example, a list of the prod-ucts bought by diﬀerent families at a grocery store would be categorical data, since it would go something like {milk, eggs, toilet paper, . . . }.)',\n",
       "  '1.2\\tMeans, Medians, and Modes',\n",
       "  'In everyday life, the word “average” is used in a variety of ways - batting averages, average life expectancies, etc. - but the meaning is similar, usually',\n",
       "  '3',\n",
       "  'the center of a distribution. In the mathematical world, where everything must be precise, we define several ways of finding the center of a set of data:',\n",
       "  'Definition 1: median.',\n",
       "  'The median is the middle number of a set of numbers arranged in numerical order. If the number of values in a set is even, then the median is the sum of the two middle values, divided by 2.',\n",
       "  'The median is not aﬀected by the magnitude of the extreme (smallest or largest) values. Thus, it is useful because it is not aﬀected by one or two abnormally small or large values, and because it is very simple to calculate. (For example, to obtain a relatively accurate average life of a particular type of lightbulb, you could measure the median life by installing several bulbs and measuring how much time passed before half of them died. Alternatives would probably involve measuring the life of each bulb.)',\n",
       "  'Definition 2: mode.',\n",
       "  'The mode is the most frequent value in a set. A set can have more than one mode; if it has two, it is said to be bimodal.',\n",
       "  '——————————–',\n",
       "  'Example 1:',\n",
       "  'The mode of {1, 1, 2, 3, 5, 8} is 1.',\n",
       "  'The modes of {1, 3, 5, 7, 9, 9, 21, 25, 25, 31} are 9 and 25. Thus, the set is bimodal.',\n",
       "  '——————————–',\n",
       "  'The mode is useful when the members of a set are very diﬀerent - take, for example, the statement “there were more Ds on that test than any other letter grade” (that is, in the set {A, B, C, D, E}, D is the mode). On the other hand, the fact that the mode is absolute (for example, 2.9999 and 3 are considered just as diﬀerent as 3 and 100 are) can make the mode a poor choice for determing a “center”. For example, the mode of the set {1, 2.3, 2.3, 5.14, 5.15, 5.16, 5.17, 5.18, 10.2} is 2.3, even though there are many values that are close to, but not exactly equal to, 5.16.',\n",
       "  'Definition 3: mean.',\n",
       "  'The mean is the sum of all the values in a set, divided by the number of values. The mean of a whole population is usually denoted by µ, while the mean of a sample is usually denoted by x. (Note that this is the arithmetic mean; there are other means, which will be discussed later.)',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  'The mean is sensitive to ',\n",
       "  'any',\n",
       "  ' change in value, unlike the median and mode, where a change to an extreme (in the case of a median) or uncommon (in the case of a mode) value usually has no eﬀect.',\n",
       "  'One disadvantage of the mean is that a small number of extreme values can distort its value. For example, the mean of the set ',\n",
       "  '{',\n",
       "  '1, 1, 1, 2, 2, 3, 3, 3, 200',\n",
       "  '}',\n",
       "  ' is 24, even though almost all of the members were very small. A variation called the ',\n",
       "  'trimmed mean',\n",
       "  ', where the smallest and largest quarters of the values are removed before the mean is taken, can solve this problem.',\n",
       "  '1.3\\tVariability',\n",
       "  'Definition 4: range',\n",
       "  '.',\n",
       "  'The range is the diﬀerence between the largest and smallest values of a set.',\n",
       "  'The range of a set is simple to calculate, but is not very useful because it depends on the extreme values, which may be distorted. An alternative form, similar to the trimmed mean, is the interquartile range, or IQR, which is the range of the set with the smallest and largest quarters removed. If Q1 and Q3 are the medians of the lower and upper halves of a data set (the values that split the data into quarters, if you will), then the IQR is simply Q3 ',\n",
       "  '−',\n",
       "  ' Q1.',\n",
       "  'The IQR is useful for determining outliers, or extreme values, such as the element ',\n",
       "  '{',\n",
       "  '200',\n",
       "  '}',\n",
       "  ' of the set at the end of section ',\n",
       "  '. An outlier is said to be a number more than 1.5 IQRs below Q1 or above Q3.',\n",
       "  'Definition 5: variance',\n",
       "  '.',\n",
       "  'The variance is a measure of how items are dispersed about their mean. The variance σ',\n",
       "  '2',\n",
       "  ' of a whole population is given by the equation',\n",
       "  '',\n",
       "  'The variance s',\n",
       "  '2',\n",
       "  ' of a sample is calculated diﬀerently:',\n",
       "  '',\n",
       "  'Definition 6: standard deviation',\n",
       "  '.',\n",
       "  'The standard deviation σ (or s for a sample) is the square root of the variance. (Thus, for a population, the standard deviation is the',\n",
       "  '5',\n",
       "  'square root of the average of the squared deviations from the mean. For a sample, the standard deviation is the square root of the sum of the squared deviations from the mean, divided by the number of samples minus 1. Try saying that five times fast.)',\n",
       "  'Definition 7: relative variability.',\n",
       "  'The relative variability of a set is its standard deviation divided by its mean. The relative variability is useful for comparing several variances.',\n",
       "  '1.4\\tLinear Transformations',\n",
       "  'A linear transformation of a data set is one where each element is increased by or multiplied by a constant. This aﬀects the mean, the standard deviation, the ',\n",
       "  'IQR',\n",
       "  ', and other important numbers in diﬀerent ways.',\n",
       "  'Addition. If a constant ',\n",
       "  'c',\n",
       "  ' is added to each member of a set, the mean will be ',\n",
       "  'c',\n",
       "  ' more than it was before the constant was added; the standard deviation and variance will not be aﬀected; and the ',\n",
       "  'IQR',\n",
       "  ' will not be aﬀected. We will prove these facts below, letting ',\n",
       "  'µ',\n",
       "  ' and ',\n",
       "  'σ',\n",
       "  ' be the mean and standard deviation, respectively, before adding ',\n",
       "  'c',\n",
       "  ', and ',\n",
       "  'µ',\n",
       "  't',\n",
       "  ' and ',\n",
       "  'σ',\n",
       "  't',\n",
       "  ' be the mean and standard devia-tion, respectively, after the transformation. Finally, we let the original set be {',\n",
       "  'a',\n",
       "  '1',\n",
       "  ', a',\n",
       "  '2',\n",
       "  ', . . . , a',\n",
       "  'n',\n",
       "  '}, so that the transformed set is {',\n",
       "  'a',\n",
       "  '1',\n",
       "  ' + ',\n",
       "  'c, a',\n",
       "  '2',\n",
       "  ' + ',\n",
       "  'c, . . . , a',\n",
       "  'n',\n",
       "  ' + ',\n",
       "  'c',\n",
       "  '}.',\n",
       "  '',\n",
       "  'a',\n",
       "  '1 ',\n",
       "  '+',\n",
       "  ' ',\n",
       "  'a',\n",
       "  '2 ',\n",
       "  '+',\n",
       "  ' ',\n",
       "  '· · ·',\n",
       "  ' ',\n",
       "  '+',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'n ',\n",
       "  '+',\n",
       "  ' ',\n",
       "  'cn',\n",
       "  ' ',\n",
       "  '=',\n",
       "  ' ',\n",
       "  'µ',\n",
       "  ' ',\n",
       "  '+',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'nn',\n",
       "  '',\n",
       "  'where we use the result of the first equation to replace ',\n",
       "  'µ',\n",
       "  't',\n",
       "  ' with ',\n",
       "  'µ',\n",
       "  '+',\n",
       "  'c',\n",
       "  ' in the second equation. Since the variance is just the square of the standard deviation, the fact that the standard deviation is not aﬀected means that the variance won’t be, either.',\n",
       "  'Multiplication.',\n",
       "  'Another type of transformation is multiplication. If each member of a set is multiplied by a constant ',\n",
       "  'c',\n",
       "  ', then the mean will be ',\n",
       "  'c',\n",
       "  ' times its value before the constant was multiplied; the standard deviation will be |',\n",
       "  'c',\n",
       "  '| times its value before',\n",
       "  '6',\n",
       "  'the constant was multiplied; and the ',\n",
       "  'IQR',\n",
       "  ' will be |',\n",
       "  'c',\n",
       "  '| times its value. Using the same notation as before, we have',\n",
       "  '',\n",
       "  'a',\n",
       "  '1 ',\n",
       "  '+',\n",
       "  ' ',\n",
       "  'a',\n",
       "  '2 ',\n",
       "  '+',\n",
       "  ' ',\n",
       "  '· · ·',\n",
       "  ' ',\n",
       "  '+',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'n ',\n",
       "  '·',\n",
       "  ' ',\n",
       "  'c',\n",
       "  ' ',\n",
       "  '=',\n",
       "  ' ',\n",
       "  'µ',\n",
       "  ' ',\n",
       "  '·',\n",
       "  ' ',\n",
       "  'c',\n",
       "  ' ',\n",
       "  'n',\n",
       "  '',\n",
       "  '1.5\\tPosition',\n",
       "  'There are several ways of measuring the relative position of a specific member of a set. Three are defined below:',\n",
       "  'Definition 8: simple ranking.',\n",
       "  'As the name suggests, the simplest form of ranking, where objects are arranged in some order and the rank of an object is its position in the order.',\n",
       "  'Definition 9: percentile ranking.',\n",
       "  'The percentile ranking of a specific value is the percent of scores/values that are below it.',\n",
       "  'Definition 10: z-score.',\n",
       "  'The z-score of a specific value is the number of standard deviations it is from the mean. Thus, the ',\n",
       "  'z',\n",
       "  '-score of a value ',\n",
       "  'x',\n",
       "  ' is given by the equation',\n",
       "  '',\n",
       "  '——————————–',\n",
       "  'Example 2:',\n",
       "  'In the set of grade point averages {1.1, 2.34, 2.9, 3.14, 3.29, 3.57, 4.0}, the value 3.57 has the simple ranking of 2 out of 7 and the percentile ranking of',\n",
       "  '7',\n",
       "  '——————————–',\n",
       "  'Conversely, if given a ',\n",
       "  'z',\n",
       "  '-score, we can find a corresponding value, using the equation',\n",
       "  '——————————–',\n",
       "  'Example 3:',\n",
       "  'The citizens of Utopia work an average (mean) of 251 days per year, with a standard deviation of 20 days. How many days correspond to a ',\n",
       "  'z',\n",
       "  '-score of 2.3?',\n",
       "  'Since each ',\n",
       "  'z',\n",
       "  ' corresponds to one standard deviation, a ',\n",
       "  'z',\n",
       "  '-score of 2.3 means that the desired value is 2.3 standard deviations more than the mean, or 251 + 2',\n",
       "  '.',\n",
       "  '3 · 20 = 297.',\n",
       "  '——————————–',\n",
       "  '1.6\\tDispersion Percentages',\n",
       "  'Theorem 1: empirical rule',\n",
       "  'For data with a “bell-shaped” graph, about 68% of the values lie within one standard deviation of the mean, about 95% lie withing two standard deviations, and over 99% lie within three standard deviations of the mean.',\n",
       "  'Note that since 99% of the data fall within a span of six standard deviations (',\n",
       "  'z',\n",
       "  '-scores of -3 to +3), the standard deviation of a set of values that are somewhat bell-shaped should be about ',\n",
       "  '1',\n",
       "  '6',\n",
       "  ' of the range. This can be useful in checking for arithmetic errors.',\n",
       "  'Theorem 2: Chebyshev’s Theorem',\n",
       "  'For any set of data, at least 1− ',\n",
       "  'k',\n",
       "  '1',\n",
       "  '2',\n",
       "  ' of the values lie within ',\n",
       "  'k',\n",
       "  ' standard deviations of the mean (that is, have ',\n",
       "  'z',\n",
       "  '-scores between −',\n",
       "  'k',\n",
       "  ' and +',\n",
       "  'k',\n",
       "  ').',\n",
       "  '',\n",
       "  '——————————–',\n",
       "  'Example 4:',\n",
       "  'Matt reads at an average (mean) rate of 20.6 pages per hour, with a standard deviation of 3.2. What percent of the time will he read between 15 and 26.2 pages per hour?',\n",
       "  '8',\n",
       "  '15 pages per hour corresponds to a z-score of ',\n",
       "  '15−20.6',\n",
       "  ' = −1.75, and 26.2 pages',\n",
       "  '3.2',\n",
       "  'per hour corresponds to a z-score of ',\n",
       "  '26.2−20.6',\n",
       "  '  = 1.75. Chebyshev’s Theorem',\n",
       "  '3.2',\n",
       "  'says that 1 − ',\n",
       "  '1.75',\n",
       "  '1',\n",
       "  '2',\n",
       "  ' = .673 of the values will be within 1.75 standard deviations, so 67.3% of the time, Matt’s reading speed will be between 15 and 26.2 pages per hour.',\n",
       "  '',\n",
       "  '——————————–',\n",
       "  '9',\n",
       "  'Chapter 2',\n",
       "  'Graphs and Displays',\n",
       "  '2.1\\tHistograms',\n",
       "  '2.1.1\\tIntroduction',\n",
       "  'Definition 11: histogram.',\n",
       "  'A histogram is a graphical representation of data, where relative frequencies are represented by relative areas. A histogram’s height at a specific point represents the relative frequency of a certain item.',\n",
       "  'A histogram is similar to a bar graph, except the sides of the bars are widened until there is no space between bars:',\n",
       "  '',\n",
       "  'A histogram isn’t limited to displaying frequencies. The y-axis (vertical axis) may labeled with relative frequencies. To determine relative frequency, we use the simple formula',\n",
       "  'In the example we used above, the total is 3 + 1 + 4 + 1 + 5 + 4 + 2 + 3 = 23, so the relative frequencies are as follows:',\n",
       "  '10',\n",
       "  'x-value\\tfrequency',\n",
       "  '\\t',\n",
       "  'relative frequency',\n",
       "  '',\n",
       "  '33/23 ≈ .13',\n",
       "  '11/23 ≈ .04',\n",
       "  '44/23 ≈ .17',\n",
       "  '11/23 ≈ .04',\n",
       "  '55/23 ≈ .22',\n",
       "  '44/23 ≈ .17',\n",
       "  '22/23 ≈ .09',\n",
       "  '33/23 ≈ .13',\n",
       "  'The resulting histogram is shown below. Note that it has the same shape as the histogram labeled with actual frequencies. This is true in general.',\n",
       "  '',\n",
       "  'If we were given a histogram with an unlabeled y-axis, we could still determine the relative frequencies of the items because the relative frequency is equal to the fraction of the total area that a certain column covers.',\n",
       "  '——————————–',\n",
       "  'Example 5:',\n",
       "  'Determine the relative frequency of items A-H using the histogram below:',\n",
       "  '',\n",
       "  'We cut the histogram up into rectangles of equal area:',\n",
       "  '11',\n",
       "  '',\n",
       "  'There are 21 rectangles in all. So if an item has an area of x rectangles, it will have relative frequency ',\n",
       "  '21',\n",
       "  'x',\n",
       "  ' .',\n",
       "  '',\n",
       "  '',\n",
       "  '55/21 ≈ .24',\n",
       "  '33/21 ≈ .14',\n",
       "  '11/21 ≈ .05',\n",
       "  '11/21 ≈ .05',\n",
       "  '44/21 ≈ .19',\n",
       "  '33/21 ≈ .14',\n",
       "  '——————————–',\n",
       "  '2.1.2\\tMedians, Modes, and Means Revisited',\n",
       "  'Histograms can be used to show the median, mode, and mean of a distribution. Since the mode is the most frequent value, it is the point on the histogram where the graph is highest. Since the median is in the middle of a distribution (so it divides the distribution in half), it may be represented by the line that divides the area of the histogram in half. Finally, the mean is a line that passes through the center of gravity of the histogram.',\n",
       "  'If the mean is less than the median, then the distribution is said to be skewed to the left. It will be spread widely to the left. Here is an example of a distribution that is skewed to the left:',\n",
       "  '',\n",
       "  '12',\n",
       "  'Conversely, if the mean is greater than the median, then the distribution is skewed to the right, and it will be spread widely to the right. Here is an example of a distribution that is skewed to the right:',\n",
       "  '',\n",
       "  '2.1.3\\tz-Scores and Percentile Ranks Revisited',\n",
       "  'Earlier, we exchanged frequency for relative frequency on the vertical axis of a histogram. In the same spirit, we can label the horizontal axis in terms of z-scores rather than with the names of the items in the set.',\n",
       "  'One common way that data is given is through percentile rankings. With this information, we can construct a histogram:',\n",
       "  '——————————–',\n",
       "  'Example 6:',\n",
       "  'Construct a histogram using the following data:',\n",
       "  '',\n",
       "  '50',\n",
       "  '80',\n",
       "  '95',\n",
       "  '100',\n",
       "  'From −∞to − 3, the relative frequency is 0.05. From −3 to −2, the relative frequency increases from 0.05 to 0.15, so the relative frequency at between −3 and −2 is 0.1. From −2 to −1, the relative frequency increases from 0.15 to 0.3, so the relative frequency between the two is 0.15. Using similar reasoning, we obtain the following:',\n",
       "  '13',\n",
       "  'Plotting these values on a histogram, we obtain',\n",
       "  '',\n",
       "  '——————————–',\n",
       "  '2.2\\tStem and Leaf Displays',\n",
       "  'A stem and leaf display is similar to a histogram, since it shows how many values in a set fall under a certain interval. However, it has even more information - it shows the actual values within the interval. The following example demonstrates a stem and leaf display:',\n",
       "  '——————————–',\n",
       "  'Example 7:',\n",
       "  'Here is a stem and leaf display of the set {10, 14, 19, 22, 25, 25, 28, 31, 33, 39, 39, 40, 40, 40, 41, 44, 45}:',\n",
       "  '',\n",
       "  'Stems\\tLeaves',\n",
       "  '0 4 9',\n",
       "  '22558',\n",
       "  '31399',\n",
       "  '4000145',\n",
       "  '——————————–',\n",
       "  'If we draw a line around the stem and leaf display, the result is a histogram, albeit one whose orientation is diﬀerent from the kind we are used to seeing:',\n",
       "  '14',\n",
       "  '',\n",
       "  'The stems and leaves need not be single digits, although all leaves should be the same size in order to make the display accurate.',\n",
       "  '2.3\\tFive Number Summaries and Box and Whisker Displays',\n",
       "  'The five-number summary is a group of five numbers that help describe the shape and variation of a distribution. These five numbers are Q2, the median of the set; Q1, the median of the lower half of the set; Q3, the median of the upper half of the set, and the maximum and minimum numbers that are not outliers. (See section ',\n",
       "  'for the definition of an outlier.)',\n",
       "  'The box and whisker display is a graphical representation of the five-number summary. Horizontal lines are drawn at Q1, Q2, and Q3; a box is then drawn around these lines, forming a double box with a shared side. Two perpendicular lines (”whiskers”) are drawn from the top and bottom of the box to the max-imum and minimum non-outliers. Any outliers are then plotted as individual points.',\n",
       "  'The diagram below shows the basic box and whisker format.',\n",
       "  '',\n",
       "  'Note that because a median splits a set in half, the top whisker represents the top quarter, the top box represents the second quarter, the bottom box represents the third quarter, and the bottom whisker represents the bottom quarter.',\n",
       "  '15',\n",
       "  'Chapter 3',\n",
       "  'Probability',\n",
       "  '3.1\\tIntroduction',\n",
       "  'Definition 12: probability.',\n",
       "  'The probability of a specific event is a mathematical statement about the likelihood that it will occur. All probabilities are numbers be-tween 0 and 1, inclusive; a probability of 0 means that the event will never occur, and a probability of 1 means that the event will always occur.',\n",
       "  'The sum of the probabilities of all possible outcomes of any event is 1. (This is because something will happen, so the probability of some outcome occurring is 1.)',\n",
       "  'Definition 13: complimentary event.',\n",
       "  'With respect to an event E, the complimentary event, denoted E',\n",
       "  '0',\n",
       "  ', is the event that E does not occur. For example, consider the event that it will rain tomorrow. The compliment of this event is the event that it will not rain tomorrow.',\n",
       "  'Since an event must either occur or not occur, from above, it must be the case that',\n",
       "  'Definition 14: mutually exclusive.',\n",
       "  'Two or more events are mutually exclusive if they cannot occur simultaneously.',\n",
       "  'Two events A and B are mutually exclusive if A ',\n",
       "  '∪',\n",
       "  ' B = 0 - that is, if they have no members in common.',\n",
       "  '16',\n",
       "  '——————————–',\n",
       "  'Example 8:',\n",
       "  'Let A = the event that it is Monday, B = the event that it is Tuesday, and C = the event that it is the year 2004.',\n",
       "  'A and B are mutually exclusive events, since it cannot be both Monday and Tuesday at the same time. A and C are not mutually exclusive events, since it can be a Monday in the year 2004.',\n",
       "  '——————————–',\n",
       "  'Theorem 3: Principle of Inclusion and Exclusion',\n",
       "  'Recall that when two events A and B are mutually exclusive P (A ∩ B) = 0. Using this fact and the Principle of Inclusion and Exclusion (PIE), we conclude that when two events are mutually exclusive, the probability that both of them will occur (P (A ',\n",
       "  '∪',\n",
       "  ' B)) is P (A) + P (B).',\n",
       "  'Definition 15: independent.',\n",
       "  'Two events are said to be independent if the chance that each one occurs is not aﬀected by the outcome of any of the others.',\n",
       "  '——————————–',\n",
       "  'Example 9:',\n",
       "  'Suppose that two dice are rolled. The outcome of either die will not aﬀect the outcome of the other die, so the two dice are independent. On the other hand, the event that John Kerry will become president of the U.S. and the event that John Edwards will become vice president are not independent, since the two are (were?) running mates, so if one is elected, so is the other.',\n",
       "  '——————————–',\n",
       "  'Theorem 4: Independence Principle',\n",
       "  'If two events are independent, then the probability that both will occur is equal to the product of their individual probabilities. In other words, if A and B are independent, then',\n",
       "  '17',\n",
       "  'Definition 16: factorial.',\n",
       "  'The factorial of an integer n is defined as the product of all the positive integers from 1 to n, that is',\n",
       "  'n!',\n",
       "  '\\t',\n",
       "  '(read “n factorial”) = n · (n − 1) · (n − 2) · · · 3 · 2 · 1\\t(3.4)',\n",
       "  '0! is defined to be 1.',\n",
       "  'Definition 17: combination.',\n",
       "  'A combination is a set of unordered (i.e. order does not matter) items. If we are to choose k distinct objects from a total of n objects, then there are ',\n",
       "  'n',\n",
       "  'k',\n",
       "  ' diﬀerent combinations, where',\n",
       "  'Formula 1: Binomial Formula',\n",
       "  'Suppose that an event occurs with probability p. Then the prob-ability that it will occur exactly x times out of a total of n trials is',\n",
       "  '——————————–',\n",
       "  'Example 10:',\n",
       "  'Derive a formula for calculating P(x) (the probability of x successes out of n trials, where the probability of each success is p) in terms of x, n, p, and P (x−1).',\n",
       "  'The probability of x − 1 successes is, using the binomial formula,',\n",
       "  'The probability of x successes is, again using the binomial formula,',\n",
       "  '18',\n",
       "  'We want to find an expression K such that P (x − 1) · K = P (x). Thus,',\n",
       "  '',\n",
       "  '——————————–',\n",
       "  '3.2\\tRandom Variables',\n",
       "  '3.2.1\\tDefinition',\n",
       "  'Definition 18: random variable.',\n",
       "  'A variable that may take on diﬀerent values depending on the out-come of some event.',\n",
       "  '——————————–',\n",
       "  'Example 11:',\n",
       "  'On the AP Statistics exam, ',\n",
       "  '1',\n",
       "  '5',\n",
       "  ' of the class received an 5, ',\n",
       "  '1',\n",
       "  '3',\n",
       "  ' received a 4, ',\n",
       "  '1',\n",
       "  '6',\n",
       "  ' received a 3, ',\n",
       "  '20',\n",
       "  '1',\n",
       "  ' received a 2, and ',\n",
       "  '1',\n",
       "  '3',\n",
       "  ' received an 1. If x represents the score of a randomly chosen student in the class, then x is a random variable that takes the values 1, 2, 3, 4, and 5.',\n",
       "  '',\n",
       "  '——————————–',\n",
       "  'A random variable may be discrete (it takes on a finite number of values) or continuous (it can take on any value in a certain interval, that is, an infinite number of values).',\n",
       "  'Definition 19: probability distribution.',\n",
       "  'A list or formula that gives the probability for each discrete value of a random variable.',\n",
       "  '19',\n",
       "  '3.2.2\\tExpected Value',\n",
       "  'Definition 20: expected value',\n",
       "  '.',\n",
       "  'The expected value (also called the mean) of a random variable is the sum of the product of each possible value and its corresponding probability. In mathematical terms, if the random variable is X, the possible values are x',\n",
       "  '1',\n",
       "  ', x',\n",
       "  '2',\n",
       "  ', . . . x',\n",
       "  'n',\n",
       "  ', and the corresponding probabilities are P (x',\n",
       "  '1',\n",
       "  '), P (x',\n",
       "  '2',\n",
       "  '), . . . , P (x',\n",
       "  'n',\n",
       "  ') then',\n",
       "  '——————————–',\n",
       "  'Example 12:',\n",
       "  'Investing in Sharma Furniture Co. has a ',\n",
       "  '60%',\n",
       "  ' chance of resulting in a $10,000 gain and a ',\n",
       "  '40%',\n",
       "  ' chance of resulting in a $3,000 loss. What is the expected value of investing in Sharma Furniture Co.?',\n",
       "  'E(X) = .6 ',\n",
       "  '·',\n",
       "  ' 10000 + .4 ',\n",
       "  '· −',\n",
       "  '3000 = 6000 ',\n",
       "  '−',\n",
       "  ' 1200 = 4800',\n",
       "  'Thus, the expected value is $4,800.',\n",
       "  '——————————–',\n",
       "  '3.2.3\\tVariance and Standard Deviation',\n",
       "  'Definition 21: variance of a discrete random variable',\n",
       "  '.',\n",
       "  'The variance σ',\n",
       "  '2',\n",
       "  ' of a random variable, which takes on discrete values x and has mean µ, is given by the equation',\n",
       "  'Definition 22: standard deviation of a discrete random vari-able',\n",
       "  '.',\n",
       "  'The standard deviation σ of a discrete random variable is equal to',\n",
       "  '√',\n",
       "  '',\n",
       "  'the square root of the variance, i.e. σ =',\n",
       "  '\\t',\n",
       "  'σ',\n",
       "  '2',\n",
       "  '.',\n",
       "  '——————————–',\n",
       "  'Example 13:',\n",
       "  'In a contest sponsored by Hansen sodas, you win a prize if the cap on your bottle of sodas says “WINNER”; however, you may only claim one prize. Eager to win, you blow all your savings on sodas; as a result, you have a 0.05% chance of winning $1,000,000, a 1% chance of winning $20,000, and a 90% chance of winning $10. Ignoring the money you spent on sodas, what is your expected',\n",
       "  '20',\n",
       "  'value and standard deviation?',\n",
       "  'First, we calculate the mean.',\n",
       "  '= 1000000 · 0',\n",
       "  '.',\n",
       "  '0005 + 0',\n",
       "  '.',\n",
       "  '01 · 20000 + 0',\n",
       "  '.',\n",
       "  '9 · 10 = 500 + 200 + 9 = 709',\n",
       "  'Now, the variance.',\n",
       "  'σ',\n",
       "  '2',\n",
       "  '\\t',\n",
       "  '=',\n",
       "  '\\t',\n",
       "  '1000000',\n",
       "  '2',\n",
       "  ' · 0',\n",
       "  '.',\n",
       "  '0005 + 20000',\n",
       "  '2',\n",
       "  ' · 0',\n",
       "  '.',\n",
       "  '01 + 10',\n",
       "  '2',\n",
       "  ' · ',\n",
       "  '.',\n",
       "  '9 − 709',\n",
       "  '2',\n",
       "  '500000000 + 4000000 + 90 − 502681',\n",
       "  '503497409',\n",
       "  'Finally, the standard deviation.',\n",
       "  '√',\n",
       "  '',\n",
       "  '=  503497409 ≈ 22438',\n",
       "  'The standard deviation is over $22,000! Although the expected value looks nice, there’s a good chance that you’ll get a not-so-good amount of money.',\n",
       "  '——————————–',\n",
       "  '3.2.4\\t“Shortcuts” for Binomial Random Variables',\n",
       "  'The past examples required a fair amount of arithmetic. To save time, there are simpler ways to find expected values, variances, and standard deviations of binomial random variables (that is, random variables with only two outcomes).',\n",
       "  'Theorem 5: Mean and Standard Deviation of a Binomial In a situation with two outcomes where the probability of an out-come O is ',\n",
       "  'p',\n",
       "  ' and there are ',\n",
       "  'n',\n",
       "  ' trials, the expected number of Os is ',\n",
       "  'np',\n",
       "  '. That is,',\n",
       "  'Furthermore, the variance ',\n",
       "  'σ',\n",
       "  '2',\n",
       "  ' is given by the equation',\n",
       "  'and the standard deviation ',\n",
       "  'σ',\n",
       "  ' by the equation',\n",
       "  'p',\n",
       "  '',\n",
       "  'σ ',\n",
       "  '=',\n",
       "  '\\tnp',\n",
       "  '(1',\n",
       "  ' ',\n",
       "  '−',\n",
       "  ' p',\n",
       "  ')',\n",
       "  '\\t',\n",
       "  '(3.12)',\n",
       "  '21',\n",
       "  'Chapter 4',\n",
       "  'Probability Distributions',\n",
       "  '4.1\\tBinomial Distributions',\n",
       "  'Previously, we computed the probability that a binomial event (one with two possible outcomes for each trial) would occur. In the same way, we can compute the probability of every possible combination of outcomes and create a table or histogram from it.',\n",
       "  '——————————–',\n",
       "  'Example 14:',\n",
       "  'On a five-item true or false test, Simon has an 80% chance of choosing the cor-rect answer for any of the questions. Find the complete probability distribution for the number of correct answers that Simon can get. Then determine the mean and standard deviation of the probability distribution.',\n",
       "  '22',\n",
       "  'Note that 0',\n",
       "  '.',\n",
       "  '00032 + 0',\n",
       "  '.',\n",
       "  '0064 + 0',\n",
       "  '.',\n",
       "  '0512 + 0',\n",
       "  '.',\n",
       "  '2048 + 0',\n",
       "  '.',\n",
       "  '4096 + 0',\n",
       "  '.',\n",
       "  '32768 = 1.',\n",
       "  'The expected value may be calculated in two ways: ',\n",
       "  'µ',\n",
       "  ' = ',\n",
       "  'np',\n",
       "  ' = 5 · 0',\n",
       "  '.',\n",
       "  '8 = 4 or',\n",
       "  '= 0 · 0',\n",
       "  '.',\n",
       "  '00032 + 1 · 0',\n",
       "  '.',\n",
       "  '0064 + 2 · 0',\n",
       "  '.',\n",
       "  '0512 + 3 · 0',\n",
       "  '.',\n",
       "  '2048 + 4 · 0',\n",
       "  '.',\n",
       "  '4096 + 5 · 0',\n",
       "  '.',\n",
       "  '32768 = 4. In either case, the answer is the same.',\n",
       "  'The variance is ',\n",
       "  'σ',\n",
       "  '2',\n",
       "  ' = ',\n",
       "  'np',\n",
       "  '(1−',\n",
       "  'p',\n",
       "  ') = 5·0',\n",
       "  '.',\n",
       "  '8·(1−0',\n",
       "  '.',\n",
       "  '8) = 0',\n",
       "  '.',\n",
       "  '8, so the standard deviation',\n",
       "  '√',\n",
       "  '',\n",
       "  'is ',\n",
       "  'σ',\n",
       "  ' =',\n",
       "  '\\t',\n",
       "  '0',\n",
       "  '.',\n",
       "  '8 ≈ 0',\n",
       "  '.',\n",
       "  '894.',\n",
       "  '——————————–',\n",
       "  'Binomial distributions are often used in quality control systems, where a small number of samples are tested out of a shipment, and the shipment is only accepted if the number of defects found among the sampled units falls below a specified number. In order to analyze whether a sampling plan eﬀectively screens out shipments containing a great number of defects and accepts shipments with very few defects, one must calculate the probability that a shipment will be accepted given various defect levels.',\n",
       "  '——————————–',\n",
       "  'Example 15:',\n",
       "  'A sampling plan calls for twenty units out of a shipment to be sampled. The shipment will be accepted if and only if the number of defective units is less than or equal to one. What is the probability that a shipment with defect level ',\n",
       "  'n',\n",
       "  ' will be accepted for ',\n",
       "  'n',\n",
       "  ' = 0',\n",
       "  '.',\n",
       "  '05',\n",
       "  ',',\n",
       "  ' 0',\n",
       "  '.',\n",
       "  '10',\n",
       "  ',',\n",
       "  ' 0',\n",
       "  '.',\n",
       "  '15',\n",
       "  ',',\n",
       "  ' 0',\n",
       "  '.',\n",
       "  '20',\n",
       "  ',',\n",
       "  ' 0',\n",
       "  '.',\n",
       "  '25',\n",
       "  ',',\n",
       "  ' 0',\n",
       "  '.',\n",
       "  '30?',\n",
       "  '——————————–',\n",
       "  '23',\n",
       "  '4.2\\tPoisson Distributions',\n",
       "  '4.2.1\\tDefinition',\n",
       "  'Definition 23: Poisson distribution.',\n",
       "  'In a binomial distribution, when the number of trials n is large and the probability of success p is small, the distribution approaches the Poisson distribution. In the Poisson distribution, the probability of x successes is given by the equation',\n",
       "  'where µ is the mean.',\n",
       "  'At first, the requirement that n be large, p be small, and the mean (np) be a known, moderate number seems overly restrictive. However, there are many cases where this occurs. For example, a grocer might sell 5 heads of lettuce each day. It’s impractical to say how many heads of lettuce he didn’t sell, because we do not know how many customers visited his store or how many they could have bought (and there is really no way to determine the latter). However, we can assume that there were many chances for someone to buy a head of lettuce, so n is very large. The chance of someone buying a head of lettuce at any given moment is very small, so p is small. Finally, the mean, 5 heads of lettuce per day, is known. Thus, the Poisson distribution could probably be used to describe this situation.',\n",
       "  'Here is another application of the Poisson distribution:',\n",
       "  '——————————–',\n",
       "  'Example 16:',\n",
       "  'The Morgan household gets an average of 3 telephone calls per day. Using the Poisson distribution, find the probability of n phone calls for 0 ≤ n ≤ 6 in one day. Then find the probability of n phone calls in half a day for 0 ≤ n ≤ 3.',\n",
       "  '24',\n",
       "  'Notice that P (0) + P (1) + P (2) + P (3) + P (4) + P (5) + P (6) ≈ 0.96, not 1. This is because there is still a small probability of 7, 8, etc. calls.',\n",
       "  'In half a day, we can expect an average of 1.5 calls per day. (It’s okay to have a non-integral mean, although it is true that the number of successes, x, must be an integer.) Thus,',\n",
       "  '——————————–',\n",
       "  '4.2.2\\tAs an Approximation to the Binomial',\n",
       "  'Earlier we stated that the Poisson distribution was useful because it only re-quired knowing the mean. However, even if we do know n and p, we can still use the Poisson distribution as an approximation. In general, if n ≥ 20 and p ≤ 0.05, the approximation will be “close” (of course, “close” is a relative term).',\n",
       "  '——————————–',\n",
       "  'Example 17:',\n",
       "  'A standard die is rolled 120 times. What is the probability of exactly 10 sixes?',\n",
       "  '15? 20? 25?',\n",
       "  'In this case, we know n (it’s 120) and p (it’s ',\n",
       "  '1',\n",
       "  '6',\n",
       "  ' ). However, using the binomial formula would require calculating very large and very small numbers - for the first one, ',\n",
       "  '120',\n",
       "  '10',\n",
       "  ' . (For the record, it’s 116068178638776. Try to remember that!) Instead, we’ll use the Poisson approximation, even though we will sacrifice some accuracy as p 6≤0.05.',\n",
       "  '25',\n",
       "  '——————————–',\n",
       "  '4.3\\tNormal Distributions',\n",
       "  '4.3.1\\tDefinition and Properties',\n",
       "  'The normal curve is a bell-shaped, symmetrical graph with an infinitely long base. The mean, median, and mode are all located at the center.',\n",
       "  '',\n",
       "  'A value is said to be normally distributed if its histogram is the shape of the normal curve. The probability that a normally distributed value will fall between the mean and some z-score z is the area under the curve from 0 to z:',\n",
       "  '26',\n",
       "  '',\n",
       "  '4.3.2\\tTable of Normal Curve Areas',\n",
       "  'The area from the mean to z-score z is given in the table below:',\n",
       "  '27',\n",
       "  '$0.50',\n",
       "  '——————————–',\n",
       "  'Example 18:',\n",
       "  'The price of a gallon of gasoline at the gasoline stations in Nevada is normally distributed with a mean of $2.00 and a standard deviation of $0.50.',\n",
       "  'What is the probability that at a randomly chosen gasoline station in Nevada, the price of gasoline will be between $2.00 and $2.75?',\n",
       "  'The z-score of $2.00 is 0, and the z-score of $2.75 is ',\n",
       "  '$2.75',\n",
       "  ' ',\n",
       "  '−',\n",
       "  ' ',\n",
       "  '$2.00',\n",
       "  ' = 1.5. Ac-',\n",
       "  '',\n",
       "  'cording to the table above, the area between 0 and 1.5 is 0.4332 of the total area, so the probability is 0.4332.',\n",
       "  'What is the probability that at a randomly chosen gasoline station in Nevada, the price of gasoline will be between $1.25 and $2.00?',\n",
       "  'The z-score of $1.25 is ',\n",
       "  '$1.75',\n",
       "  ' ',\n",
       "  '−',\n",
       "  ' ',\n",
       "  '$2.00',\n",
       "  ' = −1.5, and the z-score of $2.00 is 0. But $0.50',\n",
       "  '',\n",
       "  'since the normal curve is symmetric, the area between −1.5 and 0 is the same as the area between 0 and 1.5, which is, as before, 0.4332 of the total area. Thus the probability is also 0.4332.',\n",
       "  'What percent of the gasoline stations have prices between $1.13 and $3.20?',\n",
       "  'The z-score of $1.13 is ',\n",
       "  '$1.13',\n",
       "  ' ',\n",
       "  '−',\n",
       "  ' ',\n",
       "  '$2.00',\n",
       "  ' = −1.74, and the z-score of $3.20 is $0.50',\n",
       "  '',\n",
       "  '$3.20',\n",
       "  ' ',\n",
       "  '−',\n",
       "  ' ',\n",
       "  '$2.00',\n",
       "  ' = 2.40. The area between -1.74 and 2.40 is equal to the area be-$0.50',\n",
       "  '',\n",
       "  'tween -1.74 and 0 plus the area between 0 and 2.40, which is 0.4591 + 0.4918 = 0.9509 ≈ 95.1%.',\n",
       "  'What is the probability that a randomly chosen gas station will have prices greater than $3.15 per gallon?',\n",
       "  'The z score of $3.15 is ',\n",
       "  '$3.15',\n",
       "  ' ',\n",
       "  '−',\n",
       "  ' ',\n",
       "  '$2.00',\n",
       "  ' = 2.30. We want to find the probability $0.50',\n",
       "  '',\n",
       "  'P (greater than 2.30). But how can we do this? 0 isn’t greater than 2.30, so it appears that we can’t use the probabilities from the above table.',\n",
       "  'Appearances can be deceiving. First, note that, in terms of z-scores, P (between 0 and 2.30) + P (greater than 2.30) = P (greater than 0). Thus, P (greater than 2.30) = P (greater than 0) - P (between 0 and 2.30).',\n",
       "  'The probability that the gas station will be between 0 and 2.30 is 0.4893, and the probability that the gas station will be greater than 0 is 0.5, so the probability that the gas station will be greater than 2.30 is 0.5 − 0.4893 = 0.0107',\n",
       "  '——————————–',\n",
       "  '28',\n",
       "  '4.3.3\\tWorking Backwards',\n",
       "  'In the previous examples, we found percentages and probabilities given raw data. We can work in the reverse direction just as easily. For example, suppose we wanted to know what ',\n",
       "  'z',\n",
       "  '-score 90% of a normal distribution was greater than. Thus, we want 10% to be less than some score ',\n",
       "  'z',\n",
       "  ', or 40% to be between ',\n",
       "  'z',\n",
       "  ' and 0. Looking at the table, we see that a ',\n",
       "  'z',\n",
       "  '-score of 1.28 corresponds to 0.3997, which is very close to 0.4, so ',\n",
       "  'z',\n",
       "  ' is -1.28. Thus, 90% of the normal distribution has a ',\n",
       "  'z',\n",
       "  ' score greater than -1.28. Working similarly, we obtain the following facts:',\n",
       "  '90% of the normal distribution is between ',\n",
       "  'z',\n",
       "  '-scores of -1.645 and 1.645.',\n",
       "  '95% of the normal distribution is between ',\n",
       "  'z',\n",
       "  '-scores of -1.96 and 1.96.',\n",
       "  '99% of the normal distribution is between ',\n",
       "  'z',\n",
       "  '-scores of -2.58 and 2.58.',\n",
       "  '90% of the normal distribution is less than the ',\n",
       "  'z',\n",
       "  '-score of 1.28, and 90% of the normal distribution is greater than the ',\n",
       "  'z',\n",
       "  '-score of -1.28.',\n",
       "  '95% of the normal distribution is less than the ',\n",
       "  'z',\n",
       "  '-score of 1.645, and 95% of the normal distribution is greater than the ',\n",
       "  'z',\n",
       "  '-score of -1.645.',\n",
       "  '99% of the normal distribution is less than the ',\n",
       "  'z',\n",
       "  '-score of 2.33, and 99% of the normal distribution is greater than the ',\n",
       "  'z',\n",
       "  '-score of -2.33.',\n",
       "  '68.26% of the normal distribution is between ',\n",
       "  'z',\n",
       "  '-scores of -1 and 1.',\n",
       "  '95.44% of the normal distribution is between ',\n",
       "  'z',\n",
       "  '-scores of -2 and 2.',\n",
       "  '99.74% of the normal distribution is between ',\n",
       "  'z',\n",
       "  '-scores of -3 and 3.',\n",
       "  'If we do not know the mean or standard deviation, we can also work backward to find it.',\n",
       "  '——————————–',\n",
       "  'Example 19:',\n",
       "  'A normal distribution has a mean of 36, and 19% of the values are above 50. What is the standard deviation?',\n",
       "  'Since 0.19 of the distribution is above 50, 0.31 of the distribution is between 36 (the mean) and 50. Looking at the table of normal curve areas, 0.31 corresponds to a ',\n",
       "  'z',\n",
       "  '-score of 0.88, so 50 is 0.88 standard deviations above the mean. Thus, 50 = 36 + 0',\n",
       "  '.',\n",
       "  '88',\n",
       "  'σ',\n",
       "  ', or 0',\n",
       "  '.',\n",
       "  '88',\n",
       "  'σ',\n",
       "  ' = 14. Therefore ',\n",
       "  'σ',\n",
       "  ' = 15',\n",
       "  '.',\n",
       "  '91.',\n",
       "  '——————————–',\n",
       "  '29',\n",
       "  '4.3.4\\tAs an Approximation to the Binomial',\n",
       "  'The normal may also be viewed as a limiting case to the binomial, so we may use it to approximate the value of a binomial for large ',\n",
       "  'n',\n",
       "  '. However, because the binomial only takes values at integers, whereas the normal is a continuous curve, we will represent an integer value with a unit-long interval centered at that integer. (For example, 4 would be represented by the interval from 3.5 to 4.5.)',\n",
       "  '——————————–',\n",
       "  'Example 20:',\n",
       "  'Shaquille O’Neal averages 0.627 on free throws. What is the probability that out of 100 attempts, he will have made exactly 70 of them?',\n",
       "  'Then, recalling that we will represent 70 with the interval from 69.5 to 70.5, we',\n",
       "  'calculate some ',\n",
       "  'z',\n",
       "  '-scores. The ',\n",
       "  'z',\n",
       "  '-score of 69.5 is ',\n",
       "  '69.5−62.7',\n",
       "  ' = 1',\n",
       "  '.',\n",
       "  '405, and the ',\n",
       "  'z',\n",
       "  '-score',\n",
       "  '4.84',\n",
       "  'of 70.5 is ',\n",
       "  '70.5−62.7',\n",
       "  ' = 1',\n",
       "  '.',\n",
       "  '612. The area from 0 to 1.612 is 0.4463 and the area',\n",
       "  '4.84',\n",
       "  'from 0 to 1.405 is 0.4207, so the final probability is 0',\n",
       "  '.',\n",
       "  '4463 − 0',\n",
       "  '.',\n",
       "  '4207 = 0',\n",
       "  '.',\n",
       "  '0256.',\n",
       "  '——————————–',\n",
       "  'In general, the normal is considered a “good” approximation when both ',\n",
       "  'np',\n",
       "  ' and ',\n",
       "  'n',\n",
       "  '(1',\n",
       "  ' ',\n",
       "  '−',\n",
       "  ' p',\n",
       "  ') are greater than 5.',\n",
       "  '30',\n",
       "  'Chapter 5',\n",
       "  'The Population Mean',\n",
       "  'Oftentimes it is impossible or impractical to survey an entire population. For example, a manufacturer cannot test every battery, or it wouldn’t have any to sell. Instead, a sample must be taken and tested. This gives birth to many questions: what size sample should be taken to be accurate? How accurate is accurate? How can we ensure that a sample is representative of a population? What conclusions can we draw about the population using the sample? And so on. In this section, we’ll discuss samples and answer some of these questions.',\n",
       "  '5.1\\tThe Distribution of Sample Means',\n",
       "  'As mentioned before, we want to estimate a population’s mean by surveying a small sample. If the sample is very small, say it contains one member, then the mean of the sample is unlikely to be a good estimate of the population. As we increase the number of members, the estimate will improve. Thus, bigger sample size generally results in a sample mean that is closer to the population mean.',\n",
       "  'Similarly, if we survey individual members of a population, their values are unlikely to be normally distributed - individuals can easily throw things oﬀ with widely varying values. However, if we take several samples, then the sample means are likely to be normally distributed, because the individuals in each sample will generally balance each other out.',\n",
       "  'Theorem 6: Central Limit Theorem',\n",
       "  'Start with a population with a given mean ',\n",
       "  'µ',\n",
       "  ' and standard deviation',\n",
       "  'σ',\n",
       "  '. Take samples of size',\n",
       "  ' n',\n",
       "  ', where',\n",
       "  ' n ',\n",
       "  'is a suﬃciently large (generally',\n",
       "  ' ',\n",
       "  'at least 30) number, and compute the mean of each sample.',\n",
       "  'The set of all sample means will be approximately normally distributed.',\n",
       "  '31',\n",
       "  '• The mean of the set of samples will equal ',\n",
       "  'µ',\n",
       "  ', the mean of the population.',\n",
       "  '• The standard deviation, ',\n",
       "  'σ',\n",
       "  'x',\n",
       "  ', of the set of sample means will be',\n",
       "  '',\n",
       "  'σ',\n",
       "  'approximately √',\n",
       "  'n',\n",
       "  ' .',\n",
       "  '',\n",
       "  '——————————–',\n",
       "  'Example 21:',\n",
       "  'Foodland shoppers have a mean $60 grocery bill with a standard deviation of $40. What is the probability that a sample of 100 Foodland shoppers will have a mean grocery bill of over $70?',\n",
       "  'Since the sample size is greater than 30, we can apply the Central Limit Theo-rem. By this theorem, the set of sample means of size 100 has mean $60 and stan-',\n",
       "  '',\n",
       "  'Since the set of sample means of size 100 is normally distributed, we can com-pare a ',\n",
       "  'z',\n",
       "  '-score of 2.5 to the table of normal curve areas. The area between ',\n",
       "  'z',\n",
       "  ' = 0 and ',\n",
       "  'z',\n",
       "  ' = 2',\n",
       "  '.',\n",
       "  '5 is 0.4938, so the probability is 0.5 - 0.4938 = 0.0062.',\n",
       "  '——————————–',\n",
       "  '5.2\\tConfidence Interval Estimatess',\n",
       "  'We can find the probability that a sample lies within a certain interval of the population mean by using the central limit theorem and the table or normal curve areas. But this is the same as the probability that the population mean lies within a certain interval of a sample. Thus, we can determine how con-fident we are that the population mean lies within a certain interval of a sample mean.',\n",
       "  '——————————–',\n",
       "  'Example 22:',\n",
       "  'At a factory, batteries are produced with a standard deviation of 2.4 months. In a sample of 64 batteries, the mean life expectancy is 12.35. Find a 95% confidence interval estimate for the life expectancy of all batteries produced at the plant.',\n",
       "  'Since the sample has ',\n",
       "  'n',\n",
       "  ' larger than 30, the central limit theorem applies. Let',\n",
       "  'the standard deviation of the set of sample means of size 64 be ',\n",
       "  'σ',\n",
       "  'x',\n",
       "  '. Then by the ',\n",
       "  'σ',\n",
       "  'x',\n",
       "  '',\n",
       "  'central limit theorem, 2',\n",
       "  '.',\n",
       "  '4 = √',\n",
       "  '\\t',\n",
       "  ', so ',\n",
       "  'σ',\n",
       "  'x',\n",
       "  ' = 0',\n",
       "  '.',\n",
       "  '3 months.',\n",
       "  '',\n",
       "  '64',\n",
       "  '32',\n",
       "  'Looking at the table of normal curve areas (or referring to section ',\n",
       "  '), 95% of the normal curve area is between the z-scores of -1.96 and 1.96. Since the standard deviation is 0.3, a z-score of ',\n",
       "  '−',\n",
       "  '1.96 represents a raw score of -0.588 months, and a z-score of 1.96 represents a raw score of 0.588 months. So we have 95% confidence that the life expectancy will be between 12.35 ',\n",
       "  '−',\n",
       "  ' 0.588 = 11.762 months and 12.35 + 0.588 = 12.938 months.',\n",
       "  '——————————–',\n",
       "  'If we do not know σ (the standard deviation of the entire population), we use s (the standard deviation of the sample) as an estimate for σ. Recall that s is defined as',\n",
       "  '',\n",
       "  'where x takes on each individual value, x is the sample mean, and n is the sample size.',\n",
       "  '',\n",
       "  '5.3\\tChoosing a Sample Size',\n",
       "  'Note that as the degree of confidence increases, the interval must become larger; conversely, as the degree of confidence decreases the interval becomes more precise. This is true in general; if we want to be more sure that we are right, we sacrifice precision, and if we want to be closer to the actual value, we are less likely to be right.',\n",
       "  'There is a way to improve both the degree of confidence and the precision of the interval: by increasing the sample size. So it seems like greater sample size is always desirable; however, in the real world, increasing the sample size costs time and money.',\n",
       "  'Generally, we will be asked to find the minimum sample size that will result in a desired confidence level and range.',\n",
       "  '——————————–',\n",
       "  'Example 23:',\n",
       "  'A machine fills plastic bottles with Mountain Dew brand soda with a standard deviation of 0.04 L. How many filled bottles should be tested to determine the mean volume of soda to an accuracy of 0.01 L with 95% confidence?',\n",
       "  'σ',\n",
       "  'Let σ',\n",
       "  'x',\n",
       "  '  be the standard deviation of the sample.  Since σ',\n",
       "  'x',\n",
       "  '  = ',\n",
       "  '√',\n",
       "  'n',\n",
       "  ' , we have',\n",
       "  '',\n",
       "  '0.04L',\n",
       "  'σ',\n",
       "  'x',\n",
       "  ' =',\n",
       "  '\\t',\n",
       "  '√',\n",
       "  'n',\n",
       "  ' ',\n",
       "  '. Also, since 95% confidence corresponds to',\n",
       "  ' ',\n",
       "  'z-scores from -1.96 to',\n",
       "  '',\n",
       "  '33',\n",
       "  '1.96 on the normal curve, we have 0',\n",
       "  '.',\n",
       "  '01',\n",
       "  'L',\n",
       "  ' = ',\n",
       "  'σ',\n",
       "  'x',\n",
       "  ...],\n",
       " 'font_sizes': [317500,\n",
       "  317500,\n",
       "  215900,\n",
       "  152400,\n",
       "  152400,\n",
       "  317500,\n",
       "  107950,\n",
       "  107950,\n",
       "  260350,\n",
       "  317500,\n",
       "  184150,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  184150,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  165100,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  76200,\n",
       "  196850,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  114300,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  222250,\n",
       "  82550,\n",
       "  114300,\n",
       "  127000,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  165100,\n",
       "  165100,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  120650,\n",
       "  165100,\n",
       "  120650,\n",
       "  63500,\n",
       "  82550,\n",
       "  101600,\n",
       "  82550,\n",
       "  63500,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  260350,\n",
       "  317500,\n",
       "  184150,\n",
       "  152400,\n",
       "  127000,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  107950,\n",
       "  260350,\n",
       "  317500,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  184150,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  114300,\n",
       "  152400,\n",
       "  114300,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  114300,\n",
       "  152400,\n",
       "  114300,\n",
       "  114300,\n",
       "  114300,\n",
       "  152400,\n",
       "  114300,\n",
       "  114300,\n",
       "  114300,\n",
       "  152400,\n",
       "  114300,\n",
       "  114300,\n",
       "  114300,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  152400,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  107950,\n",
       "  260350,\n",
       "  317500,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  107950,\n",
       "  146050,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  107950,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  184150,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  171450,\n",
       "  171450,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  184150,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  107950,\n",
       "  127000,\n",
       "  152400,\n",
       "  127000,\n",
       "  107950,\n",
       "  95250,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  101600,\n",
       "  177800,\n",
       "  101600,\n",
       "  177800,\n",
       "  101600,\n",
       "  177800,\n",
       "  101600,\n",
       "  95250,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  95250,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  95250,\n",
       "  215900,\n",
       "  114300,\n",
       "  215900,\n",
       "  114300,\n",
       "  215900,\n",
       "  114300,\n",
       "  95250,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  241300,\n",
       "  127000,\n",
       "  241300,\n",
       "  127000,\n",
       "  241300,\n",
       "  127000,\n",
       "  95250,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  127000,\n",
       "  107950,\n",
       "  152400,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  101600,\n",
       "  101600,\n",
       "  101600,\n",
       "  101600,\n",
       "  101600,\n",
       "  133350,\n",
       "  101600,\n",
       "  101600,\n",
       "  101600,\n",
       "  101600,\n",
       "  101600,\n",
       "  63500,\n",
       "  82550,\n",
       "  101600,\n",
       "  82550,\n",
       "  82550,\n",
       "  82550,\n",
       "  82550,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  260350,\n",
       "  317500,\n",
       "  120650,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  82550,\n",
       "  127000,\n",
       "  127000,\n",
       "  95250,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  114300,\n",
       "  127000,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  127000,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  114300,\n",
       "  114300,\n",
       "  76200,\n",
       "  114300,\n",
       "  114300,\n",
       "  152400,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  114300,\n",
       "  114300,\n",
       "  152400,\n",
       "  114300,\n",
       "  114300,\n",
       "  114300,\n",
       "  127000,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  184150,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  120650,\n",
       "  120650,\n",
       "  165100,\n",
       "  120650,\n",
       "  165100,\n",
       "  120650,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  127000,\n",
       "  107950,\n",
       "  107950,\n",
       "  146050,\n",
       "  107950,\n",
       "  127000,\n",
       "  120650,\n",
       "  234950,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  120650,\n",
       "  127000,\n",
       "  107950,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  127000,\n",
       "  171450,\n",
       "  ...]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vikush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_font = mode(viki['font_sizes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_titles = [txt for txt,sz in zip(viki['full_text'],viki['font_sizes']) if sz > most_common_font]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An Introduction to the Science of Statistics:',\n",
       " 'From Theory to Implementation',\n",
       " 'Preliminary Edition',\n",
       " 'c ',\n",
       " 'Joseph C. Watkins',\n",
       " 'Contents',\n",
       " 'Preface',\n",
       " 'Who Should Use this Book',\n",
       " 'An Annotated Syllabus',\n",
       " 'Organizing and Collecting Data',\n",
       " 'Introduction to Probability',\n",
       " 'Estimation',\n",
       " 'Hypothesis Testing',\n",
       " 'Exercises and Problems',\n",
       " 'Acknowledgements',\n",
       " 'Part I',\n",
       " 'Organizing and Producing Data',\n",
       " 'Topic 1',\n",
       " 'Displaying Data',\n",
       " '1.1',\n",
       " 'Types of Data',\n",
       " '1.2',\n",
       " 'Categorical Data',\n",
       " '1.2.1',\n",
       " 'Pie Chart',\n",
       " '*',\n",
       " 'Proportion of AIDS Cases among Males by Transmission Category Diagnosed − USA, 2005',\n",
       " '16%',\n",
       " '1.2.2',\n",
       " 'Bar Charts',\n",
       " '1.3',\n",
       " 'Two-way Tables',\n",
       " '1.4',\n",
       " 'Histograms and the Empirical Cumulative Distribution Function',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " '1',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " '1.5',\n",
       " 'Scatterplots',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'Bone Lengths for Archeopteryx',\n",
       " 'Income vs. Assets (in billions of dollars)',\n",
       " '1.6',\n",
       " 'Time Plots',\n",
       " 'Worldwide Oil Production',\n",
       " '1.7',\n",
       " 'Answers to Selected Exercises',\n",
       " 'Topic 2',\n",
       " 'Describing Distributions with Numbers',\n",
       " '2.1',\n",
       " 'Measuring Center',\n",
       " '2.1.1',\n",
       " 'Medians',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(n)',\n",
       " '(k)',\n",
       " '((n+1)=2)',\n",
       " '2',\n",
       " '(x',\n",
       " '+',\n",
       " 'x',\n",
       " '):',\n",
       " '2.1.2',\n",
       " 'Means',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'n+1',\n",
       " 'n+1',\n",
       " 'n+1',\n",
       " 'n+k',\n",
       " 'i',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'n',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'j',\n",
       " 'j',\n",
       " 'i',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " '2.2',\n",
       " 'Measuring Spread',\n",
       " '2.2.1',\n",
       " 'Five Number Summary',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '3',\n",
       " '2',\n",
       " '1',\n",
       " '3',\n",
       " '2',\n",
       " '3',\n",
       " '1',\n",
       " '2.2.2',\n",
       " 'Sample Variance and Standard Deviation',\n",
       " 'x',\n",
       " '2',\n",
       " 'x',\n",
       " 'x',\n",
       " 'x',\n",
       " 's',\n",
       " 'x',\n",
       " 'x',\n",
       " 'x',\n",
       " 'n',\n",
       " 'i',\n",
       " '2',\n",
       " '1',\n",
       " '20',\n",
       " 'P',\n",
       " 'n',\n",
       " 'i=1',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'i',\n",
       " '2',\n",
       " '2',\n",
       " 'i',\n",
       " '2',\n",
       " '2',\n",
       " 'x',\n",
       " '=',\n",
       " 'n',\n",
       " 'x',\n",
       " '+',\n",
       " 'n',\n",
       " 'x',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '2.3',\n",
       " 'Quantiles and Standardized Variables',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'i',\n",
       " '=',\n",
       " 'x',\n",
       " 'x ',\n",
       " ':',\n",
       " 'x',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '2.4',\n",
       " 'Quantile-Quantile Plots',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '2.5',\n",
       " 'Answers to Selected Exercises',\n",
       " 'n',\n",
       " '+ 1',\n",
       " 'n',\n",
       " '+ 1',\n",
       " 'x',\n",
       " '=',\n",
       " 'k',\n",
       " '(x',\n",
       " '+',\n",
       " '+ ',\n",
       " 'x',\n",
       " '):',\n",
       " 'n',\n",
       " '+',\n",
       " 'k',\n",
       " 'n',\n",
       " '+',\n",
       " 'k',\n",
       " '3',\n",
       " '3',\n",
       " '1;1',\n",
       " '1;2',\n",
       " '1;n',\n",
       " '2;1',\n",
       " '2;2',\n",
       " '2;n',\n",
       " '0',\n",
       " '00',\n",
       " '00',\n",
       " '1',\n",
       " 'Topic 3',\n",
       " 'Correlation and Regression',\n",
       " '3.1',\n",
       " 'Covariance and Correlation',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " 'x',\n",
       " 'r=0.9',\n",
       " 'r=0.7',\n",
       " 'r=0.3',\n",
       " 'r=−0.8',\n",
       " '*',\n",
       " '*',\n",
       " '3.2',\n",
       " 'Linear Regression',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '\\\\',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " 'DAT A',\n",
       " '2',\n",
       " 'F IT',\n",
       " '2',\n",
       " 'RESID',\n",
       " '2',\n",
       " 'n',\n",
       " 'P',\n",
       " ':',\n",
       " '\\\\',\n",
       " '2',\n",
       " '3.2.1',\n",
       " 'Transformed Variables',\n",
       " '1',\n",
       " '1',\n",
       " 'n',\n",
       " 'n',\n",
       " '5',\n",
       " '1',\n",
       " 'dy',\n",
       " 'dt',\n",
       " '0',\n",
       " 'kt',\n",
       " '0',\n",
       " '0',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " 'd',\n",
       " '[',\n",
       " 'ES',\n",
       " 'dt',\n",
       " ']',\n",
       " 'm',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " 'max',\n",
       " '2',\n",
       " '0',\n",
       " 'V',\n",
       " '1',\n",
       " '[S',\n",
       " '1',\n",
       " ']',\n",
       " 'V',\n",
       " '1',\n",
       " '[S',\n",
       " '1',\n",
       " ']',\n",
       " 'max',\n",
       " 'm',\n",
       " '3.3',\n",
       " 'Extensions',\n",
       " '0',\n",
       " 'k',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '3.3.1',\n",
       " 'Nonlinear Regression',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '^',\n",
       " 'i',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " 'max',\n",
       " 'm',\n",
       " '*',\n",
       " '*',\n",
       " 'max',\n",
       " 'm',\n",
       " '3.3.2',\n",
       " 'Multiple Linear Regression',\n",
       " 'ij',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'X',\n",
       " 'c',\n",
       " 'ij',\n",
       " 'ik',\n",
       " 'kj',\n",
       " 'jj',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " 'T',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'T',\n",
       " 'n1',\n",
       " 'nk',\n",
       " '0',\n",
       " '1',\n",
       " 'k',\n",
       " 'T',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'T',\n",
       " 'T',\n",
       " 'x',\n",
       " '2',\n",
       " 'i',\n",
       " 'T',\n",
       " 'i=1',\n",
       " 'n',\n",
       " 'ij',\n",
       " 'j',\n",
       " 'i',\n",
       " 'i',\n",
       " '0',\n",
       " '1',\n",
       " 'i',\n",
       " '2',\n",
       " '2',\n",
       " 'i',\n",
       " 'k',\n",
       " 'k',\n",
       " 'i',\n",
       " 'i',\n",
       " '*',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " 'uspopulation',\n",
       " '= 3863670  10',\n",
       " '0:0147(year',\n",
       " '1790)  0:00002808(year',\n",
       " '1790)',\n",
       " '3.4',\n",
       " 'Answers to Selected Exercises',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " 'x',\n",
       " '2',\n",
       " 'y',\n",
       " '2',\n",
       " 'x',\n",
       " 'x',\n",
       " 'y',\n",
       " '2',\n",
       " 'y',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " 'x',\n",
       " 'y',\n",
       " '2',\n",
       " 'x+y',\n",
       " '2',\n",
       " 'x+y',\n",
       " 'x',\n",
       " 'i',\n",
       " 's',\n",
       " 'x',\n",
       " 'x',\n",
       " 'y',\n",
       " 'i',\n",
       " 's',\n",
       " 'y',\n",
       " 'y',\n",
       " 'y',\n",
       " 'x',\n",
       " '2',\n",
       " 'x  y',\n",
       " '^',\n",
       " 'y',\n",
       " 'y',\n",
       " 'y',\n",
       " 'y',\n",
       " 'i',\n",
       " 'i',\n",
       " '6',\n",
       " '5',\n",
       " '*',\n",
       " '*',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '0',\n",
       " 'Topic 4',\n",
       " 'Producing Data',\n",
       " '4.1',\n",
       " 'Preliminary Steps',\n",
       " '4.2',\n",
       " 'Professional Ethics',\n",
       " '4.3',\n",
       " 'Formal Statistical Procedures',\n",
       " '4.3.1',\n",
       " 'Observational Studies',\n",
       " '4.3.2',\n",
       " 'Randomized Controlled Experiments',\n",
       " 'Factor B: hive temperature',\n",
       " 'Factor A:',\n",
       " 'genotype',\n",
       " '4.3.3',\n",
       " 'Natural experiments',\n",
       " '4.4',\n",
       " 'Case Studies',\n",
       " '4.4.1',\n",
       " 'Observational Studies',\n",
       " '4.4.2',\n",
       " 'Experiments',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " 'Part II',\n",
       " 'Probability',\n",
       " 'Topic 5',\n",
       " 'The Basics of Probability',\n",
       " '5.1\\tIntroduction',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '5.2',\n",
       " 'Equally Likely Outcomes and the Axioms of Probability',\n",
       " '#(',\n",
       " '#( )',\n",
       " 'A)',\n",
       " '16',\n",
       " '5',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'j',\n",
       " 'j',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'j',\n",
       " 'j',\n",
       " '1',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'j',\n",
       " 'j',\n",
       " 'j',\n",
       " '5.3',\n",
       " 'Consequences of the Axioms',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " '1',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " '1',\n",
       " 'c',\n",
       " 'i',\n",
       " 'c',\n",
       " '5.4',\n",
       " 'Counting',\n",
       " '5.4.1',\n",
       " 'Fundamental Principle of Counting',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " 'k',\n",
       " '(k)',\n",
       " '5.4.2',\n",
       " 'Permutations',\n",
       " 'k',\n",
       " 'k',\n",
       " '*',\n",
       " 'n',\n",
       " '(n',\n",
       " 'k',\n",
       " ')!',\n",
       " ' ',\n",
       " ':',\n",
       " '5.4.3',\n",
       " 'Combinations',\n",
       " 'n',\n",
       " 'k',\n",
       " '3',\n",
       " 'k',\n",
       " '8',\n",
       " '3',\n",
       " '23',\n",
       " '*',\n",
       " '5.5',\n",
       " 'Answers to Selected Exercises',\n",
       " '42',\n",
       " '64',\n",
       " '21',\n",
       " '32',\n",
       " 'c',\n",
       " 'c',\n",
       " 'n',\n",
       " '[',\n",
       " '!',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " '2',\n",
       " 'k',\n",
       " 'n',\n",
       " 'n',\n",
       " 'x',\n",
       " '52',\n",
       " '5',\n",
       " 'x',\n",
       " '*',\n",
       " '*',\n",
       " 'Topic 6',\n",
       " 'Conditional Probability and Independence',\n",
       " '6.1',\n",
       " 'Restricting the Sample Space - Conditional Probability',\n",
       " '0.2',\n",
       " '−0.6',\n",
       " 'P',\n",
       " '(AjB) =',\n",
       " '=',\n",
       " '6.2\\tThe Multiplication Principle',\n",
       " '51',\n",
       " '3',\n",
       " '52',\n",
       " '4',\n",
       " '17',\n",
       " '1',\n",
       " '13',\n",
       " '1',\n",
       " '50',\n",
       " '2',\n",
       " '51',\n",
       " '3',\n",
       " '52',\n",
       " '4',\n",
       " '25',\n",
       " '1',\n",
       " '17',\n",
       " '1',\n",
       " '13',\n",
       " '1',\n",
       " '4',\n",
       " '2',\n",
       " ':',\n",
       " '4',\n",
       " 'b',\n",
       " '\\t',\n",
       " 'g',\n",
       " '2',\n",
       " 'b+g',\n",
       " '2',\n",
       " '6.3',\n",
       " 'The Law of Total Probability',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'C',\n",
       " 'c',\n",
       " 'ij',\n",
       " 'ii',\n",
       " '1',\n",
       " '2',\n",
       " '6.4',\n",
       " 'Bayes formula',\n",
       " ':',\n",
       " '= 30:',\n",
       " 'c',\n",
       " 'c',\n",
       " 'P',\n",
       " '(AjC)P',\n",
       " '(C)',\n",
       " '0:0009',\n",
       " 'c',\n",
       " 'j',\n",
       " 'n',\n",
       " '36',\n",
       " '1',\n",
       " '6',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " '1',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " 'j',\n",
       " 'j',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '= 0',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '= 1',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " '2',\n",
       " ':',\n",
       " 'ij',\n",
       " '1',\n",
       " '2',\n",
       " 'ij',\n",
       " 'ij',\n",
       " 'ij',\n",
       " '2',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'i',\n",
       " '1',\n",
       " 'c',\n",
       " '3',\n",
       " '7',\n",
       " '9',\n",
       " '1',\n",
       " 'c',\n",
       " '3',\n",
       " '7',\n",
       " '9',\n",
       " '1',\n",
       " 'c',\n",
       " '3',\n",
       " '7',\n",
       " '9',\n",
       " '1',\n",
       " '3',\n",
       " '7',\n",
       " '9',\n",
       " 'P',\n",
       " '(BjA) =',\n",
       " '=',\n",
       " ':',\n",
       " 'P',\n",
       " '(BjA)P',\n",
       " '(A)',\n",
       " 'c',\n",
       " 'c',\n",
       " 'Topic 7',\n",
       " 'Random Variables and Distribution Functions',\n",
       " '7.1\\tIntroduction',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '7.2',\n",
       " 'Distribution Functions',\n",
       " 'X',\n",
       " 'X',\n",
       " 'c',\n",
       " 'c',\n",
       " '1',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'X',\n",
       " 'X',\n",
       " ' ',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " '3',\n",
       " 'X',\n",
       " '7.3',\n",
       " 'Properties of the Distribution Function',\n",
       " 'X',\n",
       " 'X',\n",
       " 'x!',\n",
       " 'X',\n",
       " 'x!1',\n",
       " 'X',\n",
       " 'X',\n",
       " '7.3.1',\n",
       " 'Discrete Random Variables',\n",
       " 'X',\n",
       " 'X',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " 'X',\n",
       " '0',\n",
       " 'X',\n",
       " '0',\n",
       " 'X',\n",
       " '0',\n",
       " '7.3.2',\n",
       " 'Continuous Random Variables',\n",
       " 'X',\n",
       " 'F',\n",
       " '2',\n",
       " '=',\n",
       " '4',\n",
       " 'X',\n",
       " '*',\n",
       " 'X',\n",
       " 'X',\n",
       " 'x',\n",
       " '7.4',\n",
       " 'Mass Functions',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'x',\n",
       " 'X',\n",
       " 'x',\n",
       " 'X',\n",
       " 'x',\n",
       " 'X',\n",
       " 'x',\n",
       " '2',\n",
       " 'n',\n",
       " 'n!1',\n",
       " 'n',\n",
       " 'n',\n",
       " 'b+1',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " '0',\n",
       " '7.5',\n",
       " 'Density Functions',\n",
       " 'X',\n",
       " 'X',\n",
       " 'x',\n",
       " 'X',\n",
       " 'X',\n",
       " 'f',\n",
       " 'X',\n",
       " '(x) =',\n",
       " 'lim  ',\n",
       " 'F',\n",
       " ' ',\n",
       " '(x',\n",
       " ' ',\n",
       " '+',\n",
       " '  ',\n",
       " 'x)',\n",
       " '  ',\n",
       " 'F',\n",
       " ' ',\n",
       " '(x)',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'R',\n",
       " '1',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'b',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " '0',\n",
       " '0',\n",
       " 'f',\n",
       " '(x) =',\n",
       " 'e',\n",
       " 'if ',\n",
       " 'x >',\n",
       " ' ',\n",
       " '0:',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'X',\n",
       " 'Y',\n",
       " '7.6\\tMixtures',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " 'X',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " 'n',\n",
       " '1',\n",
       " 'n',\n",
       " '1',\n",
       " 'n',\n",
       " 'P',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_font_first_five = mode(vikush['font_sizes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An Introduction to',\n",
       " 'Statistics',\n",
       " 'Keone Hon',\n",
       " '<',\n",
       " '>',\n",
       " 'Contents',\n",
       " 'Chapter 1',\n",
       " 'Descriptive Statistics',\n",
       " '1.1\\tDescriptive vs. Inferential',\n",
       " '1.2\\tMeans, Medians, and Modes',\n",
       " '1.3\\tVariability',\n",
       " '2',\n",
       " '2',\n",
       " '1.4\\tLinear Transformations',\n",
       " 't',\n",
       " 't',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'a',\n",
       " '+',\n",
       " 'a',\n",
       " '+',\n",
       " '· · ·',\n",
       " '+',\n",
       " 'a',\n",
       " '+',\n",
       " 'cn',\n",
       " '=',\n",
       " 'µ',\n",
       " '+',\n",
       " 'c',\n",
       " 't',\n",
       " 'a',\n",
       " '+',\n",
       " 'a',\n",
       " '+',\n",
       " '+',\n",
       " 'a',\n",
       " '·',\n",
       " 'c',\n",
       " '=',\n",
       " 'µ',\n",
       " '·',\n",
       " 'c',\n",
       " '1.5\\tPosition',\n",
       " '1.6\\tDispersion Percentages',\n",
       " '1',\n",
       " '6',\n",
       " 'k',\n",
       " '1',\n",
       " '15−20.6',\n",
       " '1.75',\n",
       " '1',\n",
       " 'Chapter 2',\n",
       " 'Graphs and Displays',\n",
       " '2.1\\tHistograms',\n",
       " '2.1.1\\tIntroduction',\n",
       " '21',\n",
       " 'x',\n",
       " '2.1.2\\tMedians, Modes, and Means Revisited',\n",
       " '2.1.3\\tz-Scores and Percentile Ranks Revisited',\n",
       " '2.2\\tStem and Leaf Displays',\n",
       " '2.3\\tFive Number Summaries and Box and Whisker Displays',\n",
       " 'Chapter 3',\n",
       " 'Probability',\n",
       " '3.1\\tIntroduction',\n",
       " '0',\n",
       " 'n',\n",
       " 'k',\n",
       " '3.2\\tRandom Variables',\n",
       " '3.2.1\\tDefinition',\n",
       " '1',\n",
       " '5',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '6',\n",
       " '20',\n",
       " '1',\n",
       " '1',\n",
       " '3',\n",
       " '3.2.2\\tExpected Value',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '3.2.3\\tVariance and Standard Deviation',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '3.2.4\\t“Shortcuts” for Binomial Random Variables',\n",
       " '2',\n",
       " 'Chapter 4',\n",
       " 'Probability Distributions',\n",
       " '4.1\\tBinomial Distributions',\n",
       " '2',\n",
       " '4.2\\tPoisson Distributions',\n",
       " '4.2.1\\tDefinition',\n",
       " '4.2.2\\tAs an Approximation to the Binomial',\n",
       " '1',\n",
       " '6',\n",
       " '120',\n",
       " '10',\n",
       " '4.3\\tNormal Distributions',\n",
       " '4.3.1\\tDefinition and Properties',\n",
       " '4.3.2\\tTable of Normal Curve Areas',\n",
       " '$2.75',\n",
       " '−',\n",
       " '$2.00',\n",
       " '$1.75',\n",
       " '−',\n",
       " '$2.00',\n",
       " '$1.13',\n",
       " '−',\n",
       " '$2.00',\n",
       " '$3.20',\n",
       " '−',\n",
       " '$2.00',\n",
       " '$3.15',\n",
       " '−',\n",
       " '$2.00',\n",
       " '4.3.3\\tWorking Backwards',\n",
       " '4.3.4\\tAs an Approximation to the Binomial',\n",
       " '69.5−62.7',\n",
       " 'Chapter 5',\n",
       " 'The Population Mean',\n",
       " '5.1\\tThe Distribution of Sample Means',\n",
       " 'n',\n",
       " '5.2\\tConfidence Interval Estimatess',\n",
       " 'x',\n",
       " 'x',\n",
       " '5.3\\tChoosing a Sample Size',\n",
       " 'x',\n",
       " 'x',\n",
       " 'n',\n",
       " 'x',\n",
       " 'n',\n",
       " 'x',\n",
       " '5.4\\tThe Hypothesis Test',\n",
       " '0',\n",
       " 'a',\n",
       " '◦',\n",
       " '◦',\n",
       " '0',\n",
       " 'a',\n",
       " '◦',\n",
       " '◦',\n",
       " '5.5\\tMore on Errors',\n",
       " '5.5.1\\tType I Errors and ',\n",
       " 'α',\n",
       " '-Risks',\n",
       " '2',\n",
       " '5.5.2\\tType II Errors and ',\n",
       " 'β',\n",
       " '-Risks',\n",
       " '5.6\\tComparing Two Means',\n",
       " '5.6.1\\tConfidence Interval Estimates',\n",
       " '1',\n",
       " '2',\n",
       " 'Index']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_titles_first_five = [txt for txt,sz in zip(vikush['full_text'],vikush['font_sizes']) if sz > most_common_font_first_five]\n",
    "maybe_titles_first_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_bigger(docx_json):\n",
    "    most_common_font_ = mode(docx_json['font_sizes'])\n",
    "    return [txt for txt,sz in zip(docx_json['full_text'],docx_json['font_sizes']) if sz > most_common_font_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
