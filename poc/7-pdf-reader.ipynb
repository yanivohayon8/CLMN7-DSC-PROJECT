{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def findInDict(needle, haystack):\n",
    "    for key in haystack.keys():\n",
    "        try:\n",
    "            value=haystack[key]\n",
    "        except:\n",
    "            continue\n",
    "        if key==needle:\n",
    "            return value\n",
    "        if isinstance(value,dict):            \n",
    "            x=findInDict(needle,value)            \n",
    "            if x is not None:\n",
    "                return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfobject=open(\"C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf\",'rb')\n",
    "pdf=pypdf.PdfFileReader(pdfobject)\n",
    "#xfa=findInDict('/XFA',pdf.resolvedObjects)\n",
    "\n",
    "#pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Creator': ' TeX output 2005.03.30:0907',\n",
       " '/Producer': 'dvipdfm 0.13.2c, Copyright © 1998, by Mark A. Wicks',\n",
       " '/CreationDate': \"D:20050330092217+08'00'\",\n",
       " '/rgid': 'PB:228524191_AS:102309094756357@1401403797531'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getDocumentInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[IndirectObject(101, 0), IndirectObject(102, 0), IndirectObject(103, 0), IndirectObject(104, 0)]\n",
      "March30,20059:7WSPC/LectureNotesSeries:9inx6inheg05a\n",
      "TheAprioriAlgorithm{aTutorial\n",
      "MarkusHegland\n",
      "CMA,AustralianNationalUniversity\n",
      "JohnDedmanBuilding,CanberraACT0200,Australia\n",
      "E-mail:Markus.Hegland@anu.edu.au\n",
      "Associationrulesare\"if-thenrules\"withtwomeasureswhichquantify\n",
      "thesupportandoftheruleforagivendataset.Havingtheir\n",
      "origininmarketbaskedanalysis,associationrulesarenowoneofthe\n",
      "mostpopulartoolsindatamining.Thispopularityistoalargepartdue\n",
      "totheavailabilityoftalgorithms.Theandarguablymost\n",
      "tialalgorithmfortassociationrulediscoveryisApriori.\n",
      "Inthefollowingwewillreviewbasicconceptsofassociationruledis-\n",
      "coveryincludingsupport,theaprioriproperty,constraints\n",
      "andparallelalgorithms.Thecoreconsistsofareviewofthemostim-\n",
      "portantalgorithmsforassociationrulediscovery.Somefamiliaritywith\n",
      "conceptslikepredicates,probability,expectationandrandomvariables\n",
      "isassumed.\n",
      "1.Introduction\n",
      "Largeamountsofdatahavebeencollectedroutinelyinthecourseofday-\n",
      "to-daymanagementinbusiness,administration,banking,thedeliveryof\n",
      "socialandhealthservices,environmentalprotection,securityandinpol-\n",
      "itics.Suchdataisprimarilyusedforaccountingandformanagementof\n",
      "thecustomerbase.Typically,managementdatasetsareverylargeand\n",
      "constantlygrowingandcontainalargenumberofcomplexfeatures.While\n",
      "thesedatasetspropertiesofthemanagedsubjectsandrelations,and\n",
      "arethuspotentiallyofsomeusetotheirowner,theyoftenhaverelatively\n",
      "lowinformationdensity.Onerequiresrobust,simpleandcomputationally\n",
      "ttoolstoextractinformationfromsuchdatasets.Thedevelopment\n",
      "andunderstandingofsuchtoolsisthecorebusinessofdatamining.These\n",
      "toolsarebasedonideasfromcomputerscience,mathematicsandstatistics.\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pdf.getPage(1).getXmpMetadata())\n",
    "print(pdf.getPage(1).getContents())\n",
    "print(pdf.getPage(1).extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getPage(1).getContents()[1].getObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \u001b[1;31m# printer registered in self.type_pprinters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m                     \u001b[1;31m# deferred printer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sorted_for_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36m_enumerate\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_enumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;34m\"\"\"like enumerate, but with an upper limit on the number of items\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "pdf.resolvedObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'generation', 'getObject', 'idnum', 'pdf', 'readFromStream', 'writeToStream']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(pdf.getPage(1).getContents()[0]))\n",
    "pdf.getPage(1).getContents()[0].getObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfobject=open('C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf','rb')\n",
    "pdf=pypdf.PdfFileReader(pdfobject)\n",
    "pdf.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ShellError",
     "evalue": "The command `pdftoppm C:\\Users\\yaniv\\Downloads\\The_Apriori_Algorithm-a_Tutorial.pdf C:\\Users\\yaniv\\AppData\\Local\\Temp\\tmp64ep6yj6\\conv` failed with exit code 127\n------------- stdout -------------\n------------- stderr -------------\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mShellError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8e086a99f4d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtextract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtextract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tesseract'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'eng'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\__init__.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(filename, encoding, extension, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiletype_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, filename, encoding, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# output encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# http://nedbatchelder.com/text/unipain/unipain.html#35\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mbyte_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0municode_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0municode_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\pdf_parser.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_pdfminer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tesseract'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_tesseract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnknownMethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\pdf_parser.py\u001b[0m in \u001b[0;36mextract_tesseract\u001b[1;34m(self, filename, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pdftoppm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;31m# This is equivalent to getting exitcode 127 from sh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 raise exceptions.ShellError(\n\u001b[1;32m---> 91\u001b[1;33m                     \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m127\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m                 )\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mShellError\u001b[0m: The command `pdftoppm C:\\Users\\yaniv\\Downloads\\The_Apriori_Algorithm-a_Tutorial.pdf C:\\Users\\yaniv\\AppData\\Local\\Temp\\tmp64ep6yj6\\conv` failed with exit code 127\n------------- stdout -------------\n------------- stderr -------------\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "textract.process('C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf', method='tesseract',language='eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March30,20059:7WSPC/LectureNotesSeries:9inx6inheg05a\n",
      "TheAprioriAlgorithm{aTutorial\n",
      "MarkusHegland\n",
      "CMA,AustralianNationalUniversity\n",
      "JohnDedmanBuilding,CanberraACT0200,Australia\n",
      "E-mail:Markus.Hegland@anu.edu.au\n",
      "Associationrulesare\"if-thenrules\"withtwomeasureswhichquantify\n",
      "thesupportandoftheruleforagivendataset.Havingtheir\n",
      "origininmarketbaskedanalysis,associationrulesarenowoneofthe\n",
      "mostpopulartoolsindatamining.Thispopularityistoalargepartdue\n",
      "totheavailabilityoftalgorithms.Theandarguablymost\n",
      "tialalgorithmfortassociationrulediscoveryisApriori.\n",
      "Inthefollowingwewillreviewbasicconceptsofassociationruledis-\n",
      "coveryincludingsupport,theaprioriproperty,constraints\n",
      "andparallelalgorithms.Thecoreconsistsofareviewofthemostim-\n",
      "portantalgorithmsforassociationrulediscovery.Somefamiliaritywith\n",
      "conceptslikepredicates,probability,expectationandrandomvariables\n",
      "isassumed.\n",
      "1.Introduction\n",
      "Largeamountsofdatahavebeencollectedroutinelyinthecourseofday-\n",
      "to-daymanagementinbusiness,administration,banking,thedeliveryof\n",
      "socialandhealthservices,environmentalprotection,securityandinpol-\n",
      "itics.Suchdataisprimarilyusedforaccountingandformanagementof\n",
      "thecustomerbase.Typically,managementdatasetsareverylargeand\n",
      "constantlygrowingandcontainalargenumberofcomplexfeatures.While\n",
      "thesedatasetspropertiesofthemanagedsubjectsandrelations,and\n",
      "arethuspotentiallyofsomeusetotheirowner,theyoftenhaverelatively\n",
      "lowinformationdensity.Onerequiresrobust,simpleandcomputationally\n",
      "ttoolstoextractinformationfromsuchdatasets.Thedevelopment\n",
      "andunderstandingofsuchtoolsisthecorebusinessofdatamining.These\n",
      "toolsarebasedonideasfromcomputerscience,mathematicsandstatistics.\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF4\n",
    "import re\n",
    "import io\n",
    "\n",
    "pdfFileObj = open(r'C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf', 'rb')\n",
    "pdfReader = PyPDF4.PdfFileReader(pdfFileObj)\n",
    "pageObj = pdfReader.getPage(1)\n",
    "pages_text = pageObj.extractText()\n",
    "\n",
    "for line in pages_text.split('\\n'):\n",
    "    #if re.match(r\"^PDF\", line):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "rsrcmgr = PDFResourceManager()\n",
    "retstr = StringIO()\n",
    "codec = 'utf-8'  # 'utf16','utf-8'\n",
    "laparams = LAParams()\n",
    "device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "#\"\"    C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf\n",
    "fp = open('C:\\\\Users\\\\yaniv\\\\Downloads\\\\deeplearningbook-prob.pdf', 'rb')\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "password = \"\"\n",
    "maxpages = 0\n",
    "caching = True\n",
    "pagenos = set()\n",
    "for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                              password=password, caching=caching, check_extractable=True):\n",
    "    interpreter.process_page(page)\n",
    "fp.close()\n",
    "device.close()\n",
    "str_ = retstr.getvalue()\n",
    "retstr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "rsrcmgr = PDFResourceManager()\n",
    "retstr = StringIO()\n",
    "codec = 'utf-8'  # 'utf16','utf-8'\n",
    "laparams = LAParams()\n",
    "device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "#\"\"    \n",
    "fp = open('C:\\\\Users\\\\yaniv\\\\Downloads\\\\The_Apriori_Algorithm-a_Tutorial.pdf', 'rb')\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "password = \"\"\n",
    "maxpages = 0\n",
    "caching = True\n",
    "pagenos = set()\n",
    "for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                              password=password, caching=caching, check_extractable=True):\n",
    "    interpreter.process_page(page)\n",
    "fp.close()\n",
    "device.close()\n",
    "apriori = retstr.getvalue()\n",
    "retstr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 3\\n\\nProbability and Information\\nTheory\\n\\nIn this chapter, we describe probability theory and information theory.\\n\\nProbability theory is a mathematical framework for representing uncertain\\nstatements. It provides a means of quantifying uncertainty and axioms for deriving\\nnew uncertain statements. In artiﬁcial intelligence applications, we use probability\\ntheory in two major ways. First, the laws of probability tell us how AI systems\\nshould reason, so we design our algorithms to compute or approximate various\\nexpressions derived using probability theory. Second, we can use probability and\\nstatistics to theoretically analyze the behavior of proposed AI systems.\\n\\nProbability theory is a fundamental tool of many disciplines of science and\\nengineering. We provide this chapter to ensure that readers whose background is\\nprimarily in software engineering with limited exposure to probability theory can\\nunderstand the material in this book.\\n\\nWhile probability theory allows us to make uncertain statements and reason\\nin the presence of uncertainty, information allows us to quantify the amount of\\nuncertainty in a probability distribution.\\n\\nIf you are already familiar with probability theory and information theory,\\nyou may wish to skip all of this chapter except for Sec. 3.14, which describes the\\ngraphs we use to describe structured probabilistic models for machine learning. If\\nyou have absolutely no prior experience with these subjects, this chapter should\\nbe suﬃcient to successfully carry out deep learning research projects, but we do\\nsuggest that you consult an additional resource, such as Jaynes (2003).\\n\\n52\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.1 Why Probability?\\n\\nMany branches of computer science deal mostly with entities that are entirely\\ndeterministic and certain. A programmer can usually safely assume that a CPU will\\nexecute each machine instruction ﬂawlessly. Errors in hardware do occur, but are\\nrare enough that most software applications do not need to be designed to account\\nfor them. Given that many computer scientists and software engineers work in a\\nrelatively clean and certain environment, it can be surprising that machine learning\\nmakes heavy use of probability theory.\\n\\nThis is because machine learning must always deal with uncertain quantities,\\nand sometimes may also need to deal with stochastic (non-deterministic) quantities.\\nUncertainty and stochasticity can arise from many sources. Researchers have made\\ncompelling arguments for quantifying uncertainty using probability since at least\\nthe 1980s. Many of the arguments presented here are summarized from or inspired\\nby Pearl (1988).\\n\\nNearly all activities require some ability to reason in the presence of uncertainty.\\nIn fact, beyond mathematical statements that are true by deﬁnition, it is diﬃcult\\nto think of any proposition that is absolutely true or any event that is absolutely\\nguaranteed to occur.\\n\\nThere are three possible sources of uncertainty:\\n\\n1. Inherent stochasticity in the system being modeled. For example, most\\ninterpretations of quantum mechanics describe the dynamics of subatomic\\nparticles as being probabilistic. We can also create theoretical scenarios that\\nwe postulate to have random dynamics, such as a hypothetical card game\\nwhere we assume that the cards are truly shuﬄed into a random order.\\n\\n2. Incomplete observability. Even deterministic systems can appear stochastic\\nwhen we cannot observe all of the variables that drive the behavior of the\\nsystem. For example, in the Monty Hall problem, a game show contestant is\\nasked to choose between three doors and wins a prize held behind the chosen\\ndoor. Two doors lead to a goat while a third leads to a car. The outcome\\ngiven the contestant’s choice is deterministic, but from the contestant’s point\\nof view, the outcome is uncertain.\\n\\n3. Incomplete modeling. When we use a model that must discard some of\\nthe information we have observed, the discarded information results in\\nuncertainty in the model’s predictions. For example, suppose we build a\\nrobot that can exactly observe the location of every object around it. If the\\n\\n53\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nrobot discretizes space when predicting the future location of these objects,\\nthen the discretization makes the robot immediately become uncertain about\\nthe precise position of objects: each object could be anywhere within the\\ndiscrete cell that it was observed to occupy.\\n\\nIn many cases, it is more practical to use a simple but uncertain rule rather\\nthan a complex but certain one, even if the true rule is deterministic and our\\nmodeling system has the ﬁdelity to accommodate a complex rule. For example, the\\nsimple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a rule\\nof the form, “Birds ﬂy, except for very young birds that have not yet learned to\\nﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\\nincluding the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\\ncommunicate, and after all of this eﬀort is still very brittle and prone to failure.\\n\\nGiven that we need a means of representing and reasoning about uncertainty,\\nit is not immediately obvious that probability theory can provide all of the tools\\nwe want for artiﬁcial intelligence applications. Probability theory was originally\\ndeveloped to analyze the frequencies of events. It is easy to see how probability\\ntheory can be used to study events like drawing a certain hand of cards in a\\ngame of poker. These kinds of events are often repeatable. When we say that\\nan outcome has a probability p of occurring, it means that if we repeated the\\nexperiment (e.g., draw a hand of cards) inﬁnitely many times, then proportion p\\nof the repetitions would result in that outcome. This kind of reasoning does not\\nseem immediately applicable to propositions that are not repeatable. If a doctor\\nanalyzes a patient and says that the patient has a 40% chance of having the ﬂu,\\nthis means something very diﬀerent—we can not make inﬁnitely many replicas of\\nthe patient, nor is there any reason to believe that diﬀerent replicas of the patient\\nwould present with the same symptoms yet have varying underlying conditions. In\\nthe case of the doctor diagnosing the patient, we use probability to represent a\\ndegree of belief, with 1 indicating absolute certainty that the patient has the ﬂu\\nand 0 indicating absolute certainty that the patient does not have the ﬂu. The\\nformer kind of probability, related directly to the rates at which events occur, is\\nknown as frequentist probability, while the latter, related to qualitative levels of\\ncertainty, is known as Bayesian probability.\\n\\nIf we list several properties that we expect common sense reasoning about\\nuncertainty to have, then the only way to satisfy those properties is to treat\\nBayesian probabilities as behaving exactly the same as frequentist probabilities.\\nFor example, if we want to compute the probability that a player will win a poker\\ngame given that she has a certain set of cards, we use exactly the same formulas\\nas when we compute the probability that a patient has a disease given that she\\n\\n54\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nhas certain symptoms. For more details about why a small set of common sense\\nassumptions implies that the same axioms must control both kinds of probability,\\nsee Ramsey (1926).\\n\\nProbability can be seen as the extension of logic to deal with uncertainty. Logic\\nprovides a set of formal rules for determining what propositions are implied to\\nbe true or false given the assumption that some other set of propositions is true\\nor false. Probability theory provides a set of formal rules for determining the\\nlikelihood of a proposition being true given the likelihood of other propositions.\\n\\n3.2 Random Variables\\n\\nA random variable is a variable that can take on diﬀerent values randomly. We\\ntypically denote the random variable itself with a lower case letter in plain typeface,\\nand the values it can take on with lower case script letters. For example, x1 and x2\\nare both possible values that the random variable x can take on. For vector-valued\\nvariables, we would write the random variable as x and one of its values as x. On\\nits own, a random variable is just a description of the states that are possible; it\\nmust be coupled with a probability distribution that speciﬁes how likely each of\\nthese states are.\\n\\nRandom variables may be discrete or continuous. A discrete random variable\\nis one that has a ﬁnite or countably inﬁnite number of states. Note that these\\nstates are not necessarily the integers; they can also just be named states that\\nare not considered to have any numerical value. A continuous random variable is\\nassociated with a real value.\\n\\n3.3 Probability Distributions\\n\\nA probability distribution is a description of how likely a random variable or\\nset of random variables is to take on each of its possible states. The way we\\ndescribe probability distributions depends on whether the variables are discrete or\\ncontinuous.\\n\\n3.3.1 Discrete Variables and Probability Mass Functions\\n\\nA probability distribution over discrete variables may be described using a proba-\\nbility mass function (PMF). We typically denote probability mass functions with a\\ncapital P . Often we associate each random variable with a diﬀerent probability\\n\\n55\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nmass function and the reader must infer which probability mass function to use\\nbased on the identity of the random variable, rather than the name of the function;\\nP\\n\\n( )x is usually not the same as\\n\\n( )y .\\n\\nP\\n\\nThe probability mass function maps from a state of a random variable to\\nthe probability of that random variable taking on that state. The probability\\nthat x = x is denoted as P (x), with a probability of 1 indicating that x = x is\\ncertain and a probability of 0 indicating that x = x is impossible. Sometimes\\nto disambiguate which PMF to use, we write the name of the random variable\\nexplicitly: P (x = x). Sometimes we deﬁne a variable ﬁrst, then use ∼ notation to\\nspecify which distribution it follows later: x ∼ P (x .)\\n\\nProbability mass functions can act on many variables at the same time. Such\\na probability distribution over many variables is known as a joint probability\\ndistribution. P (x = x, y = y) denotes the probability that x = x and y = y\\nsimultaneously. We may also write\\n\\nfor brevity.\\n\\nP x, y\\n\\n(\\n\\n)\\n\\nTo be a probability mass function on a random variable x, a function P must\\n\\nsatisfy the following properties:\\n\\nP\\n\\n• The domain of must be the set of all possible states of x.\\n• ∀x\\n0 \\n\\n,∈ x 0 ≤ P (x) ≤ 1. An impossible event has probability and no state can\\nbe less probable than that. Likewise, an event that is guaranteed to happen\\nhas probability , and no state can have a greater chance of occurring.\\n\\n1\\n\\n• \\ue050x∈x P (x) = 1. We refer to this property as being normalized. Without this\\n\\nproperty, we could obtain probabilities greater than one by computing the\\nprobability of one of many events occurring.\\n\\nFor example, consider a single discrete random variable x with k diﬀerent states.\\nx—that is, make each of its states equally\\n\\nuniform distribution\\n\\nWe can place a\\nlikely—by setting its probability mass function to\\n\\non\\n\\nP\\n\\n( = x\\n\\nx\\ni) =\\n\\n1\\nk\\n\\n(3.1)\\n\\nfor all i. We can see that this ﬁts the requirements for a probability mass function.\\nThe value 1\\n\\nis a positive integer. We also see that\\n\\nk is positive because\\n\\nk\\n\\n\\ue058i\\n\\nP\\n\\n( = x\\n\\nx\\n\\ni) =\\ue058i\\n\\n1\\nk\\n\\n=\\n\\nk\\nk\\n\\n= 1,\\n\\n(3.2)\\n\\nso the distribution is properly normalized.\\n\\n56\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.3.2 Continuous Variables and Probability Density Functions\\n\\nWhen working with continuous random variables, we describe probability dis-\\ntributions using a probability density function (PDF) rather than a probability\\nmass function. To be a probability density function, a function p must satisfy the\\nfollowing properties:\\n\\np\\n\\n• The domain of must be the set of all possible states of x.\\n• ∀x\\n\\n0≥ Note that we do not require ( ) \\np x\\n\\n1≤ .\\n\\n.\\n\\n∈ x ( ) \\n, p x\\n( ) = 1.\\n\\n• \\ue052 p x dx\\n\\nA probability density function p(x) does not give the probability of a speciﬁc\\nstate directly, instead the probability of landing inside an inﬁnitesimal region with\\nvolume\\n\\nis given by\\n\\np x δx\\n\\n( )\\n\\nδx\\n\\n.\\n\\nWe can integrate the density function to ﬁnd the actual probability mass of a\\nset of points. Speciﬁcally, the probability that x lies in some set S is given by the\\nintegral of p(x) over that set. In the univariate example, the probability that x\\nlies in the interval\\n\\nis given by\\n\\n.\\np x dx\\n\\n[\\n]a, b\\n\\n( )\\n\\nFor an example of a probability density function corresponding to a speciﬁc\\nprobability density over a continuous random variable, consider a uniform distribu-\\ntion on an interval of the real numbers. We can do this with a function u (x; a, b),\\nwhere a and b are the endpoints of the interval, with b > a. The “;” notation means\\n“parametrized by”; we consider x to be the argument of the function, while a and\\nb are parameters that deﬁne the function. To ensure that there is no probability\\nmass outside the interval, we say u(x; a, b) = 0 for all x \\ue036∈ [a, b]\\n. Within a, b],\\n. We can see that this is nonnegative everywhere. Additionally, it\\nu x a, b\\n( ;\\nintegrates to 1. We often denote that x follows the uniform distribution on [a, b]\\nby writing x\\n\\n) = 1\\nb a−\\n∼ U a, b\\n)\\n\\n(\\n\\n[\\n\\n.\\n\\n\\ue052[\\n\\n]a,b\\n\\n3.4 Marginal Probability\\n\\nSometimes we know the probability distribution over a set of variables and we want\\nto know the probability distribution over just a subset of them. The probability\\ndistribution over the subset is known as the marginal probability distribution.\\n\\nFor example, suppose we have discrete random variables x and y , and we know\\n\\nP ,(x y . We can ﬁnd\\n\\n)\\n\\nx with the\\n\\nsum rule\\n\\n:\\n\\nx, P\\n\\n( = x\\n\\nx\\n\\nP\\n\\n( = x\\n\\nx,\\n\\ny =  )\\ny .\\n\\n(3.3)\\n\\nP ( )\\n∀ ∈x\\n\\n) =\\ue058y\\n\\n57\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe name “marginal probability” comes from the process of computing marginal\\nprobabilities on paper. When the values of P(x y, ) are written in a grid with\\ndiﬀerent values of x in rows and diﬀerent values of y in columns, it is natural to\\nsum across a row of the grid, then write P( x) in the margin of the paper just to\\nthe right of the row.\\n\\nFor continuous variables, we need to use integration instead of summation:\\n\\np x( ) =\\ue05a p x, y dy.\\n\\n(\\n\\n)\\n\\n(3.4)\\n\\n3.5 Conditional Probability\\n\\nIn many cases, we are interested in the probability of some event, given that some\\nother event has happened. This is called a conditional probability. We denote\\nthe conditional probability that y = y given x = x as P (y = y | x = x ). This\\nconditional probability can be computed with the formula\\n\\nP\\n\\n( = y\\n\\ny\\n\\n| x =  ) =\\n\\nx\\n\\nP\\n\\n( = y\\n\\ny,\\n\\nx =  )\\nx\\nx\\n)\\n\\nP\\n\\n( = x\\n\\n.\\n\\n(3.5)\\n\\nThe conditional probability is only deﬁned when P (x = x) > 0. We cannot compute\\nthe conditional probability conditioned on an event that never happens.\\n\\nIt is important not to confuse conditional probability with computing what\\nwould happen if some action were undertaken. The conditional probability that\\na person is from Germany given that they speak German is quite high, but if\\na randomly selected person is taught to speak German, their country of origin\\ndoes not change. Computing the consequences of an action is called making an\\nintervention query. Intervention queries are the domain of causal modeling, which\\nwe do not explore in this book.\\n\\n3.6 The Chain Rule of Conditional Probabilities\\n\\nAny joint probability distribution over many random variables may be decomposed\\ninto conditional distributions over only one variable:\\n\\nP (x(1), . . . , x ( )n ) = \\n\\n| x(1), . . . , x (\\nproduct rule of probability. It\\nfollows immediately from the deﬁnition of conditional probability in Eq. 3.5. For\\n\\nThis observation is known as the\\n\\n(P x(1) )Πn\\n\\ni=2P (x( )i\\n\\nchain rule\\n\\ni− ).\\n\\n(3.6)\\n\\nor\\n\\n1)\\n\\n58\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nexample, applying the deﬁnition twice, we get\\n\\nP ,\\n\\n(a b c) =\\n\\n,\\n\\nP ,\\n\\n(b c) =\\n\\nP ,\\n\\n(a b c) =\\n\\n,\\n\\nP\\n\\nP\\n\\nP\\n\\n(a b|\\n(b c| )\\n(a b|\\n\\nc)\\n, P ,\\n\\n(b c)\\n\\n( )P c\\n\\n, Pc)\\n\\n(b c| )\\n\\n( )P c .\\n\\n3.7 Independence and Conditional Independence\\n\\nTwo random variables x and y are independent if their probability distribution can\\nbe expressed as a product of two factors, one involving only x and one involving\\nonly y:\\n\\n∀ ∈x\\n\\nx, y\\n\\n∈ y\\n\\n, p\\n\\nx\\n( = \\n\\nx, y\\n\\n= ) =  ( =\\n\\np x\\n\\ny\\n\\nx) ( =  )\\ny .\\n\\np y\\n\\n(3.7)\\n\\nTwo random variables x and y are conditionally independent given a random\\nvariable z if the conditional probability distribution over x and y factorizes in this\\nway for every value of z:\\n\\n∀ ∈x\\n\\nx, y\\n\\n, z∈ y ∈ z, p\\n\\n( =x\\n\\nx,\\n\\ny = \\n\\ny\\n\\n| z =  ) =  ( = x\\n\\np\\n\\nz\\n\\nx\\n\\n| z =  ) ( = y\\n\\nz p\\n\\ny\\n\\n| z = )\\nz .\\n(3.8)\\n\\nWe can denote independence and conditional independence with compact\\nnotation: x y⊥ means that x and y are independent, while x y z⊥ | means that x\\nand y are conditionally independent given z.\\n\\n3.8 Expectation, Variance and Covariance\\n\\nor\\n\\nThe expectation\\nof some function f( x) with respect to a probability\\ndistribution P (x) is the average or mean value that f takes on when x is drawn\\nfrom . For discrete variables this can be computed with a summation:\\n\\nexpected value\\n\\nP\\n\\nEx∼P[ (f x)] =\\ue058x\\n\\nP x f x ,\\n( ) ( )\\n\\n(3.9)\\n\\nwhile for continuous variables, it is computed with an integral:\\n\\nEx∼p[ ( )] =\\n\\nf x\\n\\n\\ue05a p x f x dx.\\n\\n( ) ( )\\n\\n59\\n\\n(3.10)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nWhen the identity of the distribution is clear from the context, we may simply\\nwrite the name of the random variable that the expectation is over, as in Ex[f (x)].\\nIf it is clear which random variable the expectation is over, we may omit the\\nsubscript entirely, as in E[f (x)]. By default, we can assume that E [·] averages over\\nthe values of all the random variables inside the brackets. Likewise, when there is\\nno ambiguity, we may omit the square brackets.\\n\\nExpectations are linear, for example,\\n\\nE x[\\n\\nαf x\\n\\n( ) + ( )] = \\n\\nβg x\\n\\nαEx [ ( )] +\\n\\nf x\\n\\nβEx[ ( )]\\ng x ,\\n\\n(3.11)\\n\\nwhen\\n\\nα\\n\\nand\\n\\nβ\\n\\nare not dependent on .\\nx\\n\\nThe variance gives a measure of how much the values of a function of a random\\nvariable x vary as we sample diﬀerent value of x from its probability distribution:\\n\\nVar( ( )) = \\n\\nf x\\n\\nE\\ue068( ( )\\nf x − E f x 2\\ue069.\\n\\n[ ( )])\\n\\n(3.12)\\n\\nWhen the variance is low, the values of f (x) cluster near their expected value. The\\nsquare root of the variance is known as the standard deviation.\\n\\nThe covariance gives some sense of how much two values are linearly related to\\n\\neach other, as well as the scale of these variables:\\n\\nCov( ( )\\n\\nf x , g y\\n\\n( )) = \\n\\nE f x − E f x\\n[( ( )\\n\\n[ ( )]) ( ( )\\n\\ng y − E g y\\n\\n[ ( )])]\\n\\n.\\n\\n(3.13)\\n\\nHigh absolute values of the covariance mean that the values change very much\\nand are both far from their respective means at the same time. If the sign of the\\ncovariance is positive, then both variables tend to take on relatively high values\\nsimultaneously. If the sign of the covariance is negative, then one variable tends to\\ntake on a relatively high value at the times that the other takes on a relatively low\\nvalue and vice versa. Other measures such as correlation normalize the contribution\\nof each variable in order to measure only how much the variables are related, rather\\nthan also being aﬀected by the scale of the separate variables.\\n\\nThe notions of covariance and dependence are related, but are in fact distinct\\nconcepts. They are related because two variables that are independent have zero\\ncovariance, and two variables that have non-zero covariance are dependent. How-\\never, independence is a distinct property from covariance. For two variables to have\\nzero covariance, there must be no linear dependence between them. Independence\\nis a stronger requirement than zero covariance, because independence also excludes\\nnonlinear relationships. It is possible for two variables to be dependent but have\\nzero covariance. For example, suppose we ﬁrst sample a real number x from a\\nuniform distribution over the interval [−1 ,1]. We next sample a random variable\\n\\n60\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\ns. With probability 1\\n2, we choose the value of s to be 1. Otherwise, we choose\\nthe value of s to be − 1. We can then generate a random variable y by assigning\\ny = sx . Clearly, x and y are not independent, because x completely determines\\nthe magnitude of\\n\\n. However,\\n\\n.\\n) = 0\\n\\nCov(\\n\\nx, y\\n\\ny\\n\\nThe covariance matrix of a random vector x ∈ Rn is an n n× matrix, such that\\n(3.14)\\n\\nCov( )x i,j = Cov(xi, xj ).\\n\\nThe diagonal elements of the covariance give the variance:\\n\\nCov(xi , xi) = Var(xi).\\n\\n(3.15)\\n\\n3.9 Common Probability Distributions\\n\\nSeveral simple probability distributions are useful in many contexts in machine\\nlearning.\\n\\n3.9.1 Bernoulli Distribution\\n\\nBernoulli\\n\\nThe\\ndistribution is a distribution over a single binary random variable.\\nIt is controlled by a single parameter φ ∈ [0, 1], which gives the probability of the\\nrandom variable being equal to 1. It has the following properties:\\n\\nP\\n\\nx\\n( = 1) = \\n\\nφ\\n\\nP\\n\\nx\\n( = 0) = 1\\n) =  x (1\\nx\\n\\nφ\\n\\nP\\n\\n( = x\\n\\nEx [ ] = \\n\\nx\\n\\nφ\\n\\n−\\n)− φ 1−x\\nφ\\n\\nVar x( ) =  (1\\n\\nx\\n\\nφ − φ\\n\\n)\\n\\n(3.16)\\n\\n(3.17)\\n\\n(3.18)\\n\\n(3.19)\\n\\n(3.20)\\n\\n3.9.2 Multinoulli Distribution\\n\\nmultinoulli\\n\\nThe\\ncategorical distribution is a distribution over a single discrete\\nvariable with k diﬀerent states, where k is ﬁnite1 . The multinoulli distribution is\\n\\nor\\n\\n1 “Multinoulli” is a term that was recently coined by Gustavo Lacerdo and popularized by\\nMurphy (2012). The multinoulli distribution is a special case of the\\ndistribution. A\\nmultinomial distribution is the distribution over vectors in {0, . . . , n}k representing how many\\ntimes each of the k categories is visited when n samples are drawn from a multinoulli distribution.\\nMany texts use the term “multinomial” to refer to multinoulli distributions without clarifying\\nthat they refer only to the\\n\\nmultinomial\\n\\nn = 1\\n\\ncase.\\n\\n61\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nparametrized by a vector p ∈ [0, 1]k−1 , where p i gives the probability of the i-th\\nstate. The ﬁnal, k-th state’s probability is given by 1− 1\\ue03e p. Note that we must\\nconstrain 1 \\ue03ep ≤ 1. Multinoulli distributions are often used to refer to distributions\\nover categories of objects, so we do not usually assume that state 1 has numerical\\nvalue 1, etc. For this reason, we do not usually need to compute the expectation\\nor variance of multinoulli-distributed random variables.\\n\\nThe Bernoulli and multinoulli distributions are suﬃcient to describe any distri-\\nbution over their domain. This is because they model discrete variables for which\\nit is feasible to simply enumerate all of the states. When dealing with continuous\\nvariables, there are uncountably many states, so any distribution described by a\\nsmall number of parameters must impose strict limits on the distribution.\\n\\n3.9.3 Gaussian Distribution\\n\\nThe most commonly used distribution over real numbers is the normal distribution,\\nalso known as the Gaussian distribution:\\n\\nN ( ;x µ, σ2) =\\ue072 1\\n\\n2πσ2 exp\\ue012−\\n\\n1\\nx\\n2σ2 (\\n\\nµ− 2\\ue013 .\\n\\n)\\n\\n(3.21)\\n\\nSee Fig. 3.1 for a plot of the density function.\\nThe two parameters µ ∈ R and σ ∈ (0,∞ ) control the normal distribution.\\nThe parameter µ gives the coordinate of the central peak. This is also the mean of\\nthe distribution: E[x] = µ. The standard deviation of the distribution is given by\\nσ, and the variance by σ2.\\n\\nWhen we evaluate the PDF, we need to square and invert σ. When we need to\\nfrequently evaluate the PDF with diﬀerent parameter values, a more eﬃcient way\\nof parametrizing the distribution is to use a parameter β ∈ (0 ,∞) to control the\\nprecision or inverse variance of the distribution:\\n\\nN ( ;x µ, β−1) =\\ue072 β\\n\\n2π\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\nβ x\\n\\n( − )2\\ue013 .\\n\\nµ\\n\\n(3.22)\\n\\nNormal distributions are a sensible choice for many applications. In the absence\\nof prior knowledge about what form a distribution over the real numbers should\\ntake, the normal distribution is a good default choice for two major reasons.\\n\\nFirst, many distributions we wish to model are truly close to being normal\\ndistributions. The central limit theorem shows that the sum of many independent\\nrandom variables is approximately normally distributed. This means that in\\n\\n62\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe normal distribution\\n\\nMaximum at x ¹=\\n\\nInflection points at \\n     x ¹ ¾\\n\\n= §\\n\\n−1.5\\n\\n−1.0\\n\\n−0.5\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n)\\nx\\n(\\np\\n\\n0.40\\n\\n0.35\\n\\n0.30\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.10\\n\\n0.05\\n\\n0.00\\n\\n−2.0\\n\\nFigure 3.1: The normal distribution: The normal distribution N (x;µ, σ 2) exhibits a classic\\n“bell curve” shape, with the x coordinate of its central peak given by µ, and the width\\nof its peak controlled by σ. In this example, we depict the standard normal distribution,\\nwith\\n\\nσ = 1\\n\\nµ = 0\\n\\nand\\n\\n.\\n\\nx\\n\\npractice, many complicated systems can be modeled successfully as normally\\ndistributed noise, even if the system can be decomposed into parts with more\\nstructured behavior.\\n\\nSecond, out of all possible probability distributions with the same variance,\\nthe normal distribution encodes the maximum amount of uncertainty over the\\nreal numbers. We can thus think of the normal distribution as being the one that\\ninserts the least amount of prior knowledge into a model. Fully developing and\\njustifying this idea requires more mathematical tools, and is postponed to Sec.\\n19.4.2.\\n\\nThe normal distribution generalizes to Rn, in which case it is known as the\\nmultivariate normal distribution. It may be parametrized with a positive deﬁnite\\nsymmetric matrix\\n\\n:Σ\\n\\nx µ, Σ \\ue073\\n\\n) =\\n\\nN ( ;\\n\\n1\\n\\n(2 )π ndet(\\n\\n)Σ\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\nx µ− \\ue03eΣ−1 (\\n(\\n\\n)\\n\\nx µ− \\ue013 .\\n\\n)\\n\\n(3.23)\\n\\nThe parameter µ still gives the mean of the distribution, though now it is\\nvector-valued. The parameter Σ gives the covariance matrix of the distribution.\\nAs in the univariate case, when we wish to evaluate the PDF several times for\\n\\n63\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nmany diﬀerent values of the parameters, the covariance is not a computationally\\neﬃcient way to parametrize the distribution, since we need to invert Σ to evaluate\\nthe PDF. We can instead use a precision matrix β:\\n\\nN ( ;x µ β, −1) =\\ue073det( )β\\n\\n(2 )π n\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\n( − )\\ue013 .\\nx µ− \\ue03eβ x µ\\n(\\n\\n)\\n\\n(3.24)\\n\\nWe often ﬁx the covariance matrix to be a diagonal matrix. An even simpler\\nversion is the isotropic Gaussian distribution, whose covariance matrix is a scalar\\ntimes the identity matrix.\\n\\n3.9.4 Exponential and Laplace Distributions\\n\\nIn the context of deep learning, we often want to have a probability distribution\\nwith a sharp point at x = 0. To accomplish this, we can use the exponential\\ndistribution:\\n\\n(3.25)\\nThe exponential distribution uses the indicator function 1x≥0 to assign probability\\nzero to all negative values of\\n\\np x λ\\n( ; ) =  1x≥0exp (\\n\\n)−λx .\\n\\n.x\\n\\nλ\\n\\nA closely related probability distribution that allows us to place a sharp peak\\n\\nof probability mass at an arbitrary point\\n\\nµ\\n\\nis the\\n\\nLaplace distribution\\n\\nLaplace( ;\\n\\nx µ, γ\\n\\n) =\\n\\n1\\n2γ\\n\\nexp\\ue012−| − |\\nγ \\ue013.\\n\\nµ\\n\\nx\\n\\n(3.26)\\n\\n3.9.5 The Dirac Distribution and Empirical Distribution\\n\\nIn some cases, we wish to specify that all of the mass in a probability distribution\\nclusters around a single point. This can be accomplished by deﬁning a PDF using\\nthe Dirac delta function,\\n\\nδ x( )\\n\\n:\\n\\n( ) =  ( − )\\np x\\nµ .\\n\\nδ x\\n\\n(3.27)\\n\\nThe Dirac delta function is deﬁned such that it is zero-valued everywhere except\\n0, yet integrates to 1. The Dirac delta function is not an ordinary function that\\nassociates each value x with a real-valued output, instead it is a diﬀerent kind of\\nmathematical object called a generalized function that is deﬁned in terms of its\\nproperties when integrated. We can think of the Dirac delta function as being the\\nlimit point of a series of functions that put less and less mass on all points other\\nthan .µ\\n\\n64\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nBy deﬁning p(x) to be δ shifted by −µ we obtain an inﬁnitely narrow and\\n\\ninﬁnitely high peak of probability mass where\\n\\nx\\n\\nµ= \\n\\n.\\n\\nA common use of the Dirac delta distribution is as a component of an empirical\\n\\ndistribution,\\n\\nˆp( ) =x\\n\\n1\\nm\\n\\nm\\ue058i=1\\n\\nδ(x x− ( )i )\\n\\n(3.28)\\n\\n1\\nm on each of the m points x(1), . . . , x (\\n\\n)m forming\\nwhich puts probability mass\\na given data set or collection of samples. The Dirac delta distribution is only\\nnecessary to deﬁne the empirical distribution over continuous variables. For discrete\\nvariables, the situation is simpler: an empirical distribution can be conceptualized\\nas a multinoulli distribution, with a probability associated to each possible input\\nvalue that is simply equal to the empirical frequency of that value in the training\\nset.\\n\\nWe can view the empirical distribution formed from a dataset of training\\nexamples as specifying the distribution that we sample from when we train a model\\non this dataset. Another important perspective on the empirical distribution is\\nthat it is the probability density that maximizes the likelihood of the training\\ndata (see Sec. 5.5). Many machine learning algorithms can be conﬁgured to have\\narbitrarily high capacity. If given enough capacity, these algorithms will simply\\nlearn the empirical distribution. This is a bad outcome because the model does not\\ngeneralize at all and assigns inﬁnitesimal probability to any point in space that did\\nnot occur in the training set. A central problem in machine learning is studying\\nhow to limit the capacity of a model in a way that prevents it from simply learning\\nthe empirical distribution while also allowing it to learn complicated functions.\\n\\n3.9.6 Mixtures of Distributions\\n\\nIt is also common to deﬁne probability distributions by combining other simpler\\nprobability distributions. One common way of combining distributions is to\\nconstruct a mixture distribution. A mixture distribution is made up of several\\ncomponent distributions. On each trial, the choice of which component distribution\\ngenerates the sample is determined by sampling a component identity from a\\nmultinoulli distribution:\\n\\nP ( ) =x \\ue058i\\n\\nP\\n\\nc\\n( = \\n\\ni P\\n)\\n\\nx c|\\n(\\n\\n=\\n\\ni\\n)\\n\\n(3.29)\\n\\nwhere\\n\\nP ( )\\n\\nc is the multinoulli distribution over component identities.\\n\\n65\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nWe have already seen one example of a mixture distribution: the empirical\\ndistribution over real-valued variables is a mixture distribution with one Dirac\\ncomponent for each training example.\\n\\nThe mixture model is one simple strategy for combining probability distributions\\nto create a richer distribution. In Chapter 16, we explore the art of building complex\\nprobability distributions from simple ones in more detail.\\n\\nlatent variable\\n\\nThe mixture model allows us to brieﬂy glimpse a concept that will be of\\nparamount importance later—the\\n. A latent variable is a random\\nvariable that we cannot observe directly. The component identity variable c of the\\nmixture model provides an example. Latent variables may be related to x through\\nthe joint distribution, in this case, P (x c, ) = P (x c|\\n)P(c). The distribution P (c)\\nover the latent variable and the distribution P (x c| ) relating the latent variables\\nto the visible variables determines the shape of the distribution P ( x) even though\\nit is possible to describe P (x) without reference to the latent variable. Latent\\nvariables are discussed further in Sec. 16.5.\\n\\nA very powerful and common type of mixture model is the Gaussian mixture\\nmodel, in which the components p (x | c = i) are Gaussians. Each component has\\na separately parametrized mean µ ( )i and covariance Σ ( )i . Some mixtures can have\\nmore constraints. For example, the covariances could be shared across components\\nvia the constraint Σ( )i = Σ∀i. As with a single Gaussian distribution, the mixture\\nof Gaussians might constrain the covariance matrix for each component to be\\ndiagonal or isotropic.\\n\\nIn addition to the means and covariances, the parameters of a Gaussian mixture\\nspecify the prior probability α i = P (c = i) given to each component i. The word\\n“prior” indicates that it expresses the model’s beliefs about c before it has observed\\nx. By comparison, P(c | x) is a posterior probability, because it is computed after\\nobservation of x. A Gaussian mixture model is a universal approximator of\\ndensities, in the sense that any smooth density can be approximated with any\\nspeciﬁc, non-zero amount of error by a Gaussian mixture model with enough\\ncomponents.\\n\\nFig. 3.2 shows samples from a Gaussian mixture model.\\n\\n3.10 Useful Properties of Common Functions\\n\\nCertain functions arise often while working with probability distributions, especially\\nthe probability distributions used in deep learning models.\\n\\n66\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n2\\nx\\n\\nx1\\n\\nFigure 3.2: Samples from a Gaussian mixture model. In this example, there are three\\ncomponents. From left to right, the ﬁrst component has an isotropic covariance matrix,\\nmeaning it has the same amount of variance in each direction. The second has a diagonal\\ncovariane matrix, meaning it can control the variance separately along each axis-aligned\\ndirection. This example has more variance along the x2 axis than along the x1 axis. The\\nthird component has a full-rank covariance matrix, allowing it to control the variance\\nseparately along an abitrary basis of directions.\\n\\nOne of these functions is the logistic sigmoid:\\n\\nσ x( ) =\\n\\n1\\n\\n1 + exp(\\n\\n.\\n)−x\\n\\n(3.30)\\n\\nThe logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli\\ndistribution because its range is (0, 1), which lies within the valid range of values\\nfor the φ parameter. See Fig. 3.3 for a graph of the sigmoid function. The sigmoid\\nfunction saturates when its argument is very positive or very negative, meaning\\nthat the function becomes very ﬂat and insensitive to small changes in its input.\\n\\nAnother commonly encountered function is the\\n\\nsoftplus\\n\\nfunction (Dugas\\n\\net al.,\\n\\n2001):\\n\\nζ x\\nx .\\n( ) = log (1 + exp( ))\\n\\n(3.31)\\n\\nThe softplus function can be useful for producing the β or σ parameter of a normal\\ndistribution because its range is (0,∞). It also arises commonly when manipulating\\nexpressions involving sigmoids. The name of the softplus function comes from the\\nfact that it is a smoothed or “softened” version of\\n\\nx+ = max(0 ), x .\\n\\n(3.32)\\n\\nSee Fig. 3.4 for a graph of the softplus function.\\n\\n67\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe logistic sigmoid function\\n\\n)\\nx\\n(\\n¾\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n−10\\n\\n−5\\n\\n0\\nx\\n\\n5\\n\\n10\\n\\nFigure 3.3: The logistic sigmoid function.\\n\\nThe softplus function\\n\\n10\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n)\\nx\\n(\\n³\\n\\n0\\n−10\\n\\n−5\\n\\n0\\n\\nx\\n\\n5\\n\\n10\\n\\nFigure 3.4: The softplus function.\\n\\n68\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe following properties are all useful enough that you may wish to memorize\\n\\nthem:\\n\\nσ x( ) =\\n\\nexp( )x\\nx\\n\\nexp( ) + exp(0)\\n\\nd\\ndx\\n\\nσ x\\n\\n( ) =  (\\n\\n( ) =  ( )(1 − ( ))\\nσ x\\nσ x\\n− σ x\\n1\\nσ x\\n\\nσ −x\\n)\\n−ζ −x\\n(\\nζ x\\nσ x\\n( ) =  ( )\\n\\nlog ( ) = \\n\\n)\\n\\nd\\ndx\\n\\n1)\\n\\n(0,\\n\\n, σ\\n\\n\\ue012 x\\n1 − x\\ue013\\n∀ ∈x\\n∀x > 0, ζ−1( ) = log (exp( )\\nx −\\n\\n−1 ( ) = log\\n\\n1)\\n\\nx\\n\\nx\\n\\nζ x( ) =\\ue05a x\\n\\n−∞\\nx\\nζ\\n\\nσ y dy\\n\\n( )\\n\\n(3.33)\\n\\n(3.34)\\n\\n(3.35)\\n\\n(3.36)\\n\\n(3.37)\\n\\n(3.38)\\n\\n(3.39)\\n\\n(3.40)\\n\\n(3.41)\\nThe function σ−1(x) is called the logit in statistics, but this term is more rarely\\nused in machine learning. The ﬁnal property provides extra justiﬁcation for the\\nname “softplus,” since x+ − x− = x.\\n\\n( ) − (− ) = \\nζ x\\nx\\n\\n3.11 Bayes’ Rule\\n\\nWe often ﬁnd ourselves in a situation where we know P (y x| ) and need to know\\nP (x y|\\n). Fortunately, if we also know P (x), we can compute the desired quantity\\nusing Bayes’ rule:\\n\\nP (\\n\\nx y|\\n\\n) =\\n\\nP\\n\\nP( )x\\n\\ny x|\\n(\\n\\n)\\n\\n.\\n\\n(3.42)\\n\\nP ( )y\\n\\nP ( ) =y \\ue050x P\\n\\n(y |\\n\\nNote that while P (y) appears in the formula, it is usually feasible to compute\\n\\nx P x\\n\\n( ), so we do not need to begin with knowledge of\\n\\n)\\n\\nP\\n\\n(y .)\\n\\nBayes’ rule is straightforward to derive from the deﬁnition of conditional\\nprobability, but it is useful to know the name of this formula since many texts\\nrefer to it by name. It is named after the Reverend Thomas Bayes, who ﬁrst\\ndiscovered a special case of the formula. The general version presented here was\\nindependently discovered by Pierre-Simon Laplace.\\n\\n69\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.12 Technical Details of Continuous Variables\\n\\nA proper formal understanding of continuous random variables and probability\\ndensity functions requires developing probability theory in terms of a branch of\\nmathematics known as measure theory. Measure theory is beyond the scope of\\nthis textbook, but we can brieﬂy sketch some of the issues that measure theory is\\nemployed to resolve.\\n\\nIn Sec. 3.3.2, we saw that the probability of a continuous vector-valued x lying\\nin some set S is given by the integral of p(x ) over the set S. Some choices of set S\\ncan produce paradoxes. For example, it is possible to construct two sets S1 and\\nS2 such that p(x ∈ S1) + p(x ∈ S 2) > 1 but S1 ∩ S2 = ∅. These sets are generally\\nconstructed making very heavy use of the inﬁnite precision of real numbers, for\\nexample by making fractal-shaped sets or sets that are deﬁned by transforming\\nthe set of rational numbers2 . One of the key contributions of measure theory is to\\nprovide a characterization of the set of sets that we can compute the probability\\nof without encountering paradoxes. In this book, we only integrate over sets with\\nrelatively simple descriptions, so this aspect of measure theory never becomes a\\nrelevant concern.\\n\\nFor our purposes, measure theory is more useful for describing theorems that\\napply to most points in Rn but do not apply to some corner cases. Measure theory\\nprovides a rigorous way of describing that a set of points is negligibly small. Such\\na set is said to have “measure zero.” We do not formally deﬁne this concept in this\\ntextbook. However, it is useful to understand the intuition that a set of measure\\nzero occupies no volume in the space we are measuring. For example, within R2 , a\\nline has measure zero, while a ﬁlled polygon has positive measure. Likewise, an\\nindividual point has measure zero. Any union of countably many sets that each\\nhave measure zero also has measure zero (so the set of all the rational numbers\\nhas measure zero, for instance).\\n\\nAnother useful term from measure theory is “almost everywhere.” A property\\nthat holds almost everywhere holds throughout all of space except for on a set of\\nmeasure zero. Because the exceptions occupy a negligible amount of space, they\\ncan be safely ignored for many applications. Some important results in probability\\ntheory hold for all discrete values but only hold “almost everywhere” for continuous\\nvalues.\\n\\nAnother technical detail of continuous variables relates to handling continuous\\nrandom variables that are deterministic functions of one another. Suppose we have\\ntwo random variables, x and y, such that y = g(x), where g is an invertible, con-\\n\\n2The Banach-Tarski theorem provides a fun example of such sets.\\n\\n70\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\ntinuous, diﬀerentiable transformation. One might expect that py(y ) =p x(g−1(y )).\\nThis is actually not the case.\\n\\nAs a simple example, suppose we have scalar random variables x and y. Suppose\\nIf we use the rule p y(y) = p x(2 y) then py will be 0\\non this interval. This means\\n\\ny = x\\neverywhere except the interval [0 , 1\\n2 ]\\n\\n2 and x ∼ U(0,1).\\n\\n, and it will be\\n\\n1\\n\\n\\ue05a py ( ) =\\n\\ny dy\\n\\n1\\n2\\n\\n,\\n\\n(3.43)\\n\\nwhich violates the deﬁnition of a probability distribution.\\n\\nThis common mistake is wrong because it fails to account for the distortion\\nof space introduced by the function g. Recall that the probability of x lying in\\nan inﬁnitesimally small region with volume δx is given by p( x)δx. Since g can\\nexpand or contract space, the inﬁnitesimal volume surrounding x in x space may\\nhave diﬀerent volume in\\n\\nspace.\\n\\ny\\n\\nTo see how to correct the problem, we return to the scalar case. We need to\\n\\npreserve the property\\n\\nSolving from this, we obtain\\n\\n|py( ( ))\\n\\ng x dy|\\n\\n=\\n\\n|p x( )x dx .|\\n\\nor equivalently\\n\\nIn higher dimensions, the derivative generalizes to the determinant of the Jacobian\\nmatrix—the matrix with J i,j = ∂xi\\n∂yj\\n\\n. Thus, for real-valued vectors\\n\\nand ,\\ny\\n\\nx\\n\\npy( ) = \\n\\ny\\n\\npx ( ) = \\n\\nx\\n\\n∂x\\n\\n∂g x( )\\n\\npy( ( ))\\n\\npx (g−1( ))y\\n\\n\\ue00c\\ue00c\\ue00c\\ue00c\\n∂y\\ue00c\\ue00c\\ue00c\\ue00c\\ng x \\ue00c\\ue00c\\ue00c\\ue00c\\n∂x \\ue00c\\ue00c\\ue00c\\ue00c .\\ng x \\ue00c\\ue00c\\ue00c\\ue00cdet\\ue012∂g( )x\\n∂x \\ue013\\ue00c\\ue00c\\ue00c\\ue00c .\\n\\npx ( ) = \\n\\nx\\n\\np y( ( ))\\n\\n3.13 Information Theory\\n\\nInformation theory is a branch of applied mathematics that revolves around\\nquantifying how much information is present in a signal. It was originally invented\\nto study sending messages from discrete alphabets over a noisy channel, such as\\ncommunication via radio transmission. In this context, information theory tells how\\nto design optimal codes and calculate the expected length of messages sampled from\\n\\n71\\n\\n(3.44)\\n\\n(3.45)\\n\\n(3.46)\\n\\n(3.47)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nspeciﬁc probability distributions using various encoding schemes. In the context of\\nmachine learning, we can also apply information theory to continuous variables\\nwhere some of these message length interpretations do not apply. This ﬁeld is\\nfundamental to many areas of electrical engineering and computer science. In this\\ntextbook, we mostly use a few key ideas from information theory to characterize\\nprobability distributions or quantify similarity between probability distributions.\\nFor more detail on information theory, see Cover and Thomas (2006) or MacKay\\n(2003).\\n\\nThe basic intuition behind information theory is that learning that an unlikely\\nevent has occurred is more informative than learning that a likely event has\\noccurred. A message saying “the sun rose this morning” is so uninformative as\\nto be unnecessary to send, but a message saying “there was a solar eclipse this\\nmorning” is very informative.\\n\\nWe would like to quantify information in a way that formalizes this intuition.\\n\\nSpeciﬁcally,\\n\\n• Likely events should have low information content, and in the extreme case,\\nevents that are guaranteed to happen should have no information content\\nwhatsoever.\\n\\n• Less likely events should have higher information content.\\n• Independent events should have additive information. For example, ﬁnding\\nout that a tossed coin has come up as heads twice should convey twice as\\nmuch information as ﬁnding out that a tossed coin has come up as heads\\nonce.\\n\\nIn order to satisfy all three of these properties, we deﬁne the self-information\\n\\nof an event x\\n\\n= x\\n\\nto be\\n\\nI x\\n( ) = \\n\\nlog−\\n\\nP x .\\n( )\\n\\n(3.48)\\n\\nIn this book, we always use log to mean the natural logarithm, with base e. Our\\ndeﬁnition of I(x) is therefore written in units of\\n. One nat is the amount of\\ninformation gained by observing an event of probability 1\\ne . Other texts use base-2\\nlogarithms and units called\\nshannons\\n; information measured in bits is just\\na rescaling of information measured in nats.\\n\\nnats\\n\\nbits\\n\\nor\\n\\nWhen x is continuous, we use the same deﬁnition of information by analogy,\\nbut some of the properties from the discrete case are lost. For example, an event\\nwith unit density still has zero information, despite not being an event that is\\nguaranteed to occur.\\n\\n72\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n0.4\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\ns\\nt\\na\\nn\\n \\nn\\n\\ni\\n \\ny\\np\\no\\nr\\nt\\nn\\ne\\n \\nn\\no\\nn\\nn\\na\\nh\\nS\\n\\n0.0\\n\\n0.0\\n\\nShannon entropy of a binary random variable\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\np\\n\\n1\\n\\nFigure 3.5: This plot shows how distributions that are closer to deterministic have low\\nShannon entropy while distributions that are close to uniform have high Shannon entropy.\\nOn the horizontal axis, we plot p, the probability of a binary random variable being equal\\nto . The entropy is given by\\nlog . When p is near 0, the distribution\\nis nearly deterministic, because the random variable is nearly always 0. When p is near 1,\\nthe distribution is nearly deterministic, because the random variable is nearly always 1.\\nWhen p = 0 .5, the entropy is maximal, because the distribution is uniform over the two\\noutcomes.\\n\\n(p− 1) log(1− p )− p\\n\\np\\n\\nSelf-information deals only with a single outcome. We can quantify the amount\\n\\nof uncertainty in an entire probability distribution using the Shannon entropy:\\n\\nH( ) = \\n\\nx\\n\\nE\\n\\nI x\\n\\nx∼P[ ( )] = \\n\\n−E\\n\\nP x .\\nx∼P[log ( )]\\n\\n(3.49)\\n\\nalso denoted H( P). In other words, the Shannon entropy of a distribution is the\\nexpected amount of information in an event drawn from that distribution. It gives\\na lower bound on the number of bits (if the logarithm is base 2, otherwise the units\\nare diﬀerent) needed on average to encode symbols drawn from a distribution P.\\nDistributions that are nearly deterministic (where the outcome is nearly certain)\\nhave low entropy; distributions that are closer to uniform have high entropy. See\\nFig. 3.5 for a demonstration. When x is continous, the Shannon entropy is known\\nas the diﬀerential entropy.\\n\\nIf we have two separate probability distributions P(x) and Q(x) over the same\\nrandom variable x, we can measure how diﬀerent these two distributions are using\\nthe Kullback-Leibler (KL) divergence:\\n\\nDKL(\\n\\nP Q\\ue06b\\n\\n) = \\n\\nE x∼P\\ue014log\\n\\nP x( )\\n\\nQ x( )\\ue015 = Ex∼P [log ( )\\n\\nP x −\\n\\n73\\n\\nlog ( )]\\nQ x .\\n\\n(3.50)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nIn the case of discrete variables, it is the extra amount of information (measured\\nin bits if we use the base\\nlogarithm, but in machine learning we usually use nats\\nand the natural logarithm) needed to send a message containing symbols drawn\\nfrom probability distribution P , when we use a code that was designed to minimize\\nthe length of messages drawn from probability distribution\\n\\n.Q\\n\\n2\\n\\nThe KL divergence has many useful properties, most notably that it is non-\\nnegative. The KL divergence is 0 if and only if P and Qare the same distribution in\\nthe case of discrete variables, or equal “almost everywhere” in the case of continuous\\nvariables. Because the KL divergence is non-negative and measures the diﬀerence\\nbetween two distributions, it is often conceptualized as measuring some sort of\\ndistance between these distributions. However, it is not a true distance measure\\nbecause it is not symmetric: DKL(P Q\\ue06b ) \\ue036= DKL( Q P\\ue06b ) for some P and Q. This\\nasymmetry means that there are important consequences to the choice of whether\\nto use DKL(\\n\\n. See Fig. 3.6 for more detail.\\n\\nor DKL (\\n\\n)\\n\\nP Q\\ue06b\\n\\n)Q P\\ue06b\\n\\nA quantity that is closely related to the KL divergence is the cross-entropy\\nH (P, Q ) = H (P) + DKL(P Q\\ue06b ), which is similar to the KL divergence but lacking\\nthe term on the left:\\n(3.51)\\n\\nH P, Q(\\n\\n) = −E\\n\\nx∼P log ( )Q x .\\n\\nMinimizing the cross-entropy with respect to Q is equivalent to minimizing the\\nKL divergence, because\\n\\ndoes not participate in the omitted term.\\n\\nQ\\n\\nWhen computing many of these quantities, it is common to encounter expres-\\nsions of the form 0log 0. By convention, in the context of information theory, we\\ntreat these expressions as limx→0 x\\n\\nlog = 0.\\n\\nx\\n\\n3.14 Structured Probabilistic Models\\n\\nMachine learning algorithms often involve probability distributions over a very\\nlarge number of random variables. Often, these probability distributions involve\\ndirect interactions between relatively few variables. Using a single function to\\ndescribe the entire joint probability distribution can be very ineﬃcient (both\\ncomputationally and statistically).\\n\\nInstead of using a single function to represent a probability distribution, we\\ncan split a probability distribution into many factors that we multiply together.\\nFor example, suppose we have three random variables: a, b and c . Suppose that\\na inﬂuences the value of b and b inﬂuences the value of c, but that a and c are\\nindependent given b. We can represent the probability distribution over all three\\n\\n74\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nq∗ = argminq DKL(\\n\\n)p q\\ue06b\\n\\nq ∗ = argminq DKL (\\n\\nq p\\ue06b\\n\\n)\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np x( )\\nq∗( )x\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np( )x\\nq ∗( )x\\n\\nx\\n\\nx\\n\\nFigure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x ) and\\nwish to approximate it with another distribution q(x). We have the choice of minimizing\\neither DKL( p q\\ue06b ) or DKL( q p\\ue06b ). We illustrate the eﬀect of this choice using a mixture of\\ntwo Gaussians for p, and a single Gaussian for q. The choice of which direction of the\\nKL divergence to use is problem-dependent. Some applications require an approximation\\nthat usually places high probability anywhere that the true distribution places high\\nprobability, while other applications require an approximation that rarely places high\\nprobability anywhere that the true distribution places low probability. The choice of the\\ndirection of the KL divergence reﬂects which of these considerations takes priority for each\\napplication. (Left) The eﬀect of minimizing DKL (p q\\ue06b ). In this case, we select a q that has\\nhigh probability where p has high probability. When p has multiple modes, q chooses to\\nblur the modes together, in order to put high probability mass on all of them. (Right) The\\neﬀect of minimizing DKL (q p\\ue06b ). In this case, we select a q that has low probability where\\np has low probability. When p has multiple modes that are suﬃciently widely separated,\\nas in this ﬁgure, the KL divergence is minimized by choosing a single mode, in order to\\navoid putting probability mass in the low-probability areas between modes of p . Here, we\\nillustrate the outcome when q is chosen to emphasize the left mode. We could also have\\nachieved an equal value of the KL divergence by choosing the right mode. If the modes\\nare not separated by a suﬃciently strong low probability region, then this direction of the\\nKL divergence can still choose to blur the modes.\\n\\n75\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nvariables as a product of probability distributions over two variables:\\n\\np ,\\n\\n(a b c) =  ( )a (\\np\\n\\np\\n\\n,\\n\\nb a|\\n\\n)\\n\\nc b|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.52)\\n\\nThese factorizations can greatly reduce the number of parameters needed\\nto describe the distribution. Each factor uses a number of parameters that is\\nexponential in the number of variables in the factor. This means that we can greatly\\nreduce the cost of representing a distribution if we are able to ﬁnd a factorization\\ninto distributions over fewer variables.\\n\\nWe can describe these kinds of factorizations using graphs. Here we use the\\nword “graph” in the sense of graph theory: a set of vertices that may be connected\\nto each other with edges. When we represent the factorization of a probability\\ndistribution with a graph, we call it a structured probabilistic model\\ngraphical\\nmodel.\\n\\nor\\n\\nThere are two main kinds of structured probabilistic models: directed and\\nundirected. Both kinds of graphical models use a graph G in which each node\\nin the graph corresponds to a random variable, and an edge connecting two\\nrandom variables means that the probability distribution is able to represent direct\\ninteractions between those two random variables.\\n\\nDirected models use graphs with directed edges, and they represent factoriza-\\ntions into conditional probability distributions, as in the example above. Speciﬁcally,\\na directed model contains one factor for every random variable x i in the distribution,\\nand that factor consists of the conditional distribution over xi given the parents of\\nxi, denoted P a G(x i):\\n\\n(3.53)\\n\\np( ) =x \\ue059i\\n\\np (xi | P aG (xi )) .\\n\\nSee Fig. 3.7 for an example of a directed graph and the factorization of probability\\ndistributions it represents.\\n\\nUndirected models use graphs with undirected edges, and they represent fac-\\ntorizations into a set of functions; unlike in the directed case, these functions are\\nusually not probability distributions of any kind. Any set of nodes that are all\\nconnected to each other in G is called a clique. Each clique C ( )i\\nin an undirected\\nmodel is associated with a factor φ( )i (C ( )i ). These factors are just functions, not\\nprobability distributions. The output of each factor must be non-negative, but\\nthere is no constraint that the factor must sum or integrate to 1 like a probability\\ndistribution.\\n\\nThe probability of a conﬁguration of random variables is proportional to the\\nproduct of all of these factors—assignments that result in larger factor values are\\n\\n76\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.7: A directed graphical model over random variables a , b, c, d and e. This graph\\ncorresponds to probability distributions that can be factored as\\n\\n(a b c d e) =  ( )a (\\np ,\\np\\n\\np\\n\\n,\\n\\n,\\n\\n,\\n\\nb a|\\n\\n)\\n\\n(c a|\\np\\n\\n,\\n\\nb) (\\np\\n\\nd b|\\n\\n)\\n\\ne c|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.54)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nmore likely. Of course, there is no guarantee that this product will sum to 1. We\\ntherefore divide by a normalizing constant Z, deﬁned to be the sum or integral\\nover all states of the product of the φ functions, in order to obtain a normalized\\nprobability distribution:\\n\\np( ) =x\\n\\n1\\n\\nZ \\ue059i\\n\\nφ( )i \\ue010C( )i\\ue011 .\\n\\n(3.55)\\n\\nSee Fig. 3.8 for an example of an undirected graph and the factorization of\\nprobability distributions it represents.\\n\\nKeep in mind that these graphical representations of factorizations are a\\nlanguage for describing probability distributions. They are not mutually exclusive\\nfamilies of probability distributions. Being directed or undirected is not a property\\nof a probability distribution; it is a property of a particular\\nof a\\nprobability distribution, but any probability distribution may be described in both\\nways.\\n\\ndescription\\n\\nThroughout Part I and Part II of this book, we will use structured probabilistic\\nmodels merely as a language to describe which direct probabilistic relationships\\ndiﬀerent machine learning algorithms choose to represent. No further understanding\\nof structured probabilistic models is needed until the discussion of research topics,\\nin Part III, where we will explore structured probabilistic models in much greater\\ndetail.\\n\\n77\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.8: An undirected graphical model over random variables a, b, c, d and e . This\\ngraph corresponds to probability distributions that can be factored as\\n\\n(a b c d e) =\\np ,\\n\\n,\\n\\n,\\n\\n,\\n\\n1\\nZ\\n\\nφ (1)(\\n\\na b c\\n,\\n\\n, φ (2) (\\n\\n)\\n\\n)b d, φ(3)(\\n\\n)c e,\\n.\\n\\n(3.56)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nThis chapter has reviewed the basic concepts of probability theory that are\\nmost relevant to deep learning. One more set of fundamental mathematical tools\\nremains: numerical methods.\\n\\n78\\n\\n\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/228524191\n",
      "\n",
      "The Apriori Algorithm–a Tutorial\n",
      "\n",
      "Article · January 2008\n",
      "\n",
      "DOI: 10.1142/9789812709066_0006\n",
      "\n",
      "CITATIONS\n",
      "54\n",
      "\n",
      "1 author:\n",
      "\n",
      "Markus Hegland\n",
      "Australian National University\n",
      "\n",
      "188 PUBLICATIONS   1,925 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "READS\n",
      "3,464\n",
      "\n",
      "Some of the authors of this publication are also working on these related projects:\n",
      "\n",
      "Discrete Thin Plate Spline Smoothing View project\n",
      "\n",
      "Modelling gene regulatory networks View project\n",
      "\n",
      "All content following this page was uploaded by Markus Hegland on 29 May 2014.\n",
      "\n",
      "The user has requested enhancement of the downloaded file.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm – a Tutorial\n",
      "\n",
      "Markus Hegland\n",
      "\n",
      "CMA, Australian National University\n",
      "\n",
      "John Dedman Building, Canberra ACT 0200, Australia\n",
      "\n",
      "E-mail: Markus.Hegland@anu.edu.au\n",
      "\n",
      "Association rules are ”if-then rules” with two measures which quantify\n",
      "the support and conﬁdence of the rule for a given data set. Having their\n",
      "origin in market basked analysis, association rules are now one of the\n",
      "most popular tools in data mining. This popularity is to a large part due\n",
      "to the availability of eﬃcient algorithms. The ﬁrst and arguably most\n",
      "inﬂuential algorithm for eﬃcient association rule discovery is Apriori.\n",
      "\n",
      "In the following we will review basic concepts of association rule dis-\n",
      "covery including support, conﬁdence, the apriori property, constraints\n",
      "and parallel algorithms. The core consists of a review of the most im-\n",
      "portant algorithms for association rule discovery. Some familiarity with\n",
      "concepts like predicates, probability, expectation and random variables\n",
      "is assumed.\n",
      "\n",
      "1. Introduction\n",
      "Large amounts of data have been collected routinely in the course of day-\n",
      "to-day management in business, administration, banking, the delivery of\n",
      "social and health services, environmental protection, security and in pol-\n",
      "itics. Such data is primarily used for accounting and for management of\n",
      "the customer base. Typically, management data sets are very large and\n",
      "constantly growing and contain a large number of complex features. While\n",
      "these data sets reﬂect properties of the managed subjects and relations, and\n",
      "are thus potentially of some use to their owner, they often have relatively\n",
      "low information density. One requires robust, simple and computationally\n",
      "eﬃcient tools to extract information from such data sets. The development\n",
      "and understanding of such tools is the core business of data mining. These\n",
      "tools are based on ideas from computer science, mathematics and statistics.\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "2\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "[?] and,\n",
      "\n",
      "The introduction of association rule mining in 1993 by Agrawal, Imielin-\n",
      "ski and Swami\n",
      "in particular, the development of an eﬃcient\n",
      "algorithm by Agrawal and Srikant [?] and by Mannila, Toivonen and\n",
      "Verkamo [?] marked a shift of the focus in the young discipline of data\n",
      "mining onto rules and data bases. Consequently, besides involving the tra-\n",
      "ditional statistical and machine learning community, data mining now at-\n",
      "tracted researchers with a variety of skills ranging from computer science,\n",
      "mathematics, science, to business and administration. The urgent need for\n",
      "computational tools to extract information from data bases and for man-\n",
      "power to apply these tools has allowed a diverse community to settle in\n",
      "this new area. The data analysis aspect of data mining is more exploratory\n",
      "than in statistics and consequently, the mathematical roots of probability\n",
      "are somewhat less prominent in data mining than in statistics. Computa-\n",
      "tionally, however, data mining frequently requires the solution of large and\n",
      "complex search and optimisation problems and it is here where mathemat-\n",
      "ical methods can assist most. This is particularly the case for association\n",
      "rule mining which requires searching large data bases for complex rules.\n",
      "Mathematical modelling is required in order to generalise the original tech-\n",
      "niques used in market basket analysis to a wide variety of applications.\n",
      "Mathematical analysis provides insights into the performance of the algo-\n",
      "rithms.\n",
      "\n",
      "An association rule is an implication or if-then-rule which is supported\n",
      "by data. The motivation given in [?] for the development of association\n",
      "rules is market basket analysis which deals with the contents of point-of-\n",
      "sale transactions of large retailers. A typical association rule resulting from\n",
      "such a study could be “90 percent of all customers who buy bread and\n",
      "butter also buy milk”. Insights into customer behaviour may also be ob-\n",
      "tained through customer surveys, but the analysis of the transactional data\n",
      "has the advantage of being much cheaper and covering all current cus-\n",
      "tomers. Compared to customer surveys, the analysis of transactional data\n",
      "does have some severe limitations, however. For example, point-of-sale data\n",
      "typically does not contain any information about personal interests, age and\n",
      "occupation of customers. Nonetheless, market basket analysis can provide\n",
      "new insights into customer behaviour and has led to higher proﬁts through\n",
      "better customer relations, customer retention, better product placements,\n",
      "product development and fraud detection.\n",
      "\n",
      "Market basket analysis is not limited to retail shopping but has also\n",
      "\n",
      "been applied in other business areas including\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "3\n",
      "\n",
      "• credit card transactions,\n",
      "• telecommunication service purchases,\n",
      "• banking services,\n",
      "• insurance claims, and\n",
      "• medical patient histories.\n",
      "\n",
      "Association rule mining generalises market basket analysis and is used in\n",
      "many other areas including genomics, text data analysis and Internet in-\n",
      "trusion detection. For motivation we will in the following mostly focus on\n",
      "retail market basket analysis.\n",
      "\n",
      "When a customer passes through a point of sale, the contents of his\n",
      "market basket are registered. This results in large collections of market\n",
      "basket data which provide information about which items were sold and, in\n",
      "particular, which combinations of items were sold. The small toy example in\n",
      "the table of ﬁgure 1 shall illustrate this further. Each row corresponds to a\n",
      "\n",
      "market basket id market basket content\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "\n",
      "orange juice, soda water\n",
      "milk, orange juice, bread\n",
      "orange juice, butter\n",
      "orange juice, bread, soda water\n",
      "bread\n",
      "\n",
      "Fig. 1. Five grocery market baskets\n",
      "\n",
      "market basket or transaction containing popular retail items. An inspection\n",
      "of the table reveals that:\n",
      "\n",
      "• Four of the ﬁve baskets contain orange juice,\n",
      "• two baskets contain soda water\n",
      "• half of the baskets which contain orange juice also contain soda\n",
      "• all the baskets which contain soda also contain juice\n",
      "\n",
      "These rules are very simple as is typical for association rule mining. Sim-\n",
      "ple rules are understandable and ultimately useful. In a large retail shop\n",
      "there are usually more than 10,000 items on sale and the shop may service\n",
      "thousands of customers every day. Thus the size of the collected data is\n",
      "substantial and even the detection of simple rules like the ones above re-\n",
      "quires sophisticated algorithms. The eﬃciency of the algorithms will depend\n",
      "on the particular characteristics of the data sets. An important feature of\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "4\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "many retail data sets is that an average market basket only contains a small\n",
      "subset of all items available.\n",
      "\n",
      "The simplest model for the customers assumes that the customers choose\n",
      "products from the shelves in the shop at random. In this case the choice of\n",
      "each product is independent from any other product. Consequently, asso-\n",
      "ciation rule discovery will simply recover the likelihoods for any item to be\n",
      "chosen. While it is important to compare the performance of other models\n",
      "with this “null-hypothesis” one would usually ﬁnd that shoppers do have\n",
      "a more complex approach when they ﬁll the shopping basket (or trolley).\n",
      "They will buy breakfast items, lunch items, dinner items and snacks, party\n",
      "drinks, and Sunday dinners. They will have preferences for cheap items, for\n",
      "(particular) brand items, for high-quality, for freshness, low-fat, special diet\n",
      "and environmentally safe items. Such goals and preferences of the shopper\n",
      "will inﬂuence the choices but can not be directly observed. In some sense,\n",
      "market basket analysis should provide information about how the shop-\n",
      "pers choose. In order to understand this a bit further consider the case of\n",
      "politicians who vote according to party policy but where we will assume\n",
      "for the moment that the party membership is unknown. Is it possible to\n",
      "see an eﬀect of the party membership in voting data? For a small but real\n",
      "illustrative example consider the US Congress voting records from 1984 [?],\n",
      "see ﬁgure 2. The 16 columns of the displayed bit matrix correspond to the\n",
      "16 votes and the 435 rows to the members of congress. We have simpliﬁed\n",
      "the data slightly so that a matrix element is one (pixel set) in the case of\n",
      "votes which contains “voted for”, “paired for” and “announced for” and the\n",
      "matrix element is zero in all other cases. The left data matrix in ﬁgure 2\n",
      "is the original data where only the rows and columns have been randomly\n",
      "permuted to remove any information introduced through the way the data\n",
      "was collected. The matrix on the right side is purely random such that each\n",
      "entry is independent and only the total number of entries is maintained.\n",
      "Can you see the diﬀerence between the two bit matrices? We found that for\n",
      "most people, the diﬀerence between the two matrices is not obvious from\n",
      "visual inspection alone.\n",
      "\n",
      "Data mining aims to discover patterns in the left bit matrix and thus\n",
      "diﬀerences between the two examples. In particular, we will ﬁnd columns or\n",
      "items which display similar voting patterns and we aim to discover rules re-\n",
      "lating to the items which hold for a large proportion of members of congress.\n",
      "We will see how many of these rules can be explained by underlying mech-\n",
      "anisms (in this case party membership).\n",
      "\n",
      "In this example the selection of what are rows and what columns is\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "5\n",
      "\n",
      "Fig. 2.\n",
      "\n",
      "1984 US House of Representatives Votes, with 16 items voted on\n",
      "\n",
      "somewhat arbitrary. Instead of patterns regarding the items voted on one\n",
      "might be interested in patterns relating the members of Congress. For ex-\n",
      "ample one might be interested in statements like “if member x and member\n",
      "y vote yes then member z votes yes as well. Statements like this may reveal\n",
      "some of the interactions between the members of Congress. The duality of\n",
      "observations and objects occurs in other areas of data mining as well and\n",
      "illustrates that data size and data complexity are really two dual concepts\n",
      "which can be interchanged in many cases. This is in particular exploited in\n",
      "some newer association rule discovery algorithms which are based on formal\n",
      "concept analysis [?].\n",
      "\n",
      "By inspecting the data matrix of the voting example, one ﬁnds that the\n",
      "\n",
      "voting datarandom data\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "6\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "items have received between 34 to 63.5 percent yes votes. Pairs of items\n",
      "have received between 4 and 49 percent yes votes. The pairs with the most\n",
      "yes votes (over 45 percent) are in the columns 2/6, 4/6, 13/15, 13/16 and\n",
      "15/16. Some rules obtained for these pairs are: 92 percent of the yes votes\n",
      "in column 2 are also yes votes in column 6, 86 percent of the yes votes in\n",
      "column 4 are also yes votes in column 6 and, on the other side, 88 percent\n",
      "of the votes in column 13 are also in column 15 and 89 percent of the yes\n",
      "votes in column 16 are also yes votes in column 15. These ﬁgures suggest\n",
      "combinations of items which could be further investigated in terms of causal\n",
      "relationships between the items. Only a careful statistical analysis may\n",
      "provide some certainty on this. This and other issues concerning inference\n",
      "belong to statistics and are beyond the scope of this tutorial which focusses\n",
      "on computational issues.\n",
      "\n",
      "2. Itemsets and Associations\n",
      "In this section a formal mathematical model is derived to describe itemsets\n",
      "and associations to provide a framework for the discussion of the apriori al-\n",
      "gorithm. In order to apply ideas from market basket analysis to other areas\n",
      "one needs a general description of market baskets which can equally describe\n",
      "collections of medical services received by a patient during an episode of\n",
      "care, subsequences of amino acid sequences of a protein, and collections or\n",
      "words or concepts used on web pages. In this general description the items\n",
      "are numbered and a market basket is represented by an indicator vector.\n",
      "\n",
      "2.1. The Datamodel\n",
      "In this subsection a probabilistic model for the data is given along with\n",
      "some simple model examples. For this, we consider the voting data example\n",
      "again.\n",
      "First, the items are enumerated as 0, . . . , d − 1. Often, enumeration\n",
      "is done such that the more frequent items correspond to lower numbers\n",
      "but this is not essential. Itemsets are then sets of integers between 0 and\n",
      "d − 1. The itemsets are represented by bitvectors x ∈ X := {0, 1}d where\n",
      "item j is present in the corresponding itemset iﬀ the j-th bit is set in x.\n",
      "Consider the “micromarket” with the items juice, bread, milk, cheese and\n",
      "potatoes with item numbers 0, 1, 2, 3 and 4, respectively. The market basket\n",
      "containing bread, milk and potatoes is then mapped onto the set {1, 2, 4}\n",
      "and is represented by the bitvector (0, 1, 1, 0, 1). From the bitvector it is\n",
      "clear which elements are in the market basket or itemset and which are\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "7\n",
      "\n",
      "not.\n",
      "\n",
      "The data is a sequence of itemsets which is represented as a bitmatrix\n",
      "where each row corresponds to an itemset and the columns correspond to\n",
      "the items. For the micromarket example a dataset containing the market\n",
      "baskets {juice,bread, milk}, {potato} and {bread, potatoes} would be rep-\n",
      "resented by the matrix\n",
      "\n",
      "1 1 1 0 0\n",
      "\n",
      "0 0 0 0 1\n",
      "0 1 0 1 0\n",
      "\n",
      " .\n",
      "\n",
      "In the congressional voting example mentioned in the previous section the\n",
      "ﬁrst few rows of matrix are\n",
      "\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "\n",
      "0 1 0\n",
      "1 1 1\n",
      "1 0 0\n",
      "1 0 1\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      "1 0\n",
      "1 0\n",
      "0 1\n",
      "0 1\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "and they correspond to the following “itemsets” (or sets of yes-votes):\n",
      "\n",
      "4\n",
      "3\n",
      "6\n",
      "6\n",
      "7\n",
      "\n",
      "3\n",
      "2\n",
      "5\n",
      "4\n",
      "3\n",
      "\n",
      "6\n",
      "5\n",
      "8\n",
      "6\n",
      "9\n",
      "8\n",
      "9\n",
      "8\n",
      "9 10\n",
      "\n",
      "10 13 14\n",
      "10 11 13 14\n",
      "\n",
      "8\n",
      "9\n",
      "15\n",
      "11 12 15\n",
      "11 13 16.\n",
      "\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "It is assumed that the data matrix X ∈ {0, 1}n,d is random and thus\n",
      "the elements x(i)\n",
      "j are binary random variables. One would in general have\n",
      "to assume correlations between both rows and columns. The correlations\n",
      "between the columns might relate to the type of shopping and customer,\n",
      "e.g., young family with small kids, weekend shopping or shopping for a\n",
      "speciﬁc dish. Correlations between the rows might relate to special oﬀers of\n",
      "the retailer, time of the day and week. In the following it will be assumed\n",
      "that the rows are drawn independently from a population of market baskets.\n",
      "Thus it is assumed that there is a probability distribution function p :→\n",
      "[0, 1] with\n",
      "\n",
      "where X = {0, 1}d. The probability measure with distribution p is denoted\n",
      "by P and one has P (A) =\n",
      "\n",
      "x∈A p(x).\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "x∈X\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "p(x) = 1\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "8\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "The data can be represented as an empirical distribution with\n",
      "\n",
      "n(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "pemp(x) =\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "δ(x − x(i))\n",
      "\n",
      "where δ(x) is the indicator function where δ(0) = 1 and δ(x) = 0 if\n",
      "x (cid:54)= 0. (For simplicity the empty market basket is denoted by 0 instead\n",
      "of (0, . . . , 0).) All the information derived from the data is stored in pemp\n",
      "but some sort of “smoothing” is required if one would like to generalise in-\n",
      "sights from the empirical distribution of one itemset collection to another to\n",
      "separate the noise from the noise from the signal. Association rule discovery\n",
      "has its own form of smoothing as will be seen in the following.\n",
      "\n",
      "The task of frequent itemset mining consists of ﬁnding itemsets which\n",
      "occur frequently in market baskets. For this one recalls that the itemsets\n",
      "are partially ordered with respect to inclusion (the subset relation) and we\n",
      "write x ≤ y if the set with representation x is a subset of the set with\n",
      "representation y or x = y. With this partial order one deﬁnes the support\n",
      "of an itemset x to be\n",
      "\n",
      "s(x) = P ({z | x ≤ z})\n",
      "\n",
      "(1)\n",
      "\n",
      "which is also called the anticummulative distribution function of the prob-\n",
      "ability P . The support is a function s : X → [0, 1] and s(0) = 1. By\n",
      "construction, the support is antimonotone, i.e., if x ≤ y then p(x) ≥ p(y).\n",
      "This antimonotonicity is the basis for eﬃcient algorithms to ﬁnd all frequent\n",
      "itemsets which are deﬁned as itemsets for which s(x) ≥ σ0 > 0 for some\n",
      "user deﬁned σ0.\n",
      "\n",
      "Equation 1 can be reformulated in terms of p(x) as\n",
      "\n",
      "s(x) =\n",
      "\n",
      "p(z).\n",
      "\n",
      "(2)\n",
      "\n",
      "For given supports s(x), this is a linear system of equations which can be\n",
      "solved recursively using s(e) = p(e) (where e = (1, . . . , 1) is the maximal\n",
      "itemset) and\n",
      "\n",
      "p(x) = s(x) −\n",
      "\n",
      "p(z).\n",
      "\n",
      "z>x\n",
      "\n",
      "It follows that the support function s(x) provides an alternative description\n",
      "of the probability measure P which is equivalent to p. However, for many\n",
      "examples the form of s(x) turns out to be simpler. In the cases of market\n",
      "baskets it is highly unlikely, that market baskets contain large numbers of\n",
      "items and so approximations with s(x) = 0 for x with a large number of\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "z≥x\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "9\n",
      "\n",
      "nonzero components will usually produce good approximations of the item-\n",
      "set distribution p(x). This leads to an eﬀective smoothing mechanism for\n",
      "association rule discovery where the minimal support σ0 acts as a smooth-\n",
      "ing parameter which in principle could be determined from a test data set\n",
      "or with crossvalidation.\n",
      "\n",
      "2.1.1. Example: The Random Shopper\n",
      "In the simplest case all the bits (items) in x are chosen independently\n",
      "with probability p0. We call this the case of the “random shopper” as it\n",
      "corresponds to a shopper who ﬁlls the market basket with random items.\n",
      "The distribution is in this case\n",
      "\n",
      "p(x) = p\n",
      "\n",
      "|x|\n",
      "0 (1 − p0)d−|x|\n",
      "\n",
      "where |x| is the number of bits set in x and d is the total number of items\n",
      "available, i.e., number of components of x. As any z ≥ x has at least all the\n",
      "bits set which are set in x one gets for the support\n",
      "\n",
      "s(x) = p\n",
      "\n",
      "|x|\n",
      "0 .\n",
      "\n",
      "It follows that the frequent itemsets x are exactly the ones with few items,\n",
      "in particular, where\n",
      "\n",
      "|x| ≤ log(σ0)/ log(p0).\n",
      "\n",
      "For example, if one is interested in ﬁnding itemsets which are supported by\n",
      "one percent of the data records and if the probability of choosing any item\n",
      "is p0 = 0.1 the frequent itemsets are the ones with at most two items. For\n",
      "large shops one would typically have p0 much smaller. Note that if p0 < σ0\n",
      "one would not get any frequent itemsets at all.\n",
      "\n",
      "The random shopper is of course not a realistic model for shopping\n",
      "and one would in particular not expect to draw useful conclusions from\n",
      "the frequent itemsets. Basically, the random shopper is the market bas-\n",
      "ket equivalent of noise. The above discussion, however, might be used to\n",
      "guide the choice of σ0 to ﬁlter out the noise in market basket analysis. In\n",
      "particular, one could choose\n",
      "\n",
      "σ0 = min\n",
      "|x|=1\n",
      "\n",
      "s(x).\n",
      "\n",
      "Note that in the random shopper case the rhs is just p0. In this case, all\n",
      "the single items would be frequent.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "10\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "A slight generalisation of the random shopper example above assumes\n",
      "that the items are selected independently but with diﬀerent probabilities\n",
      "pj. In this case one gets\n",
      "\n",
      "d(cid:89)\n",
      "\n",
      "p(x) =\n",
      "\n",
      "j (1 − pj)1−xj\n",
      "pxj\n",
      "\n",
      "and\n",
      "\n",
      "j=1\n",
      "\n",
      "s(x) =\n",
      "\n",
      "d(cid:89)\n",
      "\n",
      "j=1\n",
      "\n",
      "pxj\n",
      "j .\n",
      "\n",
      "In examples one can often ﬁnd that the pj, when sorted, are approximated\n",
      "by Zipf’s law, i.e,\n",
      "\n",
      "pj = α\n",
      "j\n",
      "\n",
      "for some constant α. It follows again that itemsets with few popular items\n",
      "are most likely.\n",
      "\n",
      "However, this type of structure is not really what is of interest in as-\n",
      "sociation rule mining. To illustrate this consider again the case of the US\n",
      "Congress voting data. In ﬁgure ?? the support for single itemsets are dis-\n",
      "played for the case of the actual data matrix and for a random permutation\n",
      "of all the matrix elements. The supports are between 0.32 and 0.62 for the\n",
      "original data where for the randomly permuted case the supports are be-\n",
      "tween 0.46 and 0.56. Note that these supports are computed from the data,\n",
      "in theory, the permuted case should have constant supports somewhere\n",
      "slightly below 0.5. More interesting than the variation of the supports of\n",
      "single items is the case when 2 items are considered. Supports for the votes\n",
      "displayed in ﬁgure ??. are of the form “V2 and Vx”. Note that “V2 and\n",
      "V2” is included for reference, even though this itemset has only one item.\n",
      "In the random data case where the vote for any pair {V2,Vx} (where Vx\n",
      "is not V2) is the square of the vote for the single item V2 as predicted\n",
      "by the “random shopper theory” above. One notes that some pairs have\n",
      "signiﬁcantly higher supports than the random ones and others signiﬁcantly\n",
      "lower supports. This type of behaviour is not captured by the “random\n",
      "shopper” model above even if the case of variable supports for single items\n",
      "are allowed. The following example attempts to model some this behaviour.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "11\n",
      "\n",
      "Fig. 3. Supports for all the votes in the US congress data (split by party).\n",
      "\n",
      "2.1.2. Example: Multiple Shopper and Item Classes\n",
      "Consider now the case of some simple structure. Assume that there are\n",
      "two types of shoppers and two types of items. Assume that the items are\n",
      "x = (x0, x1) where the vectors xi correspond to items of class i. In practice,\n",
      "the type of items might have to be discovered as well. Consider that the\n",
      "shoppers of type i have a probability πi of ﬁlling a market basket where here\n",
      "i = 0, 1. Assume that it is not known to which type of shopper a market\n",
      "basket belongs. Finally, assume that the diﬀerence between the two types\n",
      "of shoppers relates to how likely they are to put items of the two types into\n",
      "their market basket. Let pi,j denote the probability that shopper of type i\n",
      "puts an item of type j into the market basket. In this case the probability\n",
      "\n",
      "V2V8V3V14V15V9V10V12voting datavote numberproportion yes votes0.00.10.20.30.40.50.60.7V2V8V3V14V15V9V10V12random datavote numberproportion yes votes0.00.10.20.30.40.50.60.7\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "12\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Fig. 4. Supports for pairs of votes in the US congress data (split by party).\n",
      "\n",
      "|x1|\n",
      "01 (1−p00)d0−|x0|(1−p01)d1−|x1|+π1 p\n",
      "\n",
      "distribution is\n",
      "|x0|\n",
      "p(x) = π0 p\n",
      "00 p\n",
      "This is a mixture model with two components. Recovery of the parameters\n",
      "from the data of mixture models uses the EM algorithm and is discussed\n",
      "in detail in [?]. Note that π0 + π1 = 1.\n",
      "\n",
      "|x0|\n",
      "10 p\n",
      "\n",
      "For frequent itemset mining, however, the support function is considered\n",
      "\n",
      "|x1|\n",
      "11 (1−p10)d0−|x0|(1−p11)d1−|x1|.\n",
      "\n",
      "and similarly to the random shopper case can be shown to be:\n",
      "\n",
      "s(x) = π0p\n",
      "\n",
      "|x0|\n",
      "00 p\n",
      "\n",
      "|x1|\n",
      "01 + π1p\n",
      "\n",
      "|x0|\n",
      "10 p\n",
      "\n",
      "|x1|\n",
      "11 .\n",
      "\n",
      "Assume that the shopper of type i is unlikely to purchase items of the other\n",
      "type. Thus p00 and p11 are much larger than p10 and p01. In this case the\n",
      "\n",
      "V2V8V3V14V15V9V10V12voting datavote number of second vote in pairproportion yes votes0.00.10.20.30.40.50.60.7V2V8V3V14V15V9V10V12random datavote number of second vote in pairproportion yes votes0.00.10.20.30.40.50.60.7\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "13\n",
      "\n",
      "frequent itemsets are going to be small (as before), moreover, one has either\n",
      "x0 = 0 or x1 = 0, thus, frequent itemsets will only contain items of one\n",
      "type. Thus in this case frequent itemset mining acts as a ﬁlter to retrieve\n",
      "“pure” itemsets.\n",
      "\n",
      "A simple application to the voting data could consider two types of\n",
      "politicians. A question to further study would be how closely these two\n",
      "types correspond with the party lines.\n",
      "\n",
      "One can now consider generalisations of this case by including more\n",
      "than two types, combining with diﬀerent probabilities (Zipf’s law) for the\n",
      "diﬀerent items in the same class and even use itemtypes and customer types\n",
      "which overlap. These generalisations lead to graphical models and Bayesian\n",
      "nets [?,?]. The “association rule approach” in these cases distinguishes itself\n",
      "by using support functions, frequent itemsets and in particular, is based on\n",
      "binary data. A statistical approach to this type of data is “discriminant\n",
      "analysis” [?].\n",
      "\n",
      "2.2. The Size of Itemsets\n",
      "The size of the itemsets is a key factor in determining the performance of\n",
      "association rule discovery algorithms. The size of an itemset is the equal to\n",
      "the number of bits set, one has\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "|x| =\n",
      "\n",
      "xi\n",
      "\n",
      "if the components of x are interpreted as integers 0 or 1 this is a real valued\n",
      "random variable or function deﬁned on X. The expectation of a general real\n",
      "random variable f is\n",
      "\n",
      "The expectation is monotone in the sense that f ≥ g ⇒ E(f) ≥ E(g) and\n",
      "the expectation of a constant function is the function value. The variance\n",
      "of the random variable corresponding to the function f is\n",
      "\n",
      "For an arbitrary but ﬁxed itemset x ∈ X consider the function\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "x∈X\n",
      "\n",
      "E(f) =\n",
      "\n",
      "p(x)f(x).\n",
      "\n",
      "var(f) = E\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(f − E(f))2(cid:1)\n",
      "d(cid:89)\n",
      "\n",
      ".\n",
      "\n",
      "ax(z) =\n",
      "\n",
      "zxi\n",
      "i\n",
      "\n",
      ".\n",
      "\n",
      "Thus function takes values which are either 0 or 1 and ax(z) = 1 iﬀ x ≤ z\n",
      "as in this case all the components zi which occur to power 1 in ax(z)\n",
      "\n",
      "i=1\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "14\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "are one. Note that ax(z) is a monotone function of z and antimonotone\n",
      "function of x. Moreover, one has for the expectation E(ax) = s(x) and\n",
      "variance var(ax) = s(x)(1 − s(x)). The term 1 − s(x) is the support of the\n",
      "complement of the itemset x. The values for the expectation and variance\n",
      "are obtained directly from the deﬁnition of the expectation and the fact\n",
      "that ax(z)2 = ax(z) (holds for any function with values 0 and 1).\n",
      "\n",
      "Our main example of a random variable is the length of an itemset,\n",
      "\n",
      "d(cid:88)\n",
      "(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "f(x) = |x| =\n",
      "\n",
      "xj.\n",
      "\n",
      "E(|x|) =\n",
      "\n",
      "s(z).\n",
      "\n",
      "|z|=1\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "|x||z|=1\n",
      "\n",
      "The average length of itemsets is the expectation of f and one can see that\n",
      "\n",
      "The variance of the length is also expressed in terms of the support as\n",
      "\n",
      "var(|x|) =\n",
      "\n",
      "(s(x ∨ z) − s(x)s(z))\n",
      "\n",
      "where x ∨ z corresponds to the union of the itemsets x and z.\n",
      "\n",
      "With the expectation and using the Markov inequality one gets a simple\n",
      "\n",
      "bound on the probability of large itemsets as\n",
      "\n",
      "P ({x | |x| ≥ m}) ≤ E(|x|)/m.\n",
      "\n",
      "The expected length is easily obtained directly from the data and this bound\n",
      "gives an easy upper bound on probability of large itemsets. Of course one\n",
      "could just as easily get a histogram for the size of itemsets directly from\n",
      "the data. Using the above equation one gets an estimate for the average\n",
      "support of one-itemsets as E(|x|)/d.\n",
      "Consider now the special example of a random shopper discussed pre-\n",
      "viously. In this case one gets E(|x|) = dp0. The distribution of the length\n",
      "is in this case binomial and one has:\n",
      "d\n",
      "r\n",
      "\n",
      "0] (1 − p0)d−r.\n",
      "pr\n",
      "\n",
      "P (|x| = r) =\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "Moreover, for very large d and small p0 one gets a good approximation\n",
      "using the Poisson distribution\n",
      "\n",
      "p(|x| = r) ≈ 1\n",
      "\n",
      "r! e−λλr.\n",
      "\n",
      "with λ = E(|x|).\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "15\n",
      "\n",
      "The apriori algorithm which will be discussed in the following works\n",
      "best when long itemsets are unlikely. Thus in order to choose a suitable\n",
      "algorithm, it is important to check if this is the case, e.g., by using the\n",
      "histogram for the length |x|.\n",
      "\n",
      "2.3. The Itemset Lattice\n",
      "The search for frequent itemsets beneﬁts from a structured search space as\n",
      "the itemsets form a lattice. This lattice is also intuitive and itemsets close\n",
      "to 0 in the lattice are often the ones which are of most interest and lend\n",
      "themselves to interpretation and further discussion.\n",
      "\n",
      "As X consists of sets or bitvectors, one has a natural partial ordering\n",
      "which is induced by the subset relation. In terms of the bitvectors one can\n",
      "deﬁne this component-wise as\n",
      "\n",
      "x ≤ y :⇔ xi ≤ yi.\n",
      "\n",
      "Alternatively,\n",
      "\n",
      "x ≤ y :⇔ (xi = 1 ⇒ yi = 1,\n",
      "\n",
      "i = 1, . . . , d) .\n",
      "\n",
      "If xi = 1 and x ≤ y then it follows that yi = 1. Thus if the corresponding\n",
      "itemset to x contains item i then the itemset corresponding to y has to\n",
      "contain the same item. In other words, the itemset corresponding to x is a\n",
      "subset of the one corresponding to y.\n",
      "and so if x ≤ y then |x| ≤ |y|.\n",
      "\n",
      "Subsets have at most the same number of elements as their supersets\n",
      "\n",
      "Bitvectors also allow a total order by interpreting it as an integer φ(x)\n",
      "\n",
      "by\n",
      "\n",
      "φ(x) =\n",
      "\n",
      "d−1(cid:88)\n",
      "\n",
      "xi2i.\n",
      "\n",
      "Now as φ is a bijection it induces a total order on X deﬁned as x ≺ y iﬀ\n",
      "φ(x) < φ(y). This is the colex order and the colex order extends the partial\n",
      "order as x ≤ y ⇒ x ≺ y.\n",
      "\n",
      "i=0\n",
      "\n",
      "The partial order order has a smallest element which consists of the\n",
      "empty set, corresponding to the bitvector x = (0, . . . , 0) and a largest ele-\n",
      "ment which is just the set of all items Zd, corresponding to the bitvector\n",
      "(1, . . . , 1). Furthermore, for each pair x, y ∈ X there is a greatest lower\n",
      "bound and a least upper bound. These are just\n",
      "\n",
      "x ∨ y = z\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "16\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "where zi = max{xi, yi} for i = 0, . . . , d − 1 and similarly for x ∧ y. Conse-\n",
      "quently, the partially ordered set X forms a Boolean lattice. We denote the\n",
      "maximal and minimal elements of X by e and 0.\n",
      "\n",
      "In general, partial order is deﬁned by\n",
      "\n",
      "Deﬁnition 1: A partially ordered set (X,≤) consists of a set X with a\n",
      "binary relation ≤ such that for all x, x(cid:48), x(cid:48)(cid:48) ∈ X:\n",
      "\n",
      "• x ≤ x\n",
      "• If x ≤ x(cid:48) and x(cid:48) ≤ x then x = x(cid:48)\n",
      "• If x ≤ x(cid:48) and x(cid:48) ≤ x(cid:48)(cid:48) then x ≤ x(cid:48)(cid:48)\n",
      "\n",
      "(reﬂexivity)\n",
      "(antisymmetry)\n",
      "(transitivity)\n",
      "\n",
      "A lattice is a partially ordered set with glb and lub:\n",
      "Deﬁnition 2: A lattice (X,≤) is a partially ordered set such that for each\n",
      "pair of of elements of X there is greatest lower bound and a least upper\n",
      "bound.\n",
      "\n",
      "We will call a lattice distributive if the distributive law holds:\n",
      "\n",
      "x ∧ (x(cid:48) ∨ x(cid:48)(cid:48)) = (x ∧ x(cid:48)) ∨ (x ∧ x(cid:48)(cid:48)).\n",
      "\n",
      "where x∨ y is the maximum of the two elements which contains ones when-\n",
      "ever at least one of the two elements and x ∧ x(cid:48) contains ones where both\n",
      "elements contain a one. Then we can deﬁne:\n",
      "Deﬁnition 3: A lattice (X,≤) is a Boolean lattice if\n",
      "(1) (X,≤) is distributive\n",
      "(2) It has a maximal element e and a minimal element 0 such that for all\n",
      "\n",
      "(3) Each element x as a (unique) complement x(cid:48) such that x ∧ x(cid:48) = 0 and\n",
      "\n",
      "0 ≤ x ≤ e.\n",
      "\n",
      "x ∈ X:\n",
      "\n",
      "x ∨ x(cid:48) = e.\n",
      "\n",
      "The maximal and minimal elements 0 and e satisfy 0∨ x = x and e∧ x = x.\n",
      "In algebra one considers the properties of the conjunctives ∨ and ∧ and a\n",
      "set which has conjunctives which have the properties of a Boolean lattice\n",
      "is called a Boolean algebra. We will now consider some of the properties of\n",
      "Boolean algebras.\n",
      "\n",
      "The smallest nontrivial elements of X are the atoms:\n",
      "\n",
      "Deﬁnition 4: The set of atoms A of a lattice is deﬁned by\n",
      "A = {x ∈ X|x (cid:54)= 0 and if x(cid:48) ≤ x then x(cid:48) = x}.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "17\n",
      "\n",
      "(cid:87)\n",
      "\n",
      "The atoms generate the lattice, in particular, one has:\n",
      "Lemma 5: Let X be a ﬁnite Boolean lattice. Then, for each x ∈ X one has\n",
      "\n",
      "(cid:95)\n",
      "\n",
      "x =\n",
      "\n",
      "{z ∈ A(X) | z ≤ x}.\n",
      "\n",
      "(cid:87)\n",
      "\n",
      "Ax ≤ x.\n",
      "\n",
      "Proof: Let Ax := {z ∈ A(B)|z ≤ x}. Thus x is an upper bound for Ax,\n",
      "i.e.,\n",
      "Ax ≤ y. We need to show\n",
      "Now let y be any upper bound for Ax, i.e.,\n",
      "that x ≤ y.\n",
      "Consider x ∧ y(cid:48). If this is 0 then from distributivity one gets x = (x ∧\n",
      "y) ∨ (x ∧ y(cid:48)) = x ∧ y ≤ y. Conversely, if it is not true that x ≤ y then\n",
      "x ∧ y(cid:48) > 0. This happens if what we would like to show doesn’t hold.\n",
      "In this case there is an atom z ≤ x ∧ y(cid:48) and it follows that z ∈ Ax. As\n",
      "y is an upper bound we have y ≥ z and so 0 = y ∧ y(cid:48) ≥ x ∧ y(cid:48) which is\n",
      "impossible as we assumed x ∧ y(cid:48) > 0. Thus it follows that x ≤ y.\n",
      "\n",
      "The set of atoms associated with any element is unique, and the Boolean\n",
      "lattice itself is isomorph to the powerset of the set of atoms. This is the key\n",
      "structural theorem of Boolean lattices and is the reason why we can talk\n",
      "about sets (itemsets) in general for association rule discovery.\n",
      "\n",
      "Theorem 6: A ﬁnite Boolean algebra X is isomorphic to the power set\n",
      "2A(X) of the set of atoms. The isomorphism is given by\n",
      "\n",
      "η : x ∈ X (cid:55)→ {z ∈ A(X) | z ≤ x}\n",
      "\n",
      "and the inverse is\n",
      "\n",
      "η−1 : S (cid:55)→\n",
      "\n",
      "(cid:95)\n",
      "\n",
      "S.\n",
      "\n",
      "(cid:80)d\n",
      "\n",
      "In our case the atoms are the d basis vectors e1, . . . , ed and any element\n",
      "of X can be represented as a set of basis vectors, in particular x =\n",
      "i=1 ξiei\n",
      "where ξi ∈ {0, 1}. For the proof of the above theorem and further informa-\n",
      "tion on lattices and partially ordered sets see [?]. The signiﬁcance of the\n",
      "theorem lays in the fact that if X is an arbitrary Boolean lattice it is equiva-\n",
      "lent to the powerset of atoms (which can be represented by bitvectors) and\n",
      "so one can ﬁnd association rules on any Boolean lattice which conceptually\n",
      "generalises the association rule algorithms.\n",
      "\n",
      "In ﬁgure ?? we show the lattice of patterns for a simple market basket\n",
      "case which is just a power set. The corresponding lattice for the bitvectors\n",
      "is in ﬁgure ??. We represent the lattice using an undirected graph where the\n",
      "nodes are the elements of X and edges are introduced between any element\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "18\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Fig. 5. Lattice of breakfast itemsets\n",
      "\n",
      "Fig. 6. Lattice of bitvectors.\n",
      "\n",
      "and its covering elements. A covering element of x ∈ X is an x(cid:48) ≥ x such\n",
      "that no element is “in between” x and x(cid:48), i.e., any element x(cid:48)(cid:48) with x(cid:48)(cid:48) ≥ x\n",
      "and x(cid:48) ≥ x(cid:48)(cid:48) is either equal to x or to x(cid:48).\n",
      "\n",
      "In Figure ?? we display the graphs of the ﬁrst few Boolean lattices.\n",
      "We will graph speciﬁc lattices with (Hasse) diagrams [?] and later use the\n",
      "\n",
      "Fig. 7. The ﬁrst Boolean Lattices\n",
      "\n",
      "positive plane R2\n",
      "\n",
      "+ to illustrate general aspects of the lattices.\n",
      "\n",
      "{}{bread} {coffee}{juice}{milk}{bread, coffee} {milk, coffee}{milk, bread}{milk, juice}{bread. juice} {coffee, juice}{milk, coffee, juice}{bread, coffee, juice} {milk, bread, coffee}{milk, bread, juice}{milk, bread, coffee, juice}0000011011001011110101111111010010001010100100110010000101011110\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "19\n",
      "\n",
      "In the following we may sometimes also refer to the elements x of X\n",
      "as item sets, market baskets or even patterns depending on the context.\n",
      "As the data set is a ﬁnite collection of elements of a lattice the closure of\n",
      "this collection with respect to ∧ and ∨ (the inf and sup operators) in the\n",
      "lattice forms again a boolean lattice. The powerset of the set of elements of\n",
      "this lattice is then the sigma algebra which is fundamental to the measure\n",
      "which is deﬁned by the data.\n",
      "\n",
      "The partial order in the set X allows us to introduce the cumulative\n",
      "distribution function as the probability that we observe a bitvector less\n",
      "than a given x ∈ X:\n",
      "\n",
      "F (x) = P ({x(cid:48)|x(cid:48) ≤ x}) .\n",
      "\n",
      "By deﬁnition, the cumulative distribution function is monotone, i.e.\n",
      "\n",
      "x ≤ x(cid:48) ⇒ F (x) ≤ F (x(cid:48)).\n",
      "\n",
      "A second cumulative distribution function is obtained from the dual order\n",
      "as:\n",
      "\n",
      "F ∂(x) = P ({x(cid:48)|x(cid:48) ≤ x}) .\n",
      "\n",
      "It turns out that this dual cumulative distribution function is the one which\n",
      "is more useful in the discussion of association rules and frequent itemsets\n",
      "as one has for the support s(x):\n",
      "\n",
      "s(x) = F ∂(x).\n",
      "\n",
      "From this it follows directly that s(x) is antimonotone, i.e., that\n",
      "\n",
      "x ≤ y ⇒ s(x) ≥ s(y).\n",
      "\n",
      "The aim of frequent itemset mining is to ﬁnd sets of itemsets, i.e., subsets\n",
      "\n",
      "of X. In particular, one aims to determine\n",
      "\n",
      "L = {x | s(x) ≥ σ0}.\n",
      "From the antimonotonicity of s, it follows that\n",
      "\n",
      "x ∈ L and x ≥ y ⇒ y ∈ L.\n",
      "\n",
      "Such a set is called an down-set, decreasing set or order ideal. This algebraic\n",
      "characterisation will turn out to be crucial for the development and analysis\n",
      "of algorithms.\n",
      "\n",
      "The set of downsets is a subset of the power set of X, however, there\n",
      "are still a very large number of possible downsets. For example, in the case\n",
      "of d = 6, the set X has 64 elements and the power set has 264 ≈ 1.81019\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "20\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "elements and the number of downsets is 7,828,354. The simplest downsets\n",
      "are generated by one element and are\n",
      "\n",
      "For example, one has\n",
      "\n",
      "↓ x = {z | z ≤ x}\n",
      "\n",
      "X =↓ e.\n",
      "\n",
      "Consider the set of itemsets for which the (empirical) support as deﬁned by\n",
      "a data base D is nonzero. This is the set of all itemsets which are at least\n",
      "contained in one data record. It is\n",
      "\n",
      "(cid:91)\n",
      "\n",
      "x∈D\n",
      "\n",
      "(cid:91)\n",
      "\n",
      "L(0) =\n",
      "\n",
      "↓ x.\n",
      "\n",
      "L(0) =\n",
      "\n",
      "x∈Dmax\n",
      "\n",
      "↓ x.\n",
      "\n",
      "This formula can be simpliﬁed by considering only the maximal elements\n",
      "Dmax in D:\n",
      "\n",
      "Any general downset can be represented as a union of the “simple\n",
      "downsets” generated by one element. It follows that for some set Z ⊂ X of\n",
      "maximal elements one then has\n",
      "\n",
      "(cid:91)\n",
      "\n",
      "x∈Z\n",
      "\n",
      "L =\n",
      "\n",
      "↓ x.\n",
      "\n",
      "The aim of frequent itemset mining is to determine Z for a given σ0. Al-\n",
      "gorithms to determine such Z will be discussed in the next sections. Note\n",
      "that L ⊂ L(0 and that the representation of L can be considerably more\n",
      "complex than the representation of L(0). As illustration, consider the case\n",
      "where e is in the data base. In this case L(0) = X =↓ e but e is usually not\n",
      "going to be a frequent itemset.\n",
      "\n",
      "2.4. General Search for Itemsets and Search for Rules\n",
      "The previous subsections considered the search for itemsets which were\n",
      "frequently occurring in a database. One might be interested in more general\n",
      "characterisations, maybe searching for itemsets for which the income from\n",
      "their sale amounted to some minimum ﬁgure or which combined only certain\n",
      "items together. Thus one has a criterion or predicate a(x) which is true\n",
      "for items of interest. It is assumed that the evaluation of this criterion is\n",
      "expensive and requires reading the database. One would now like to ﬁnd all\n",
      "“interesting” items and would like to do this without having to consider all\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "21\n",
      "\n",
      "possible itemsets. This search for interesting itemsets is considerably more\n",
      "challenging. In some cases one ﬁnds, however, that the sets to be found\n",
      "are again downsets and then similar algorithms can be employed. Often,\n",
      "however, one has to resort to heuristics and approximate methods.\n",
      "\n",
      "Once frequent itemsets are available one can ﬁnd strong rules. These\n",
      "rules are of the type “if itemset x is in a record than so is itemset y. Such\n",
      "a rule is written as x ⇒ y and is deﬁned by a pair of itemsets (x, y) ∈ X2.\n",
      "The proportion of records for which this rule holds is called the conﬁdence,\n",
      "it is deﬁned formally as\n",
      "\n",
      "c(x ⇒ y) = s(x ∨ y)\n",
      "s(x)\n",
      "\n",
      ".\n",
      "\n",
      "A strong rule is given by a rule x ⇒ y for which x ∨ y is frequent, i.e.,\n",
      "s(x ∨ y) ≥ σ0 for a given σ0 > 0 and for which c(x ⇒ y) ≥ γ0 for a given\n",
      "γ0 > 0. The constants σ0, γ0 are provided by the user and their careful\n",
      "choice is crucial to the detection of sensible rules. From the deﬁnition it\n",
      "follows that the conﬁdence is the conditional anticumulative distribution,\n",
      "i.e.,\n",
      "\n",
      "c(ax ⇒ ay) = F δ(y|x)\n",
      "\n",
      "where F δ(y|x) = F δ(y ∨ x)/F δ(x) is the conditional cumulative distribu-\n",
      "tion function. Now there are several problems with strong association rules\n",
      "which have been addressed in the literature:\n",
      "• The straight-forward interpretation of the rules may lead to wrong in-\n",
      "• The number of strong rules found can be very small.\n",
      "• The number of strong rules can be very large.\n",
      "• Most of the strong rules found can be inferred from domain knowledge\n",
      "• The strong rules found do not lend themselves to any actions and are\n",
      "\n",
      "and do not lead to new insights.\n",
      "\n",
      "ferences.\n",
      "\n",
      "hard to interpret.\n",
      "\n",
      "We will address various of these challenges in the following. At this stage\n",
      "the association rule mining problem consists of the following:\n",
      "\n",
      "Find all strong association rules in a given data set D.\n",
      "\n",
      "A simple procedure would now visit each frequent itemset z and look at\n",
      "all pairs z1, z2 such that z = z1 ∨ z2 and z1 ∧ z2 = 0 and consider all rules\n",
      "az1 ⇒ az2. This procedure can be improved by taking into account that\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "22\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Theorem 7: Let z = z1 ∨ z2 = z3 ∨ z4 and z1 ∧ z2 = z3 ∧ z4 = 0. Then if\n",
      "az1 ⇒ az2 is a strong association rule and z3 ≥ z1 then so is az3 ⇒ az4.\n",
      "\n",
      "This is basically “the apriori property for the rules” and allows pruning\n",
      "the tree of possible rules quite a lot. The theorem is again used as a nec-\n",
      "essary condition. We start the algorithm by considering z = z1 ∨ z2 with\n",
      "1-itemsets for z2 and looking at all strong rules. Then, if we consider a\n",
      "2-itemset for z2 both subsets y < z2 need to be consequents of strong rules\n",
      "in order for z2 to be a candidate of a consequent. By constructing the con-\n",
      "sequents taking into account that all their nearest neighbours (their cover\n",
      "in lattice terminology) need to be consequents as well. Due to the inter-\n",
      "pretability problem one is mostly interested in small consequent itemsets\n",
      "so that this is not really a big consideration. See [?] for eﬃcient algorithms\n",
      "for the direct search for association rules.\n",
      "\n",
      "3. The Apriori Algorithm\n",
      "The aim of association rule discovery is the derivation of if-then-rules based\n",
      "on the itemsets x deﬁned in the previous subsection. An example of such\n",
      "a rule is “if a market basket contains orange juice then it also contains\n",
      "bread”. In this section the Apriori algorithm to ﬁnd all frequent itemsets\n",
      "is discussed. The classical Apriori algorithm as suggested by Agrawal et\n",
      "al. in [?] is one of the most important data mining algorithms. It uses a\n",
      "breadth ﬁrst search approach, ﬁrst ﬁnding all frequent 1-itemsets, and then\n",
      "discovering 2-itemsets and continues by ﬁnding increasingly larger frequent\n",
      "itemsets. The three subsections of this section consider ﬁrst the problem of\n",
      "the determination of the support of any itemset and the storage of the data\n",
      "in memory, then the actual apriori algorithm and ﬁnally the estimation of\n",
      "the size of candidate itemsets which allows the prediction of computational\n",
      "time as the size of the candidates is a determining factor in the complexity\n",
      "of the apriori algorithm.\n",
      "\n",
      "3.1. Time complexity – computing supports\n",
      "(cid:80)n\n",
      "The data is a sequence x(1), . . . , x(n) of binary vectors. We can thus repre-\n",
      "sent the data as a n by d binary matrix. The number of nonzero elements\n",
      "i=1 |x(i)|. This is approximated by the n times expected length, i.e.,\n",
      "is\n",
      "nE(|x|). So the proportion of nonzero elements is E(|x|)/d. This can be very\n",
      "small, especially for the case of market baskets, where out of often more\n",
      "than 10,000 items usually less than 100 items are purchased. Thus less than\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "23\n",
      "\n",
      "one percent of all the items are nonzero. In this case it makes sense to store\n",
      "the matrix in a sparse format. Here we will consider two ways to store\n",
      "the matrix, either by rows or by columns. The matrix corresponding to an\n",
      "earlier example is\n",
      "\n",
      " .\n",
      "\n",
      "\n",
      "\n",
      "1 1 0 0 0\n",
      "1 0 1 1 0\n",
      "1 0 0 0 1\n",
      "1 1 0 1 0\n",
      "0 0 0 0 1\n",
      "\n",
      "First we discuss the horizontal organisation. A row is represented simply\n",
      "by the indices of the nonzero elements. And the matrix is represented as a\n",
      "tuple of rows. For example, the above matrix is represented as\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(1, 2) (1, 3, 4) (1, 5) (1, 2, 4) (5)\n",
      "\n",
      "In practice we also need pointers which tell us where the row starts if\n",
      "contiguous locations are used in memory.\n",
      "Now assume that we have any row x and a az and would like to ﬁnd\n",
      "out if x supports az, i.e., if z ≤ x. If both the vectors are represented in the\n",
      "sparse format this means that we would like to ﬁnd out if the indices of z\n",
      "are a subset of the indices of x. There are several diﬀerent ways to do this\n",
      "and we will choose the one which uses an auxiliary bitvector v ∈ X (in full\n",
      "format) which is initialised to zero. The proposed algorithm has 3 steps:\n",
      "(1) Expand x into a bitvector v: v ← x\n",
      "(2) Extract the value of v for the elements of z, i.e., v[z]. If they are all\n",
      "nonzero, i.e., if v[z] = e then z ≤ x.\n",
      "(3) Set v to zero again, i.e., v ← 0\n",
      "We assume that the time per nonzero element for all the steps is the same\n",
      "τ and we get for the time:\n",
      "\n",
      "T = (2|x| + |z|)τ.\n",
      "\n",
      "In practice we will have to determine if az(x) holds for mk diﬀerent\n",
      "vectors z which have all the same length k. Rather than doing the above\n",
      "algorithm mk times one can extract x once and one so gets the algorithm\n",
      "(1) Extract x into v: v ← x\n",
      "(2) For all j = 1, . . . , mk check if v[z(j)] = e, i.e., if z(j) ≤ x.\n",
      "(3) Set v to zero again, i.e., v ← 0\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "24\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "With the same assumptions as above we get (|z(j)| = k) for the time:\n",
      "\n",
      "T = (2|x| + mkk)τ.\n",
      "\n",
      "Finally, running this algorithm for all the rows x(i) and vectors z(j) of\n",
      "\n",
      "diﬀerent lengths, one gets the total time\n",
      "\n",
      "T =\n",
      "\n",
      "and the expected time is\n",
      "\n",
      "|x(i)| + mkkn)τ\n",
      "\n",
      "E(T ) =\n",
      "\n",
      "(2E(|x|) + mkk)nτ.\n",
      "\n",
      "(cid:88)\n",
      "n(cid:88)\n",
      "(cid:88)\n",
      "\n",
      "(2\n",
      "\n",
      "i=1\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "Note that the sum over k is for k between one and d but only the k for\n",
      "which mk > 0 need to be considered. The complexity has two parts. The\n",
      "ﬁrst part is proportional to E(|x|)n which corresponds to the number of\n",
      "data points times the average complexity of each data point. This part\n",
      "thus encapsulates the data dependency. The second part is proportional to\n",
      "mkkn where the factor mkk refers to the complexity of the search space\n",
      "which has to be visited for each record n. For k = 1 we have m1 = 1 as\n",
      "we need to consider all the components. Thus the second part is larger\n",
      "than dnτ, in fact, we would probably have to consider all the pairs so that\n",
      "it would be larger than d2nτ which is much larger than the ﬁrst part as\n",
      "2E(|x|) ≤ 2d. Thus the major cost is due to the search through the possible\n",
      "patterns and one typically has a good approximation\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "E(T ) ≈\n",
      "\n",
      "mkknτ.\n",
      "\n",
      "An alternative is based on the vertical organisation where the binary\n",
      "matrix (or Boolean relational table) is stored column-wise. This may require\n",
      "slightly less storage as the row wise storage as we only needs pointers to\n",
      "each column and one typically has more rows than columns. In this vertical\n",
      "storage scheme the matrix considered earlier would be represented as\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(1, 2, 3, 4) (1, 4) (2) (2, 4) (3, 5)\n",
      "\n",
      "The storage savings in the vertical format however, are oﬀset by extra\n",
      "storage costs for an auxiliary vector with n elements.\n",
      "\n",
      "For any az the algorithm considers only the columns for which the com-\n",
      "ponents zj are one. The algorithm determines the intersection (or elemen-\n",
      "twise product) of all the columns j with zj = 1. This is done by using the\n",
      "auxiliary array v which holds the current intersection. We initially set it\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "25\n",
      "\n",
      "to the ﬁrst column j with zj = 1, later extract all the values at the points\n",
      "deﬁned by the nonzero elements for the next column j(cid:48) for which zj(cid:48) = 1,\n",
      "then zero the original ones in v and ﬁnally set the extracted values into the\n",
      "v. More concisely, we have the algorithm, where xj stands for the whole\n",
      "column j in the data matrix.\n",
      "\n",
      "(1) Get j such that zj = 1, mark as visited\n",
      "(2) Extract column xj into v: v ← xj\n",
      "(3) Repeat until no nonzero elements in z unvisited:\n",
      "\n",
      "(a) Get unvisited j such that zj = 1, mark as visited\n",
      "(b) Extract elements of v corresponding to xj, i.e., w ← v[xj]\n",
      "(c) Set v to zero, v ← 0\n",
      "(d) Set v to w, v ← w\n",
      "\n",
      "(4) Get the support s(az) = |w|\n",
      "\n",
      "So we access v three times for each column, once for the extraction of\n",
      "elements, once for setting it to zero and once for resetting the elements.\n",
      "Thus for the determination of the support of z in the data base we have\n",
      "the time complexity of\n",
      "\n",
      "d(cid:88)\n",
      "\n",
      "n(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "T = 3τ\n",
      "\n",
      "x(i)\n",
      "j zj.\n",
      "\n",
      "A more careful analysis shows that this is actually an upper bound for the\n",
      "complexity. Now this is done for mk arrays z(s,k) of size k and for all k.\n",
      "Thus we get the total time for the determination of the support of all az\n",
      "to be\n",
      "\n",
      "We can get a simple upper bound for this using x(i)\n",
      "\n",
      "j ≤ 1 as\n",
      "\n",
      "T = 3τ\n",
      "\n",
      "x(i)\n",
      "j z(s,k)\n",
      "\n",
      "j\n",
      "\n",
      ".\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "mk(cid:88)\n",
      "\n",
      "d(cid:88)\n",
      "\n",
      "n(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "s=1\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "T ≤ 3\n",
      "\n",
      "mkknτ\n",
      "\n",
      "(cid:80)d\n",
      "\n",
      "j=1 z(s,k)\n",
      "\n",
      "because\n",
      "= k. This is roughly 3 times what we got for the pre-\n",
      "vious algorithm. However, the x(i)\n",
      "j are random with an expectation E(x(i)\n",
      "j )\n",
      "which is typically much less than one and have an average expected length\n",
      "\n",
      "j\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "26\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "of E(|x|)/d. If we introduce this into the equation for T we get the approx-\n",
      "imation\n",
      "\n",
      "E(T ) ≈ 3E(|x|)\n",
      "\n",
      "d\n",
      "\n",
      "mkknτ\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "which can be substantially smaller than the time for the previous algorithm.\n",
      "Finally, we should point out that there are many other possible al-\n",
      "gorithms and other possible data formats. Practical experience and more\n",
      "careful analysis shows that one method may be more suitable for one data\n",
      "set where the other is better for another data set. Thus one carefully has\n",
      "to consider the speciﬁcs of a data set. Another consideration is also the size\n",
      "k and number mk of the z considered. It is clear from the above that it is\n",
      "essential to carefully choose the “candidates” az for which the support will\n",
      "be determined. This will further be discussed in the next sections. There is\n",
      "one term which occurred in both algorithms above and which characterises\n",
      "the complexity of the search through multiple levels of az, it is:\n",
      "\n",
      "∞(cid:88)\n",
      "\n",
      "C =\n",
      "\n",
      "mkk.\n",
      "\n",
      "We will use this constant later in the discussion of the eﬃciency of the\n",
      "search procedures.\n",
      "\n",
      "k=1\n",
      "\n",
      "3.2. The algorithm\n",
      "Some principles of the apriori algorithm are suggested in [?]. In partic-\n",
      "ular, the authors suggest a breadth-ﬁrst search algorithm and utilise the\n",
      "apriori principle to avoid unnecessary processing. However, the problem\n",
      "with this early algorithm is that it generates candidate itemsets for each\n",
      "record and also cannot make use of the vertical data organisation. Con-\n",
      "sequently, two groups of authors suggested at the same conference a new\n",
      "and faster algorithm which determines candidate itemsets before each data\n",
      "base scan [?,?]. This approach has substantially improved performance and\n",
      "is capable of utilising the vertical data organisation. We will discuss this\n",
      "algorithm using the currently accepted term frequent itemsets. (This was\n",
      "in the earlier literature called “large itemsets” or “covering itemsets”.)\n",
      "\n",
      "The apriori algorithm visits the lattice of itemsets in a level-wise fashion,\n",
      "see Figure ?? and Algorithm ??. Thus it is a breadth-ﬁrst-search or BFS\n",
      "procedure. At each level the data base is scanned to determine the support\n",
      "of items in the candidate itemset Ck. Recall from the last section that\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "27\n",
      "\n",
      "Fig. 8. Level sets of Boolean lattices\n",
      "\n",
      "Algorithm 1 Apriori\n",
      "\n",
      "C1 = A(X) is the set of all one-itemsets, k = 1\n",
      "while Ck (cid:54)= ∅ do\n",
      "\n",
      "scan database to determine support of all ay with y ∈ Ck\n",
      "extract frequent itemsets from Ck into Lk\n",
      "generate Ck+1\n",
      "k := k + 1.\n",
      "\n",
      "end while\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "the major determining parameter for the complexity of the algorithm is\n",
      "C =\n",
      "\n",
      "k mkk where mk = |Ck|.\n",
      "\n",
      "It is often pointed out that much of the time is spent in dealing with\n",
      "pairs of items. We know that m1 = d as one needs to consider all single\n",
      "items. Furthermore, one would not have any items which alone are not\n",
      "frequent and so one has m2 = d(d − 1)/2. Thus we get the lower bound for\n",
      "C:\n",
      "\n",
      "C ≤ m1 + 2m2 = d2.\n",
      "\n",
      "As one sees in practice that this is a large portion of the total computations\n",
      "one has a good approximation C ≈ d2. Including also the dependence on\n",
      "the data size we get for the time complexity of apriori:\n",
      "\n",
      "T = O(d2n).\n",
      "\n",
      "Thus we have scalability in the data size but quadratic dependence on the\n",
      "dimension or number of attributes.\n",
      "Consider the ﬁrst (row-wise) storage where T ≈ d2nτ. If we have\n",
      "d = 10, 000 items and n = 1, 000, 000 data records and the speed of the\n",
      "computations is such that τ = 1ns the apriori algorithm would require 105\n",
      "seconds which is around 30 hours, more than one day. Thus the time spent\n",
      "for the algorithm is clearly considerable.\n",
      "\n",
      "L0L1L2L3L4\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "28\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "In case of the second (column-wise) storage scheme we have T ≈\n",
      "3E(|x|)dnτ. Note that in this case for ﬁxed size of the market baskets the\n",
      "complexity is now\n",
      "\n",
      "T = O(dn).\n",
      "\n",
      "If we take the same data set as before and we assume that the average\n",
      "market basket contains 100 items (E(|x|) = 100) then the apriori algorithm\n",
      "would require only 300 seconds or ﬁve minutes, clearly a big improvement\n",
      "over the row-wise algorithm.\n",
      "\n",
      "3.3. Determination and size of the candidate itemsets\n",
      "The computational complexity of the apriori algorithm is dominated by\n",
      "data scanning, i.e., evaluation of az(x) for data records x. We found earlier\n",
      "k mkk. As mk\n",
      "that the complexity can be described by the constant C =\n",
      "is the number of k itemsets, i.e., bitvectors where exactly k bits are one we\n",
      ". On the other hand the apriori algorithm will ﬁnd the set Lk\n",
      "of the actual frequent itemsets, thus Lk ⊂ Ck and so |Lk| ≤ mk. Thus one\n",
      "gets for the constant C the bounds\n",
      "\n",
      "get mk ≤(cid:0)m\n",
      "(cid:1)\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k|Lk| ≤ C =\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "mkk ≤ d(cid:88)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "d\n",
      "k\n",
      "\n",
      "k.\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "k=0\n",
      "\n",
      "The upper bound is hopeless for any large size d and we need to get better\n",
      "bounds. This depends very much on how the candidate itemsets Ck are\n",
      "chosen. We choose C1 to be the set of all 1-itemsets, and C2 to be the set\n",
      "of all 2-itemsets so that we get m1 = 1 and m2 = d(d − 1)/2.\n",
      "\n",
      "The apriori algorithm determines alternatively Ck and Lk such that\n",
      "\n",
      "successively the following chain of sets is generated:\n",
      "\n",
      "C1 = L1 → C2 → L2 → C3 → L3 → C4 → ···\n",
      "\n",
      "How should we now choose the Ck? We know, that the sequence Lk satisﬁes\n",
      "the apriori property, which can be reformulated as\n",
      "Deﬁnition 8: If y is a frequent k-itemset (i.e., y ∈ Lk) and if z ≤ y then\n",
      "z is a frequent |z|-itemset, i.e., z ∈ L|z|.\n",
      "Thus in extending a sequence L1, L2, . . . , Lk by a Ck+1 we can choose the\n",
      "candidate itemset such that the extended sequence still satisﬁes the apriori\n",
      "property. This still leaves a lot of freedom to choose the candidate itemset.\n",
      "In particular, the empty set would always be admissible. We need to ﬁnd\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "29\n",
      "\n",
      "a set which contains Lk+1. The apriori algorithm chooses the largest set\n",
      "Ck+1 which satisﬁes the apriori condition. But is this really necessary? It\n",
      "is if we can ﬁnd a data set for which the extended sequence is the set of\n",
      "frequent itemsets. This is shown in the next proposition:\n",
      "\n",
      "Proof: Set x(i) ∈(cid:83)\n",
      "sets, i.e., for any z ∈(cid:83)\n",
      "\n",
      "Proposition 9: Let L1, . . . , Lm be any sequence of sets of k-itemsets which\n",
      "satisﬁes the apriori condition. Then there exists a dataset D and a σ > 0\n",
      "such that the Lk are frequent itemsets for this dataset with minimal support\n",
      "σ.\n",
      "\n",
      "k Lk, i = 1, . . . , n to be sequence of all maximal item-\n",
      "k Lk there is an x(i) such that z ≤ x(i) and x(i) (cid:54)≤ x(j)\n",
      "for i (cid:54)= j. Choose σ = 1/n. Then the Lk are the sets of frequent itemsets\n",
      "for this data set.\n",
      "\n",
      "For any collection Lk there might be other data sets as well, the one chosen\n",
      "above is the minimal one. The sequence of the Ck is now characterised by:\n",
      "(1) C1 = L1\n",
      "(2) If y ∈ Ck and z ≤ y then z ∈ Ck(cid:48) where k(cid:48) = |z|.\n",
      "In this case we will say that the sequence Ck satisﬁes the apriori condition.\n",
      "It turns out that this characterisation is strong enough to get good upper\n",
      "bounds for the size of mk = |Ck|.\n",
      "However, before we go any further in the study of bounds for |Ck| we\n",
      "provide a construction of a sequence Ck which satisﬁes the apriori condi-\n",
      "tion. A ﬁrst method uses Lk to construct Ck+1 which it chooses to be the\n",
      "maximal set such that the sequence L1, . . . , Lk, Ck+1 satisﬁes the apriori\n",
      "property. One can see by induction that then the sequence C1, . . . , Ck+1\n",
      "will also satisfy the apriori property. A more general approach constructs\n",
      "Ck+1, . . . , Ck+p such that L1, . . . , Lk,Ck+1, . . . , Ck+p satisﬁes the apriori\n",
      "property. As p increases the granularity gets larger and this method may\n",
      "work well for larger itemsets. However, choosing larger p also amounts to\n",
      "larger Ck and thus some overhead. We will only discuss the case of p = 1\n",
      "here.\n",
      "\n",
      "The generation of Ck+1 is done in two steps. First a slightly larger set is\n",
      "constructed and then all the elements which break the apriori property are\n",
      "removed. For the ﬁrst step the join operation is used. To explain join let the\n",
      "elements of L1 (the atoms) be enumerated as e1, . . . , ed. Any itemset can\n",
      "then be constructed as join of these atoms. We denote a general itemset by\n",
      "\n",
      "e(j1, . . . , jk) = ej1 ∨ ··· ∨ ejk\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "30\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "where j1 < j2 < ··· < jk. The join of any k-itemset with itself is then\n",
      "deﬁned as\n",
      "Lk (cid:111)(cid:110) Lk := {e(j1, . . . , jk+1) | e(j1, . . . , jk) ∈ Lk,\n",
      "Thus Lk (cid:111)(cid:110) Lk is the set of all k + 1 itemsets for which 2 subsets with\n",
      "k items each are frequent. As this condition also holds for all elements in\n",
      "Ck+1 one has Ck+1 ⊂ Lk (cid:111)(cid:110) Lk. The Ck+1 is then obtained by removing\n",
      "elements which contain infrequent subsets.\n",
      "if (1, 0, 1, 0, 0) ∈ L2 and (0, 1, 1, 0, 0) ∈ L2 then\n",
      "(1, 1, 1, 0, 0) ∈ L2 (cid:111)(cid:110) L2. If, in addition, (1, 1, 0, 0, 0) ∈ L2 then (1, 1, 1, 0, 0) ∈\n",
      "C3.\n",
      "\n",
      "For example,\n",
      "\n",
      "e(j1, . . . , jk−1, jk+1) ∈ Lk}.\n",
      "\n",
      "Having developed a construction for Ck we can now determine the\n",
      "bounds for the size of the candidate itemsets based purely on combinatorial\n",
      "considerations. The main tool for our discussion is the Kruskal-Katona the-\n",
      "orem. The proof of this theorem and much of the discussion follows closely\n",
      "the exposition in Chapter 5 of [?]. The bounds developed in this way have\n",
      "ﬁrst been developed in [?].\n",
      "We will denote the set of all possible k-itemsets or bitvectors with ex-\n",
      "actly k bits as Ik. Subsets of this set are sometimes also called hypergraphs\n",
      "in the literature. The set of candidate itemsets Ck ⊂ Ik.\n",
      "k − 1 subsets of the elements of Ck:\n",
      "\n",
      "Given a set of k-itemsets Ck the lower shadow of Ck is the set of all\n",
      "\n",
      "∂(Ck) := {y ∈ Ik−1 | y < z for some z ∈ Ck}.\n",
      "\n",
      "This is the set of bitvectors which have k − 1 bits set at places where some\n",
      "z ∈ Ck has them set. The shadow ∂Ck can be smaller or larger than the\n",
      "Ck. In general, one has for the size |∂Ck| ≥ k independent of the size of Ck.\n",
      "So, for example, if k = 20 then |∂Ck| ≥ 20 even if |Ck| = 1. (In this case we\n",
      "actually have |∂Ck| = 20.) For example, we have ∂C1 = ∅, and |∂C2| ≤ d.\n",
      "It follows now that the sequence of sets of itemsets Ck satisﬁes the\n",
      "\n",
      "apriori condition iﬀ\n",
      "\n",
      "∂(Ck) ⊂ Ck−1.\n",
      "\n",
      "The Kruskal-Katona Theorem provides an estimate of the size of the\n",
      "shadow.\n",
      "\n",
      "First, recall the mapping φ : X → N, deﬁned by:\n",
      "\n",
      "d−1(cid:88)\n",
      "\n",
      "i=0\n",
      "\n",
      "φ(x) =\n",
      "\n",
      "2ixi,\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "31\n",
      "\n",
      "and the induced order\n",
      "\n",
      "y ≺ z :⇔ φ(y) < φ(z)\n",
      "\n",
      "which is the colex (or colexicographic) order. In this order the itemset\n",
      "{3, 5, 6, 9} ≺ {3, 4, 7, 9} as the largest items determine the order. (In the\n",
      "lexicographic ordering the order of these two sets would be reversed.)\n",
      "Let [m] := {0, . . . , m − 1} and [m](k) be the set of all k-itemsets where\n",
      "k bits are set in the ﬁrst m positions and all other bits can be either 0 or 1.\n",
      "In the colex order any z where bits m (and beyond) are set are larger than\n",
      "any of the elements in [m](k). Thus [m]k is just the set of the ﬁrst\n",
      "bitvectors with k bits set.\n",
      "\n",
      "(cid:0)m−1\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "k\n",
      "\n",
      "We will now construct the sequence of the ﬁrst m bitvectors for any m.\n",
      "This corresponds to the ﬁrst numbers, which, in the binary representation\n",
      "have m ones set. Consider, for example the case of d = 5 and k = 2. For this\n",
      "case all the bitvectors are in table ??. (Printed with the lowest signiﬁcant\n",
      "bit on the right hand side for legibility.)\n",
      "\n",
      "(0,0,0,1,1)\n",
      "(0,0,1,0,1)\n",
      "(0,0,1,1,0)\n",
      "(0,1,0,0,1)\n",
      "(0,1,0,1,0)\n",
      "(0,1,1,0,0)\n",
      "(1,0,0,0,1)\n",
      "(1,0,0,1,0)\n",
      "(1,0,1,0,0)\n",
      "(1,1,0,0,0)\n",
      "\n",
      "3\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "12\n",
      "17\n",
      "18\n",
      "20\n",
      "24\n",
      "\n",
      "As before, we denote by ej the j − th atom and by e(j1, . . . , jk) the\n",
      "bitvector with bits j1, . . . , jk set to one. Furthermore, we introduce the\n",
      "element-wise join of a bitvector and a set C of bitvectors as:\n",
      "\n",
      "C ∨ y := {z ∨ y | z ∈ C}.\n",
      "\n",
      "For 0 ≤ s ≤ ms < ms+1 < ··· < mk we introduce the following set of\n",
      "k-itemsets:\n",
      "\n",
      "B(k)(mk, . . . , ms) :=\n",
      "\n",
      "[mj](j) ∨ e(mj+1, . . . , mk)\n",
      "\n",
      "⊂ Ik.\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "k(cid:91)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "j=s\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "32\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "k(cid:88)\n",
      "\n",
      "j=s\n",
      "\n",
      "mj\n",
      "j\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "As only term j does not contain itemsets with item mj (all the others\n",
      "\n",
      "do) the terms are pairwise disjoint and so the union contains\n",
      "\n",
      "|B(k)(mk, . . . , ms)| = b(k)(mk, . . . , ms) :=\n",
      "\n",
      "k-itemsets. This set contains the ﬁrst (in colex order) bitvectors with k bits\n",
      "set. By splitting oﬀ the last term in the union one then sees:\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "B(k−1)(mk−1, . . . , ms) ∨ emk\n",
      "\n",
      "∪ [mk](k)\n",
      "\n",
      "(3)\n",
      "\n",
      "B(k)(mk, . . . , ms) =\n",
      "\n",
      "and consequently\n",
      "\n",
      "b(k)(mk, . . . , ms) = b(k−1)(mk−1, . . . , ms) + b(k)(mk).\n",
      "\n",
      "Consider the example of table ?? of all bitvectors up to (1, 0, 0, 1, 0).\n",
      "There are 8 bitvectors which come earlier in the colex order. The highest\n",
      "bit set for the largest element is bit 5. As we consider all smaller elements\n",
      "we need to have all two-itemsets where the 2 bits are distributed between\n",
      "positions 1 to 4 and there are\n",
      "= 6 such bitvectors. The other cases have\n",
      "the top bit ﬁxed at position 5 and the other bit is either on position 1 or\n",
      "two thus there are\n",
      "= 2 bitvectors for which the top bit is ﬁxed. Thus\n",
      "we get a total of\n",
      "\n",
      "(cid:0)2\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "(cid:0)4\n",
      "(cid:18)\n",
      "\n",
      "4\n",
      "2\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "2\n",
      "1\n",
      "\n",
      "+\n",
      "\n",
      "= 8\n",
      "\n",
      "bitvectors up to (and including) bitvector (1, 0, 0, 1, 0) for which 2 bits are\n",
      "set. This construction is generalised in the following.\n",
      "\n",
      "In the following we will show that b(k)(mk, . . . , ms) provides a unique\n",
      "representation for the integers. We will make frequent use of the identity:\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "t + 1\n",
      "\n",
      "r\n",
      "\n",
      "− 1 =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "r(cid:88)\n",
      "\n",
      "t − r + l\n",
      "\n",
      "l=1\n",
      "\n",
      "l\n",
      "\n",
      ".\n",
      "\n",
      "(4)\n",
      "\n",
      "Lemma 10: For every m, k ∈ N there are numbers ms < ··· < mk such\n",
      "that\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "k(cid:88)\n",
      "\n",
      "j=s\n",
      "\n",
      "mj\n",
      "j\n",
      "\n",
      "m =\n",
      "\n",
      "(5)\n",
      "\n",
      "and the mj are uniquely determined by m.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "33\n",
      "\n",
      "Assume a decomposition of the form ?? is given. Using equation ?? one\n",
      "\n",
      "Proof: The proof is by induction over m. In the case of m = 1 one sees\n",
      "immediately that there can only be one term in the sum of equation (??),\n",
      "thus s = k and the only choice is mk = k.\n",
      "Now assume that equation ?? is true for some m(cid:48) = m − 1. We show\n",
      "uniqueness for m. We only need to show that mk is uniquely determined\n",
      "as the uniqueness of the other mj follows from the induction hypothesis\n",
      "\n",
      "(cid:1)\n",
      "\n",
      ".\n",
      "\n",
      "k\n",
      "\n",
      "gets:\n",
      "\n",
      "applied to m(cid:48) = m − m −(cid:0)mk\n",
      "(cid:18)\n",
      "(cid:19)\n",
      "(cid:18)\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "mk\n",
      "k\n",
      "mk\n",
      "k\n",
      "mk + 1\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "m =\n",
      "\n",
      "≤\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "=\n",
      "\n",
      "k\n",
      "\n",
      "as mk−1 ≤ mk − 1 etc. Thus we get\n",
      "≤ m ≤\n",
      "\n",
      "mk\n",
      "k\n",
      "\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "mk−1\n",
      "k − 1\n",
      "mk − 1\n",
      "k − 1\n",
      "− 1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "ms\n",
      "s\n",
      "mk − k + 1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "\n",
      "+ ··· +\n",
      "\n",
      "+ ··· +\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "mk + 1\n",
      "\n",
      "k\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "− 1.\n",
      "\n",
      "With other words, mk is the largest integer such that\n",
      "provides a unique characterisation of mk which proves uniqueness.\n",
      "\n",
      "k\n",
      "\n",
      "Assume that the mj be constructed according to the method outlined\n",
      "in the ﬁrst part of this proof. One can check that equation ?? holds for\n",
      "these mj using the characterisation.\n",
      "\n",
      "What remains to be shown is mj+1 > mj and using inductions, it is\n",
      "enough to show that mk−1 < mk. If, on the contrary, this does not hold\n",
      "and mk−1 ≥ mk, then one gets from ??:\n",
      "\n",
      "(cid:0)mk\n",
      "\n",
      "(cid:1) ≤ m. This\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "mk\n",
      "k\n",
      "mk\n",
      "k\n",
      "mk\n",
      "k\n",
      "mk\n",
      "k\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "mk−1\n",
      "k − 1\n",
      "mk\n",
      "k − 1\n",
      "mk − 1\n",
      "k − 1\n",
      "mk−1\n",
      "k − 1\n",
      "\n",
      "(cid:19)\n",
      "(cid:19)\n",
      "\n",
      "m ≥\n",
      "\n",
      "≥\n",
      "\n",
      "=\n",
      "\n",
      "≥\n",
      "\n",
      "= m + 1\n",
      "\n",
      "which is not possible.\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "+ 1\n",
      "\n",
      "mk − k + 1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "\n",
      "(cid:18)\n",
      "(cid:18)\n",
      "\n",
      "+ ··· +\n",
      "\n",
      "+ ··· +\n",
      "\n",
      "ms\n",
      "s\n",
      "\n",
      "+ 1 by induction hyp.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "34\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Let N (k) be the set of all k-itemsets of integers. It turns out that the\n",
      "\n",
      "j\n",
      "\n",
      "j=s\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:0)mj\n",
      "\n",
      "B(k) occur as natural subsets of N (k):\n",
      "\n",
      "itemsets of N (k) (in colex order).\n",
      "\n",
      "Theorem 11: The set B(k)(mk, . . . , ms) consists of the ﬁrst m =\n",
      "\n",
      "(cid:80)k\n",
      "elements are still [mk](k). The remaining m −(cid:0)mk\n",
      "\n",
      "Proof: The proof is by induction over k − s. If k = s and thus m =\n",
      "then the ﬁrst elements of N (k) are just [mk](k). If k > s then the ﬁrst\n",
      "\n",
      "elements all contain\n",
      "bit mk. By the induction hypothesis the ﬁrst bk−1(mk−1, . . . , ms) elements\n",
      "containing bit mk are Bk−1(mk−1, . . . , ms)∨ emk and the rest follows from\n",
      "??.\n",
      "\n",
      "(cid:0)mk\n",
      "(cid:0)mk\n",
      "\n",
      "(cid:1)\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "The shadow of the ﬁrst k-itemsets B(k)(mk, . . . , ms) are the ﬁrst k − 1-\n",
      "\n",
      "itemsets, or more precisely:\n",
      "\n",
      "Lemma 12:\n",
      "\n",
      "∂B(k)(mk, . . . , ms) = B(k−1)(mk, . . . , ms).\n",
      "\n",
      "Proof: First we observe that in the case of s = k the shadow is simply set\n",
      "of all k − 1 itemsets:\n",
      "\n",
      "∂[mk]k = [mk](k−1).\n",
      "\n",
      "This can be used as anchor for the induction over k − s. As was shown\n",
      "\n",
      "earlier, one has in general:\n",
      "\n",
      "B(k)(mk, . . . , ms) = [mk]k ∪\n",
      "and, as the shadow is additive, as\n",
      "∂B(k)(mk, . . . , ms) = [mk](k−1)∪\n",
      "Note that B(k−1)(mk−1, . . . , ms) ⊂ [ml](k−1).\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:16)\n",
      "B(k−1)(mk−1, . . . , ms) ∨ emk\n",
      "(cid:17)\n",
      "\n",
      "B(k−2)(mk−1, . . . , ms) ∨ emk\n",
      "\n",
      "= B(k−1)(mk, . . . , ms).\n",
      "\n",
      "The shadow is important for the apriori property and we would thus\n",
      "like to determine the shadow, or at least its size for more arbitrary k-\n",
      "itemsets as they occur in the apriori algorithm. Getting bounds is feasible\n",
      "but one requires special technology to do this. This is going to be developed\n",
      "further in the sequel. We would like to reduce the case of general sets of\n",
      "k-itemsets to the case of the previous lemma, where we know the shadow.\n",
      "So we would like to ﬁnd a mapping which maps the set of k-itemsets to the\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "35\n",
      "\n",
      "ﬁrst k itemsets in colex order without changing the size of the shadow. We\n",
      "will see that this can almost be done in the following. The way to move the\n",
      "itemsets to earlier ones (or to “compress” them) is done by moving later\n",
      "bits to earlier positions.\n",
      "\n",
      "So we try to get the k itemsets close to B(k)(mk, . . . , ms) in some sense,\n",
      "so that the size of the shadow can be estimated. In order to simplify no-\n",
      "tation we will introduce z + ej for z ∨ ej when ej (cid:54)≤ z and the reverse\n",
      "operation (removing the j-th bit) by z − ej when ej ≤ z. Now we introduce\n",
      "compression of a bitvector as\n",
      "\n",
      "(cid:40)\n",
      "\n",
      "Rij(z) =\n",
      "\n",
      "z − ej + ei\n",
      "z\n",
      "\n",
      "if ei (cid:54)≤ z and ej ≤ z\n",
      "else\n",
      "\n",
      "Thus we simply move the bit in position j to position i if there is a bit in\n",
      "position j and position i is empty. If not, then we don’t do anything. So we\n",
      "did not change the number of bits set. Also, if i < j then we move the bit\n",
      "to an earlier position so that Rij(z) ≤ z. For our earlier example, when we\n",
      "number the bits from the right, starting with 0 we get R1,3((0, 1, 1, 0, 0)) =\n",
      "(0, 0, 1, 1, 0) and R31((0, 0, 0, 1, 1)) = (0, 0, 0, 1, 1). This is a “compression”\n",
      "as it moves a collection of k-itemsets closer together and closer to the vector\n",
      "z = 0 in terms of the colex order.\n",
      "\n",
      "The mapping Rij is not injective as\n",
      "\n",
      "Rij(z) = Rij(y)\n",
      "\n",
      "when y = Rij(z) and this is the only case. Now consider for any set C of\n",
      "ij (C) ∩ C. These are those elements of C which stay\n",
      "bitvectors the set R−1\n",
      "in C when compressed by Rij. The compression operator for bitsets is now\n",
      "deﬁned as\n",
      "\n",
      "˜Ri,j(C) = Rij(C) ∪ (C ∩ R−1\n",
      "\n",
      "ij (C)).\n",
      "\n",
      "Thus the points which stay in C under Rij are retained and the points\n",
      "which are mapped outside C are added. Note that by this we have avoided\n",
      "the problem with the non-injectivity as only points which stay in C can\n",
      "be mapped onto each other. The size of the compressed set is thus the\n",
      "same. However, the elements in the ﬁrst part have been mapped to earlier\n",
      "elements in the colex order. In our earlier example, for i, j = 1, 3 we get\n",
      "\n",
      "C = {(0, 0, 0, 1, 1), (0, 1, 1, 0, 0), (1, 1, 0, 0, 0), (0, 1, 0, 1, 0)}\n",
      "\n",
      "we get\n",
      "\n",
      "˜Ri,j(C) = {(0, 0, 0, 1, 1), (0, 0, 1, 1, 0), (1, 1, 0, 0, 0), (0, 1, 0, 1, 0)}.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "36\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Corresponding to this compression of sets we introduce a mapping ˜Ri,j\n",
      "(which depends on C) by ˜Ri,j(y) = y if Ri,j(y) ∈ C and ˜Ri,j(y) = Ri,j(y)\n",
      "else. In our example this maps corresponding elements in the sets onto each\n",
      "other. In preparation, for the next lemma we need the simple little result:\n",
      "Lemma 13: Let C be any set of k itemsets and z ∈ C, ej ≤ z, ei (cid:54)≤ z. Then\n",
      "\n",
      "z − ej + ei ∈ ˜Ri,j(C).\n",
      "\n",
      "Proof: There are two cases to consider:\n",
      "(1) Either z − ej + ei (cid:54)∈ C in which case z − ej + ei = ˜Ri,j(z).\n",
      "(2) Or z − ej + ei ∈ C and as z − ej + ei = ˜Ri,j(z − ej + ei) one gets again\n",
      "\n",
      "z − ej + ei ∈ ˜Ri,j(C).\n",
      "\n",
      "The next result shows that in terms of the shadow, the “compression”\n",
      "˜Ri,j really is a compression as the shadow of a compressed set can never\n",
      "be larger than the shadow of the original set. We suggest therefor to call it\n",
      "compression lemma.\n",
      "\n",
      "Lemma 14: Let C be a set of k itemsets. Then one has\n",
      "\n",
      "∂ ˜Ri,j(C) ⊂ ˜Ri,j(∂C).\n",
      "Proof: Let x ∈ ∂ ˜Ri,j(C). We need to show that\n",
      "\n",
      "x ∈ ˜Ri,j(∂C)\n",
      "\n",
      "and we will enumerate all possible cases.\n",
      "\n",
      "First notice that there exists a ek (cid:54)≤ x such that\n",
      "\n",
      "x + ek ∈ ˜Ri,j(C)\n",
      "\n",
      "so there is an y ∈ C such that\n",
      "\n",
      "x + ek = ˜Ri,j(y).\n",
      "\n",
      "(1) In the ﬁrst two cases ˜Ri,j(y) (cid:54)= y and so one has (by the previous\n",
      "\n",
      "lemma)\n",
      "\n",
      "x + ek = y − ej + ei,\n",
      "\n",
      "for some y ∈ C, ej ≤ y, ei (cid:54)≤ y.\n",
      "\n",
      "(a) First consider i (cid:54)= k. Then there is a bitvector z such that y = z+ek\n",
      "\n",
      "and z ∈ ∂C. Thus we get\n",
      "\n",
      "x = z − ej + ei ∈ ˜Ri,j(∂C)\n",
      "\n",
      "as z ∈ ∂C and with lemma ??.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "37\n",
      "\n",
      "(b) Now consider i = k. In this case x + ei = y − ej + ei and so\n",
      "\n",
      "x = y − ej ∈ ∂C. As ej (cid:54)≤ x one gets\n",
      "\n",
      "x = ˜Ri,j(x) ∈ ˜Ri,j(∂C).\n",
      "\n",
      "(2) In the remaining cases ˜Ri,j(y) = y, i.e., x + ek = ˜Ri,j(x + ek). Thus\n",
      "x + ek = y ∈ C and so x ∈ ∂C. Note that ˜Ri,j actually depends on ∂C!\n",
      "(a) In the case where ej (cid:54)≤ x one has x = ˜Ri,j(x) ∈ ˜Ri,j(∂C).\n",
      "(b) In the other case ej ≤ x. We will show that x = ˜Ri,j(x) and, as\n",
      "\n",
      "x ∈ ∂C one gets x ∈ ˜Ri,j(∂C).\n",
      "i. If k (cid:54)= i then one can only have x + ek = ˜Ri,j(x + ek) if either\n",
      "ei ≤ x, in which case x = ˜Ri,j(x), or x− ej + ei + ek ∈ C in which\n",
      "case x − ej + ei ∈ ∂C and so x = ˜Ri,j(x).\n",
      "ii. Finally, if k = i, then x + ei ∈ C and so x − ej + ei ∈ ∂C thus\n",
      "\n",
      "x = ˜Ri,j(x).\n",
      "\n",
      "The operator ˜Ri,j maps sets of k itemsets onto sets of k itemsets and\n",
      "does not change the number of elements in a set of k itemsets. One now\n",
      "says that a set of k itemsets C is compressed if ˜Ri,j(C) = C for all i < j.\n",
      "This means that for any z ∈ C one has again Rij(z) ∈ C. Now we can move\n",
      "to prove the key theorem:\n",
      "Theorem 15: Let k ≥ 1, A ⊂ N (k), s ≤ ms < ··· < mk and\n",
      "\n",
      "then\n",
      "\n",
      "|A| ≥ b(k)(mk, . . . , ms)\n",
      "\n",
      "|∂A| ≥ b(k−1)(mk, . . . , ms).\n",
      "\n",
      "Proof: First we note that the shadow is a monotone function of the un-\n",
      "derlying set, i.e., if A1 ⊂ A2 then ∂A1 ⊂ ∂A2. From this it follows that it\n",
      "is enough to show that the bound holds for |A| = b(k)(mk, . . . , ms).\n",
      "\n",
      "Furthermore, it is suﬃcient to show this bound for compressed A as\n",
      "compression at most reduces the size of the shadow and we are looking for\n",
      "a lower bound. Thus we will assume A to be compressed in the following.\n",
      "The proof uses double induction over k and m = |A|. First we show\n",
      "that the theorem holds for the cases of k = 1 for any m and m = 1 for any\n",
      "k. In the induction step we show that if the theorem holds for 1, . . . , k − 1\n",
      "and any m and for 1, . . . , m − 1 and k then it also holds for k and m, see\n",
      "ﬁgure (??).\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "38\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Fig. 9. Double induction\n",
      "\n",
      "In the case of k = 1 (as A is compressed) one has:\n",
      "A = B(1)(m) = {e0, . . . , em−1}\n",
      "\n",
      "and so\n",
      "\n",
      "∂A = ∂B(1)(m) = {0}\n",
      "\n",
      "hence |∂A| = 1 = b(0)(m).\n",
      "\n",
      "In the case of m = |A| = 1 one has:\n",
      "\n",
      "and so:\n",
      "\n",
      "A = B(k)(k) = {e(0, . . . , k − 1)}\n",
      "\n",
      "∂A = ∂B(k)(k) = [k](k−1)\n",
      "\n",
      "hence |∂A| = k = b(k−1)(k).\n",
      "The key step of the proof is a partition of A into bitvectors with bit 0 set\n",
      "and such for which bit 0 is not set: A = A0∪A1 where A0 = {x ∈ A|x0 = 0}\n",
      "and A1 = {x ∈ A|x0 = 1}.\n",
      "(1) If x ∈ ∂A0 then x + ej ∈ A0 for some j > 0. As A is compressed it\n",
      "must also contain x + e0 = R0j(x + ej) ∈ A1 and so x ∈ A1 − e0 thus\n",
      "\n",
      "|∂A0| ≤ |A1 − e0| = |A1|.\n",
      "\n",
      "(2) A special case is A = B(k)(mk, . . . , ms) where one has |A0| = b(k)(mk−\n",
      "\n",
      "1, . . . , ms − 1) and |A1| = b(k−1)(mk − 1, . . . , ms − 1) and thus\n",
      "m = b(k)(mk, . . . , ms) = b(k)(mk−1, . . . , ms−1)+b(k−1)(mk−1, . . . , ms−1)\n",
      "\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u00011m−1mkk−1\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "39\n",
      "\n",
      "(3) Now partition ∂A1 into 2 parts:\n",
      "\n",
      "∂A1 = (A1 − e0) ∪ (∂(A1 − e0) + e0).\n",
      "\n",
      "It follows from previous inequalities and the induction hypothesis that\n",
      "|∂A1| = |A1−e0|+|∂(A1−e0)+e0| = |A1|+|∂(A1−e0)| ≥ b(k−1)(mk−\n",
      "1, . . . , ms − 1) + b(k−2)(mk − 1, . . . , ms − 1) = b(k−1)(mk, . . . , ms) and\n",
      "hence\n",
      "\n",
      "|∂A| ≥ |∂A1| ≥ b(k−1)(mk, . . . , ms)\n",
      "\n",
      "This theorem is the tool to derive the bounds for the size of future candidate\n",
      "itemsets based on a current itemset and the apriori principle.\n",
      "\n",
      "Theorem 16: Let the sequence Ck satisfy the apriori property and let\n",
      "\n",
      "|Ck| = b(k)(mk, . . . , mr).\n",
      "\n",
      "|Ck+p| ≤ b(k+p)(mk, . . . , mr)\n",
      "\n",
      "Then\n",
      "\n",
      "for all p ≤ r.\n",
      "\n",
      "Proof: The reason for the condition on p is that the shadows are well\n",
      "deﬁned.\n",
      "First, we choose r such that mr ≤ r + p − 1, mr+1 ≤ r + 1 + p − 1, . . .,\n",
      "ms−1 ≤ s − 1 + p − 1 and ms ≥ s + p − 1. Note that s = r and s = k + 1\n",
      "may be possible.\n",
      "\n",
      "Now we get an upper bound for the size |Ck|:\n",
      "\n",
      "|Ck| = b(k)(mk, . . . , mr)\n",
      "\n",
      "≤ b(k)(mk, . . . , ms) +\n",
      "\n",
      "= b(k)(mk, . . . , ms) +\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "s−1(cid:88)\n",
      "(cid:18)\n",
      "\n",
      "j=1\n",
      "\n",
      "j + p − 1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "j\n",
      "s + p − 1\n",
      "s − 1\n",
      "\n",
      "− 1\n",
      "\n",
      "according to a previous lemma.\n",
      "\n",
      "If the theorem does not hold then |Ck+p| > b(k+p)(mj, . . . , mr) and thus\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "|Ck+p| ≥ b(k+p)(mj, . . . , mr) + 1\n",
      "≥ b(k+p)(mk, . . . , ms) +\n",
      "= b(k+p)(mk, . . . , ms, s + p − 1).\n",
      "\n",
      "s + p − 1\n",
      "s + p − 1\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "40\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "Here we can apply the previous theorem to get a lower bound for Ck:\n",
      "\n",
      "|Ck| ≥ b(k)(mk, . . . , ms, s + p − 1).\n",
      "\n",
      "This, however is contradicting the higher upper bound we got previously\n",
      "and so we have to have |Ck+p| ≤ b(k+p)(mj, . . . , mr).\n",
      "\n",
      "As a simple consequence one also gets tightness:\n",
      "\n",
      "Corollary 17: For any m and k there exists a Ck with |Ck| = m =\n",
      "b(k+p)(mk, . . . , ms+1). such that\n",
      "\n",
      "|Ck+p| = b(k+p)(mk, . . . , ms+1).\n",
      "\n",
      "Proof: The Ck consists of the ﬁrst m k-itemsets in the colexicographic\n",
      "ordering.\n",
      "\n",
      "by the theory. A consequence of the theorem is that for Lk with |Lk| ≤(cid:0)mk\n",
      "one has |Ck+p| ≤(cid:0) mk\n",
      "\n",
      "In practice one would know not only the size but also the contents of any\n",
      "Ck and from that one can get a much better bound than the one provided\n",
      ". In particular, one has Ck+p = ∅ for k > mp − p.\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "k\n",
      "\n",
      "k+p\n",
      "\n",
      "4. Extensions\n",
      "\n",
      "4.1. Apriori Tid\n",
      "One variant of the apriori algorithm discussed above computes supports\n",
      "of itemsets by doing intersections of columns. Some of these intersections\n",
      "are repeated over time and, in particular, entries of the Boolean matrix\n",
      "are revisited which have no impact on the support. The Apriori TID [?]\n",
      "algorithm provides a solution to some of these problems. For computing\n",
      "the supports for larger itemsets it does not revisit the original table but\n",
      "transforms the table as it goes along. The new columns correspond to the\n",
      "candidate itemsets. In this way each new candidate itemset only requires\n",
      "the intersection of two old ones.\n",
      "\n",
      "The following demonstrates with an example how this works. The exam-\n",
      "ple is adapted from [?]. In the ﬁrst row the itemsets from Ck are depicted.\n",
      "The minimal support is 50 percent or 2 rows. The initial matrix of the tid\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "41\n",
      "\n",
      "algorithm is equal to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 2 3 4 5\n",
      "1 0 1 1 0\n",
      "0 1 1 0 1\n",
      "1 1 1 0 1\n",
      "0 1 0 0 1\n",
      "\n",
      "Note that the column (or item) four is not frequent and is not considered\n",
      "for Ck. After one step of the Apriori tid one gets the matrix:\n",
      "\n",
      "(1, 2) (1, 3) (1, 5) (2, 3) (2, 5) (3, 5)\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Here one can see directly that the itemsets (1, 2) and (1, 5) are not frequent.\n",
      "It follows that there remains only one candidate itemset with three items,\n",
      "\n",
      "namely (2, 3, 5) and the matrix is\n",
      "\n",
      "\n",
      "\n",
      "(2, 3, 5)\n",
      "\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "Let z(j1, . . . , jk) denote the elements of Ck. Then the elements in the trans-\n",
      "formed Boolean matrix are az(j1,...,jk)(xi).\n",
      "We will again use an auxiliary array v ∈ {0, 1}n. The apriori tid algo-\n",
      "rithm uses the join considered earlier in order to construct a matrix for\n",
      "the frequent itemsets Lk+1 from Lk. (As in the previous algorithms it is\n",
      "assumed that all matrices are stored in memory. The case of very large data\n",
      "sets which do not ﬁt into memory will be discussed later.) The key part of\n",
      "the algorithm, i.e., the step from k to k + 1 is then:\n",
      "\n",
      "(1) Select a pair of frequent k-itemsets (y, z), mark as read\n",
      "(2) expand xy =\n",
      "(3) extract elements using xz, i.e., w ← v[xz]\n",
      "(4) compress result and reset v to zero, v ← 0\n",
      "\n",
      "i , i.e., v ← xy\n",
      "\n",
      "i xyi\n",
      "\n",
      "There are three major steps where the auxiliary vector v is accessed. The\n",
      "\n",
      "\n",
      "\n",
      "(cid:86)\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "42\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "time complexity for this is\n",
      "\n",
      "T =\n",
      "\n",
      "n(cid:88)\n",
      "\n",
      "(2|(x(i))y| + |(x(i))z|)τ.\n",
      "\n",
      "This has to be done for all elements y∨ z where y, z ∈ Lk. Thus the average\n",
      "complexity is\n",
      "\n",
      "i=1\n",
      "\n",
      "E(T ) =\n",
      "\n",
      "3nmkE(xy)τ\n",
      "\n",
      "for some “average” y and xy =\n",
      "i . Now for all elements in Lk the\n",
      "support is larger than σ, thus E(xy) ≥ σ. So we get a lower bound for the\n",
      "complexity:\n",
      "\n",
      "i xyi\n",
      "\n",
      "E(T ) ≥\n",
      "\n",
      "3nmkστ.\n",
      "\n",
      "We can also obtain a simple upper bound if we observe that E(xy) ≤\n",
      "E(|x|)/d which is true “on average”. From this we get\n",
      "\n",
      "(cid:88)\n",
      "(cid:86)\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "E(T ) ≤\n",
      "\n",
      "3nmk\n",
      "\n",
      "E(|x|)\n",
      "\n",
      "τ.\n",
      "\n",
      "d\n",
      "\n",
      "Another approximation (typically a lower bound) is obtained if we assume\n",
      "that the components of x are independent. In this case E(xy) ≈ (E(|x|)/d)k\n",
      "and thus\n",
      "\n",
      "E(T ) ≥\n",
      "\n",
      "3nmk(E(|x|)/d)kτ.\n",
      "\n",
      "From this we would expect that for some rk ∈ [1, k] we get the approxima-\n",
      "tion\n",
      "\n",
      "E(T ) ≈\n",
      "\n",
      "3nmk(E(|x|)/d)rk τ.\n",
      "\n",
      "Now recall that the original column-wise apriori implementation required\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "(cid:88)\n",
      "\n",
      "k\n",
      "\n",
      "and so the “speedup” we can achieve by using this new algorithm is around\n",
      "\n",
      "E(T ) ≈\n",
      "\n",
      "3nmkk(E(|x|)/d)τ\n",
      "\n",
      "k\n",
      "\n",
      "(cid:80)\n",
      "(cid:80)\n",
      "k(E(|x|)/d)rk−1mk\n",
      "\n",
      "k kmk\n",
      "\n",
      ".\n",
      "\n",
      "S ≈\n",
      "\n",
      "which can be substantial as both k ≥ 1 and E(|x|)/d)rk−1 < 1. We can see\n",
      "that there are two reasons for the decrease in work: First we have reused\n",
      "earlier computations of xj1 ∧ ··· ∧ xjk and second we are able to make use\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "43\n",
      "\n",
      "of the lower support of the k-itemsets for larger k. While this second eﬀect\n",
      "does strongly depend on rk and thus the data, the ﬁrst eﬀect always holds,\n",
      "so we get a speedup of at least\n",
      "\n",
      "(cid:80)\n",
      "k kmk(cid:80)\n",
      "\n",
      "k mk\n",
      "\n",
      ",\n",
      "\n",
      "S ≥\n",
      "\n",
      "i.e., the average size of the k-itemsets. Note that the role of the number mk\n",
      "of candidate itemsets maybe slightly diminished but this is still the core\n",
      "parameter which determines the complexity of the algorithm and the need\n",
      "to reduce the size of the frequent itemsets is not diminished.\n",
      "\n",
      "4.2. Constrained association rules\n",
      "The number of frequent itemsets found by the apriori algorithm will of-\n",
      "ten be too large or too small. While the prime mechanism of controlling\n",
      "the discovered itemsets is the minimal support σ, this may often not be\n",
      "enough. Small collections of frequent itemsets may often contain mostly\n",
      "well known associations whereas large collections may reﬂect mostly ran-\n",
      "dom ﬂuctuations. There are eﬀective other ways to control the amount of\n",
      "itemsets obtained. First, in the case of too many itemsets one can use con-\n",
      "straints to ﬁlter out trivial or otherwise uninteresting itemsets. In the case\n",
      "of too few frequent itemsets one can also change the attributes or features\n",
      "which deﬁne the vector x. In particular, one can introduce new “more gen-\n",
      "eral” attributes. For example, one might ﬁnd that rules including the item\n",
      "“ginger beer” are not frequent. However, rules including “soft drinks” will\n",
      "have much higher support and may lead to interesting new rules. Thus one\n",
      "introduces new more general items. However, including more general items\n",
      "while maintaining the original special items leads to duplications in the\n",
      "itemsets, in our example the itemset containing ginger beer and soft drinks\n",
      "is identical to the set which only contains ginger beer. In order to avoid this\n",
      "one can again introduce constraints, which, in our example would identify\n",
      "the itemset containing ginger beer only with the one containing softdrink\n",
      "and ginger beer.\n",
      "\n",
      "Constraints are conditions for the frequent itemsets of interest. These\n",
      "\n",
      "conditions take the form “predicate = true” with some predicates\n",
      "\n",
      "b1(z), . . . , bs(z).\n",
      "\n",
      "Thus one is looking for frequent k-itemsets L∗\n",
      "i.e.,\n",
      "\n",
      "k for which the bj are true,\n",
      "\n",
      "k := {z ∈ Lk | bj(z) = 1}.\n",
      "L∗\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "44\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "These constraints will reduce the amount of frequent itemsets which need\n",
      "to be further processed, but can they also assist in making the algorithms\n",
      "more eﬃcient? This will be discussed next after we have considered some\n",
      "examples. Note that the constraints are not necessarily simple conjunctions!\n",
      "Examples:\n",
      "• We have mentioned the rule that any frequent itemset should not con-\n",
      "tain an item and its generalisation, e.g., it should not contain both soft\n",
      "drinks and ginger beer as this is identical to ginger beer. The constraint\n",
      "is of the form b(x) = ¬ay(x) where y is the itemset where the “softdrink\n",
      "and ginger beer bits” are set.\n",
      "• In some cases, frequent itemsets have been well established earlier. An\n",
      "example are crisps and soft drinks. There is no need to rediscover this\n",
      "association. Here the constraint is of the form b(x) = ¬δy(x) where y\n",
      "denotes the itemset “softdrinks and chips”.\n",
      "• In some cases, the domain knowledge tells us that some itemsets are pre-\n",
      "scribed, like in the case of a medical schedule which prescribes certain\n",
      "procedures to be done jointly but others should not be jointly. Finding\n",
      "these rules is not interesting. Here the constraint would exclude certain\n",
      "z, i.e., b(z) = ¬δy(z) where y is the element to exclude.\n",
      "• In some cases, the itemsets are related by deﬁnition. For example the\n",
      "predicates deﬁned by |z| > 2 is a consequence of |z| > 4. Having discov-\n",
      "ered the second one relieves us of the need to discover the ﬁrst one. This,\n",
      "however, is a diﬀerent type of constraint which needs to be considered\n",
      "when deﬁning the search space.\n",
      "A general algorithm for the determination of the L∗\n",
      "\n",
      "k determines at every\n",
      "step the Lk (which are required for the continuation) and from those out-\n",
      "puts the elements of L∗\n",
      "k. The algorithm is exactly the same as apriori or apri-\n",
      "ori tid except that not all frequent itemsets are output. See Algorithm ??.\n",
      "The work is almost exactly the same as for the original apriori algorithm.\n",
      "Now we would like to understand how the constraints can impact the\n",
      "computational performance, after all, one will require less rules in the\n",
      "end and the discovery of less rules should be faster. This, however, is not\n",
      "straight-forward as the constrained frequent itemsets L∗\n",
      "k do not necessar-\n",
      "ily satisfy the apriori property. There is, however an important class of\n",
      "constraints for which the apriori property holds:\n",
      "\n",
      "Theorem 18: If the constraints bj, j = 1, . . . , m are anti-monotone then\n",
      "k} satisﬁes the apriori condition.\n",
      "the set of constrained frequent itemsets {L∗\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "45\n",
      "\n",
      "Algorithm 2 Apriori with general constraints\n",
      "C1 = A(X) is the set of all one-itemsets, k = 1\n",
      "while Ck (cid:54)= ∅ do\n",
      "\n",
      "scan database to determine support of all z ∈ Ck\n",
      "extract frequent itemsets from Ck into Lk\n",
      "use the constraints to extract the constrained frequent itemsets in L∗\n",
      "generate Ck+1\n",
      "k := k + 1.\n",
      "\n",
      "k\n",
      "\n",
      "end while\n",
      "\n",
      "Proof: Let y ∈ L∗\n",
      "itemsets) Lk satisfy the apriori condition one has z ∈ Lsize(z).\n",
      "\n",
      "k ⊂ Lk and the (unconstrained frequent\n",
      "\n",
      "k and z ≤ y. As L∗\n",
      "\n",
      "As the bj are antimonotone and y ∈ L∗\n",
      "\n",
      "k one has\n",
      "\n",
      "bj(z) ≥ bj(y) = 1\n",
      "\n",
      "and so bj(z) = 1 from which it follows that z ∈ L∗\n",
      "\n",
      "size(z).\n",
      "\n",
      "When the apriori condition holds one can generate the candidate item-\n",
      "sets Ck in the (constrained) apriori algorithm from the sets L∗\n",
      "k instead of\n",
      "from the larger Lk. However, the constraints need to be anti-monotone. We\n",
      "know that constraints of the form az(j) are monotone and thus constraints\n",
      "of the form bj = ¬az(j) are antimonotone. Such constraints say that a cer-\n",
      "tain combination of items should not occur in the itemset. An example of\n",
      "this is the case of ginger beer and soft drinks. Thus we will have simpler\n",
      "frequent itemsets in general if we apply such a rule. Note that itemsets have\n",
      "played three diﬀerent roles so far:\n",
      "\n",
      "(1) as data points x(i)\n",
      "(2) as potentially frequent itemsets z and\n",
      "(3) to deﬁne constraints ¬az(j).\n",
      "\n",
      "The constraints of the kind bj = ¬az(j) are now used to reduce the\n",
      "candidate itemsets Ck prior to the data scan (this is how we save most).\n",
      "Even better, it turns out that the conditions only need to be checked for\n",
      "level k = |z(j)| where k is the size of the itemset deﬁning the constraint.\n",
      "(This gives a minor saving.) This is summarised in the next theorem:\n",
      "Theorem 19: Let the constraints be bj = ¬az(j) for j = 1, . . . , s. Further-\n",
      "more let the candidate k-itemsets for L∗\n",
      "\n",
      "k be sets of k-itemsets such that\n",
      "\n",
      "k = {y ∈ Ik |\n",
      "C∗\n",
      "\n",
      "if z < y then z ∈ L|z| and bj(y) = 1}\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "46\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "and a further set deﬁned by\n",
      "˜Ck = {y ∈ Ik | if z < y then z ∈ L|z| and if |z(j)| = k then bj(y) = 1 }.\n",
      "Then ˜Ck = C∗\n",
      "k .\n",
      "Proof: We need to show that every element y ∈ ˜Ck satisﬁes the constraints\n",
      "bj(y) = 1. Remember that |y| = k. There are three cases:\n",
      "• If |z(j)| = |y| then the constraint is satisﬁed by deﬁnition\n",
      "• If |z(j)| > |y| then z(j) (cid:54)≤ y and so bj(y) = 1\n",
      "• Consider the case |z(j)| < |y|. If bj(y) = 0 then az(j)(y) = 1 and\n",
      "so z(j) ≤ y. As |z(j)| < |y| it follows z(j) < y. Thus it follows that\n",
      "z(j) ∈ L∗\n",
      "z(j) and, consequently, bj(z(j)) = 1 or z(j) (cid:54)≤ z(j) which is not\n",
      "true. It follows that in this case we have bj(y) = 1.\n",
      "\n",
      "From this it follows that ˜Ck ⊂ C∗\n",
      "the deﬁnition of the sets.\n",
      "\n",
      "k. The converse is a direct consequence of\n",
      "\n",
      "Thus we get a variant of the apriori algorithm which checks the constraints\n",
      "only for one level, and moreover, this is done to reduce the number of\n",
      "candidate itemsets. This is Algorithm ??.\n",
      "\n",
      "Algorithm 3 Apriori with antimonotone constraints\n",
      "\n",
      "C1 = A(X) is the set of all one-itemsets, k = 1\n",
      "while Ck (cid:54)= ∅ do\n",
      "\n",
      "extract elements of Ck which satisfy the constraints az(j)(x) = 0 for\n",
      "|z(j)| = k and put into C∗\n",
      "scan database to determine support of all y ∈ C∗\n",
      "extract frequent itemsets from C∗\n",
      "generate Ck+1 (as per ordinary apriori)\n",
      "k := k + 1.\n",
      "\n",
      "k into L∗\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "end while\n",
      "\n",
      "4.3. Partitioned algorithms\n",
      "The previous algorithms assumed that all the data was able to ﬁt into\n",
      "main memory and was resident in one place. Also, the algorithm was for\n",
      "one processor. We will look here into partitioned algorithms which lead to\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "47\n",
      "\n",
      "parallel, distributed and out-of-core algorithms with few synchronisation\n",
      "points and little disk access. The algorithms have been suggested in [?].\n",
      "\n",
      "We assume that the data is partitioned into equal parts as\n",
      "\n",
      "D = [D1, D2, . . . , Dp]\n",
      "\n",
      "where D1 = (x(1), . . . , x(n/p)), D2 = (x(n/p+1), . . . , x(2n/p)), etc. While we\n",
      "assume equal distribution it is simple to generalise the discussions below to\n",
      "non-equal distributions.\n",
      "\n",
      "In each partition Dj an estimate for the support s(a) of a predicate\n",
      "can be determined and we will call this ˆsj(a). If ˆs(a) is the estimate of the\n",
      "support in D then one has\n",
      "\n",
      "p(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "ˆs(a) =\n",
      "\n",
      "1\n",
      "p\n",
      "\n",
      "ˆsj(a).\n",
      "\n",
      "This leads to a straight-forward parallel implementation of the apriori al-\n",
      "gorithm: The extraction of the Lk can either be done on all the processors\n",
      "\n",
      "Algorithm 4 Parallel Apriori\n",
      "\n",
      "C1 = A(X) is the set of all one-itemsets, k = 1\n",
      "while Ck (cid:54)= ∅ do\n",
      "\n",
      "scan database to determine support of all z ∈ Ck on each Dj and sum\n",
      "up the results\n",
      "extract frequent itemsets from Ck into Lk\n",
      "generate Ck+1\n",
      "k := k + 1.\n",
      "\n",
      "end while\n",
      "\n",
      "redundantly or on one master processor and the result can then be com-\n",
      "municated. The parallel algorithm also leads to an out-of-core algorithm\n",
      "which does the counting of the supports in blocks. One can equally develop\n",
      "an apriori-tid variant as well.\n",
      "\n",
      "There is a disadvantage of this straight-forward approach, however. It\n",
      "does require many synchronisation points, respectively, many scans of the\n",
      "disk, one for each level. As the disks are slow and synchronisation expensive\n",
      "this will cost some time. We will not discuss and algorithm suggested by [?]\n",
      "which substantially reduces disk scans or synchronisation points at the cost\n",
      "of some redundant computations. First we observe that\n",
      "\n",
      "min\n",
      "\n",
      "k\n",
      "\n",
      "ˆsk(a) ≤ ˆs(a) ≤ max\n",
      "\n",
      "k\n",
      "\n",
      "ˆsk(a)\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "48\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "which follows from the summation formula above. A consequence of this is\n",
      "\n",
      "Theorem 20: Each a which is frequent in D is at least frequent in one\n",
      "Dj.\n",
      "\n",
      "Proof: If for some frequent a this would not hold then one would get\n",
      "\n",
      "max ˆsj(a) < σ0\n",
      "\n",
      "if σ0 is the threshold for frequent a. By the observation above ˆs(a) < σ0\n",
      "which contradicts the assumption that a is frequent.\n",
      "\n",
      "Using this one gets an algorithm which generates in a ﬁrst step frequent k-\n",
      "itemsets Lk,j for each Dj and each k. This requires one scan of the data, or\n",
      "can be done on one processor, respectively. The union of all these frequent\n",
      "itemset is then used as a set of candidate itemsets and the supports of all\n",
      "these candidates is found in a second scan of the data. The parallel variant of\n",
      "the algorithm is then Algorithm ??. Note that the supports for all the levels\n",
      "\n",
      "(cid:83)p\n",
      "\n",
      "Algorithm 5 Parallel Association Rules\n",
      "\n",
      "j=1 Lk,j and broadcast\n",
      "\n",
      "determine the frequent k-itemsets Lk,j for all Dj in parallel\n",
      "C p\n",
      "k :=\n",
      "determine supports ˆsk for all candidates and all partitions in parallel\n",
      "collect all the supports, sum up and extract the frequent elements from\n",
      "C p\n",
      "k.\n",
      "\n",
      "k are collected simultaneously thus they require only two synchronisation\n",
      "points. Also, the apriori property holds for the C p\n",
      "k:\n",
      "\n",
      "Proposition 21: The sequence C p\n",
      "\n",
      "k satisﬁes the apriori property, i.e.,\n",
      "\n",
      "z ∈ C p\n",
      "\n",
      "k & y ≤ z ⇒ y ∈ C p|y|.\n",
      "\n",
      "Proof: If z ∈ C p\n",
      "the apriori property on Dj one has y ∈ L|y|,j and so y ∈ C p|y|.\n",
      "\n",
      "k & y ≤ z then there exists a j such that z ∈ Lk,j. By\n",
      "\n",
      "In order to understand the eﬃciency of the algorithm one needs to esti-\n",
      "mate the size of the C p\n",
      "k. In the (computationally best case, all the frequent\n",
      "itemsets are identiﬁed on the partitions and thus\n",
      "\n",
      "C p\n",
      "k = Lk,j = Lk.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "49\n",
      "\n",
      "We can use any algorithm to determine the frequent itemsets on one par-\n",
      "tition, and, if we assume that the algorithm is scalable in the data size the\n",
      "time to determine the frequent itemsets on all processors is equal to 1/p of\n",
      "the time required to determine the frequent itemsets on one processor as\n",
      "the data is 1/p on each processor. In addition we require to reads of the\n",
      "data base which has an expectation of nλτDisk/p where λ is the average\n",
      "size of the market baskets and τDisk is the time for one disk access. There\n",
      "is also some time required for the communication which is proportional to\n",
      "the size of the frequent itemsets. We will leave the further analysis which\n",
      "follows the same lines as our earlier analysis to the reader at this stage.\n",
      "\n",
      "As the partition is random, one can actually get away with the determi-\n",
      "nation of the supports for a small subset of C p\n",
      "k, as we only need to determine\n",
      "the support for az for which the supports have not been determined in the\n",
      "ﬁrst scan. One may also wish to choose the minimal support σ for the ﬁrst\n",
      "scan slightly lower in order to further reduce the amount of second scans\n",
      "required.\n",
      "\n",
      "4.4. Mining Sequences\n",
      "The following is an example of how one may construct more complex struc-\n",
      "tures from the market baskets. We consider here a special case of sequences,\n",
      "see [?,?]. Let the data be of the form\n",
      "\n",
      "(x1, . . . , xm)\n",
      "\n",
      "where each xi is an itemset (not a component as in our earlier notation.\n",
      "Examples of sequences correspond to the shopping behaviour of customers\n",
      "of retailers over time, or the sequence of services a patient receives over time.\n",
      "The focus is thus not on individual market-baskets but on the customers.\n",
      "We do not discuss the temporal aspects, just the sequential ones.\n",
      "\n",
      "In deﬁning our space of features we include the empty sequence () but\n",
      "\n",
      "not components of the sequences are 0, i.e.,\n",
      "\n",
      "xi (cid:54)= 0.\n",
      "\n",
      "The rationale for this is that sequences correspond to actions which occur in\n",
      "some order and 0 would correspond to a non-action. We are not interested\n",
      "in the times when a shopper went to the store and didn’t buy anything at\n",
      "all. Any empty component itemsets in the data will also be removed.\n",
      "\n",
      "The sequences also have an intrinsic partial ordering\n",
      "\n",
      "x ≤ y\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "50\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "which holds for (x1, . . . , xm) and (y1, . . . , yk) when ever there is a sequence\n",
      "1 ≤ i1 < i2 < ··· < im ≤ k such that\n",
      "\n",
      "xi ≤ yis,\n",
      "\n",
      "s = 1, . . . , m.\n",
      "\n",
      "One can now verify that this deﬁnes a partial order on the set of se-\n",
      "quences introduced above. However, the set of sequences does not form\n",
      "a lattice as there are not necessarily unique lowest upper or greatest\n",
      "lower bounds. For example, the two sequences ((0, 1), (1, 1), (1, 0)) and\n",
      "((0, 1), (0, 1)) have the two (joint) upper bounds ((0, 1), (1, 1), (1, 0), (0, 1))\n",
      "and ((0, 1), (0, 1), (1, 1), (1, 0) which have now common lower bound which\n",
      "is still an upper bound for both original sequences. This makes the search\n",
      "for frequent itemsets somewhat harder.\n",
      "Another diﬀerence is that the complexity of the mining tasks has grown\n",
      "considerably, with |I| items one has 2|I| market-baskets and thus 2|I|m\n",
      "diﬀerent sequences of length ≤ m. Thus it is essential to be able to deal\n",
      "with the computational complexity of this problem. Note in particular, that\n",
      "the probability of any particular sequence is going to be extremely small.\n",
      "However, one will be able to make statements about the support of small\n",
      "subsequences which correspond to shopping or treatment patterns.\n",
      "\n",
      "Based on the ordering, the support of a sequence x is the set of all\n",
      "\n",
      "sequences larger than x is\n",
      "\n",
      "s(x) = P ({x|x ≤ y}) .\n",
      "\n",
      "This is estimated by the number of sequences in the data base which are in\n",
      "the support. Note that the itemsets now occur as length 1 sequences and\n",
      "thus the support of the itemsets can be identiﬁed with the support of the\n",
      "corresponding 1 sequence. As our focus is now on sequences this is diﬀerent\n",
      "from the support we get if we look just at the distribution of the itemsets.\n",
      "The length of a sequence is the number of non-empty components. Thus\n",
      "we can now deﬁne an apriori algorithm as before. This would start with the\n",
      "determination of all the frequent 1 sequences which correspond to all the\n",
      "frequent itemsets. Thus the ﬁrst step of the sequence mining algorithm is\n",
      "just the ordinary apriori algorithm. Then the apriori algorithm continues\n",
      "as before, where the candidate generation step is similar but now we join\n",
      "any two sequences which have all components identical except for the last\n",
      "(non-empty) one. Then one gets a sequence of length m + 1 from two such\n",
      "sequences of length m by concatenating the last component of the second\n",
      "sequence on to the ﬁrst one. After that one still needs to check if all subse-\n",
      "quences are frequent to do some pruning.\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "51\n",
      "\n",
      "There has been some arbitrariness in some of the choices. Alternatives\n",
      "choose the size of a sequence as the sum of the sizes of the itemsets. In this\n",
      "case the candidate generation procedure becomes slightly more complex,\n",
      "see [?].\n",
      "\n",
      "4.5. The FP tree algorithm\n",
      "The Apriori algorithm is very eﬀective for discovering a reasonable num-\n",
      "ber of small frequent itemsets. However it does show severe performance\n",
      "problems for the discovery of large numbers of frequent itemsets. If, for\n",
      "example, there are 106 frequent items then the set of candidate 2-itemsets\n",
      "contains 5 · 1011 itemsets which all require testing. In addition, the Apriori\n",
      "algorithm has problems with the discovery of very long frequent itemsets.\n",
      "For the discovery of an itemset with 100 items the algorithm requires scan-\n",
      "ning the data for all the 2100 subsets in 100 scans. The bottleneck in the\n",
      "algorithm is the creation of the candidate itemsets, more precisely, the num-\n",
      "ber of candidate itemsets which need to be created during the mining. The\n",
      "reason for this large number is that the candidate itemsets are visited in a\n",
      "breadth-ﬁrst way.\n",
      "\n",
      "The FP tree algorithm addresses these issues and scans the data in a\n",
      "depth-ﬁrst way. The data is only scanned twice. In a ﬁrst scan, the frequent\n",
      "items (or 1-itemsets) are determined. The data items are then ordered based\n",
      "on their frequency and the infrequent items are removed. In the second scan,\n",
      "the data base is mapped onto a tree structure. Except for the root all the\n",
      "nodes are labelled with items, each item can correspond to multiple nodes.\n",
      "We will explain the algorithm with the help of an example, see table ?? for\n",
      "the original data and the records with the frequent itemsets only (here we\n",
      "look for support > 0.5).\n",
      "\n",
      "items\n",
      "\n",
      "f, a, c, d, g, i, m, p\n",
      "\n",
      "a, b, c, f, l, m, o\n",
      "b, f, h, j, o, w\n",
      "\n",
      "b, c, k, s, p\n",
      "\n",
      "s > 0.5\n",
      "\n",
      "f, c, a, m, p\n",
      "f, c, a, b, m\n",
      "\n",
      "f, b\n",
      "c, b, p\n",
      "\n",
      "a, f, c, e, l, p, m, n\n",
      "\n",
      "f, c, a, m, p\n",
      "\n",
      "Initially the tree consists only of the root. Then the ﬁrst record is read\n",
      "and a path is attached to the root such that the node labelled with the\n",
      "ﬁrst item of the record (items are ordered by their frequency) is adjacent to\n",
      "\n",
      "\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "52\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "the root, the second item labels the next neighbour and so on. In addition\n",
      "to the item, the label also contains the number 1, see Step 1 in ﬁgure ??.\n",
      "Then the second record is included such that any common preﬁx (in the\n",
      "\n",
      "Fig. 10. Construction of the FP-Tree\n",
      "\n",
      "example the items f,c,a is shared with the previous record and the remaining\n",
      "items are added in a splitted path. The numeric parts of the labels of the\n",
      "shared preﬁx nodes are increased by one, see Step 2 in the ﬁgure. This is\n",
      "then done with all the other records until the whole data base is stored\n",
      "in the tree. As the most common items were ordered ﬁrst, there is a big\n",
      "likelihood that many preﬁxes will be shared which results in substantial\n",
      "saving or compression of the data base. Note that no information is lost\n",
      "with respect to the supports. The FP tree structure is completed by adding\n",
      "a header table which contains all items together with pointers to their ﬁrst\n",
      "occurrence in the tree. The other occurrences are then linked together so\n",
      "that all occurrences of an item can easily be retrieved, see ﬁgure ??.\n",
      "\n",
      "The FP tree does never break a long pattern into smaller patterns the\n",
      "way the Apriori algorithm does. Long patterns can be directly retrieved\n",
      "from the FP tree. The FP tree also contains the full relevant information\n",
      "about the data base. It is compact, as all infrequent items are removed and\n",
      "the highly frequent items share nodes in the tree. The number of nodes is\n",
      "never less than the size of the data base measured in the sum of the sizes\n",
      "of the records but there is anecdotal evidence that compression rates can\n",
      "be over 100.\n",
      "\n",
      "The FP tree is used to ﬁnd all association rules containing particular\n",
      "items. Starting with the least frequent items, all rules containing those items\n",
      "\n",
      "c : 1a : 1m : 1p : 1f : 1c : 2a : 2m : 1p : 1f : 2b : 1m : 1c : 3b : 1b : 1p : 1a : 3m : 2p : 2f : 4c : 1b : 1m : 1Step 1Step 2Step 5\f",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "The Apriori Algorithm\n",
      "\n",
      "53\n",
      "\n",
      "Fig. 11. Final FP-Tree\n",
      "\n",
      "can be found simply by generating for each item the conditional data base\n",
      "which consists for each path which contains the item of those items which\n",
      "are between that item and the root. (The lower items don’t need to be\n",
      "considered, as they are considered together with other items.) These con-\n",
      "ditional pattern bases can then again be put into FP-trees, the conditional\n",
      "FP-trees and for those trees all the rules containing the previously selected\n",
      "and any other item will be extracted. If the conditional pattern base con-\n",
      "tains only one item, that item has to be the itemset. The frequencies of\n",
      "these itemsets can be obtained from the number labels.\n",
      "\n",
      "An additional speed-up is obtained by mining long preﬁx paths sepa-\n",
      "rately and combine the results at the end. Of course any chain does not\n",
      "need to be broken into parts necessarily as all the frequent subsets, together\n",
      "with their frequencies are easily obtained directly.\n",
      "\n",
      "5. Conclusion\n",
      "Data mining deals with the processing of large, complex and noisy data.\n",
      "Robust tools are required to recover weak signals. These tools require highly\n",
      "eﬃcient algorithms which scale with data size and complexity. Association\n",
      "rule discovery is one of the most popular and successful tools in data mining.\n",
      "Eﬃcient algorithms are available. The developments in association rule dis-\n",
      "covery combine concepts and insights from probability and combinatorics.\n",
      "The original algorithm “Apriori” was developed in the early years of data\n",
      "mining and is still widely used. Numerous variants and extensions exist of\n",
      "which a small selection was covered in this tutorial.\n",
      "\n",
      "The most recent work in association rules uses concepts from graph\n",
      "\n",
      "c : 3b : 1b : 1p : 1a : 3m : 2p : 2f : 4{}c : 1b : 1m : 1443333fcabmpitemsupportheader table\f",
      "View publication stats\n",
      "View publication stats\n",
      "\n",
      "March 30, 2005 9:7 WSPC/Lecture Notes Series: 9in x 6in\n",
      "\n",
      "heg05a\n",
      "\n",
      "54\n",
      "\n",
      "M. Hegland\n",
      "\n",
      "theory, formal concept analysis and and statistics and links association\n",
      "rules with graphical models and with hidden Markov models.\n",
      "\n",
      "In this tutorial some of the mathematical basis of association rules was\n",
      "covered but no attempt has been made to cover the vast literature discussing\n",
      "with numerous algorithms.\n",
      "\n",
      "Acknowledgements\n",
      "I would like to thank Zuowei Shen, for his patience and support during the\n",
      "preparation of this manuscript. Much of the work has arisen in discussions\n",
      "and collaboration with John Maindonald, Peter Christen, Ole Nielsen, Steve\n",
      "Roberts and Graham Williams.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#apriori = str_\n",
    "print(apriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 3\n",
      "\n",
      "Probability and Information\n",
      "Theory\n",
      "\n",
      "In this chapter, we describe probability theory and information theory.\n",
      "\n",
      "Probability theory is a mathematical framework for representing uncertain\n",
      "statements. It provides a means of quantifying uncertainty and axioms for deriving\n",
      "new uncertain statements. In artiﬁcial intelligence applications, we use probability\n",
      "theory in two major ways. First, the laws of probability tell us how AI systems\n",
      "should reason, so we design our algorithms to compute or approximate various\n",
      "expressions derived using probability theory. Second, we can use probability and\n",
      "statistics to theoretically analyze the behavior of proposed AI systems.\n",
      "\n",
      "Probability theory is a fundamental tool of many disciplines of science and\n",
      "engineering. We provide this chapter to ensure that readers whose background is\n",
      "primarily in software engineering with limited exposure to probability theory can\n",
      "understand the material in this book.\n",
      "\n",
      "While probability theory allows us to make uncertain statements and reason\n",
      "in the presence of uncertainty, information allows us to quantify the amount of\n",
      "uncertainty in a probability distribution.\n",
      "\n",
      "If you are already familiar with probability theory and information theory,\n",
      "you may wish to skip all of this chapter except for Sec. 3.14, which describes the\n",
      "graphs we use to describe structured probabilistic models for machine learning. If\n",
      "you have absolutely no prior experience with these subjects, this chapter should\n",
      "be suﬃcient to successfully carry out deep learning research projects, but we do\n",
      "suggest that you consult an additional resource, such as Jaynes (2003).\n",
      "\n",
      "52\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "3.1 Why Probability?\n",
      "\n",
      "Many branches of computer science deal mostly with entities that are entirely\n",
      "deterministic and certain. A programmer can usually safely assume that a CPU will\n",
      "execute each machine instruction ﬂawlessly. Errors in hardware do occur, but are\n",
      "rare enough that most software applications do not need to be designed to account\n",
      "for them. Given that many computer scientists and software engineers work in a\n",
      "relatively clean and certain environment, it can be surprising that machine learning\n",
      "makes heavy use of probability theory.\n",
      "\n",
      "This is because machine learning must always deal with uncertain quantities,\n",
      "and sometimes may also need to deal with stochastic (non-deterministic) quantities.\n",
      "Uncertainty and stochasticity can arise from many sources. Researchers have made\n",
      "compelling arguments for quantifying uncertainty using probability since at least\n",
      "the 1980s. Many of the arguments presented here are summarized from or inspired\n",
      "by Pearl (1988).\n",
      "\n",
      "Nearly all activities require some ability to reason in the presence of uncertainty.\n",
      "In fact, beyond mathematical statements that are true by deﬁnition, it is diﬃcult\n",
      "to think of any proposition that is absolutely true or any event that is absolutely\n",
      "guaranteed to occur.\n",
      "\n",
      "There are three possible sources of uncertainty:\n",
      "\n",
      "1. Inherent stochasticity in the system being modeled. For example, most\n",
      "interpretations of quantum mechanics describe the dynamics of subatomic\n",
      "particles as being probabilistic. We can also create theoretical scenarios that\n",
      "we postulate to have random dynamics, such as a hypothetical card game\n",
      "where we assume that the cards are truly shuﬄed into a random order.\n",
      "\n",
      "2. Incomplete observability. Even deterministic systems can appear stochastic\n",
      "when we cannot observe all of the variables that drive the behavior of the\n",
      "system. For example, in the Monty Hall problem, a game show contestant is\n",
      "asked to choose between three doors and wins a prize held behind the chosen\n",
      "door. Two doors lead to a goat while a third leads to a car. The outcome\n",
      "given the contestant’s choice is deterministic, but from the contestant’s point\n",
      "of view, the outcome is uncertain.\n",
      "\n",
      "3. Incomplete modeling. When we use a model that must discard some of\n",
      "the information we have observed, the discarded information results in\n",
      "uncertainty in the model’s predictions. For example, suppose we build a\n",
      "robot that can exactly observe the location of every object around it. If the\n",
      "\n",
      "53\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "robot discretizes space when predicting the future location of these objects,\n",
      "then the discretization makes the robot immediately become uncertain about\n",
      "the precise position of objects: each object could be anywhere within the\n",
      "discrete cell that it was observed to occupy.\n",
      "\n",
      "In many cases, it is more practical to use a simple but uncertain rule rather\n",
      "than a complex but certain one, even if the true rule is deterministic and our\n",
      "modeling system has the ﬁdelity to accommodate a complex rule. For example, the\n",
      "simple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a rule\n",
      "of the form, “Birds ﬂy, except for very young birds that have not yet learned to\n",
      "ﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\n",
      "including the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\n",
      "communicate, and after all of this eﬀort is still very brittle and prone to failure.\n",
      "\n",
      "Given that we need a means of representing and reasoning about uncertainty,\n",
      "it is not immediately obvious that probability theory can provide all of the tools\n",
      "we want for artiﬁcial intelligence applications. Probability theory was originally\n",
      "developed to analyze the frequencies of events. It is easy to see how probability\n",
      "theory can be used to study events like drawing a certain hand of cards in a\n",
      "game of poker. These kinds of events are often repeatable. When we say that\n",
      "an outcome has a probability p of occurring, it means that if we repeated the\n",
      "experiment (e.g., draw a hand of cards) inﬁnitely many times, then proportion p\n",
      "of the repetitions would result in that outcome. This kind of reasoning does not\n",
      "seem immediately applicable to propositions that are not repeatable. If a doctor\n",
      "analyzes a patient and says that the patient has a 40% chance of having the ﬂu,\n",
      "this means something very diﬀerent—we can not make inﬁnitely many replicas of\n",
      "the patient, nor is there any reason to believe that diﬀerent replicas of the patient\n",
      "would present with the same symptoms yet have varying underlying conditions. In\n",
      "the case of the doctor diagnosing the patient, we use probability to represent a\n",
      "degree of belief, with 1 indicating absolute certainty that the patient has the ﬂu\n",
      "and 0 indicating absolute certainty that the patient does not have the ﬂu. The\n",
      "former kind of probability, related directly to the rates at which events occur, is\n",
      "known as frequentist probability, while the latter, related to qualitative levels of\n",
      "certainty, is known as Bayesian probability.\n",
      "\n",
      "If we list several properties that we expect common sense reasoning about\n",
      "uncertainty to have, then the only way to satisfy those properties is to treat\n",
      "Bayesian probabilities as behaving exactly the same as frequentist probabilities.\n",
      "For example, if we want to compute the probability that a player will win a poker\n",
      "game given that she has a certain set of cards, we use exactly the same formulas\n",
      "as when we compute the probability that a patient has a disease given that she\n",
      "\n",
      "54\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "has certain symptoms. For more details about why a small set of common sense\n",
      "assumptions implies that the same axioms must control both kinds of probability,\n",
      "see Ramsey (1926).\n",
      "\n",
      "Probability can be seen as the extension of logic to deal with uncertainty. Logic\n",
      "provides a set of formal rules for determining what propositions are implied to\n",
      "be true or false given the assumption that some other set of propositions is true\n",
      "or false. Probability theory provides a set of formal rules for determining the\n",
      "likelihood of a proposition being true given the likelihood of other propositions.\n",
      "\n",
      "3.2 Random Variables\n",
      "\n",
      "A random variable is a variable that can take on diﬀerent values randomly. We\n",
      "typically denote the random variable itself with a lower case letter in plain typeface,\n",
      "and the values it can take on with lower case script letters. For example, x1 and x2\n",
      "are both possible values that the random variable x can take on. For vector-valued\n",
      "variables, we would write the random variable as x and one of its values as x. On\n",
      "its own, a random variable is just a description of the states that are possible; it\n",
      "must be coupled with a probability distribution that speciﬁes how likely each of\n",
      "these states are.\n",
      "\n",
      "Random variables may be discrete or continuous. A discrete random variable\n",
      "is one that has a ﬁnite or countably inﬁnite number of states. Note that these\n",
      "states are not necessarily the integers; they can also just be named states that\n",
      "are not considered to have any numerical value. A continuous random variable is\n",
      "associated with a real value.\n",
      "\n",
      "3.3 Probability Distributions\n",
      "\n",
      "A probability distribution is a description of how likely a random variable or\n",
      "set of random variables is to take on each of its possible states. The way we\n",
      "describe probability distributions depends on whether the variables are discrete or\n",
      "continuous.\n",
      "\n",
      "3.3.1 Discrete Variables and Probability Mass Functions\n",
      "\n",
      "A probability distribution over discrete variables may be described using a proba-\n",
      "bility mass function (PMF). We typically denote probability mass functions with a\n",
      "capital P . Often we associate each random variable with a diﬀerent probability\n",
      "\n",
      "55\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "mass function and the reader must infer which probability mass function to use\n",
      "based on the identity of the random variable, rather than the name of the function;\n",
      "P\n",
      "\n",
      "( )x is usually not the same as\n",
      "\n",
      "( )y .\n",
      "\n",
      "P\n",
      "\n",
      "The probability mass function maps from a state of a random variable to\n",
      "the probability of that random variable taking on that state. The probability\n",
      "that x = x is denoted as P (x), with a probability of 1 indicating that x = x is\n",
      "certain and a probability of 0 indicating that x = x is impossible. Sometimes\n",
      "to disambiguate which PMF to use, we write the name of the random variable\n",
      "explicitly: P (x = x). Sometimes we deﬁne a variable ﬁrst, then use ∼ notation to\n",
      "specify which distribution it follows later: x ∼ P (x .)\n",
      "\n",
      "Probability mass functions can act on many variables at the same time. Such\n",
      "a probability distribution over many variables is known as a joint probability\n",
      "distribution. P (x = x, y = y) denotes the probability that x = x and y = y\n",
      "simultaneously. We may also write\n",
      "\n",
      "for brevity.\n",
      "\n",
      "P x, y\n",
      "\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      "To be a probability mass function on a random variable x, a function P must\n",
      "\n",
      "satisfy the following properties:\n",
      "\n",
      "P\n",
      "\n",
      "• The domain of must be the set of all possible states of x.\n",
      "• ∀x\n",
      "0 \n",
      "\n",
      ",∈ x 0 ≤ P (x) ≤ 1. An impossible event has probability and no state can\n",
      "be less probable than that. Likewise, an event that is guaranteed to happen\n",
      "has probability , and no state can have a greater chance of occurring.\n",
      "\n",
      "1\n",
      "\n",
      "• x∈x P (x) = 1. We refer to this property as being normalized. Without this\n",
      "\n",
      "property, we could obtain probabilities greater than one by computing the\n",
      "probability of one of many events occurring.\n",
      "\n",
      "For example, consider a single discrete random variable x with k diﬀerent states.\n",
      "x—that is, make each of its states equally\n",
      "\n",
      "uniform distribution\n",
      "\n",
      "We can place a\n",
      "likely—by setting its probability mass function to\n",
      "\n",
      "on\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      "x\n",
      "i) =\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "(3.1)\n",
      "\n",
      "for all i. We can see that this ﬁts the requirements for a probability mass function.\n",
      "The value 1\n",
      "\n",
      "is a positive integer. We also see that\n",
      "\n",
      "k is positive because\n",
      "\n",
      "k\n",
      "\n",
      "i\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      "x\n",
      "\n",
      "i) =i\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "=\n",
      "\n",
      "k\n",
      "k\n",
      "\n",
      "= 1,\n",
      "\n",
      "(3.2)\n",
      "\n",
      "so the distribution is properly normalized.\n",
      "\n",
      "56\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "3.3.2 Continuous Variables and Probability Density Functions\n",
      "\n",
      "When working with continuous random variables, we describe probability dis-\n",
      "tributions using a probability density function (PDF) rather than a probability\n",
      "mass function. To be a probability density function, a function p must satisfy the\n",
      "following properties:\n",
      "\n",
      "p\n",
      "\n",
      "• The domain of must be the set of all possible states of x.\n",
      "• ∀x\n",
      "\n",
      "0≥ Note that we do not require ( ) \n",
      "p x\n",
      "\n",
      "1≤ .\n",
      "\n",
      ".\n",
      "\n",
      "∈ x ( ) \n",
      ", p x\n",
      "( ) = 1.\n",
      "\n",
      "•  p x dx\n",
      "\n",
      "A probability density function p(x) does not give the probability of a speciﬁc\n",
      "state directly, instead the probability of landing inside an inﬁnitesimal region with\n",
      "volume\n",
      "\n",
      "is given by\n",
      "\n",
      "p x δx\n",
      "\n",
      "( )\n",
      "\n",
      "δx\n",
      "\n",
      ".\n",
      "\n",
      "We can integrate the density function to ﬁnd the actual probability mass of a\n",
      "set of points. Speciﬁcally, the probability that x lies in some set S is given by the\n",
      "integral of p(x) over that set. In the univariate example, the probability that x\n",
      "lies in the interval\n",
      "\n",
      "is given by\n",
      "\n",
      ".\n",
      "p x dx\n",
      "\n",
      "[\n",
      "]a, b\n",
      "\n",
      "( )\n",
      "\n",
      "For an example of a probability density function corresponding to a speciﬁc\n",
      "probability density over a continuous random variable, consider a uniform distribu-\n",
      "tion on an interval of the real numbers. We can do this with a function u (x; a, b),\n",
      "where a and b are the endpoints of the interval, with b > a. The “;” notation means\n",
      "“parametrized by”; we consider x to be the argument of the function, while a and\n",
      "b are parameters that deﬁne the function. To ensure that there is no probability\n",
      "mass outside the interval, we say u(x; a, b) = 0 for all x ∈ [a, b]\n",
      ". Within a, b],\n",
      ". We can see that this is nonnegative everywhere. Additionally, it\n",
      "u x a, b\n",
      "( ;\n",
      "integrates to 1. We often denote that x follows the uniform distribution on [a, b]\n",
      "by writing x\n",
      "\n",
      ") = 1\n",
      "b a−\n",
      "∼ U a, b\n",
      ")\n",
      "\n",
      "(\n",
      "\n",
      "[\n",
      "\n",
      ".\n",
      "\n",
      "[\n",
      "\n",
      "]a,b\n",
      "\n",
      "3.4 Marginal Probability\n",
      "\n",
      "Sometimes we know the probability distribution over a set of variables and we want\n",
      "to know the probability distribution over just a subset of them. The probability\n",
      "distribution over the subset is known as the marginal probability distribution.\n",
      "\n",
      "For example, suppose we have discrete random variables x and y , and we know\n",
      "\n",
      "P ,(x y . We can ﬁnd\n",
      "\n",
      ")\n",
      "\n",
      "x with the\n",
      "\n",
      "sum rule\n",
      "\n",
      ":\n",
      "\n",
      "x, P\n",
      "\n",
      "( = x\n",
      "\n",
      "x\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      "x,\n",
      "\n",
      "y =  )\n",
      "y .\n",
      "\n",
      "(3.3)\n",
      "\n",
      "P ( )\n",
      "∀ ∈x\n",
      "\n",
      ") =y\n",
      "\n",
      "57\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "The name “marginal probability” comes from the process of computing marginal\n",
      "probabilities on paper. When the values of P(x y, ) are written in a grid with\n",
      "diﬀerent values of x in rows and diﬀerent values of y in columns, it is natural to\n",
      "sum across a row of the grid, then write P( x) in the margin of the paper just to\n",
      "the right of the row.\n",
      "\n",
      "For continuous variables, we need to use integration instead of summation:\n",
      "\n",
      "p x( ) = p x, y dy.\n",
      "\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      "(3.4)\n",
      "\n",
      "3.5 Conditional Probability\n",
      "\n",
      "In many cases, we are interested in the probability of some event, given that some\n",
      "other event has happened. This is called a conditional probability. We denote\n",
      "the conditional probability that y = y given x = x as P (y = y | x = x ). This\n",
      "conditional probability can be computed with the formula\n",
      "\n",
      "P\n",
      "\n",
      "( = y\n",
      "\n",
      "y\n",
      "\n",
      "| x =  ) =\n",
      "\n",
      "x\n",
      "\n",
      "P\n",
      "\n",
      "( = y\n",
      "\n",
      "y,\n",
      "\n",
      "x =  )\n",
      "x\n",
      "x\n",
      ")\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      ".\n",
      "\n",
      "(3.5)\n",
      "\n",
      "The conditional probability is only deﬁned when P (x = x) > 0. We cannot compute\n",
      "the conditional probability conditioned on an event that never happens.\n",
      "\n",
      "It is important not to confuse conditional probability with computing what\n",
      "would happen if some action were undertaken. The conditional probability that\n",
      "a person is from Germany given that they speak German is quite high, but if\n",
      "a randomly selected person is taught to speak German, their country of origin\n",
      "does not change. Computing the consequences of an action is called making an\n",
      "intervention query. Intervention queries are the domain of causal modeling, which\n",
      "we do not explore in this book.\n",
      "\n",
      "3.6 The Chain Rule of Conditional Probabilities\n",
      "\n",
      "Any joint probability distribution over many random variables may be decomposed\n",
      "into conditional distributions over only one variable:\n",
      "\n",
      "P (x(1), . . . , x ( )n ) = \n",
      "\n",
      "| x(1), . . . , x (\n",
      "product rule of probability. It\n",
      "follows immediately from the deﬁnition of conditional probability in Eq. 3.5. For\n",
      "\n",
      "This observation is known as the\n",
      "\n",
      "(P x(1) )Πn\n",
      "\n",
      "i=2P (x( )i\n",
      "\n",
      "chain rule\n",
      "\n",
      "i− ).\n",
      "\n",
      "(3.6)\n",
      "\n",
      "or\n",
      "\n",
      "1)\n",
      "\n",
      "58\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "example, applying the deﬁnition twice, we get\n",
      "\n",
      "P ,\n",
      "\n",
      "(a b c) =\n",
      "\n",
      ",\n",
      "\n",
      "P ,\n",
      "\n",
      "(b c) =\n",
      "\n",
      "P ,\n",
      "\n",
      "(a b c) =\n",
      "\n",
      ",\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "(a b|\n",
      "(b c| )\n",
      "(a b|\n",
      "\n",
      "c)\n",
      ", P ,\n",
      "\n",
      "(b c)\n",
      "\n",
      "( )P c\n",
      "\n",
      ", Pc)\n",
      "\n",
      "(b c| )\n",
      "\n",
      "( )P c .\n",
      "\n",
      "3.7 Independence and Conditional Independence\n",
      "\n",
      "Two random variables x and y are independent if their probability distribution can\n",
      "be expressed as a product of two factors, one involving only x and one involving\n",
      "only y:\n",
      "\n",
      "∀ ∈x\n",
      "\n",
      "x, y\n",
      "\n",
      "∈ y\n",
      "\n",
      ", p\n",
      "\n",
      "x\n",
      "( = \n",
      "\n",
      "x, y\n",
      "\n",
      "= ) =  ( =\n",
      "\n",
      "p x\n",
      "\n",
      "y\n",
      "\n",
      "x) ( =  )\n",
      "y .\n",
      "\n",
      "p y\n",
      "\n",
      "(3.7)\n",
      "\n",
      "Two random variables x and y are conditionally independent given a random\n",
      "variable z if the conditional probability distribution over x and y factorizes in this\n",
      "way for every value of z:\n",
      "\n",
      "∀ ∈x\n",
      "\n",
      "x, y\n",
      "\n",
      ", z∈ y ∈ z, p\n",
      "\n",
      "( =x\n",
      "\n",
      "x,\n",
      "\n",
      "y = \n",
      "\n",
      "y\n",
      "\n",
      "| z =  ) =  ( = x\n",
      "\n",
      "p\n",
      "\n",
      "z\n",
      "\n",
      "x\n",
      "\n",
      "| z =  ) ( = y\n",
      "\n",
      "z p\n",
      "\n",
      "y\n",
      "\n",
      "| z = )\n",
      "z .\n",
      "(3.8)\n",
      "\n",
      "We can denote independence and conditional independence with compact\n",
      "notation: x y⊥ means that x and y are independent, while x y z⊥ | means that x\n",
      "and y are conditionally independent given z.\n",
      "\n",
      "3.8 Expectation, Variance and Covariance\n",
      "\n",
      "or\n",
      "\n",
      "The expectation\n",
      "of some function f( x) with respect to a probability\n",
      "distribution P (x) is the average or mean value that f takes on when x is drawn\n",
      "from . For discrete variables this can be computed with a summation:\n",
      "\n",
      "expected value\n",
      "\n",
      "P\n",
      "\n",
      "Ex∼P[ (f x)] =x\n",
      "\n",
      "P x f x ,\n",
      "( ) ( )\n",
      "\n",
      "(3.9)\n",
      "\n",
      "while for continuous variables, it is computed with an integral:\n",
      "\n",
      "Ex∼p[ ( )] =\n",
      "\n",
      "f x\n",
      "\n",
      " p x f x dx.\n",
      "\n",
      "( ) ( )\n",
      "\n",
      "59\n",
      "\n",
      "(3.10)\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "When the identity of the distribution is clear from the context, we may simply\n",
      "write the name of the random variable that the expectation is over, as in Ex[f (x)].\n",
      "If it is clear which random variable the expectation is over, we may omit the\n",
      "subscript entirely, as in E[f (x)]. By default, we can assume that E [·] averages over\n",
      "the values of all the random variables inside the brackets. Likewise, when there is\n",
      "no ambiguity, we may omit the square brackets.\n",
      "\n",
      "Expectations are linear, for example,\n",
      "\n",
      "E x[\n",
      "\n",
      "αf x\n",
      "\n",
      "( ) + ( )] = \n",
      "\n",
      "βg x\n",
      "\n",
      "αEx [ ( )] +\n",
      "\n",
      "f x\n",
      "\n",
      "βEx[ ( )]\n",
      "g x ,\n",
      "\n",
      "(3.11)\n",
      "\n",
      "when\n",
      "\n",
      "α\n",
      "\n",
      "and\n",
      "\n",
      "β\n",
      "\n",
      "are not dependent on .\n",
      "x\n",
      "\n",
      "The variance gives a measure of how much the values of a function of a random\n",
      "variable x vary as we sample diﬀerent value of x from its probability distribution:\n",
      "\n",
      "Var( ( )) = \n",
      "\n",
      "f x\n",
      "\n",
      "E( ( )\n",
      "f x − E f x 2.\n",
      "\n",
      "[ ( )])\n",
      "\n",
      "(3.12)\n",
      "\n",
      "When the variance is low, the values of f (x) cluster near their expected value. The\n",
      "square root of the variance is known as the standard deviation.\n",
      "\n",
      "The covariance gives some sense of how much two values are linearly related to\n",
      "\n",
      "each other, as well as the scale of these variables:\n",
      "\n",
      "Cov( ( )\n",
      "\n",
      "f x , g y\n",
      "\n",
      "( )) = \n",
      "\n",
      "E f x − E f x\n",
      "[( ( )\n",
      "\n",
      "[ ( )]) ( ( )\n",
      "\n",
      "g y − E g y\n",
      "\n",
      "[ ( )])]\n",
      "\n",
      ".\n",
      "\n",
      "(3.13)\n",
      "\n",
      "High absolute values of the covariance mean that the values change very much\n",
      "and are both far from their respective means at the same time. If the sign of the\n",
      "covariance is positive, then both variables tend to take on relatively high values\n",
      "simultaneously. If the sign of the covariance is negative, then one variable tends to\n",
      "take on a relatively high value at the times that the other takes on a relatively low\n",
      "value and vice versa. Other measures such as correlation normalize the contribution\n",
      "of each variable in order to measure only how much the variables are related, rather\n",
      "than also being aﬀected by the scale of the separate variables.\n",
      "\n",
      "The notions of covariance and dependence are related, but are in fact distinct\n",
      "concepts. They are related because two variables that are independent have zero\n",
      "covariance, and two variables that have non-zero covariance are dependent. How-\n",
      "ever, independence is a distinct property from covariance. For two variables to have\n",
      "zero covariance, there must be no linear dependence between them. Independence\n",
      "is a stronger requirement than zero covariance, because independence also excludes\n",
      "nonlinear relationships. It is possible for two variables to be dependent but have\n",
      "zero covariance. For example, suppose we ﬁrst sample a real number x from a\n",
      "uniform distribution over the interval [−1 ,1]. We next sample a random variable\n",
      "\n",
      "60\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "s. With probability 1\n",
      "2, we choose the value of s to be 1. Otherwise, we choose\n",
      "the value of s to be − 1. We can then generate a random variable y by assigning\n",
      "y = sx . Clearly, x and y are not independent, because x completely determines\n",
      "the magnitude of\n",
      "\n",
      ". However,\n",
      "\n",
      ".\n",
      ") = 0\n",
      "\n",
      "Cov(\n",
      "\n",
      "x, y\n",
      "\n",
      "y\n",
      "\n",
      "The covariance matrix of a random vector x ∈ Rn is an n n× matrix, such that\n",
      "(3.14)\n",
      "\n",
      "Cov( )x i,j = Cov(xi, xj ).\n",
      "\n",
      "The diagonal elements of the covariance give the variance:\n",
      "\n",
      "Cov(xi , xi) = Var(xi).\n",
      "\n",
      "(3.15)\n",
      "\n",
      "3.9 Common Probability Distributions\n",
      "\n",
      "Several simple probability distributions are useful in many contexts in machine\n",
      "learning.\n",
      "\n",
      "3.9.1 Bernoulli Distribution\n",
      "\n",
      "Bernoulli\n",
      "\n",
      "The\n",
      "distribution is a distribution over a single binary random variable.\n",
      "It is controlled by a single parameter φ ∈ [0, 1], which gives the probability of the\n",
      "random variable being equal to 1. It has the following properties:\n",
      "\n",
      "P\n",
      "\n",
      "x\n",
      "( = 1) = \n",
      "\n",
      "φ\n",
      "\n",
      "P\n",
      "\n",
      "x\n",
      "( = 0) = 1\n",
      ") =  x (1\n",
      "x\n",
      "\n",
      "φ\n",
      "\n",
      "P\n",
      "\n",
      "( = x\n",
      "\n",
      "Ex [ ] = \n",
      "\n",
      "x\n",
      "\n",
      "φ\n",
      "\n",
      "−\n",
      ")− φ 1−x\n",
      "φ\n",
      "\n",
      "Var x( ) =  (1\n",
      "\n",
      "x\n",
      "\n",
      "φ − φ\n",
      "\n",
      ")\n",
      "\n",
      "(3.16)\n",
      "\n",
      "(3.17)\n",
      "\n",
      "(3.18)\n",
      "\n",
      "(3.19)\n",
      "\n",
      "(3.20)\n",
      "\n",
      "3.9.2 Multinoulli Distribution\n",
      "\n",
      "multinoulli\n",
      "\n",
      "The\n",
      "categorical distribution is a distribution over a single discrete\n",
      "variable with k diﬀerent states, where k is ﬁnite1 . The multinoulli distribution is\n",
      "\n",
      "or\n",
      "\n",
      "1 “Multinoulli” is a term that was recently coined by Gustavo Lacerdo and popularized by\n",
      "Murphy (2012). The multinoulli distribution is a special case of the\n",
      "distribution. A\n",
      "multinomial distribution is the distribution over vectors in {0, . . . , n}k representing how many\n",
      "times each of the k categories is visited when n samples are drawn from a multinoulli distribution.\n",
      "Many texts use the term “multinomial” to refer to multinoulli distributions without clarifying\n",
      "that they refer only to the\n",
      "\n",
      "multinomial\n",
      "\n",
      "n = 1\n",
      "\n",
      "case.\n",
      "\n",
      "61\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "parametrized by a vector p ∈ [0, 1]k−1 , where p i gives the probability of the i-th\n",
      "state. The ﬁnal, k-th state’s probability is given by 1− 1 p. Note that we must\n",
      "constrain 1 p ≤ 1. Multinoulli distributions are often used to refer to distributions\n",
      "over categories of objects, so we do not usually assume that state 1 has numerical\n",
      "value 1, etc. For this reason, we do not usually need to compute the expectation\n",
      "or variance of multinoulli-distributed random variables.\n",
      "\n",
      "The Bernoulli and multinoulli distributions are suﬃcient to describe any distri-\n",
      "bution over their domain. This is because they model discrete variables for which\n",
      "it is feasible to simply enumerate all of the states. When dealing with continuous\n",
      "variables, there are uncountably many states, so any distribution described by a\n",
      "small number of parameters must impose strict limits on the distribution.\n",
      "\n",
      "3.9.3 Gaussian Distribution\n",
      "\n",
      "The most commonly used distribution over real numbers is the normal distribution,\n",
      "also known as the Gaussian distribution:\n",
      "\n",
      "N ( ;x µ, σ2) = 1\n",
      "\n",
      "2πσ2 exp−\n",
      "\n",
      "1\n",
      "x\n",
      "2σ2 (\n",
      "\n",
      "µ− 2 .\n",
      "\n",
      ")\n",
      "\n",
      "(3.21)\n",
      "\n",
      "See Fig. 3.1 for a plot of the density function.\n",
      "The two parameters µ ∈ R and σ ∈ (0,∞ ) control the normal distribution.\n",
      "The parameter µ gives the coordinate of the central peak. This is also the mean of\n",
      "the distribution: E[x] = µ. The standard deviation of the distribution is given by\n",
      "σ, and the variance by σ2.\n",
      "\n",
      "When we evaluate the PDF, we need to square and invert σ. When we need to\n",
      "frequently evaluate the PDF with diﬀerent parameter values, a more eﬃcient way\n",
      "of parametrizing the distribution is to use a parameter β ∈ (0 ,∞) to control the\n",
      "precision or inverse variance of the distribution:\n",
      "\n",
      "N ( ;x µ, β−1) = β\n",
      "\n",
      "2π\n",
      "\n",
      "exp−\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "β x\n",
      "\n",
      "( − )2 .\n",
      "\n",
      "µ\n",
      "\n",
      "(3.22)\n",
      "\n",
      "Normal distributions are a sensible choice for many applications. In the absence\n",
      "of prior knowledge about what form a distribution over the real numbers should\n",
      "take, the normal distribution is a good default choice for two major reasons.\n",
      "\n",
      "First, many distributions we wish to model are truly close to being normal\n",
      "distributions. The central limit theorem shows that the sum of many independent\n",
      "random variables is approximately normally distributed. This means that in\n",
      "\n",
      "62\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "The normal distribution\n",
      "\n",
      "Maximum at x ¹=\n",
      "\n",
      "Inflection points at \n",
      "     x ¹ ¾\n",
      "\n",
      "= §\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1.0\n",
      "\n",
      "−0.5\n",
      "\n",
      "0.0\n",
      "\n",
      "0.5\n",
      "\n",
      "1.0\n",
      "\n",
      "1.5\n",
      "\n",
      "2.0\n",
      "\n",
      ")\n",
      "x\n",
      "(\n",
      "p\n",
      "\n",
      "0.40\n",
      "\n",
      "0.35\n",
      "\n",
      "0.30\n",
      "\n",
      "0.25\n",
      "\n",
      "0.20\n",
      "\n",
      "0.15\n",
      "\n",
      "0.10\n",
      "\n",
      "0.05\n",
      "\n",
      "0.00\n",
      "\n",
      "−2.0\n",
      "\n",
      "Figure 3.1: The normal distribution: The normal distribution N (x;µ, σ 2) exhibits a classic\n",
      "“bell curve” shape, with the x coordinate of its central peak given by µ, and the width\n",
      "of its peak controlled by σ. In this example, we depict the standard normal distribution,\n",
      "with\n",
      "\n",
      "σ = 1\n",
      "\n",
      "µ = 0\n",
      "\n",
      "and\n",
      "\n",
      ".\n",
      "\n",
      "x\n",
      "\n",
      "practice, many complicated systems can be modeled successfully as normally\n",
      "distributed noise, even if the system can be decomposed into parts with more\n",
      "structured behavior.\n",
      "\n",
      "Second, out of all possible probability distributions with the same variance,\n",
      "the normal distribution encodes the maximum amount of uncertainty over the\n",
      "real numbers. We can thus think of the normal distribution as being the one that\n",
      "inserts the least amount of prior knowledge into a model. Fully developing and\n",
      "justifying this idea requires more mathematical tools, and is postponed to Sec.\n",
      "19.4.2.\n",
      "\n",
      "The normal distribution generalizes to Rn, in which case it is known as the\n",
      "multivariate normal distribution. It may be parametrized with a positive deﬁnite\n",
      "symmetric matrix\n",
      "\n",
      ":Σ\n",
      "\n",
      "x µ, Σ \n",
      "\n",
      ") =\n",
      "\n",
      "N ( ;\n",
      "\n",
      "1\n",
      "\n",
      "(2 )π ndet(\n",
      "\n",
      ")Σ\n",
      "\n",
      "exp−\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x µ− Σ−1 (\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      "x µ−  .\n",
      "\n",
      ")\n",
      "\n",
      "(3.23)\n",
      "\n",
      "The parameter µ still gives the mean of the distribution, though now it is\n",
      "vector-valued. The parameter Σ gives the covariance matrix of the distribution.\n",
      "As in the univariate case, when we wish to evaluate the PDF several times for\n",
      "\n",
      "63\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "many diﬀerent values of the parameters, the covariance is not a computationally\n",
      "eﬃcient way to parametrize the distribution, since we need to invert Σ to evaluate\n",
      "the PDF. We can instead use a precision matrix β:\n",
      "\n",
      "N ( ;x µ β, −1) =det( )β\n",
      "\n",
      "(2 )π n\n",
      "\n",
      "exp−\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "( − ) .\n",
      "x µ− β x µ\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      "(3.24)\n",
      "\n",
      "We often ﬁx the covariance matrix to be a diagonal matrix. An even simpler\n",
      "version is the isotropic Gaussian distribution, whose covariance matrix is a scalar\n",
      "times the identity matrix.\n",
      "\n",
      "3.9.4 Exponential and Laplace Distributions\n",
      "\n",
      "In the context of deep learning, we often want to have a probability distribution\n",
      "with a sharp point at x = 0. To accomplish this, we can use the exponential\n",
      "distribution:\n",
      "\n",
      "(3.25)\n",
      "The exponential distribution uses the indicator function 1x≥0 to assign probability\n",
      "zero to all negative values of\n",
      "\n",
      "p x λ\n",
      "( ; ) =  1x≥0exp (\n",
      "\n",
      ")−λx .\n",
      "\n",
      ".x\n",
      "\n",
      "λ\n",
      "\n",
      "A closely related probability distribution that allows us to place a sharp peak\n",
      "\n",
      "of probability mass at an arbitrary point\n",
      "\n",
      "µ\n",
      "\n",
      "is the\n",
      "\n",
      "Laplace distribution\n",
      "\n",
      "Laplace( ;\n",
      "\n",
      "x µ, γ\n",
      "\n",
      ") =\n",
      "\n",
      "1\n",
      "2γ\n",
      "\n",
      "exp−| − |\n",
      "γ .\n",
      "\n",
      "µ\n",
      "\n",
      "x\n",
      "\n",
      "(3.26)\n",
      "\n",
      "3.9.5 The Dirac Distribution and Empirical Distribution\n",
      "\n",
      "In some cases, we wish to specify that all of the mass in a probability distribution\n",
      "clusters around a single point. This can be accomplished by deﬁning a PDF using\n",
      "the Dirac delta function,\n",
      "\n",
      "δ x( )\n",
      "\n",
      ":\n",
      "\n",
      "( ) =  ( − )\n",
      "p x\n",
      "µ .\n",
      "\n",
      "δ x\n",
      "\n",
      "(3.27)\n",
      "\n",
      "The Dirac delta function is deﬁned such that it is zero-valued everywhere except\n",
      "0, yet integrates to 1. The Dirac delta function is not an ordinary function that\n",
      "associates each value x with a real-valued output, instead it is a diﬀerent kind of\n",
      "mathematical object called a generalized function that is deﬁned in terms of its\n",
      "properties when integrated. We can think of the Dirac delta function as being the\n",
      "limit point of a series of functions that put less and less mass on all points other\n",
      "than .µ\n",
      "\n",
      "64\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "By deﬁning p(x) to be δ shifted by −µ we obtain an inﬁnitely narrow and\n",
      "\n",
      "inﬁnitely high peak of probability mass where\n",
      "\n",
      "x\n",
      "\n",
      "µ= \n",
      "\n",
      ".\n",
      "\n",
      "A common use of the Dirac delta distribution is as a component of an empirical\n",
      "\n",
      "distribution,\n",
      "\n",
      "ˆp( ) =x\n",
      "\n",
      "1\n",
      "m\n",
      "\n",
      "mi=1\n",
      "\n",
      "δ(x x− ( )i )\n",
      "\n",
      "(3.28)\n",
      "\n",
      "1\n",
      "m on each of the m points x(1), . . . , x (\n",
      "\n",
      ")m forming\n",
      "which puts probability mass\n",
      "a given data set or collection of samples. The Dirac delta distribution is only\n",
      "necessary to deﬁne the empirical distribution over continuous variables. For discrete\n",
      "variables, the situation is simpler: an empirical distribution can be conceptualized\n",
      "as a multinoulli distribution, with a probability associated to each possible input\n",
      "value that is simply equal to the empirical frequency of that value in the training\n",
      "set.\n",
      "\n",
      "We can view the empirical distribution formed from a dataset of training\n",
      "examples as specifying the distribution that we sample from when we train a model\n",
      "on this dataset. Another important perspective on the empirical distribution is\n",
      "that it is the probability density that maximizes the likelihood of the training\n",
      "data (see Sec. 5.5). Many machine learning algorithms can be conﬁgured to have\n",
      "arbitrarily high capacity. If given enough capacity, these algorithms will simply\n",
      "learn the empirical distribution. This is a bad outcome because the model does not\n",
      "generalize at all and assigns inﬁnitesimal probability to any point in space that did\n",
      "not occur in the training set. A central problem in machine learning is studying\n",
      "how to limit the capacity of a model in a way that prevents it from simply learning\n",
      "the empirical distribution while also allowing it to learn complicated functions.\n",
      "\n",
      "3.9.6 Mixtures of Distributions\n",
      "\n",
      "It is also common to deﬁne probability distributions by combining other simpler\n",
      "probability distributions. One common way of combining distributions is to\n",
      "construct a mixture distribution. A mixture distribution is made up of several\n",
      "component distributions. On each trial, the choice of which component distribution\n",
      "generates the sample is determined by sampling a component identity from a\n",
      "multinoulli distribution:\n",
      "\n",
      "P ( ) =x i\n",
      "\n",
      "P\n",
      "\n",
      "c\n",
      "( = \n",
      "\n",
      "i P\n",
      ")\n",
      "\n",
      "x c|\n",
      "(\n",
      "\n",
      "=\n",
      "\n",
      "i\n",
      ")\n",
      "\n",
      "(3.29)\n",
      "\n",
      "where\n",
      "\n",
      "P ( )\n",
      "\n",
      "c is the multinoulli distribution over component identities.\n",
      "\n",
      "65\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "We have already seen one example of a mixture distribution: the empirical\n",
      "distribution over real-valued variables is a mixture distribution with one Dirac\n",
      "component for each training example.\n",
      "\n",
      "The mixture model is one simple strategy for combining probability distributions\n",
      "to create a richer distribution. In Chapter 16, we explore the art of building complex\n",
      "probability distributions from simple ones in more detail.\n",
      "\n",
      "latent variable\n",
      "\n",
      "The mixture model allows us to brieﬂy glimpse a concept that will be of\n",
      "paramount importance later—the\n",
      ". A latent variable is a random\n",
      "variable that we cannot observe directly. The component identity variable c of the\n",
      "mixture model provides an example. Latent variables may be related to x through\n",
      "the joint distribution, in this case, P (x c, ) = P (x c|\n",
      ")P(c). The distribution P (c)\n",
      "over the latent variable and the distribution P (x c| ) relating the latent variables\n",
      "to the visible variables determines the shape of the distribution P ( x) even though\n",
      "it is possible to describe P (x) without reference to the latent variable. Latent\n",
      "variables are discussed further in Sec. 16.5.\n",
      "\n",
      "A very powerful and common type of mixture model is the Gaussian mixture\n",
      "model, in which the components p (x | c = i) are Gaussians. Each component has\n",
      "a separately parametrized mean µ ( )i and covariance Σ ( )i . Some mixtures can have\n",
      "more constraints. For example, the covariances could be shared across components\n",
      "via the constraint Σ( )i = Σ∀i. As with a single Gaussian distribution, the mixture\n",
      "of Gaussians might constrain the covariance matrix for each component to be\n",
      "diagonal or isotropic.\n",
      "\n",
      "In addition to the means and covariances, the parameters of a Gaussian mixture\n",
      "specify the prior probability α i = P (c = i) given to each component i. The word\n",
      "“prior” indicates that it expresses the model’s beliefs about c before it has observed\n",
      "x. By comparison, P(c | x) is a posterior probability, because it is computed after\n",
      "observation of x. A Gaussian mixture model is a universal approximator of\n",
      "densities, in the sense that any smooth density can be approximated with any\n",
      "speciﬁc, non-zero amount of error by a Gaussian mixture model with enough\n",
      "components.\n",
      "\n",
      "Fig. 3.2 shows samples from a Gaussian mixture model.\n",
      "\n",
      "3.10 Useful Properties of Common Functions\n",
      "\n",
      "Certain functions arise often while working with probability distributions, especially\n",
      "the probability distributions used in deep learning models.\n",
      "\n",
      "66\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "2\n",
      "x\n",
      "\n",
      "x1\n",
      "\n",
      "Figure 3.2: Samples from a Gaussian mixture model. In this example, there are three\n",
      "components. From left to right, the ﬁrst component has an isotropic covariance matrix,\n",
      "meaning it has the same amount of variance in each direction. The second has a diagonal\n",
      "covariane matrix, meaning it can control the variance separately along each axis-aligned\n",
      "direction. This example has more variance along the x2 axis than along the x1 axis. The\n",
      "third component has a full-rank covariance matrix, allowing it to control the variance\n",
      "separately along an abitrary basis of directions.\n",
      "\n",
      "One of these functions is the logistic sigmoid:\n",
      "\n",
      "σ x( ) =\n",
      "\n",
      "1\n",
      "\n",
      "1 + exp(\n",
      "\n",
      ".\n",
      ")−x\n",
      "\n",
      "(3.30)\n",
      "\n",
      "The logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli\n",
      "distribution because its range is (0, 1), which lies within the valid range of values\n",
      "for the φ parameter. See Fig. 3.3 for a graph of the sigmoid function. The sigmoid\n",
      "function saturates when its argument is very positive or very negative, meaning\n",
      "that the function becomes very ﬂat and insensitive to small changes in its input.\n",
      "\n",
      "Another commonly encountered function is the\n",
      "\n",
      "softplus\n",
      "\n",
      "function (Dugas\n",
      "\n",
      "et al.,\n",
      "\n",
      "2001):\n",
      "\n",
      "ζ x\n",
      "x .\n",
      "( ) = log (1 + exp( ))\n",
      "\n",
      "(3.31)\n",
      "\n",
      "The softplus function can be useful for producing the β or σ parameter of a normal\n",
      "distribution because its range is (0,∞). It also arises commonly when manipulating\n",
      "expressions involving sigmoids. The name of the softplus function comes from the\n",
      "fact that it is a smoothed or “softened” version of\n",
      "\n",
      "x+ = max(0 ), x .\n",
      "\n",
      "(3.32)\n",
      "\n",
      "See Fig. 3.4 for a graph of the softplus function.\n",
      "\n",
      "67\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "The logistic sigmoid function\n",
      "\n",
      ")\n",
      "x\n",
      "(\n",
      "¾\n",
      "\n",
      "1.0\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.0\n",
      "\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "x\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "Figure 3.3: The logistic sigmoid function.\n",
      "\n",
      "The softplus function\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      ")\n",
      "x\n",
      "(\n",
      "³\n",
      "\n",
      "0\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "x\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "Figure 3.4: The softplus function.\n",
      "\n",
      "68\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "The following properties are all useful enough that you may wish to memorize\n",
      "\n",
      "them:\n",
      "\n",
      "σ x( ) =\n",
      "\n",
      "exp( )x\n",
      "x\n",
      "\n",
      "exp( ) + exp(0)\n",
      "\n",
      "d\n",
      "dx\n",
      "\n",
      "σ x\n",
      "\n",
      "( ) =  (\n",
      "\n",
      "( ) =  ( )(1 − ( ))\n",
      "σ x\n",
      "σ x\n",
      "− σ x\n",
      "1\n",
      "σ x\n",
      "\n",
      "σ −x\n",
      ")\n",
      "−ζ −x\n",
      "(\n",
      "ζ x\n",
      "σ x\n",
      "( ) =  ( )\n",
      "\n",
      "log ( ) = \n",
      "\n",
      ")\n",
      "\n",
      "d\n",
      "dx\n",
      "\n",
      "1)\n",
      "\n",
      "(0,\n",
      "\n",
      ", σ\n",
      "\n",
      " x\n",
      "1 − x\n",
      "∀ ∈x\n",
      "∀x > 0, ζ−1( ) = log (exp( )\n",
      "x −\n",
      "\n",
      "−1 ( ) = log\n",
      "\n",
      "1)\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "ζ x( ) = x\n",
      "\n",
      "−∞\n",
      "x\n",
      "ζ\n",
      "\n",
      "σ y dy\n",
      "\n",
      "( )\n",
      "\n",
      "(3.33)\n",
      "\n",
      "(3.34)\n",
      "\n",
      "(3.35)\n",
      "\n",
      "(3.36)\n",
      "\n",
      "(3.37)\n",
      "\n",
      "(3.38)\n",
      "\n",
      "(3.39)\n",
      "\n",
      "(3.40)\n",
      "\n",
      "(3.41)\n",
      "The function σ−1(x) is called the logit in statistics, but this term is more rarely\n",
      "used in machine learning. The ﬁnal property provides extra justiﬁcation for the\n",
      "name “softplus,” since x+ − x− = x.\n",
      "\n",
      "( ) − (− ) = \n",
      "ζ x\n",
      "x\n",
      "\n",
      "3.11 Bayes’ Rule\n",
      "\n",
      "We often ﬁnd ourselves in a situation where we know P (y x| ) and need to know\n",
      "P (x y|\n",
      "). Fortunately, if we also know P (x), we can compute the desired quantity\n",
      "using Bayes’ rule:\n",
      "\n",
      "P (\n",
      "\n",
      "x y|\n",
      "\n",
      ") =\n",
      "\n",
      "P\n",
      "\n",
      "P( )x\n",
      "\n",
      "y x|\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      ".\n",
      "\n",
      "(3.42)\n",
      "\n",
      "P ( )y\n",
      "\n",
      "P ( ) =y x P\n",
      "\n",
      "(y |\n",
      "\n",
      "Note that while P (y) appears in the formula, it is usually feasible to compute\n",
      "\n",
      "x P x\n",
      "\n",
      "( ), so we do not need to begin with knowledge of\n",
      "\n",
      ")\n",
      "\n",
      "P\n",
      "\n",
      "(y .)\n",
      "\n",
      "Bayes’ rule is straightforward to derive from the deﬁnition of conditional\n",
      "probability, but it is useful to know the name of this formula since many texts\n",
      "refer to it by name. It is named after the Reverend Thomas Bayes, who ﬁrst\n",
      "discovered a special case of the formula. The general version presented here was\n",
      "independently discovered by Pierre-Simon Laplace.\n",
      "\n",
      "69\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "3.12 Technical Details of Continuous Variables\n",
      "\n",
      "A proper formal understanding of continuous random variables and probability\n",
      "density functions requires developing probability theory in terms of a branch of\n",
      "mathematics known as measure theory. Measure theory is beyond the scope of\n",
      "this textbook, but we can brieﬂy sketch some of the issues that measure theory is\n",
      "employed to resolve.\n",
      "\n",
      "In Sec. 3.3.2, we saw that the probability of a continuous vector-valued x lying\n",
      "in some set S is given by the integral of p(x ) over the set S. Some choices of set S\n",
      "can produce paradoxes. For example, it is possible to construct two sets S1 and\n",
      "S2 such that p(x ∈ S1) + p(x ∈ S 2) > 1 but S1 ∩ S2 = ∅. These sets are generally\n",
      "constructed making very heavy use of the inﬁnite precision of real numbers, for\n",
      "example by making fractal-shaped sets or sets that are deﬁned by transforming\n",
      "the set of rational numbers2 . One of the key contributions of measure theory is to\n",
      "provide a characterization of the set of sets that we can compute the probability\n",
      "of without encountering paradoxes. In this book, we only integrate over sets with\n",
      "relatively simple descriptions, so this aspect of measure theory never becomes a\n",
      "relevant concern.\n",
      "\n",
      "For our purposes, measure theory is more useful for describing theorems that\n",
      "apply to most points in Rn but do not apply to some corner cases. Measure theory\n",
      "provides a rigorous way of describing that a set of points is negligibly small. Such\n",
      "a set is said to have “measure zero.” We do not formally deﬁne this concept in this\n",
      "textbook. However, it is useful to understand the intuition that a set of measure\n",
      "zero occupies no volume in the space we are measuring. For example, within R2 , a\n",
      "line has measure zero, while a ﬁlled polygon has positive measure. Likewise, an\n",
      "individual point has measure zero. Any union of countably many sets that each\n",
      "have measure zero also has measure zero (so the set of all the rational numbers\n",
      "has measure zero, for instance).\n",
      "\n",
      "Another useful term from measure theory is “almost everywhere.” A property\n",
      "that holds almost everywhere holds throughout all of space except for on a set of\n",
      "measure zero. Because the exceptions occupy a negligible amount of space, they\n",
      "can be safely ignored for many applications. Some important results in probability\n",
      "theory hold for all discrete values but only hold “almost everywhere” for continuous\n",
      "values.\n",
      "\n",
      "Another technical detail of continuous variables relates to handling continuous\n",
      "random variables that are deterministic functions of one another. Suppose we have\n",
      "two random variables, x and y, such that y = g(x), where g is an invertible, con-\n",
      "\n",
      "2The Banach-Tarski theorem provides a fun example of such sets.\n",
      "\n",
      "70\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "tinuous, diﬀerentiable transformation. One might expect that py(y ) =p x(g−1(y )).\n",
      "This is actually not the case.\n",
      "\n",
      "As a simple example, suppose we have scalar random variables x and y. Suppose\n",
      "If we use the rule p y(y) = p x(2 y) then py will be 0\n",
      "on this interval. This means\n",
      "\n",
      "y = x\n",
      "everywhere except the interval [0 , 1\n",
      "2 ]\n",
      "\n",
      "2 and x ∼ U(0,1).\n",
      "\n",
      ", and it will be\n",
      "\n",
      "1\n",
      "\n",
      " py ( ) =\n",
      "\n",
      "y dy\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      ",\n",
      "\n",
      "(3.43)\n",
      "\n",
      "which violates the deﬁnition of a probability distribution.\n",
      "\n",
      "This common mistake is wrong because it fails to account for the distortion\n",
      "of space introduced by the function g. Recall that the probability of x lying in\n",
      "an inﬁnitesimally small region with volume δx is given by p( x)δx. Since g can\n",
      "expand or contract space, the inﬁnitesimal volume surrounding x in x space may\n",
      "have diﬀerent volume in\n",
      "\n",
      "space.\n",
      "\n",
      "y\n",
      "\n",
      "To see how to correct the problem, we return to the scalar case. We need to\n",
      "\n",
      "preserve the property\n",
      "\n",
      "Solving from this, we obtain\n",
      "\n",
      "|py( ( ))\n",
      "\n",
      "g x dy|\n",
      "\n",
      "=\n",
      "\n",
      "|p x( )x dx .|\n",
      "\n",
      "or equivalently\n",
      "\n",
      "In higher dimensions, the derivative generalizes to the determinant of the Jacobian\n",
      "matrix—the matrix with J i,j = ∂xi\n",
      "∂yj\n",
      "\n",
      ". Thus, for real-valued vectors\n",
      "\n",
      "and ,\n",
      "y\n",
      "\n",
      "x\n",
      "\n",
      "py( ) = \n",
      "\n",
      "y\n",
      "\n",
      "px ( ) = \n",
      "\n",
      "x\n",
      "\n",
      "∂x\n",
      "\n",
      "∂g x( )\n",
      "\n",
      "py( ( ))\n",
      "\n",
      "px (g−1( ))y\n",
      "\n",
      "\n",
      "∂y\n",
      "g x \n",
      "∂x  .\n",
      "g x det∂g( )x\n",
      "∂x  .\n",
      "\n",
      "px ( ) = \n",
      "\n",
      "x\n",
      "\n",
      "p y( ( ))\n",
      "\n",
      "3.13 Information Theory\n",
      "\n",
      "Information theory is a branch of applied mathematics that revolves around\n",
      "quantifying how much information is present in a signal. It was originally invented\n",
      "to study sending messages from discrete alphabets over a noisy channel, such as\n",
      "communication via radio transmission. In this context, information theory tells how\n",
      "to design optimal codes and calculate the expected length of messages sampled from\n",
      "\n",
      "71\n",
      "\n",
      "(3.44)\n",
      "\n",
      "(3.45)\n",
      "\n",
      "(3.46)\n",
      "\n",
      "(3.47)\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "speciﬁc probability distributions using various encoding schemes. In the context of\n",
      "machine learning, we can also apply information theory to continuous variables\n",
      "where some of these message length interpretations do not apply. This ﬁeld is\n",
      "fundamental to many areas of electrical engineering and computer science. In this\n",
      "textbook, we mostly use a few key ideas from information theory to characterize\n",
      "probability distributions or quantify similarity between probability distributions.\n",
      "For more detail on information theory, see Cover and Thomas (2006) or MacKay\n",
      "(2003).\n",
      "\n",
      "The basic intuition behind information theory is that learning that an unlikely\n",
      "event has occurred is more informative than learning that a likely event has\n",
      "occurred. A message saying “the sun rose this morning” is so uninformative as\n",
      "to be unnecessary to send, but a message saying “there was a solar eclipse this\n",
      "morning” is very informative.\n",
      "\n",
      "We would like to quantify information in a way that formalizes this intuition.\n",
      "\n",
      "Speciﬁcally,\n",
      "\n",
      "• Likely events should have low information content, and in the extreme case,\n",
      "events that are guaranteed to happen should have no information content\n",
      "whatsoever.\n",
      "\n",
      "• Less likely events should have higher information content.\n",
      "• Independent events should have additive information. For example, ﬁnding\n",
      "out that a tossed coin has come up as heads twice should convey twice as\n",
      "much information as ﬁnding out that a tossed coin has come up as heads\n",
      "once.\n",
      "\n",
      "In order to satisfy all three of these properties, we deﬁne the self-information\n",
      "\n",
      "of an event x\n",
      "\n",
      "= x\n",
      "\n",
      "to be\n",
      "\n",
      "I x\n",
      "( ) = \n",
      "\n",
      "log−\n",
      "\n",
      "P x .\n",
      "( )\n",
      "\n",
      "(3.48)\n",
      "\n",
      "In this book, we always use log to mean the natural logarithm, with base e. Our\n",
      "deﬁnition of I(x) is therefore written in units of\n",
      ". One nat is the amount of\n",
      "information gained by observing an event of probability 1\n",
      "e . Other texts use base-2\n",
      "logarithms and units called\n",
      "shannons\n",
      "; information measured in bits is just\n",
      "a rescaling of information measured in nats.\n",
      "\n",
      "nats\n",
      "\n",
      "bits\n",
      "\n",
      "or\n",
      "\n",
      "When x is continuous, we use the same deﬁnition of information by analogy,\n",
      "but some of the properties from the discrete case are lost. For example, an event\n",
      "with unit density still has zero information, despite not being an event that is\n",
      "guaranteed to occur.\n",
      "\n",
      "72\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      " \n",
      "n\n",
      "\n",
      "i\n",
      " \n",
      "y\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "n\n",
      "e\n",
      " \n",
      "n\n",
      "o\n",
      "n\n",
      "n\n",
      "a\n",
      "h\n",
      "S\n",
      "\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "\n",
      "Shannon entropy of a binary random variable\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1.0\n",
      "\n",
      "p\n",
      "\n",
      "1\n",
      "\n",
      "Figure 3.5: This plot shows how distributions that are closer to deterministic have low\n",
      "Shannon entropy while distributions that are close to uniform have high Shannon entropy.\n",
      "On the horizontal axis, we plot p, the probability of a binary random variable being equal\n",
      "to . The entropy is given by\n",
      "log . When p is near 0, the distribution\n",
      "is nearly deterministic, because the random variable is nearly always 0. When p is near 1,\n",
      "the distribution is nearly deterministic, because the random variable is nearly always 1.\n",
      "When p = 0 .5, the entropy is maximal, because the distribution is uniform over the two\n",
      "outcomes.\n",
      "\n",
      "(p− 1) log(1− p )− p\n",
      "\n",
      "p\n",
      "\n",
      "Self-information deals only with a single outcome. We can quantify the amount\n",
      "\n",
      "of uncertainty in an entire probability distribution using the Shannon entropy:\n",
      "\n",
      "H( ) = \n",
      "\n",
      "x\n",
      "\n",
      "E\n",
      "\n",
      "I x\n",
      "\n",
      "x∼P[ ( )] = \n",
      "\n",
      "−E\n",
      "\n",
      "P x .\n",
      "x∼P[log ( )]\n",
      "\n",
      "(3.49)\n",
      "\n",
      "also denoted H( P). In other words, the Shannon entropy of a distribution is the\n",
      "expected amount of information in an event drawn from that distribution. It gives\n",
      "a lower bound on the number of bits (if the logarithm is base 2, otherwise the units\n",
      "are diﬀerent) needed on average to encode symbols drawn from a distribution P.\n",
      "Distributions that are nearly deterministic (where the outcome is nearly certain)\n",
      "have low entropy; distributions that are closer to uniform have high entropy. See\n",
      "Fig. 3.5 for a demonstration. When x is continous, the Shannon entropy is known\n",
      "as the diﬀerential entropy.\n",
      "\n",
      "If we have two separate probability distributions P(x) and Q(x) over the same\n",
      "random variable x, we can measure how diﬀerent these two distributions are using\n",
      "the Kullback-Leibler (KL) divergence:\n",
      "\n",
      "DKL(\n",
      "\n",
      "P Q\n",
      "\n",
      ") = \n",
      "\n",
      "E x∼Plog\n",
      "\n",
      "P x( )\n",
      "\n",
      "Q x( ) = Ex∼P [log ( )\n",
      "\n",
      "P x −\n",
      "\n",
      "73\n",
      "\n",
      "log ( )]\n",
      "Q x .\n",
      "\n",
      "(3.50)\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "In the case of discrete variables, it is the extra amount of information (measured\n",
      "in bits if we use the base\n",
      "logarithm, but in machine learning we usually use nats\n",
      "and the natural logarithm) needed to send a message containing symbols drawn\n",
      "from probability distribution P , when we use a code that was designed to minimize\n",
      "the length of messages drawn from probability distribution\n",
      "\n",
      ".Q\n",
      "\n",
      "2\n",
      "\n",
      "The KL divergence has many useful properties, most notably that it is non-\n",
      "negative. The KL divergence is 0 if and only if P and Qare the same distribution in\n",
      "the case of discrete variables, or equal “almost everywhere” in the case of continuous\n",
      "variables. Because the KL divergence is non-negative and measures the diﬀerence\n",
      "between two distributions, it is often conceptualized as measuring some sort of\n",
      "distance between these distributions. However, it is not a true distance measure\n",
      "because it is not symmetric: DKL(P Q ) = DKL( Q P ) for some P and Q. This\n",
      "asymmetry means that there are important consequences to the choice of whether\n",
      "to use DKL(\n",
      "\n",
      ". See Fig. 3.6 for more detail.\n",
      "\n",
      "or DKL (\n",
      "\n",
      ")\n",
      "\n",
      "P Q\n",
      "\n",
      ")Q P\n",
      "\n",
      "A quantity that is closely related to the KL divergence is the cross-entropy\n",
      "H (P, Q ) = H (P) + DKL(P Q ), which is similar to the KL divergence but lacking\n",
      "the term on the left:\n",
      "(3.51)\n",
      "\n",
      "H P, Q(\n",
      "\n",
      ") = −E\n",
      "\n",
      "x∼P log ( )Q x .\n",
      "\n",
      "Minimizing the cross-entropy with respect to Q is equivalent to minimizing the\n",
      "KL divergence, because\n",
      "\n",
      "does not participate in the omitted term.\n",
      "\n",
      "Q\n",
      "\n",
      "When computing many of these quantities, it is common to encounter expres-\n",
      "sions of the form 0log 0. By convention, in the context of information theory, we\n",
      "treat these expressions as limx→0 x\n",
      "\n",
      "log = 0.\n",
      "\n",
      "x\n",
      "\n",
      "3.14 Structured Probabilistic Models\n",
      "\n",
      "Machine learning algorithms often involve probability distributions over a very\n",
      "large number of random variables. Often, these probability distributions involve\n",
      "direct interactions between relatively few variables. Using a single function to\n",
      "describe the entire joint probability distribution can be very ineﬃcient (both\n",
      "computationally and statistically).\n",
      "\n",
      "Instead of using a single function to represent a probability distribution, we\n",
      "can split a probability distribution into many factors that we multiply together.\n",
      "For example, suppose we have three random variables: a, b and c . Suppose that\n",
      "a inﬂuences the value of b and b inﬂuences the value of c, but that a and c are\n",
      "independent given b. We can represent the probability distribution over all three\n",
      "\n",
      "74\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "q∗ = argminq DKL(\n",
      "\n",
      ")p q\n",
      "\n",
      "q ∗ = argminq DKL (\n",
      "\n",
      "q p\n",
      "\n",
      ")\n",
      "\n",
      "y\n",
      "t\n",
      "i\n",
      "s\n",
      "n\n",
      "e\n",
      "D\n",
      "y\n",
      "t\n",
      "i\n",
      "l\n",
      "i\n",
      "\n",
      "b\n",
      "a\n",
      "b\n",
      "o\n",
      "r\n",
      "P\n",
      "\n",
      "p x( )\n",
      "q∗( )x\n",
      "\n",
      "y\n",
      "t\n",
      "i\n",
      "s\n",
      "n\n",
      "e\n",
      "D\n",
      "y\n",
      "t\n",
      "i\n",
      "l\n",
      "i\n",
      "\n",
      "b\n",
      "a\n",
      "b\n",
      "o\n",
      "r\n",
      "P\n",
      "\n",
      "p( )x\n",
      "q ∗( )x\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "Figure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x ) and\n",
      "wish to approximate it with another distribution q(x). We have the choice of minimizing\n",
      "either DKL( p q ) or DKL( q p ). We illustrate the eﬀect of this choice using a mixture of\n",
      "two Gaussians for p, and a single Gaussian for q. The choice of which direction of the\n",
      "KL divergence to use is problem-dependent. Some applications require an approximation\n",
      "that usually places high probability anywhere that the true distribution places high\n",
      "probability, while other applications require an approximation that rarely places high\n",
      "probability anywhere that the true distribution places low probability. The choice of the\n",
      "direction of the KL divergence reﬂects which of these considerations takes priority for each\n",
      "application. (Left) The eﬀect of minimizing DKL (p q ). In this case, we select a q that has\n",
      "high probability where p has high probability. When p has multiple modes, q chooses to\n",
      "blur the modes together, in order to put high probability mass on all of them. (Right) The\n",
      "eﬀect of minimizing DKL (q p ). In this case, we select a q that has low probability where\n",
      "p has low probability. When p has multiple modes that are suﬃciently widely separated,\n",
      "as in this ﬁgure, the KL divergence is minimized by choosing a single mode, in order to\n",
      "avoid putting probability mass in the low-probability areas between modes of p . Here, we\n",
      "illustrate the outcome when q is chosen to emphasize the left mode. We could also have\n",
      "achieved an equal value of the KL divergence by choosing the right mode. If the modes\n",
      "are not separated by a suﬃciently strong low probability region, then this direction of the\n",
      "KL divergence can still choose to blur the modes.\n",
      "\n",
      "75\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "variables as a product of probability distributions over two variables:\n",
      "\n",
      "p ,\n",
      "\n",
      "(a b c) =  ( )a (\n",
      "p\n",
      "\n",
      "p\n",
      "\n",
      ",\n",
      "\n",
      "b a|\n",
      "\n",
      ")\n",
      "\n",
      "c b|\n",
      "(\n",
      "p\n",
      "\n",
      ")\n",
      "\n",
      ".\n",
      "\n",
      "(3.52)\n",
      "\n",
      "These factorizations can greatly reduce the number of parameters needed\n",
      "to describe the distribution. Each factor uses a number of parameters that is\n",
      "exponential in the number of variables in the factor. This means that we can greatly\n",
      "reduce the cost of representing a distribution if we are able to ﬁnd a factorization\n",
      "into distributions over fewer variables.\n",
      "\n",
      "We can describe these kinds of factorizations using graphs. Here we use the\n",
      "word “graph” in the sense of graph theory: a set of vertices that may be connected\n",
      "to each other with edges. When we represent the factorization of a probability\n",
      "distribution with a graph, we call it a structured probabilistic model\n",
      "graphical\n",
      "model.\n",
      "\n",
      "or\n",
      "\n",
      "There are two main kinds of structured probabilistic models: directed and\n",
      "undirected. Both kinds of graphical models use a graph G in which each node\n",
      "in the graph corresponds to a random variable, and an edge connecting two\n",
      "random variables means that the probability distribution is able to represent direct\n",
      "interactions between those two random variables.\n",
      "\n",
      "Directed models use graphs with directed edges, and they represent factoriza-\n",
      "tions into conditional probability distributions, as in the example above. Speciﬁcally,\n",
      "a directed model contains one factor for every random variable x i in the distribution,\n",
      "and that factor consists of the conditional distribution over xi given the parents of\n",
      "xi, denoted P a G(x i):\n",
      "\n",
      "(3.53)\n",
      "\n",
      "p( ) =x i\n",
      "\n",
      "p (xi | P aG (xi )) .\n",
      "\n",
      "See Fig. 3.7 for an example of a directed graph and the factorization of probability\n",
      "distributions it represents.\n",
      "\n",
      "Undirected models use graphs with undirected edges, and they represent fac-\n",
      "torizations into a set of functions; unlike in the directed case, these functions are\n",
      "usually not probability distributions of any kind. Any set of nodes that are all\n",
      "connected to each other in G is called a clique. Each clique C ( )i\n",
      "in an undirected\n",
      "model is associated with a factor φ( )i (C ( )i ). These factors are just functions, not\n",
      "probability distributions. The output of each factor must be non-negative, but\n",
      "there is no constraint that the factor must sum or integrate to 1 like a probability\n",
      "distribution.\n",
      "\n",
      "The probability of a conﬁguration of random variables is proportional to the\n",
      "product of all of these factors—assignments that result in larger factor values are\n",
      "\n",
      "76\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "aa\n",
      "\n",
      "bb\n",
      "\n",
      "dd\n",
      "\n",
      "cc\n",
      "\n",
      "ee\n",
      "\n",
      "Figure 3.7: A directed graphical model over random variables a , b, c, d and e. This graph\n",
      "corresponds to probability distributions that can be factored as\n",
      "\n",
      "(a b c d e) =  ( )a (\n",
      "p ,\n",
      "p\n",
      "\n",
      "p\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "b a|\n",
      "\n",
      ")\n",
      "\n",
      "(c a|\n",
      "p\n",
      "\n",
      ",\n",
      "\n",
      "b) (\n",
      "p\n",
      "\n",
      "d b|\n",
      "\n",
      ")\n",
      "\n",
      "e c|\n",
      "(\n",
      "p\n",
      "\n",
      ")\n",
      "\n",
      ".\n",
      "\n",
      "(3.54)\n",
      "\n",
      "This graph allows us to quickly see some properties of the distribution. For example, a\n",
      "and c interact directly, but a and e interact only indirectly via c.\n",
      "\n",
      "more likely. Of course, there is no guarantee that this product will sum to 1. We\n",
      "therefore divide by a normalizing constant Z, deﬁned to be the sum or integral\n",
      "over all states of the product of the φ functions, in order to obtain a normalized\n",
      "probability distribution:\n",
      "\n",
      "p( ) =x\n",
      "\n",
      "1\n",
      "\n",
      "Z i\n",
      "\n",
      "φ( )i C( )i .\n",
      "\n",
      "(3.55)\n",
      "\n",
      "See Fig. 3.8 for an example of an undirected graph and the factorization of\n",
      "probability distributions it represents.\n",
      "\n",
      "Keep in mind that these graphical representations of factorizations are a\n",
      "language for describing probability distributions. They are not mutually exclusive\n",
      "families of probability distributions. Being directed or undirected is not a property\n",
      "of a probability distribution; it is a property of a particular\n",
      "of a\n",
      "probability distribution, but any probability distribution may be described in both\n",
      "ways.\n",
      "\n",
      "description\n",
      "\n",
      "Throughout Part I and Part II of this book, we will use structured probabilistic\n",
      "models merely as a language to describe which direct probabilistic relationships\n",
      "diﬀerent machine learning algorithms choose to represent. No further understanding\n",
      "of structured probabilistic models is needed until the discussion of research topics,\n",
      "in Part III, where we will explore structured probabilistic models in much greater\n",
      "detail.\n",
      "\n",
      "77\n",
      "\n",
      "\f",
      "CHAPTER 3. PROBABILITY AND INFORMATION THEORY\n",
      "\n",
      "aa\n",
      "\n",
      "bb\n",
      "\n",
      "dd\n",
      "\n",
      "cc\n",
      "\n",
      "ee\n",
      "\n",
      "Figure 3.8: An undirected graphical model over random variables a, b, c, d and e . This\n",
      "graph corresponds to probability distributions that can be factored as\n",
      "\n",
      "(a b c d e) =\n",
      "p ,\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "1\n",
      "Z\n",
      "\n",
      "φ (1)(\n",
      "\n",
      "a b c\n",
      ",\n",
      "\n",
      ", φ (2) (\n",
      "\n",
      ")\n",
      "\n",
      ")b d, φ(3)(\n",
      "\n",
      ")c e,\n",
      ".\n",
      "\n",
      "(3.56)\n",
      "\n",
      "This graph allows us to quickly see some properties of the distribution. For example, a\n",
      "and c interact directly, but a and e interact only indirectly via c.\n",
      "\n",
      "This chapter has reviewed the basic concepts of probability theory that are\n",
      "most relevant to deep learning. One more set of fundamental mathematical tools\n",
      "remains: numerical methods.\n",
      "\n",
      "78\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "cry = str_\n",
    "print(cry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\yaniv\\\\Downloads\\\\deeplearningbook-prob.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pypdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ed07d9f3ea64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpdfobject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpypdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdfobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pypdf' is not defined"
     ]
    }
   ],
   "source": [
    "pdfobject=open(path,'rb')\n",
    "pdf=pypdf.PdfFileReader(pdfobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d7fc26c9707a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetXmpMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "print(pdf.getPage(1).getXmpMetadata())\n",
    "print(pdf.getPage(1).getContents())\n",
    "print(pdf.getPage(1).extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getPage(3).extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Filter': '/FlateDecode'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.getPage(1).getContents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_addTransformationMatrix',\n",
       " '_contentStreamRename',\n",
       " '_mergePage',\n",
       " '_mergeResources',\n",
       " '_pushPopGS',\n",
       " '_rotate',\n",
       " 'addTransformation',\n",
       " 'artBox',\n",
       " 'bleedBox',\n",
       " 'clear',\n",
       " 'compressContentStreams',\n",
       " 'copy',\n",
       " 'createBlankPage',\n",
       " 'cropBox',\n",
       " 'extractText',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'getContents',\n",
       " 'getObject',\n",
       " 'getXmpMetadata',\n",
       " 'indirectRef',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'mediaBox',\n",
       " 'mergePage',\n",
       " 'mergeRotatedPage',\n",
       " 'mergeRotatedScaledPage',\n",
       " 'mergeRotatedScaledTranslatedPage',\n",
       " 'mergeRotatedTranslatedPage',\n",
       " 'mergeScaledPage',\n",
       " 'mergeScaledTranslatedPage',\n",
       " 'mergeTransformedPage',\n",
       " 'mergeTranslatedPage',\n",
       " 'pdf',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'raw_get',\n",
       " 'readFromStream',\n",
       " 'rotateClockwise',\n",
       " 'rotateCounterClockwise',\n",
       " 'scale',\n",
       " 'scaleBy',\n",
       " 'scaleTo',\n",
       " 'setdefault',\n",
       " 'trimBox',\n",
       " 'update',\n",
       " 'values',\n",
       " 'writeToStream',\n",
       " 'xmpMetadata']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pdf.getPage(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_match  = '\\n\\n3.1 Why Probability?\\n\\n'\n",
    "str_to_match = '\\n\\n3.1 why probability\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 3\\n\\nProbability and Information\\nTheory\\n\\nIn this chapter, we describe probability theory and information theory.\\n\\nProbability theory is a mathematical framework for representing uncertain\\nstatements. It provides a means of quantifying uncertainty and axioms for deriving\\nnew uncertain statements. In artiﬁcial intelligence applications, we use probability\\ntheory in two major ways. First, the laws of probability tell us how AI systems\\nshould reason, so we design our algorithms to compute or approximate various\\nexpressions derived using probability theory. Second, we can use probability and\\nstatistics to theoretically analyze the behavior of proposed AI systems.\\n\\nProbability theory is a fundamental tool of many disciplines of science and\\nengineering. We provide this chapter to ensure that readers whose background is\\nprimarily in software engineering with limited exposure to probability theory can\\nunderstand the material in this book.\\n\\nWhile probability theory allows us to make uncertain statements and reason\\nin the presence of uncertainty, information allows us to quantify the amount of\\nuncertainty in a probability distribution.\\n\\nIf you are already familiar with probability theory and information theory,\\nyou may wish to skip all of this chapter except for Sec. 3.14, which describes the\\ngraphs we use to describe structured probabilistic models for machine learning. If\\nyou have absolutely no prior experience with these subjects, this chapter should\\nbe suﬃcient to successfully carry out deep learning research projects, but we do\\nsuggest that you consult an additional resource, such as Jaynes (2003).\\n\\n52\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.1 Why Probability?\\n\\nMany branches of computer science deal mostly with entities that are entirely\\ndeterministic and certain. A programmer can usually safely assume that a CPU will\\nexecute each machine instruction ﬂawlessly. Errors in hardware do occur, but are\\nrare enough that most software applications do not need to be designed to account\\nfor them. Given that many computer scientists and software engineers work in a\\nrelatively clean and certain environment, it can be surprising that machine learning\\nmakes heavy use of probability theory.\\n\\nThis is because machine learning must always deal with uncertain quantities,\\nand sometimes may also need to deal with stochastic (non-deterministic) quantities.\\nUncertainty and stochasticity can arise from many sources. Researchers have made\\ncompelling arguments for quantifying uncertainty using probability since at least\\nthe 1980s. Many of the arguments presented here are summarized from or inspired\\nby Pearl (1988).\\n\\nNearly all activities require some ability to reason in the presence of uncertainty.\\nIn fact, beyond mathematical statements that are true by deﬁnition, it is diﬃcult\\nto think of any proposition that is absolutely true or any event that is absolutely\\nguaranteed to occur.\\n\\nThere are three possible sources of uncertainty:\\n\\n1. Inherent stochasticity in the system being modeled. For example, most\\ninterpretations of quantum mechanics describe the dynamics of subatomic\\nparticles as being probabilistic. We can also create theoretical scenarios that\\nwe postulate to have random dynamics, such as a hypothetical card game\\nwhere we assume that the cards are truly shuﬄed into a random order.\\n\\n2. Incomplete observability. Even deterministic systems can appear stochastic\\nwhen we cannot observe all of the variables that drive the behavior of the\\nsystem. For example, in the Monty Hall problem, a game show contestant is\\nasked to choose between three doors and wins a prize held behind the chosen\\ndoor. Two doors lead to a goat while a third leads to a car. The outcome\\ngiven the contestant’s choice is deterministic, but from the contestant’s point\\nof view, the outcome is uncertain.\\n\\n3. Incomplete modeling. When we use a model that must discard some of\\nthe information we have observed, the discarded information results in\\nuncertainty in the model’s predictions. For example, suppose we build a\\nrobot that can exactly observe the location of every object around it. If the\\n\\n53\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nrobot discretizes space when predicting the future location of these objects,\\nthen the discretization makes the robot immediately become uncertain about\\nthe precise position of objects: each object could be anywhere within the\\ndiscrete cell that it was observed to occupy.\\n\\nIn many cases, it is more practical to use a simple but uncertain rule rather\\nthan a complex but certain one, even if the true rule is deterministic and our\\nmodeling system has the ﬁdelity to accommodate a complex rule. For example, the\\nsimple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a rule\\nof the form, “Birds ﬂy, except for very young birds that have not yet learned to\\nﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds\\nincluding the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain and\\ncommunicate, and after all of this eﬀort is still very brittle and prone to failure.\\n\\nGiven that we need a means of representing and reasoning about uncertainty,\\nit is not immediately obvious that probability theory can provide all of the tools\\nwe want for artiﬁcial intelligence applications. Probability theory was originally\\ndeveloped to analyze the frequencies of events. It is easy to see how probability\\ntheory can be used to study events like drawing a certain hand of cards in a\\ngame of poker. These kinds of events are often repeatable. When we say that\\nan outcome has a probability p of occurring, it means that if we repeated the\\nexperiment (e.g., draw a hand of cards) inﬁnitely many times, then proportion p\\nof the repetitions would result in that outcome. This kind of reasoning does not\\nseem immediately applicable to propositions that are not repeatable. If a doctor\\nanalyzes a patient and says that the patient has a 40% chance of having the ﬂu,\\nthis means something very diﬀerent—we can not make inﬁnitely many replicas of\\nthe patient, nor is there any reason to believe that diﬀerent replicas of the patient\\nwould present with the same symptoms yet have varying underlying conditions. In\\nthe case of the doctor diagnosing the patient, we use probability to represent a\\ndegree of belief, with 1 indicating absolute certainty that the patient has the ﬂu\\nand 0 indicating absolute certainty that the patient does not have the ﬂu. The\\nformer kind of probability, related directly to the rates at which events occur, is\\nknown as frequentist probability, while the latter, related to qualitative levels of\\ncertainty, is known as Bayesian probability.\\n\\nIf we list several properties that we expect common sense reasoning about\\nuncertainty to have, then the only way to satisfy those properties is to treat\\nBayesian probabilities as behaving exactly the same as frequentist probabilities.\\nFor example, if we want to compute the probability that a player will win a poker\\ngame given that she has a certain set of cards, we use exactly the same formulas\\nas when we compute the probability that a patient has a disease given that she\\n\\n54\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nhas certain symptoms. For more details about why a small set of common sense\\nassumptions implies that the same axioms must control both kinds of probability,\\nsee Ramsey (1926).\\n\\nProbability can be seen as the extension of logic to deal with uncertainty. Logic\\nprovides a set of formal rules for determining what propositions are implied to\\nbe true or false given the assumption that some other set of propositions is true\\nor false. Probability theory provides a set of formal rules for determining the\\nlikelihood of a proposition being true given the likelihood of other propositions.\\n\\n3.2 Random Variables\\n\\nA random variable is a variable that can take on diﬀerent values randomly. We\\ntypically denote the random variable itself with a lower case letter in plain typeface,\\nand the values it can take on with lower case script letters. For example, x1 and x2\\nare both possible values that the random variable x can take on. For vector-valued\\nvariables, we would write the random variable as x and one of its values as x. On\\nits own, a random variable is just a description of the states that are possible; it\\nmust be coupled with a probability distribution that speciﬁes how likely each of\\nthese states are.\\n\\nRandom variables may be discrete or continuous. A discrete random variable\\nis one that has a ﬁnite or countably inﬁnite number of states. Note that these\\nstates are not necessarily the integers; they can also just be named states that\\nare not considered to have any numerical value. A continuous random variable is\\nassociated with a real value.\\n\\n3.3 Probability Distributions\\n\\nA probability distribution is a description of how likely a random variable or\\nset of random variables is to take on each of its possible states. The way we\\ndescribe probability distributions depends on whether the variables are discrete or\\ncontinuous.\\n\\n3.3.1 Discrete Variables and Probability Mass Functions\\n\\nA probability distribution over discrete variables may be described using a proba-\\nbility mass function (PMF). We typically denote probability mass functions with a\\ncapital P . Often we associate each random variable with a diﬀerent probability\\n\\n55\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nmass function and the reader must infer which probability mass function to use\\nbased on the identity of the random variable, rather than the name of the function;\\nP\\n\\n( )x is usually not the same as\\n\\n( )y .\\n\\nP\\n\\nThe probability mass function maps from a state of a random variable to\\nthe probability of that random variable taking on that state. The probability\\nthat x = x is denoted as P (x), with a probability of 1 indicating that x = x is\\ncertain and a probability of 0 indicating that x = x is impossible. Sometimes\\nto disambiguate which PMF to use, we write the name of the random variable\\nexplicitly: P (x = x). Sometimes we deﬁne a variable ﬁrst, then use ∼ notation to\\nspecify which distribution it follows later: x ∼ P (x .)\\n\\nProbability mass functions can act on many variables at the same time. Such\\na probability distribution over many variables is known as a joint probability\\ndistribution. P (x = x, y = y) denotes the probability that x = x and y = y\\nsimultaneously. We may also write\\n\\nfor brevity.\\n\\nP x, y\\n\\n(\\n\\n)\\n\\nTo be a probability mass function on a random variable x, a function P must\\n\\nsatisfy the following properties:\\n\\nP\\n\\n• The domain of must be the set of all possible states of x.\\n• ∀x\\n0 \\n\\n,∈ x 0 ≤ P (x) ≤ 1. An impossible event has probability and no state can\\nbe less probable than that. Likewise, an event that is guaranteed to happen\\nhas probability , and no state can have a greater chance of occurring.\\n\\n1\\n\\n• \\ue050x∈x P (x) = 1. We refer to this property as being normalized. Without this\\n\\nproperty, we could obtain probabilities greater than one by computing the\\nprobability of one of many events occurring.\\n\\nFor example, consider a single discrete random variable x with k diﬀerent states.\\nx—that is, make each of its states equally\\n\\nuniform distribution\\n\\nWe can place a\\nlikely—by setting its probability mass function to\\n\\non\\n\\nP\\n\\n( = x\\n\\nx\\ni) =\\n\\n1\\nk\\n\\n(3.1)\\n\\nfor all i. We can see that this ﬁts the requirements for a probability mass function.\\nThe value 1\\n\\nis a positive integer. We also see that\\n\\nk is positive because\\n\\nk\\n\\n\\ue058i\\n\\nP\\n\\n( = x\\n\\nx\\n\\ni) =\\ue058i\\n\\n1\\nk\\n\\n=\\n\\nk\\nk\\n\\n= 1,\\n\\n(3.2)\\n\\nso the distribution is properly normalized.\\n\\n56\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.3.2 Continuous Variables and Probability Density Functions\\n\\nWhen working with continuous random variables, we describe probability dis-\\ntributions using a probability density function (PDF) rather than a probability\\nmass function. To be a probability density function, a function p must satisfy the\\nfollowing properties:\\n\\np\\n\\n• The domain of must be the set of all possible states of x.\\n• ∀x\\n\\n0≥ Note that we do not require ( ) \\np x\\n\\n1≤ .\\n\\n.\\n\\n∈ x ( ) \\n, p x\\n( ) = 1.\\n\\n• \\ue052 p x dx\\n\\nA probability density function p(x) does not give the probability of a speciﬁc\\nstate directly, instead the probability of landing inside an inﬁnitesimal region with\\nvolume\\n\\nis given by\\n\\np x δx\\n\\n( )\\n\\nδx\\n\\n.\\n\\nWe can integrate the density function to ﬁnd the actual probability mass of a\\nset of points. Speciﬁcally, the probability that x lies in some set S is given by the\\nintegral of p(x) over that set. In the univariate example, the probability that x\\nlies in the interval\\n\\nis given by\\n\\n.\\np x dx\\n\\n[\\n]a, b\\n\\n( )\\n\\nFor an example of a probability density function corresponding to a speciﬁc\\nprobability density over a continuous random variable, consider a uniform distribu-\\ntion on an interval of the real numbers. We can do this with a function u (x; a, b),\\nwhere a and b are the endpoints of the interval, with b > a. The “;” notation means\\n“parametrized by”; we consider x to be the argument of the function, while a and\\nb are parameters that deﬁne the function. To ensure that there is no probability\\nmass outside the interval, we say u(x; a, b) = 0 for all x \\ue036∈ [a, b]\\n. Within a, b],\\n. We can see that this is nonnegative everywhere. Additionally, it\\nu x a, b\\n( ;\\nintegrates to 1. We often denote that x follows the uniform distribution on [a, b]\\nby writing x\\n\\n) = 1\\nb a−\\n∼ U a, b\\n)\\n\\n(\\n\\n[\\n\\n.\\n\\n\\ue052[\\n\\n]a,b\\n\\n3.4 Marginal Probability\\n\\nSometimes we know the probability distribution over a set of variables and we want\\nto know the probability distribution over just a subset of them. The probability\\ndistribution over the subset is known as the marginal probability distribution.\\n\\nFor example, suppose we have discrete random variables x and y , and we know\\n\\nP ,(x y . We can ﬁnd\\n\\n)\\n\\nx with the\\n\\nsum rule\\n\\n:\\n\\nx, P\\n\\n( = x\\n\\nx\\n\\nP\\n\\n( = x\\n\\nx,\\n\\ny =  )\\ny .\\n\\n(3.3)\\n\\nP ( )\\n∀ ∈x\\n\\n) =\\ue058y\\n\\n57\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe name “marginal probability” comes from the process of computing marginal\\nprobabilities on paper. When the values of P(x y, ) are written in a grid with\\ndiﬀerent values of x in rows and diﬀerent values of y in columns, it is natural to\\nsum across a row of the grid, then write P( x) in the margin of the paper just to\\nthe right of the row.\\n\\nFor continuous variables, we need to use integration instead of summation:\\n\\np x( ) =\\ue05a p x, y dy.\\n\\n(\\n\\n)\\n\\n(3.4)\\n\\n3.5 Conditional Probability\\n\\nIn many cases, we are interested in the probability of some event, given that some\\nother event has happened. This is called a conditional probability. We denote\\nthe conditional probability that y = y given x = x as P (y = y | x = x ). This\\nconditional probability can be computed with the formula\\n\\nP\\n\\n( = y\\n\\ny\\n\\n| x =  ) =\\n\\nx\\n\\nP\\n\\n( = y\\n\\ny,\\n\\nx =  )\\nx\\nx\\n)\\n\\nP\\n\\n( = x\\n\\n.\\n\\n(3.5)\\n\\nThe conditional probability is only deﬁned when P (x = x) > 0. We cannot compute\\nthe conditional probability conditioned on an event that never happens.\\n\\nIt is important not to confuse conditional probability with computing what\\nwould happen if some action were undertaken. The conditional probability that\\na person is from Germany given that they speak German is quite high, but if\\na randomly selected person is taught to speak German, their country of origin\\ndoes not change. Computing the consequences of an action is called making an\\nintervention query. Intervention queries are the domain of causal modeling, which\\nwe do not explore in this book.\\n\\n3.6 The Chain Rule of Conditional Probabilities\\n\\nAny joint probability distribution over many random variables may be decomposed\\ninto conditional distributions over only one variable:\\n\\nP (x(1), . . . , x ( )n ) = \\n\\n| x(1), . . . , x (\\nproduct rule of probability. It\\nfollows immediately from the deﬁnition of conditional probability in Eq. 3.5. For\\n\\nThis observation is known as the\\n\\n(P x(1) )Πn\\n\\ni=2P (x( )i\\n\\nchain rule\\n\\ni− ).\\n\\n(3.6)\\n\\nor\\n\\n1)\\n\\n58\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nexample, applying the deﬁnition twice, we get\\n\\nP ,\\n\\n(a b c) =\\n\\n,\\n\\nP ,\\n\\n(b c) =\\n\\nP ,\\n\\n(a b c) =\\n\\n,\\n\\nP\\n\\nP\\n\\nP\\n\\n(a b|\\n(b c| )\\n(a b|\\n\\nc)\\n, P ,\\n\\n(b c)\\n\\n( )P c\\n\\n, Pc)\\n\\n(b c| )\\n\\n( )P c .\\n\\n3.7 Independence and Conditional Independence\\n\\nTwo random variables x and y are independent if their probability distribution can\\nbe expressed as a product of two factors, one involving only x and one involving\\nonly y:\\n\\n∀ ∈x\\n\\nx, y\\n\\n∈ y\\n\\n, p\\n\\nx\\n( = \\n\\nx, y\\n\\n= ) =  ( =\\n\\np x\\n\\ny\\n\\nx) ( =  )\\ny .\\n\\np y\\n\\n(3.7)\\n\\nTwo random variables x and y are conditionally independent given a random\\nvariable z if the conditional probability distribution over x and y factorizes in this\\nway for every value of z:\\n\\n∀ ∈x\\n\\nx, y\\n\\n, z∈ y ∈ z, p\\n\\n( =x\\n\\nx,\\n\\ny = \\n\\ny\\n\\n| z =  ) =  ( = x\\n\\np\\n\\nz\\n\\nx\\n\\n| z =  ) ( = y\\n\\nz p\\n\\ny\\n\\n| z = )\\nz .\\n(3.8)\\n\\nWe can denote independence and conditional independence with compact\\nnotation: x y⊥ means that x and y are independent, while x y z⊥ | means that x\\nand y are conditionally independent given z.\\n\\n3.8 Expectation, Variance and Covariance\\n\\nor\\n\\nThe expectation\\nof some function f( x) with respect to a probability\\ndistribution P (x) is the average or mean value that f takes on when x is drawn\\nfrom . For discrete variables this can be computed with a summation:\\n\\nexpected value\\n\\nP\\n\\nEx∼P[ (f x)] =\\ue058x\\n\\nP x f x ,\\n( ) ( )\\n\\n(3.9)\\n\\nwhile for continuous variables, it is computed with an integral:\\n\\nEx∼p[ ( )] =\\n\\nf x\\n\\n\\ue05a p x f x dx.\\n\\n( ) ( )\\n\\n59\\n\\n(3.10)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nWhen the identity of the distribution is clear from the context, we may simply\\nwrite the name of the random variable that the expectation is over, as in Ex[f (x)].\\nIf it is clear which random variable the expectation is over, we may omit the\\nsubscript entirely, as in E[f (x)]. By default, we can assume that E [·] averages over\\nthe values of all the random variables inside the brackets. Likewise, when there is\\nno ambiguity, we may omit the square brackets.\\n\\nExpectations are linear, for example,\\n\\nE x[\\n\\nαf x\\n\\n( ) + ( )] = \\n\\nβg x\\n\\nαEx [ ( )] +\\n\\nf x\\n\\nβEx[ ( )]\\ng x ,\\n\\n(3.11)\\n\\nwhen\\n\\nα\\n\\nand\\n\\nβ\\n\\nare not dependent on .\\nx\\n\\nThe variance gives a measure of how much the values of a function of a random\\nvariable x vary as we sample diﬀerent value of x from its probability distribution:\\n\\nVar( ( )) = \\n\\nf x\\n\\nE\\ue068( ( )\\nf x − E f x 2\\ue069.\\n\\n[ ( )])\\n\\n(3.12)\\n\\nWhen the variance is low, the values of f (x) cluster near their expected value. The\\nsquare root of the variance is known as the standard deviation.\\n\\nThe covariance gives some sense of how much two values are linearly related to\\n\\neach other, as well as the scale of these variables:\\n\\nCov( ( )\\n\\nf x , g y\\n\\n( )) = \\n\\nE f x − E f x\\n[( ( )\\n\\n[ ( )]) ( ( )\\n\\ng y − E g y\\n\\n[ ( )])]\\n\\n.\\n\\n(3.13)\\n\\nHigh absolute values of the covariance mean that the values change very much\\nand are both far from their respective means at the same time. If the sign of the\\ncovariance is positive, then both variables tend to take on relatively high values\\nsimultaneously. If the sign of the covariance is negative, then one variable tends to\\ntake on a relatively high value at the times that the other takes on a relatively low\\nvalue and vice versa. Other measures such as correlation normalize the contribution\\nof each variable in order to measure only how much the variables are related, rather\\nthan also being aﬀected by the scale of the separate variables.\\n\\nThe notions of covariance and dependence are related, but are in fact distinct\\nconcepts. They are related because two variables that are independent have zero\\ncovariance, and two variables that have non-zero covariance are dependent. How-\\never, independence is a distinct property from covariance. For two variables to have\\nzero covariance, there must be no linear dependence between them. Independence\\nis a stronger requirement than zero covariance, because independence also excludes\\nnonlinear relationships. It is possible for two variables to be dependent but have\\nzero covariance. For example, suppose we ﬁrst sample a real number x from a\\nuniform distribution over the interval [−1 ,1]. We next sample a random variable\\n\\n60\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\ns. With probability 1\\n2, we choose the value of s to be 1. Otherwise, we choose\\nthe value of s to be − 1. We can then generate a random variable y by assigning\\ny = sx . Clearly, x and y are not independent, because x completely determines\\nthe magnitude of\\n\\n. However,\\n\\n.\\n) = 0\\n\\nCov(\\n\\nx, y\\n\\ny\\n\\nThe covariance matrix of a random vector x ∈ Rn is an n n× matrix, such that\\n(3.14)\\n\\nCov( )x i,j = Cov(xi, xj ).\\n\\nThe diagonal elements of the covariance give the variance:\\n\\nCov(xi , xi) = Var(xi).\\n\\n(3.15)\\n\\n3.9 Common Probability Distributions\\n\\nSeveral simple probability distributions are useful in many contexts in machine\\nlearning.\\n\\n3.9.1 Bernoulli Distribution\\n\\nBernoulli\\n\\nThe\\ndistribution is a distribution over a single binary random variable.\\nIt is controlled by a single parameter φ ∈ [0, 1], which gives the probability of the\\nrandom variable being equal to 1. It has the following properties:\\n\\nP\\n\\nx\\n( = 1) = \\n\\nφ\\n\\nP\\n\\nx\\n( = 0) = 1\\n) =  x (1\\nx\\n\\nφ\\n\\nP\\n\\n( = x\\n\\nEx [ ] = \\n\\nx\\n\\nφ\\n\\n−\\n)− φ 1−x\\nφ\\n\\nVar x( ) =  (1\\n\\nx\\n\\nφ − φ\\n\\n)\\n\\n(3.16)\\n\\n(3.17)\\n\\n(3.18)\\n\\n(3.19)\\n\\n(3.20)\\n\\n3.9.2 Multinoulli Distribution\\n\\nmultinoulli\\n\\nThe\\ncategorical distribution is a distribution over a single discrete\\nvariable with k diﬀerent states, where k is ﬁnite1 . The multinoulli distribution is\\n\\nor\\n\\n1 “Multinoulli” is a term that was recently coined by Gustavo Lacerdo and popularized by\\nMurphy (2012). The multinoulli distribution is a special case of the\\ndistribution. A\\nmultinomial distribution is the distribution over vectors in {0, . . . , n}k representing how many\\ntimes each of the k categories is visited when n samples are drawn from a multinoulli distribution.\\nMany texts use the term “multinomial” to refer to multinoulli distributions without clarifying\\nthat they refer only to the\\n\\nmultinomial\\n\\nn = 1\\n\\ncase.\\n\\n61\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nparametrized by a vector p ∈ [0, 1]k−1 , where p i gives the probability of the i-th\\nstate. The ﬁnal, k-th state’s probability is given by 1− 1\\ue03e p. Note that we must\\nconstrain 1 \\ue03ep ≤ 1. Multinoulli distributions are often used to refer to distributions\\nover categories of objects, so we do not usually assume that state 1 has numerical\\nvalue 1, etc. For this reason, we do not usually need to compute the expectation\\nor variance of multinoulli-distributed random variables.\\n\\nThe Bernoulli and multinoulli distributions are suﬃcient to describe any distri-\\nbution over their domain. This is because they model discrete variables for which\\nit is feasible to simply enumerate all of the states. When dealing with continuous\\nvariables, there are uncountably many states, so any distribution described by a\\nsmall number of parameters must impose strict limits on the distribution.\\n\\n3.9.3 Gaussian Distribution\\n\\nThe most commonly used distribution over real numbers is the normal distribution,\\nalso known as the Gaussian distribution:\\n\\nN ( ;x µ, σ2) =\\ue072 1\\n\\n2πσ2 exp\\ue012−\\n\\n1\\nx\\n2σ2 (\\n\\nµ− 2\\ue013 .\\n\\n)\\n\\n(3.21)\\n\\nSee Fig. 3.1 for a plot of the density function.\\nThe two parameters µ ∈ R and σ ∈ (0,∞ ) control the normal distribution.\\nThe parameter µ gives the coordinate of the central peak. This is also the mean of\\nthe distribution: E[x] = µ. The standard deviation of the distribution is given by\\nσ, and the variance by σ2.\\n\\nWhen we evaluate the PDF, we need to square and invert σ. When we need to\\nfrequently evaluate the PDF with diﬀerent parameter values, a more eﬃcient way\\nof parametrizing the distribution is to use a parameter β ∈ (0 ,∞) to control the\\nprecision or inverse variance of the distribution:\\n\\nN ( ;x µ, β−1) =\\ue072 β\\n\\n2π\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\nβ x\\n\\n( − )2\\ue013 .\\n\\nµ\\n\\n(3.22)\\n\\nNormal distributions are a sensible choice for many applications. In the absence\\nof prior knowledge about what form a distribution over the real numbers should\\ntake, the normal distribution is a good default choice for two major reasons.\\n\\nFirst, many distributions we wish to model are truly close to being normal\\ndistributions. The central limit theorem shows that the sum of many independent\\nrandom variables is approximately normally distributed. This means that in\\n\\n62\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe normal distribution\\n\\nMaximum at x ¹=\\n\\nInflection points at \\n     x ¹ ¾\\n\\n= §\\n\\n−1.5\\n\\n−1.0\\n\\n−0.5\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n)\\nx\\n(\\np\\n\\n0.40\\n\\n0.35\\n\\n0.30\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.10\\n\\n0.05\\n\\n0.00\\n\\n−2.0\\n\\nFigure 3.1: The normal distribution: The normal distribution N (x;µ, σ 2) exhibits a classic\\n“bell curve” shape, with the x coordinate of its central peak given by µ, and the width\\nof its peak controlled by σ. In this example, we depict the standard normal distribution,\\nwith\\n\\nσ = 1\\n\\nµ = 0\\n\\nand\\n\\n.\\n\\nx\\n\\npractice, many complicated systems can be modeled successfully as normally\\ndistributed noise, even if the system can be decomposed into parts with more\\nstructured behavior.\\n\\nSecond, out of all possible probability distributions with the same variance,\\nthe normal distribution encodes the maximum amount of uncertainty over the\\nreal numbers. We can thus think of the normal distribution as being the one that\\ninserts the least amount of prior knowledge into a model. Fully developing and\\njustifying this idea requires more mathematical tools, and is postponed to Sec.\\n19.4.2.\\n\\nThe normal distribution generalizes to Rn, in which case it is known as the\\nmultivariate normal distribution. It may be parametrized with a positive deﬁnite\\nsymmetric matrix\\n\\n:Σ\\n\\nx µ, Σ \\ue073\\n\\n) =\\n\\nN ( ;\\n\\n1\\n\\n(2 )π ndet(\\n\\n)Σ\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\nx µ− \\ue03eΣ−1 (\\n(\\n\\n)\\n\\nx µ− \\ue013 .\\n\\n)\\n\\n(3.23)\\n\\nThe parameter µ still gives the mean of the distribution, though now it is\\nvector-valued. The parameter Σ gives the covariance matrix of the distribution.\\nAs in the univariate case, when we wish to evaluate the PDF several times for\\n\\n63\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nmany diﬀerent values of the parameters, the covariance is not a computationally\\neﬃcient way to parametrize the distribution, since we need to invert Σ to evaluate\\nthe PDF. We can instead use a precision matrix β:\\n\\nN ( ;x µ β, −1) =\\ue073det( )β\\n\\n(2 )π n\\n\\nexp\\ue012−\\n\\n1\\n2\\n\\n( − )\\ue013 .\\nx µ− \\ue03eβ x µ\\n(\\n\\n)\\n\\n(3.24)\\n\\nWe often ﬁx the covariance matrix to be a diagonal matrix. An even simpler\\nversion is the isotropic Gaussian distribution, whose covariance matrix is a scalar\\ntimes the identity matrix.\\n\\n3.9.4 Exponential and Laplace Distributions\\n\\nIn the context of deep learning, we often want to have a probability distribution\\nwith a sharp point at x = 0. To accomplish this, we can use the exponential\\ndistribution:\\n\\n(3.25)\\nThe exponential distribution uses the indicator function 1x≥0 to assign probability\\nzero to all negative values of\\n\\np x λ\\n( ; ) =  1x≥0exp (\\n\\n)−λx .\\n\\n.x\\n\\nλ\\n\\nA closely related probability distribution that allows us to place a sharp peak\\n\\nof probability mass at an arbitrary point\\n\\nµ\\n\\nis the\\n\\nLaplace distribution\\n\\nLaplace( ;\\n\\nx µ, γ\\n\\n) =\\n\\n1\\n2γ\\n\\nexp\\ue012−| − |\\nγ \\ue013.\\n\\nµ\\n\\nx\\n\\n(3.26)\\n\\n3.9.5 The Dirac Distribution and Empirical Distribution\\n\\nIn some cases, we wish to specify that all of the mass in a probability distribution\\nclusters around a single point. This can be accomplished by deﬁning a PDF using\\nthe Dirac delta function,\\n\\nδ x( )\\n\\n:\\n\\n( ) =  ( − )\\np x\\nµ .\\n\\nδ x\\n\\n(3.27)\\n\\nThe Dirac delta function is deﬁned such that it is zero-valued everywhere except\\n0, yet integrates to 1. The Dirac delta function is not an ordinary function that\\nassociates each value x with a real-valued output, instead it is a diﬀerent kind of\\nmathematical object called a generalized function that is deﬁned in terms of its\\nproperties when integrated. We can think of the Dirac delta function as being the\\nlimit point of a series of functions that put less and less mass on all points other\\nthan .µ\\n\\n64\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nBy deﬁning p(x) to be δ shifted by −µ we obtain an inﬁnitely narrow and\\n\\ninﬁnitely high peak of probability mass where\\n\\nx\\n\\nµ= \\n\\n.\\n\\nA common use of the Dirac delta distribution is as a component of an empirical\\n\\ndistribution,\\n\\nˆp( ) =x\\n\\n1\\nm\\n\\nm\\ue058i=1\\n\\nδ(x x− ( )i )\\n\\n(3.28)\\n\\n1\\nm on each of the m points x(1), . . . , x (\\n\\n)m forming\\nwhich puts probability mass\\na given data set or collection of samples. The Dirac delta distribution is only\\nnecessary to deﬁne the empirical distribution over continuous variables. For discrete\\nvariables, the situation is simpler: an empirical distribution can be conceptualized\\nas a multinoulli distribution, with a probability associated to each possible input\\nvalue that is simply equal to the empirical frequency of that value in the training\\nset.\\n\\nWe can view the empirical distribution formed from a dataset of training\\nexamples as specifying the distribution that we sample from when we train a model\\non this dataset. Another important perspective on the empirical distribution is\\nthat it is the probability density that maximizes the likelihood of the training\\ndata (see Sec. 5.5). Many machine learning algorithms can be conﬁgured to have\\narbitrarily high capacity. If given enough capacity, these algorithms will simply\\nlearn the empirical distribution. This is a bad outcome because the model does not\\ngeneralize at all and assigns inﬁnitesimal probability to any point in space that did\\nnot occur in the training set. A central problem in machine learning is studying\\nhow to limit the capacity of a model in a way that prevents it from simply learning\\nthe empirical distribution while also allowing it to learn complicated functions.\\n\\n3.9.6 Mixtures of Distributions\\n\\nIt is also common to deﬁne probability distributions by combining other simpler\\nprobability distributions. One common way of combining distributions is to\\nconstruct a mixture distribution. A mixture distribution is made up of several\\ncomponent distributions. On each trial, the choice of which component distribution\\ngenerates the sample is determined by sampling a component identity from a\\nmultinoulli distribution:\\n\\nP ( ) =x \\ue058i\\n\\nP\\n\\nc\\n( = \\n\\ni P\\n)\\n\\nx c|\\n(\\n\\n=\\n\\ni\\n)\\n\\n(3.29)\\n\\nwhere\\n\\nP ( )\\n\\nc is the multinoulli distribution over component identities.\\n\\n65\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nWe have already seen one example of a mixture distribution: the empirical\\ndistribution over real-valued variables is a mixture distribution with one Dirac\\ncomponent for each training example.\\n\\nThe mixture model is one simple strategy for combining probability distributions\\nto create a richer distribution. In Chapter 16, we explore the art of building complex\\nprobability distributions from simple ones in more detail.\\n\\nlatent variable\\n\\nThe mixture model allows us to brieﬂy glimpse a concept that will be of\\nparamount importance later—the\\n. A latent variable is a random\\nvariable that we cannot observe directly. The component identity variable c of the\\nmixture model provides an example. Latent variables may be related to x through\\nthe joint distribution, in this case, P (x c, ) = P (x c|\\n)P(c). The distribution P (c)\\nover the latent variable and the distribution P (x c| ) relating the latent variables\\nto the visible variables determines the shape of the distribution P ( x) even though\\nit is possible to describe P (x) without reference to the latent variable. Latent\\nvariables are discussed further in Sec. 16.5.\\n\\nA very powerful and common type of mixture model is the Gaussian mixture\\nmodel, in which the components p (x | c = i) are Gaussians. Each component has\\na separately parametrized mean µ ( )i and covariance Σ ( )i . Some mixtures can have\\nmore constraints. For example, the covariances could be shared across components\\nvia the constraint Σ( )i = Σ∀i. As with a single Gaussian distribution, the mixture\\nof Gaussians might constrain the covariance matrix for each component to be\\ndiagonal or isotropic.\\n\\nIn addition to the means and covariances, the parameters of a Gaussian mixture\\nspecify the prior probability α i = P (c = i) given to each component i. The word\\n“prior” indicates that it expresses the model’s beliefs about c before it has observed\\nx. By comparison, P(c | x) is a posterior probability, because it is computed after\\nobservation of x. A Gaussian mixture model is a universal approximator of\\ndensities, in the sense that any smooth density can be approximated with any\\nspeciﬁc, non-zero amount of error by a Gaussian mixture model with enough\\ncomponents.\\n\\nFig. 3.2 shows samples from a Gaussian mixture model.\\n\\n3.10 Useful Properties of Common Functions\\n\\nCertain functions arise often while working with probability distributions, especially\\nthe probability distributions used in deep learning models.\\n\\n66\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n2\\nx\\n\\nx1\\n\\nFigure 3.2: Samples from a Gaussian mixture model. In this example, there are three\\ncomponents. From left to right, the ﬁrst component has an isotropic covariance matrix,\\nmeaning it has the same amount of variance in each direction. The second has a diagonal\\ncovariane matrix, meaning it can control the variance separately along each axis-aligned\\ndirection. This example has more variance along the x2 axis than along the x1 axis. The\\nthird component has a full-rank covariance matrix, allowing it to control the variance\\nseparately along an abitrary basis of directions.\\n\\nOne of these functions is the logistic sigmoid:\\n\\nσ x( ) =\\n\\n1\\n\\n1 + exp(\\n\\n.\\n)−x\\n\\n(3.30)\\n\\nThe logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli\\ndistribution because its range is (0, 1), which lies within the valid range of values\\nfor the φ parameter. See Fig. 3.3 for a graph of the sigmoid function. The sigmoid\\nfunction saturates when its argument is very positive or very negative, meaning\\nthat the function becomes very ﬂat and insensitive to small changes in its input.\\n\\nAnother commonly encountered function is the\\n\\nsoftplus\\n\\nfunction (Dugas\\n\\net al.,\\n\\n2001):\\n\\nζ x\\nx .\\n( ) = log (1 + exp( ))\\n\\n(3.31)\\n\\nThe softplus function can be useful for producing the β or σ parameter of a normal\\ndistribution because its range is (0,∞). It also arises commonly when manipulating\\nexpressions involving sigmoids. The name of the softplus function comes from the\\nfact that it is a smoothed or “softened” version of\\n\\nx+ = max(0 ), x .\\n\\n(3.32)\\n\\nSee Fig. 3.4 for a graph of the softplus function.\\n\\n67\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe logistic sigmoid function\\n\\n)\\nx\\n(\\n¾\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n−10\\n\\n−5\\n\\n0\\nx\\n\\n5\\n\\n10\\n\\nFigure 3.3: The logistic sigmoid function.\\n\\nThe softplus function\\n\\n10\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n)\\nx\\n(\\n³\\n\\n0\\n−10\\n\\n−5\\n\\n0\\n\\nx\\n\\n5\\n\\n10\\n\\nFigure 3.4: The softplus function.\\n\\n68\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nThe following properties are all useful enough that you may wish to memorize\\n\\nthem:\\n\\nσ x( ) =\\n\\nexp( )x\\nx\\n\\nexp( ) + exp(0)\\n\\nd\\ndx\\n\\nσ x\\n\\n( ) =  (\\n\\n( ) =  ( )(1 − ( ))\\nσ x\\nσ x\\n− σ x\\n1\\nσ x\\n\\nσ −x\\n)\\n−ζ −x\\n(\\nζ x\\nσ x\\n( ) =  ( )\\n\\nlog ( ) = \\n\\n)\\n\\nd\\ndx\\n\\n1)\\n\\n(0,\\n\\n, σ\\n\\n\\ue012 x\\n1 − x\\ue013\\n∀ ∈x\\n∀x > 0, ζ−1( ) = log (exp( )\\nx −\\n\\n−1 ( ) = log\\n\\n1)\\n\\nx\\n\\nx\\n\\nζ x( ) =\\ue05a x\\n\\n−∞\\nx\\nζ\\n\\nσ y dy\\n\\n( )\\n\\n(3.33)\\n\\n(3.34)\\n\\n(3.35)\\n\\n(3.36)\\n\\n(3.37)\\n\\n(3.38)\\n\\n(3.39)\\n\\n(3.40)\\n\\n(3.41)\\nThe function σ−1(x) is called the logit in statistics, but this term is more rarely\\nused in machine learning. The ﬁnal property provides extra justiﬁcation for the\\nname “softplus,” since x+ − x− = x.\\n\\n( ) − (− ) = \\nζ x\\nx\\n\\n3.11 Bayes’ Rule\\n\\nWe often ﬁnd ourselves in a situation where we know P (y x| ) and need to know\\nP (x y|\\n). Fortunately, if we also know P (x), we can compute the desired quantity\\nusing Bayes’ rule:\\n\\nP (\\n\\nx y|\\n\\n) =\\n\\nP\\n\\nP( )x\\n\\ny x|\\n(\\n\\n)\\n\\n.\\n\\n(3.42)\\n\\nP ( )y\\n\\nP ( ) =y \\ue050x P\\n\\n(y |\\n\\nNote that while P (y) appears in the formula, it is usually feasible to compute\\n\\nx P x\\n\\n( ), so we do not need to begin with knowledge of\\n\\n)\\n\\nP\\n\\n(y .)\\n\\nBayes’ rule is straightforward to derive from the deﬁnition of conditional\\nprobability, but it is useful to know the name of this formula since many texts\\nrefer to it by name. It is named after the Reverend Thomas Bayes, who ﬁrst\\ndiscovered a special case of the formula. The general version presented here was\\nindependently discovered by Pierre-Simon Laplace.\\n\\n69\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n3.12 Technical Details of Continuous Variables\\n\\nA proper formal understanding of continuous random variables and probability\\ndensity functions requires developing probability theory in terms of a branch of\\nmathematics known as measure theory. Measure theory is beyond the scope of\\nthis textbook, but we can brieﬂy sketch some of the issues that measure theory is\\nemployed to resolve.\\n\\nIn Sec. 3.3.2, we saw that the probability of a continuous vector-valued x lying\\nin some set S is given by the integral of p(x ) over the set S. Some choices of set S\\ncan produce paradoxes. For example, it is possible to construct two sets S1 and\\nS2 such that p(x ∈ S1) + p(x ∈ S 2) > 1 but S1 ∩ S2 = ∅. These sets are generally\\nconstructed making very heavy use of the inﬁnite precision of real numbers, for\\nexample by making fractal-shaped sets or sets that are deﬁned by transforming\\nthe set of rational numbers2 . One of the key contributions of measure theory is to\\nprovide a characterization of the set of sets that we can compute the probability\\nof without encountering paradoxes. In this book, we only integrate over sets with\\nrelatively simple descriptions, so this aspect of measure theory never becomes a\\nrelevant concern.\\n\\nFor our purposes, measure theory is more useful for describing theorems that\\napply to most points in Rn but do not apply to some corner cases. Measure theory\\nprovides a rigorous way of describing that a set of points is negligibly small. Such\\na set is said to have “measure zero.” We do not formally deﬁne this concept in this\\ntextbook. However, it is useful to understand the intuition that a set of measure\\nzero occupies no volume in the space we are measuring. For example, within R2 , a\\nline has measure zero, while a ﬁlled polygon has positive measure. Likewise, an\\nindividual point has measure zero. Any union of countably many sets that each\\nhave measure zero also has measure zero (so the set of all the rational numbers\\nhas measure zero, for instance).\\n\\nAnother useful term from measure theory is “almost everywhere.” A property\\nthat holds almost everywhere holds throughout all of space except for on a set of\\nmeasure zero. Because the exceptions occupy a negligible amount of space, they\\ncan be safely ignored for many applications. Some important results in probability\\ntheory hold for all discrete values but only hold “almost everywhere” for continuous\\nvalues.\\n\\nAnother technical detail of continuous variables relates to handling continuous\\nrandom variables that are deterministic functions of one another. Suppose we have\\ntwo random variables, x and y, such that y = g(x), where g is an invertible, con-\\n\\n2The Banach-Tarski theorem provides a fun example of such sets.\\n\\n70\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\ntinuous, diﬀerentiable transformation. One might expect that py(y ) =p x(g−1(y )).\\nThis is actually not the case.\\n\\nAs a simple example, suppose we have scalar random variables x and y. Suppose\\nIf we use the rule p y(y) = p x(2 y) then py will be 0\\non this interval. This means\\n\\ny = x\\neverywhere except the interval [0 , 1\\n2 ]\\n\\n2 and x ∼ U(0,1).\\n\\n, and it will be\\n\\n1\\n\\n\\ue05a py ( ) =\\n\\ny dy\\n\\n1\\n2\\n\\n,\\n\\n(3.43)\\n\\nwhich violates the deﬁnition of a probability distribution.\\n\\nThis common mistake is wrong because it fails to account for the distortion\\nof space introduced by the function g. Recall that the probability of x lying in\\nan inﬁnitesimally small region with volume δx is given by p( x)δx. Since g can\\nexpand or contract space, the inﬁnitesimal volume surrounding x in x space may\\nhave diﬀerent volume in\\n\\nspace.\\n\\ny\\n\\nTo see how to correct the problem, we return to the scalar case. We need to\\n\\npreserve the property\\n\\nSolving from this, we obtain\\n\\n|py( ( ))\\n\\ng x dy|\\n\\n=\\n\\n|p x( )x dx .|\\n\\nor equivalently\\n\\nIn higher dimensions, the derivative generalizes to the determinant of the Jacobian\\nmatrix—the matrix with J i,j = ∂xi\\n∂yj\\n\\n. Thus, for real-valued vectors\\n\\nand ,\\ny\\n\\nx\\n\\npy( ) = \\n\\ny\\n\\npx ( ) = \\n\\nx\\n\\n∂x\\n\\n∂g x( )\\n\\npy( ( ))\\n\\npx (g−1( ))y\\n\\n\\ue00c\\ue00c\\ue00c\\ue00c\\n∂y\\ue00c\\ue00c\\ue00c\\ue00c\\ng x \\ue00c\\ue00c\\ue00c\\ue00c\\n∂x \\ue00c\\ue00c\\ue00c\\ue00c .\\ng x \\ue00c\\ue00c\\ue00c\\ue00cdet\\ue012∂g( )x\\n∂x \\ue013\\ue00c\\ue00c\\ue00c\\ue00c .\\n\\npx ( ) = \\n\\nx\\n\\np y( ( ))\\n\\n3.13 Information Theory\\n\\nInformation theory is a branch of applied mathematics that revolves around\\nquantifying how much information is present in a signal. It was originally invented\\nto study sending messages from discrete alphabets over a noisy channel, such as\\ncommunication via radio transmission. In this context, information theory tells how\\nto design optimal codes and calculate the expected length of messages sampled from\\n\\n71\\n\\n(3.44)\\n\\n(3.45)\\n\\n(3.46)\\n\\n(3.47)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nspeciﬁc probability distributions using various encoding schemes. In the context of\\nmachine learning, we can also apply information theory to continuous variables\\nwhere some of these message length interpretations do not apply. This ﬁeld is\\nfundamental to many areas of electrical engineering and computer science. In this\\ntextbook, we mostly use a few key ideas from information theory to characterize\\nprobability distributions or quantify similarity between probability distributions.\\nFor more detail on information theory, see Cover and Thomas (2006) or MacKay\\n(2003).\\n\\nThe basic intuition behind information theory is that learning that an unlikely\\nevent has occurred is more informative than learning that a likely event has\\noccurred. A message saying “the sun rose this morning” is so uninformative as\\nto be unnecessary to send, but a message saying “there was a solar eclipse this\\nmorning” is very informative.\\n\\nWe would like to quantify information in a way that formalizes this intuition.\\n\\nSpeciﬁcally,\\n\\n• Likely events should have low information content, and in the extreme case,\\nevents that are guaranteed to happen should have no information content\\nwhatsoever.\\n\\n• Less likely events should have higher information content.\\n• Independent events should have additive information. For example, ﬁnding\\nout that a tossed coin has come up as heads twice should convey twice as\\nmuch information as ﬁnding out that a tossed coin has come up as heads\\nonce.\\n\\nIn order to satisfy all three of these properties, we deﬁne the self-information\\n\\nof an event x\\n\\n= x\\n\\nto be\\n\\nI x\\n( ) = \\n\\nlog−\\n\\nP x .\\n( )\\n\\n(3.48)\\n\\nIn this book, we always use log to mean the natural logarithm, with base e. Our\\ndeﬁnition of I(x) is therefore written in units of\\n. One nat is the amount of\\ninformation gained by observing an event of probability 1\\ne . Other texts use base-2\\nlogarithms and units called\\nshannons\\n; information measured in bits is just\\na rescaling of information measured in nats.\\n\\nnats\\n\\nbits\\n\\nor\\n\\nWhen x is continuous, we use the same deﬁnition of information by analogy,\\nbut some of the properties from the discrete case are lost. For example, an event\\nwith unit density still has zero information, despite not being an event that is\\nguaranteed to occur.\\n\\n72\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n0.4\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\ns\\nt\\na\\nn\\n \\nn\\n\\ni\\n \\ny\\np\\no\\nr\\nt\\nn\\ne\\n \\nn\\no\\nn\\nn\\na\\nh\\nS\\n\\n0.0\\n\\n0.0\\n\\nShannon entropy of a binary random variable\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\np\\n\\n1\\n\\nFigure 3.5: This plot shows how distributions that are closer to deterministic have low\\nShannon entropy while distributions that are close to uniform have high Shannon entropy.\\nOn the horizontal axis, we plot p, the probability of a binary random variable being equal\\nto . The entropy is given by\\nlog . When p is near 0, the distribution\\nis nearly deterministic, because the random variable is nearly always 0. When p is near 1,\\nthe distribution is nearly deterministic, because the random variable is nearly always 1.\\nWhen p = 0 .5, the entropy is maximal, because the distribution is uniform over the two\\noutcomes.\\n\\n(p− 1) log(1− p )− p\\n\\np\\n\\nSelf-information deals only with a single outcome. We can quantify the amount\\n\\nof uncertainty in an entire probability distribution using the Shannon entropy:\\n\\nH( ) = \\n\\nx\\n\\nE\\n\\nI x\\n\\nx∼P[ ( )] = \\n\\n−E\\n\\nP x .\\nx∼P[log ( )]\\n\\n(3.49)\\n\\nalso denoted H( P). In other words, the Shannon entropy of a distribution is the\\nexpected amount of information in an event drawn from that distribution. It gives\\na lower bound on the number of bits (if the logarithm is base 2, otherwise the units\\nare diﬀerent) needed on average to encode symbols drawn from a distribution P.\\nDistributions that are nearly deterministic (where the outcome is nearly certain)\\nhave low entropy; distributions that are closer to uniform have high entropy. See\\nFig. 3.5 for a demonstration. When x is continous, the Shannon entropy is known\\nas the diﬀerential entropy.\\n\\nIf we have two separate probability distributions P(x) and Q(x) over the same\\nrandom variable x, we can measure how diﬀerent these two distributions are using\\nthe Kullback-Leibler (KL) divergence:\\n\\nDKL(\\n\\nP Q\\ue06b\\n\\n) = \\n\\nE x∼P\\ue014log\\n\\nP x( )\\n\\nQ x( )\\ue015 = Ex∼P [log ( )\\n\\nP x −\\n\\n73\\n\\nlog ( )]\\nQ x .\\n\\n(3.50)\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nIn the case of discrete variables, it is the extra amount of information (measured\\nin bits if we use the base\\nlogarithm, but in machine learning we usually use nats\\nand the natural logarithm) needed to send a message containing symbols drawn\\nfrom probability distribution P , when we use a code that was designed to minimize\\nthe length of messages drawn from probability distribution\\n\\n.Q\\n\\n2\\n\\nThe KL divergence has many useful properties, most notably that it is non-\\nnegative. The KL divergence is 0 if and only if P and Qare the same distribution in\\nthe case of discrete variables, or equal “almost everywhere” in the case of continuous\\nvariables. Because the KL divergence is non-negative and measures the diﬀerence\\nbetween two distributions, it is often conceptualized as measuring some sort of\\ndistance between these distributions. However, it is not a true distance measure\\nbecause it is not symmetric: DKL(P Q\\ue06b ) \\ue036= DKL( Q P\\ue06b ) for some P and Q. This\\nasymmetry means that there are important consequences to the choice of whether\\nto use DKL(\\n\\n. See Fig. 3.6 for more detail.\\n\\nor DKL (\\n\\n)\\n\\nP Q\\ue06b\\n\\n)Q P\\ue06b\\n\\nA quantity that is closely related to the KL divergence is the cross-entropy\\nH (P, Q ) = H (P) + DKL(P Q\\ue06b ), which is similar to the KL divergence but lacking\\nthe term on the left:\\n(3.51)\\n\\nH P, Q(\\n\\n) = −E\\n\\nx∼P log ( )Q x .\\n\\nMinimizing the cross-entropy with respect to Q is equivalent to minimizing the\\nKL divergence, because\\n\\ndoes not participate in the omitted term.\\n\\nQ\\n\\nWhen computing many of these quantities, it is common to encounter expres-\\nsions of the form 0log 0. By convention, in the context of information theory, we\\ntreat these expressions as limx→0 x\\n\\nlog = 0.\\n\\nx\\n\\n3.14 Structured Probabilistic Models\\n\\nMachine learning algorithms often involve probability distributions over a very\\nlarge number of random variables. Often, these probability distributions involve\\ndirect interactions between relatively few variables. Using a single function to\\ndescribe the entire joint probability distribution can be very ineﬃcient (both\\ncomputationally and statistically).\\n\\nInstead of using a single function to represent a probability distribution, we\\ncan split a probability distribution into many factors that we multiply together.\\nFor example, suppose we have three random variables: a, b and c . Suppose that\\na inﬂuences the value of b and b inﬂuences the value of c, but that a and c are\\nindependent given b. We can represent the probability distribution over all three\\n\\n74\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nq∗ = argminq DKL(\\n\\n)p q\\ue06b\\n\\nq ∗ = argminq DKL (\\n\\nq p\\ue06b\\n\\n)\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np x( )\\nq∗( )x\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np( )x\\nq ∗( )x\\n\\nx\\n\\nx\\n\\nFigure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x ) and\\nwish to approximate it with another distribution q(x). We have the choice of minimizing\\neither DKL( p q\\ue06b ) or DKL( q p\\ue06b ). We illustrate the eﬀect of this choice using a mixture of\\ntwo Gaussians for p, and a single Gaussian for q. The choice of which direction of the\\nKL divergence to use is problem-dependent. Some applications require an approximation\\nthat usually places high probability anywhere that the true distribution places high\\nprobability, while other applications require an approximation that rarely places high\\nprobability anywhere that the true distribution places low probability. The choice of the\\ndirection of the KL divergence reﬂects which of these considerations takes priority for each\\napplication. (Left) The eﬀect of minimizing DKL (p q\\ue06b ). In this case, we select a q that has\\nhigh probability where p has high probability. When p has multiple modes, q chooses to\\nblur the modes together, in order to put high probability mass on all of them. (Right) The\\neﬀect of minimizing DKL (q p\\ue06b ). In this case, we select a q that has low probability where\\np has low probability. When p has multiple modes that are suﬃciently widely separated,\\nas in this ﬁgure, the KL divergence is minimized by choosing a single mode, in order to\\navoid putting probability mass in the low-probability areas between modes of p . Here, we\\nillustrate the outcome when q is chosen to emphasize the left mode. We could also have\\nachieved an equal value of the KL divergence by choosing the right mode. If the modes\\nare not separated by a suﬃciently strong low probability region, then this direction of the\\nKL divergence can still choose to blur the modes.\\n\\n75\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nvariables as a product of probability distributions over two variables:\\n\\np ,\\n\\n(a b c) =  ( )a (\\np\\n\\np\\n\\n,\\n\\nb a|\\n\\n)\\n\\nc b|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.52)\\n\\nThese factorizations can greatly reduce the number of parameters needed\\nto describe the distribution. Each factor uses a number of parameters that is\\nexponential in the number of variables in the factor. This means that we can greatly\\nreduce the cost of representing a distribution if we are able to ﬁnd a factorization\\ninto distributions over fewer variables.\\n\\nWe can describe these kinds of factorizations using graphs. Here we use the\\nword “graph” in the sense of graph theory: a set of vertices that may be connected\\nto each other with edges. When we represent the factorization of a probability\\ndistribution with a graph, we call it a structured probabilistic model\\ngraphical\\nmodel.\\n\\nor\\n\\nThere are two main kinds of structured probabilistic models: directed and\\nundirected. Both kinds of graphical models use a graph G in which each node\\nin the graph corresponds to a random variable, and an edge connecting two\\nrandom variables means that the probability distribution is able to represent direct\\ninteractions between those two random variables.\\n\\nDirected models use graphs with directed edges, and they represent factoriza-\\ntions into conditional probability distributions, as in the example above. Speciﬁcally,\\na directed model contains one factor for every random variable x i in the distribution,\\nand that factor consists of the conditional distribution over xi given the parents of\\nxi, denoted P a G(x i):\\n\\n(3.53)\\n\\np( ) =x \\ue059i\\n\\np (xi | P aG (xi )) .\\n\\nSee Fig. 3.7 for an example of a directed graph and the factorization of probability\\ndistributions it represents.\\n\\nUndirected models use graphs with undirected edges, and they represent fac-\\ntorizations into a set of functions; unlike in the directed case, these functions are\\nusually not probability distributions of any kind. Any set of nodes that are all\\nconnected to each other in G is called a clique. Each clique C ( )i\\nin an undirected\\nmodel is associated with a factor φ( )i (C ( )i ). These factors are just functions, not\\nprobability distributions. The output of each factor must be non-negative, but\\nthere is no constraint that the factor must sum or integrate to 1 like a probability\\ndistribution.\\n\\nThe probability of a conﬁguration of random variables is proportional to the\\nproduct of all of these factors—assignments that result in larger factor values are\\n\\n76\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.7: A directed graphical model over random variables a , b, c, d and e. This graph\\ncorresponds to probability distributions that can be factored as\\n\\n(a b c d e) =  ( )a (\\np ,\\np\\n\\np\\n\\n,\\n\\n,\\n\\n,\\n\\nb a|\\n\\n)\\n\\n(c a|\\np\\n\\n,\\n\\nb) (\\np\\n\\nd b|\\n\\n)\\n\\ne c|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.54)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nmore likely. Of course, there is no guarantee that this product will sum to 1. We\\ntherefore divide by a normalizing constant Z, deﬁned to be the sum or integral\\nover all states of the product of the φ functions, in order to obtain a normalized\\nprobability distribution:\\n\\np( ) =x\\n\\n1\\n\\nZ \\ue059i\\n\\nφ( )i \\ue010C( )i\\ue011 .\\n\\n(3.55)\\n\\nSee Fig. 3.8 for an example of an undirected graph and the factorization of\\nprobability distributions it represents.\\n\\nKeep in mind that these graphical representations of factorizations are a\\nlanguage for describing probability distributions. They are not mutually exclusive\\nfamilies of probability distributions. Being directed or undirected is not a property\\nof a probability distribution; it is a property of a particular\\nof a\\nprobability distribution, but any probability distribution may be described in both\\nways.\\n\\ndescription\\n\\nThroughout Part I and Part II of this book, we will use structured probabilistic\\nmodels merely as a language to describe which direct probabilistic relationships\\ndiﬀerent machine learning algorithms choose to represent. No further understanding\\nof structured probabilistic models is needed until the discussion of research topics,\\nin Part III, where we will explore structured probabilistic models in much greater\\ndetail.\\n\\n77\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.8: An undirected graphical model over random variables a, b, c, d and e . This\\ngraph corresponds to probability distributions that can be factored as\\n\\n(a b c d e) =\\np ,\\n\\n,\\n\\n,\\n\\n,\\n\\n1\\nZ\\n\\nφ (1)(\\n\\na b c\\n,\\n\\n, φ (2) (\\n\\n)\\n\\n)b d, φ(3)(\\n\\n)c e,\\n.\\n\\n(3.56)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nThis chapter has reviewed the basic concepts of probability theory that are\\nmost relevant to deep learning. One more set of fundamental mathematical tools\\nremains: numerical methods.\\n\\n78\\n\\n\\x0c'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'([\\n]{2,2}(\\d\\.)+\\d* [A-Za-z0-9? ]+[\\n]{1,2})', re.UNICODE)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p = re.compile('[\\n]{2,2}(\\d.){1,} [A-Za-z0-9][\\n]{2,2}')\n",
    "#p = re.compile('[a-z]*')\n",
    "\n",
    "#p = re.compile('[\\n]{2,2}[\\d|.]+ [A-Za-z0-9? ]+[\\n]{1,2}')\n",
    "p = re.compile(r'([\\n]{2,2}(\\d\\.)+\\d* [A-Za-z0-9? ]+[\\n]{1,2})')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n3.1 Why Probability?\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.match(string_to_match).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\n3.1 Why Probability?\\n\\n', '3.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.search(cry).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_all = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_all_iter = p.finditer(cry)\n",
    "start_end_sections_titles = [m.span() for m in cry_all_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_all = p.findall(apriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1677, 1701),\n",
       " (7804, 7828),\n",
       " (8773, 8806),\n",
       " (9058, 9117),\n",
       " (11612, 11676),\n",
       " (13399, 13427),\n",
       " (14373, 14404),\n",
       " (15429, 15480),\n",
       " (16105, 16154),\n",
       " (20560, 20600),\n",
       " (20689, 20721),\n",
       " (21118, 21152),\n",
       " (22776, 22807),\n",
       " (26294, 26341),\n",
       " (26895, 26954),\n",
       " (29422, 29457),\n",
       " (32308, 32354),\n",
       " (35989, 36039),\n",
       " (40066, 40093),\n",
       " (46553, 46593)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end_sections_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_section = 0\n",
    "section_text = []\n",
    "for range_ in start_end_sections_titles:\n",
    "    section_text.append(cry[start_section:range_[0]])\n",
    "    start_section = range_[1]\n",
    "    \n",
    "section_text.append(cry[start_section:])\n",
    "section_text = section_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning algorithms often involve probability distributions over a very\\nlarge number of random variables. Often, these probability distributions involve\\ndirect interactions between relatively few variables. Using a single function to\\ndescribe the entire joint probability distribution can be very ineﬃcient (both\\ncomputationally and statistically).\\n\\nInstead of using a single function to represent a probability distribution, we\\ncan split a probability distribution into many factors that we multiply together.\\nFor example, suppose we have three random variables: a, b and c . Suppose that\\na inﬂuences the value of b and b inﬂuences the value of c, but that a and c are\\nindependent given b. We can represent the probability distribution over all three\\n\\n74\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nq∗ = argminq DKL(\\n\\n)p q\\ue06b\\n\\nq ∗ = argminq DKL (\\n\\nq p\\ue06b\\n\\n)\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np x( )\\nq∗( )x\\n\\ny\\nt\\ni\\ns\\nn\\ne\\nD\\ny\\nt\\ni\\nl\\ni\\n\\nb\\na\\nb\\no\\nr\\nP\\n\\np( )x\\nq ∗( )x\\n\\nx\\n\\nx\\n\\nFigure 3.6: The KL divergence is asymmetric. Suppose we have a distribution p(x ) and\\nwish to approximate it with another distribution q(x). We have the choice of minimizing\\neither DKL( p q\\ue06b ) or DKL( q p\\ue06b ). We illustrate the eﬀect of this choice using a mixture of\\ntwo Gaussians for p, and a single Gaussian for q. The choice of which direction of the\\nKL divergence to use is problem-dependent. Some applications require an approximation\\nthat usually places high probability anywhere that the true distribution places high\\nprobability, while other applications require an approximation that rarely places high\\nprobability anywhere that the true distribution places low probability. The choice of the\\ndirection of the KL divergence reﬂects which of these considerations takes priority for each\\napplication. (Left) The eﬀect of minimizing DKL (p q\\ue06b ). In this case, we select a q that has\\nhigh probability where p has high probability. When p has multiple modes, q chooses to\\nblur the modes together, in order to put high probability mass on all of them. (Right) The\\neﬀect of minimizing DKL (q p\\ue06b ). In this case, we select a q that has low probability where\\np has low probability. When p has multiple modes that are suﬃciently widely separated,\\nas in this ﬁgure, the KL divergence is minimized by choosing a single mode, in order to\\navoid putting probability mass in the low-probability areas between modes of p . Here, we\\nillustrate the outcome when q is chosen to emphasize the left mode. We could also have\\nachieved an equal value of the KL divergence by choosing the right mode. If the modes\\nare not separated by a suﬃciently strong low probability region, then this direction of the\\nKL divergence can still choose to blur the modes.\\n\\n75\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\nvariables as a product of probability distributions over two variables:\\n\\np ,\\n\\n(a b c) =  ( )a (\\np\\n\\np\\n\\n,\\n\\nb a|\\n\\n)\\n\\nc b|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.52)\\n\\nThese factorizations can greatly reduce the number of parameters needed\\nto describe the distribution. Each factor uses a number of parameters that is\\nexponential in the number of variables in the factor. This means that we can greatly\\nreduce the cost of representing a distribution if we are able to ﬁnd a factorization\\ninto distributions over fewer variables.\\n\\nWe can describe these kinds of factorizations using graphs. Here we use the\\nword “graph” in the sense of graph theory: a set of vertices that may be connected\\nto each other with edges. When we represent the factorization of a probability\\ndistribution with a graph, we call it a structured probabilistic model\\ngraphical\\nmodel.\\n\\nor\\n\\nThere are two main kinds of structured probabilistic models: directed and\\nundirected. Both kinds of graphical models use a graph G in which each node\\nin the graph corresponds to a random variable, and an edge connecting two\\nrandom variables means that the probability distribution is able to represent direct\\ninteractions between those two random variables.\\n\\nDirected models use graphs with directed edges, and they represent factoriza-\\ntions into conditional probability distributions, as in the example above. Speciﬁcally,\\na directed model contains one factor for every random variable x i in the distribution,\\nand that factor consists of the conditional distribution over xi given the parents of\\nxi, denoted P a G(x i):\\n\\n(3.53)\\n\\np( ) =x \\ue059i\\n\\np (xi | P aG (xi )) .\\n\\nSee Fig. 3.7 for an example of a directed graph and the factorization of probability\\ndistributions it represents.\\n\\nUndirected models use graphs with undirected edges, and they represent fac-\\ntorizations into a set of functions; unlike in the directed case, these functions are\\nusually not probability distributions of any kind. Any set of nodes that are all\\nconnected to each other in G is called a clique. Each clique C ( )i\\nin an undirected\\nmodel is associated with a factor φ( )i (C ( )i ). These factors are just functions, not\\nprobability distributions. The output of each factor must be non-negative, but\\nthere is no constraint that the factor must sum or integrate to 1 like a probability\\ndistribution.\\n\\nThe probability of a conﬁguration of random variables is proportional to the\\nproduct of all of these factors—assignments that result in larger factor values are\\n\\n76\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.7: A directed graphical model over random variables a , b, c, d and e. This graph\\ncorresponds to probability distributions that can be factored as\\n\\n(a b c d e) =  ( )a (\\np ,\\np\\n\\np\\n\\n,\\n\\n,\\n\\n,\\n\\nb a|\\n\\n)\\n\\n(c a|\\np\\n\\n,\\n\\nb) (\\np\\n\\nd b|\\n\\n)\\n\\ne c|\\n(\\np\\n\\n)\\n\\n.\\n\\n(3.54)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nmore likely. Of course, there is no guarantee that this product will sum to 1. We\\ntherefore divide by a normalizing constant Z, deﬁned to be the sum or integral\\nover all states of the product of the φ functions, in order to obtain a normalized\\nprobability distribution:\\n\\np( ) =x\\n\\n1\\n\\nZ \\ue059i\\n\\nφ( )i \\ue010C( )i\\ue011 .\\n\\n(3.55)\\n\\nSee Fig. 3.8 for an example of an undirected graph and the factorization of\\nprobability distributions it represents.\\n\\nKeep in mind that these graphical representations of factorizations are a\\nlanguage for describing probability distributions. They are not mutually exclusive\\nfamilies of probability distributions. Being directed or undirected is not a property\\nof a probability distribution; it is a property of a particular\\nof a\\nprobability distribution, but any probability distribution may be described in both\\nways.\\n\\ndescription\\n\\nThroughout Part I and Part II of this book, we will use structured probabilistic\\nmodels merely as a language to describe which direct probabilistic relationships\\ndiﬀerent machine learning algorithms choose to represent. No further understanding\\nof structured probabilistic models is needed until the discussion of research topics,\\nin Part III, where we will explore structured probabilistic models in much greater\\ndetail.\\n\\n77\\n\\n\\x0cCHAPTER 3. PROBABILITY AND INFORMATION THEORY\\n\\naa\\n\\nbb\\n\\ndd\\n\\ncc\\n\\nee\\n\\nFigure 3.8: An undirected graphical model over random variables a, b, c, d and e . This\\ngraph corresponds to probability distributions that can be factored as\\n\\n(a b c d e) =\\np ,\\n\\n,\\n\\n,\\n\\n,\\n\\n1\\nZ\\n\\nφ (1)(\\n\\na b c\\n,\\n\\n, φ (2) (\\n\\n)\\n\\n)b d, φ(3)(\\n\\n)c e,\\n.\\n\\n(3.56)\\n\\nThis graph allows us to quickly see some properties of the distribution. For example, a\\nand c interact directly, but a and e interact only indirectly via c.\\n\\nThis chapter has reviewed the basic concepts of probability theory that are\\nmost relevant to deep learning. One more set of fundamental mathematical tools\\nremains: numerical methods.\\n\\n78\\n\\n\\x0c'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_text[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load('en',disable=['parser','ner'])\n",
    "allowed_postags=['NOUN', 'ADJ', 'VERB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sc_text in section_text:\n",
    "    documents = []\n",
    "    doc_tokened = sc_text.split(\".\")\n",
    "    for index,doc in enumerate(doc_tokened):\n",
    "        doc_text_no_punc = simple_preprocess(doc,deacc=True) \n",
    "        tokenized_text_non_stop_words = [ word for word in doc_text_no_punc \\\n",
    "                                         if word not in stop_words]\n",
    "        text_non_stop_words = ' '.join(tokenized_text_non_stop_words)\n",
    "        tokenized_lemmas = nlp(text_non_stop_words)\n",
    "        tokenized_lemmas = [token.lemma_ for token in tokenized_lemmas \\\n",
    "                            if token.pos_ in allowed_postags]\n",
    "        documents.append(tokenized_lemmas)\n",
    "    corpus.append(list(filter(lambda x: len(x) >= 2, documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['many',\n",
       "  'branch',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'deal',\n",
       "  'entity',\n",
       "  'deterministic',\n",
       "  'certain'],\n",
       " ['assume', 'execute', 'machine', 'instruction'],\n",
       " ['error',\n",
       "  'hardware',\n",
       "  'occur',\n",
       "  'rare',\n",
       "  'enough',\n",
       "  'software',\n",
       "  'application',\n",
       "  'need',\n",
       "  'design',\n",
       "  'account'],\n",
       " ['give',\n",
       "  'many',\n",
       "  'computer',\n",
       "  'scientist',\n",
       "  'software',\n",
       "  'engineer',\n",
       "  'work',\n",
       "  'clean',\n",
       "  'certain',\n",
       "  'environment',\n",
       "  'surprising',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'make',\n",
       "  'heavy',\n",
       "  'use',\n",
       "  'probability',\n",
       "  'theory'],\n",
       " ['machine',\n",
       "  'must',\n",
       "  'deal',\n",
       "  'uncertain',\n",
       "  'quantity',\n",
       "  'may',\n",
       "  'need',\n",
       "  'deal',\n",
       "  'stochastic',\n",
       "  'non',\n",
       "  'deterministic',\n",
       "  'quantity'],\n",
       " ['uncertainty', 'stochasticity', 'arise', 'many', 'source'],\n",
       " ['researcher',\n",
       "  'make',\n",
       "  'compelling',\n",
       "  'argument',\n",
       "  'quantify',\n",
       "  'uncertainty',\n",
       "  'use',\n",
       "  'probability',\n",
       "  'least'],\n",
       " ['many', 'argument', 'present', 'summarize', 'inspired', 'pearl'],\n",
       " ['activity', 'require', 'ability', 'reason', 'presence', 'uncertainty'],\n",
       " ['fact',\n",
       "  'mathematical',\n",
       "  'statement',\n",
       "  'true',\n",
       "  'deﬁnition',\n",
       "  'diﬃcult',\n",
       "  'think',\n",
       "  'proposition',\n",
       "  'true',\n",
       "  'event',\n",
       "  'guarantee',\n",
       "  'occur'],\n",
       " ['possible', 'source', 'uncertainty'],\n",
       " ['inherent', 'stochasticity', 'system', 'model'],\n",
       " ['example', 'interpretation', 'mechanic', 'describe', 'particle'],\n",
       " ['create',\n",
       "  'theoretical',\n",
       "  'scenario',\n",
       "  'postulate',\n",
       "  'random',\n",
       "  'dynamic',\n",
       "  'hypothetical',\n",
       "  'card',\n",
       "  'game',\n",
       "  'assume',\n",
       "  'card',\n",
       "  'shuﬄed',\n",
       "  'random',\n",
       "  'order'],\n",
       " ['incomplete', 'observability'],\n",
       " ['deterministic',\n",
       "  'system',\n",
       "  'appear',\n",
       "  'stochastic',\n",
       "  'can',\n",
       "  'observe',\n",
       "  'variable',\n",
       "  'drive',\n",
       "  'behavior',\n",
       "  'system'],\n",
       " ['contestant',\n",
       "  'ask',\n",
       "  'choose',\n",
       "  'door',\n",
       "  'win',\n",
       "  'prize',\n",
       "  'hold',\n",
       "  'choose',\n",
       "  'door'],\n",
       " ['door', 'lead', 'lead', 'car'],\n",
       " ['give',\n",
       "  'contestant',\n",
       "  'choice',\n",
       "  'deterministic',\n",
       "  'contestant',\n",
       "  'point',\n",
       "  'view',\n",
       "  'outcome',\n",
       "  'uncertain'],\n",
       " ['incomplete', 'modeling'],\n",
       " ['use',\n",
       "  'model',\n",
       "  'must',\n",
       "  'discard',\n",
       "  'information',\n",
       "  'observe',\n",
       "  'discard',\n",
       "  'information',\n",
       "  'result',\n",
       "  'uncertainty',\n",
       "  'model',\n",
       "  'prediction'],\n",
       " ['example', 'suppose', 'build', 'robot', 'observe', 'location', 'object'],\n",
       " ['robot',\n",
       "  'discretize',\n",
       "  'space',\n",
       "  'predict',\n",
       "  'future',\n",
       "  'location',\n",
       "  'object',\n",
       "  'discretization',\n",
       "  'make',\n",
       "  'robot',\n",
       "  'become',\n",
       "  'uncertain',\n",
       "  'precise',\n",
       "  'position',\n",
       "  'object',\n",
       "  'object',\n",
       "  'could',\n",
       "  'discrete',\n",
       "  'cell',\n",
       "  'observe',\n",
       "  'occupy'],\n",
       " ['many',\n",
       "  'case',\n",
       "  'practical',\n",
       "  'use',\n",
       "  'simple',\n",
       "  'uncertain',\n",
       "  'rule',\n",
       "  'complex',\n",
       "  'certain',\n",
       "  'true',\n",
       "  'rule',\n",
       "  'deterministic',\n",
       "  'modeling',\n",
       "  'system',\n",
       "  'ﬁdelity',\n",
       "  'accommodate',\n",
       "  'complex',\n",
       "  'rule'],\n",
       " ['simple',\n",
       "  'rule',\n",
       "  'bird',\n",
       "  'cheap',\n",
       "  'develop',\n",
       "  'useful',\n",
       "  'rule',\n",
       "  'form',\n",
       "  'bird',\n",
       "  'young',\n",
       "  'bird',\n",
       "  'learn',\n",
       "  'sick',\n",
       "  'injure',\n",
       "  'bird',\n",
       "  'lose',\n",
       "  'ability',\n",
       "  'ﬂightless',\n",
       "  'specie',\n",
       "  'bird',\n",
       "  'include',\n",
       "  'cassowary',\n",
       "  'ostrich',\n",
       "  'kiwi'],\n",
       " ['expensive',\n",
       "  'develop',\n",
       "  'maintain',\n",
       "  'communicate',\n",
       "  'eﬀort',\n",
       "  'brittle',\n",
       "  'prone',\n",
       "  'failure'],\n",
       " ['give',\n",
       "  'need',\n",
       "  'mean',\n",
       "  'represent',\n",
       "  'reason',\n",
       "  'uncertainty',\n",
       "  'obvious',\n",
       "  'probability',\n",
       "  'theory',\n",
       "  'provide',\n",
       "  'tool',\n",
       "  'want',\n",
       "  'artiﬁcial',\n",
       "  'intelligence',\n",
       "  'application'],\n",
       " ['probability', 'theory', 'develop', 'analyze', 'frequency', 'event'],\n",
       " ['see',\n",
       "  'probability',\n",
       "  'theory',\n",
       "  'use',\n",
       "  'study',\n",
       "  'event',\n",
       "  'draw',\n",
       "  'certain',\n",
       "  'hand',\n",
       "  'card',\n",
       "  'game',\n",
       "  'poker'],\n",
       " ['kind', 'event', 'repeatable'],\n",
       " ['say',\n",
       "  'outcome',\n",
       "  'probability',\n",
       "  'occurring',\n",
       "  'mean',\n",
       "  'repeat',\n",
       "  'experiment'],\n",
       " ['draw',\n",
       "  'hand',\n",
       "  'card',\n",
       "  'many',\n",
       "  'time',\n",
       "  'proportion',\n",
       "  'repetition',\n",
       "  'would',\n",
       "  'result',\n",
       "  'outcome'],\n",
       " ['reasoning', 'seem', 'applicable', 'proposition', 'repeatable'],\n",
       " ['doctor',\n",
       "  'analyze',\n",
       "  'say',\n",
       "  'patient',\n",
       "  'chance',\n",
       "  'mean',\n",
       "  'diﬀerent',\n",
       "  'make',\n",
       "  'many',\n",
       "  'replicas',\n",
       "  'patient',\n",
       "  'reason',\n",
       "  'believe',\n",
       "  'replicas',\n",
       "  'patient',\n",
       "  'would',\n",
       "  'present',\n",
       "  'symptom',\n",
       "  'vary',\n",
       "  'underlie',\n",
       "  'condition'],\n",
       " ['case',\n",
       "  'doctor',\n",
       "  'diagnose',\n",
       "  'patient',\n",
       "  'use',\n",
       "  'probability',\n",
       "  'represent',\n",
       "  'degree',\n",
       "  'belief',\n",
       "  'indicate',\n",
       "  'absolute',\n",
       "  'certainty',\n",
       "  'patient',\n",
       "  'indicate',\n",
       "  'absolute',\n",
       "  'certainty',\n",
       "  'patient',\n",
       "  'ﬂu'],\n",
       " ['former',\n",
       "  'kind',\n",
       "  'probability',\n",
       "  'relate',\n",
       "  'rate',\n",
       "  'event',\n",
       "  'occur',\n",
       "  'know',\n",
       "  'frequentist',\n",
       "  'probability',\n",
       "  'relate',\n",
       "  'qualitative',\n",
       "  'level',\n",
       "  'certainty',\n",
       "  'know',\n",
       "  'bayesian',\n",
       "  'probability'],\n",
       " ['list',\n",
       "  'several',\n",
       "  'property',\n",
       "  'expect',\n",
       "  'common',\n",
       "  'sense',\n",
       "  'reason',\n",
       "  'uncertainty',\n",
       "  'way',\n",
       "  'satisfy',\n",
       "  'property',\n",
       "  'treat',\n",
       "  'probability',\n",
       "  'behave',\n",
       "  'frequentist',\n",
       "  'probability'],\n",
       " ['example',\n",
       "  'want',\n",
       "  'compute',\n",
       "  'probability',\n",
       "  'player',\n",
       "  'win',\n",
       "  'poker',\n",
       "  'game',\n",
       "  'give',\n",
       "  'certain',\n",
       "  'set',\n",
       "  'card',\n",
       "  'use',\n",
       "  'formulas',\n",
       "  'compute',\n",
       "  'probability',\n",
       "  'patient',\n",
       "  'disease',\n",
       "  'give',\n",
       "  'chapter'],\n",
       " ['probability', 'information', 'theory', 'certain', 'symptom'],\n",
       " ['detail',\n",
       "  'small',\n",
       "  'set',\n",
       "  'common',\n",
       "  'sense',\n",
       "  'assumption',\n",
       "  'imply',\n",
       "  'axiom',\n",
       "  'must',\n",
       "  'control',\n",
       "  'kind',\n",
       "  'probability',\n",
       "  'see'],\n",
       " ['probability', 'see', 'extension', 'logic', 'deal', 'uncertainty'],\n",
       " ['logic',\n",
       "  'provide',\n",
       "  'set',\n",
       "  'formal',\n",
       "  'rule',\n",
       "  'determine',\n",
       "  'proposition',\n",
       "  'imply',\n",
       "  'true',\n",
       "  'false',\n",
       "  'give',\n",
       "  'assumption',\n",
       "  'set',\n",
       "  'proposition',\n",
       "  'true',\n",
       "  'false'],\n",
       " ['theory',\n",
       "  'provide',\n",
       "  'set',\n",
       "  'formal',\n",
       "  'rule',\n",
       "  'determine',\n",
       "  'likelihood',\n",
       "  'proposition',\n",
       "  'true',\n",
       "  'give',\n",
       "  'likelihood',\n",
       "  'proposition']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x2115c204c88>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
