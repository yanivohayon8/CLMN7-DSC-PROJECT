{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.models.pipeline import pipeline\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from shutil import copyfile\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "from src.models.train_model import functionsBuilder\n",
    "from src.models.audio import downloadAudioFromYoutube\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundbase_dir = '../data/raw/groundbase'\n",
    "transcripts_dir = os.path.join(groundbase_dir,'transcripts')\n",
    "topic_dataset_path = os.path.join(groundbase_dir,'dataset.csv')\n",
    "transcript_filespath = glob.glob(groundbase_dir + '/transcripts/*.json')\n",
    "\n",
    "'''Read the transcript'''\n",
    "transcripts_jsons = {}\n",
    "for fl in transcript_filespath:\n",
    "    with open(fl,encoding=\"utf8\") as f:\n",
    "        transcript =ast.literal_eval(f.read()) #json.load(f)\n",
    "        vid = fl.split('\\\\')[-1].split('.')[0]\n",
    "        #print(vid)\n",
    "        transcripts_jsons[vid] = transcript\n",
    "#print(transcripts_jsons)\n",
    "\n",
    "'''Read the videos metadata to perform on them the segmentation'''\n",
    "df_videos = pd.read_csv(topic_dataset_path)\n",
    "\n",
    "''' Transfer topic shifts time to seconds units instead HH:MM:SS'''\n",
    "\n",
    "def topic_shifts_seconds(topic_shifts):\n",
    "    tp_shift_sec=[]\n",
    "    for tp in topic_shifts:\n",
    "        intervals = tp.split(':')\n",
    "        seconds = int(intervals[2])\n",
    "        minutes = int(intervals[1]) * 60\n",
    "        hours = int(intervals[0]) * 60 *60\n",
    "        tp_shift_sec.append(seconds + minutes + hours)\n",
    "    return tp_shift_sec\n",
    "\n",
    "\n",
    "for video_id in transcripts_jsons.keys():    \n",
    "    df_videos.at[df_videos['video id'] == video_id,'topic shifts(ends)'] =\\\n",
    "    topic_shifts_seconds(\\\n",
    "                         df_videos[df_videos['video id'] == \\\n",
    "                                   video_id]['topic shifts(ends)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''From the get optimized by bayesian we get that for the video '''\n",
    "# the precision is about 66% \n",
    "vid = 'dkAr9ThdSUU'\n",
    "params = {'n_clusters': 18, 'sim_thresh': 0.6, 'step_size': 49, 'window_size': 150}\n",
    "workflow = 'sliding_window-tfidf-cosine-median_(3,3)-spectral_clustering'\n",
    "\n",
    "groundbase = df_videos.loc[df_videos['video id'] == vid,'topic shifts(ends)'].values.tolist()[:-1]\n",
    "transcripts = transcripts_jsons[vid]\n",
    "#print(grounbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This running may not work at first time do not give up and run it couple of times'''\n",
    "\n",
    "shift_times,topic_words = pipeline.run_for_baye(groundbase,transcripts,slicing_method='sliding_window',\n",
    "                      window_size=params['window_size'],step_size_sd=params['step_size'],\n",
    "                      #silence_threshold=-30,slice_length=1000,step_size_audio=10,wav_file_path=\"../../data/raw/audio/Mod-01 Lec-01 Foundation of Scientific Computing-01.wav\",                \n",
    "                      vector_method='tfidf',vectorizing_params=None,\n",
    "                      similarity_method='cosine',\n",
    "                      filter_params={\"filter_type\":'median',\n",
    "                                     \"mask_shape\":(3,3),\n",
    "                                     \"sim_thresh\":params['sim_thresh'],\n",
    "                                     \"is_min_thresh\":True\n",
    "                                     },\n",
    "                     clustering_params={\n",
    "                             'algorithm':'spectral_clustering',\n",
    "                             'n_clusters':params['n_clusters']\n",
    "                             },return_value='division')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shift_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151, 141, 400, 193, 187, 196, 157, 198, 83, 155, 129, 102, 143, 113, 90, 185]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(tp) for tp in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding top words for topic 0\n",
      "information\n",
      "question\n",
      "random\n",
      "try\n",
      "Finding top words for topic 1\n",
      "encrypt\n",
      "guess\n",
      "information\n",
      "Finding top words for topic 2\n",
      "problem\n",
      "security\n",
      "tail\n",
      "Finding top words for topic 3\n",
      "number\n",
      "probability\n",
      "time\n",
      "Finding top words for topic 4\n",
      "discrete\n",
      "probability\n",
      "value\n",
      "Finding top words for topic 5\n",
      "probability\n",
      "value\n",
      "variable\n",
      "Finding top words for topic 6\n",
      "number\n",
      "probability\n",
      "success\n",
      "Finding top words for topic 7\n",
      "number\n",
      "power\n",
      "time\n",
      "Finding top words for topic 8\n",
      "experiment\n",
      "get\n",
      "power\n",
      "sigma\n",
      "time\n",
      "work\n",
      "Finding top words for topic 9\n",
      "collision\n",
      "consider\n",
      "least\n",
      "probability\n",
      "student\n",
      "Finding top words for topic 10\n",
      "person\n",
      "power\n",
      "probability\n",
      "Finding top words for topic 11\n",
      "equal\n",
      "find\n",
      "people\n",
      "power\n",
      "probability\n",
      "Finding top words for topic 12\n",
      "collision\n",
      "cycle\n",
      "point\n",
      "Finding top words for topic 13\n",
      "complexity\n",
      "cycle\n",
      "equal\n",
      "space\n",
      "tree\n",
      "Finding top words for topic 14\n",
      "collision\n",
      "dash\n",
      "point\n",
      "Finding top words for topic 15\n",
      "bit\n",
      "equal\n",
      "information\n",
      "probability\n",
      "uncertainty\n",
      "value\n"
     ]
    }
   ],
   "source": [
    "for tp_i,tp_words in enumerate(topic_words):\n",
    "    print('Finding top words for topic %s' %(tp_i))\n",
    "    raw_text = ' '.join(tp_words)\n",
    "    myvectorizer = CountVectorizer()\n",
    "    mytf = myvectorizer.fit_transform([raw_text]).toarray()\n",
    "    #print(mytf)\n",
    "    maxes = heapq.nlargest(3,mytf[0])\n",
    "    indexes = []\n",
    "    for i,bal in enumerate(mytf[0]):\n",
    "        if bal in maxes:\n",
    "            indexes.append(i)\n",
    "    [ print(myvectorizer.get_feature_names()[_]) for i,_ in enumerate(indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
