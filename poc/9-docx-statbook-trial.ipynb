{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "from docx import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_italic_bold_docx(doc_path):\n",
    "    document = Document(doc_path)\n",
    "    bolds=[]\n",
    "    italics=[]\n",
    "    full_text = []\n",
    "    font_sizes = []\n",
    "    for para in document.paragraphs:\n",
    "        #print(\"next paragraph:\")\n",
    "        #rint(para.text)\n",
    "        for run in para.runs:\n",
    "            #print('next run')\n",
    "            #print(run.text)\n",
    "            full_text.append(run.text)\n",
    "            font_sizes.append(run.font.size)\n",
    "            if run.italic :\n",
    "                italics.append(run.text)\n",
    "            if run.bold :\n",
    "                bolds.append(run.text)\n",
    "\n",
    "    return {'bold_phrases':bolds,'italic_phrases':italics,'full_text':full_text,'font_sizes':font_sizes}\n",
    "\n",
    "def get_txt_bigger(docx_json):\n",
    "    most_common_font_ = mode(docx_json['font_sizes'])\n",
    "    #return [txt for txt,sz in zip(docx_json['full_text'],docx_json['font_sizes']) if sz > most_common_font_]\n",
    "    return [txt for txt,sz in zip(docx_json['full_text'],docx_json['font_sizes']) if sz > most_common_font_],[i for i,(txt,sz) in enumerate(zip(docx_json['full_text'],docx_json['font_sizes'])) if sz > most_common_font_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chapters_format_statbooks(lines_bigger,key_word_ = 'Chapter'):\n",
    "    topic_titles = []\n",
    "    p_Start_Section = re.compile('{} \\d?'.format(key_word_))\n",
    "    p_subsection = re.compile(r'^((\\d+\\.)+\\d*)$')\n",
    "    p_words = re.compile('[A-Za-z]+')\n",
    "    new_lines = []\n",
    "    for i,doc in enumerate(lines_bigger):\n",
    "        matching = p_Start_Section.match(doc)\n",
    "\n",
    "        if matching is not None:\n",
    "            topic_titles.append(lines_bigger[i+1])\n",
    "\n",
    "        matching = p_subsection.match(doc)\n",
    "        if matching is not None:\n",
    "            matching = p_words.match(lines_bigger[i+1])\n",
    "            if matching is not None:\n",
    "                #lines[i:i+1] = [''.join(lines[i:i+1])]\n",
    "                #topic_titles.append(lines[i])\n",
    "                new_title = (\"%s %s\" %(lines_bigger[i],lines_bigger[i+1]))\n",
    "                topic_titles.append(new_title)\n",
    "                #new_lines.append(new_title)\n",
    "        #new_lines.append(doc)\n",
    "    return topic_titles#,new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_statbook_doc = read_italic_bold_docx('../data/raw/pdf/7kLHJ-F33GI/statbook.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An Introduction to the Science of Statistics:',\n",
       " 'From Theory to Implementation',\n",
       " 'Preliminary Edition',\n",
       " 'c ',\n",
       " 'Joseph C. Watkins',\n",
       " 'Contents',\n",
       " 'i',\n",
       " '',\n",
       " 'ii',\n",
       " '',\n",
       " 'iii',\n",
       " '',\n",
       " 'iv',\n",
       " '',\n",
       " 'v',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '',\n",
       " 'vi',\n",
       " 'Preface',\n",
       " 'Statistical thinking will one day be as necessary a qualification for efficient citizenship as the ability to read and write. – Samuel Wilkes, 1951, paraphrasing H. G. Wells from Mankind in the Making',\n",
       " 'The value of statistical thinking is now accepted by researchers and practitioners from a broad range of endeavors. This viewpoint has become common wisdom in a world of big data. The challenge for statistics educators is to adapt their pedagogy to accommodate the circumstances associated to the information age. This choice of pedagogy should be attuned to the quantitative capabilities and scientific background of the students as well as the intended use of their newly acquired knowledge of statistics.',\n",
       " 'Many university students, presumed to be proficient in college algebra, are taught a variety of procedures and standard tests under a well-developed pedagogy. This approach is sufficiently refined so that students have a good intuitive understanding of the underlying principles presented in the course. However, if the statistical needs presented by a given scientific question fall outside the battery of methods presented in the standard curriculum, then students are typically at a loss to adjust the procedures to accommodate the additional demand.',\n",
       " 'On the other hand, undergraduate students majoring in mathematics frequently have a course on the theory of statistics as a part of their program of study. In this case, the standard curriculum repeatedly finds itself close to the very practically minded subject that statistics is. However, the demands of the syllabus provide very little time to explore these applications with any sustained attention.',\n",
       " 'Our goal is to find a middle ground.',\n",
       " 'Despite the fact that calculus is a routine tool in the development of statistics, the benefits to students who have learned calculus are infrequently employed in the statistics curriculum. The objective of this book is to meet this need with a one semester course in statistics that moves forward in recognition of the coherent body of knowledge provided by statistical theory having an eye consistently on the application of the subject. Such a course may not be able to achieve the same degree of completeness now presented by the two more standard courses described above. However, it ought to able to achieve some important goals:',\n",
       " 'leaving students capable of understanding what statistical thinking is and how to integrate this with scientific procedures and quantitative modeling and',\n",
       " 'learning how to ask statistics experts productive questions, and how to implement their ideas using statistical software and other computational tools.',\n",
       " 'Inevitably, many important topics are not included in this book. In addition, I have chosen to incorporate abbre-viated introductions of some more advanced topics. Such topics can be skipped in a first pass through the material. However, one value of a textbook is that it can serve as a reference in future years. The context for some parts of the exposition will become more clear as students continue their own education in statistics. In these cases, the more advanced pieces can serve as a bridge from this book to more well developed accounts. My goal is not to compose a stand alone treatise, but rather to build a foundation that allows those who have worked through this book to introduce themselves to many exciting topics both in statistics and in its areas of application.',\n",
       " 'vii',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '',\n",
       " 'Who Should Use this Book',\n",
       " 'The major prerequisites are comfort with calculus and a strong interest in questions that can benefit from statistical analysis. Willingness to engage in explorations utilizing statistical software is an important additional requirement. The original audience for the course associated to this book are undergraduate students minoring in mathematics. These student have typically completed a course in multivariate calculus. Many have been exposed to either linear algebra or differential equations. They enroll in this course because they want to obtain a better understanding of their own core subject. Even though we regularly rely on the mechanics of calculus and occasionally need to work with matrices, this is not a textbook for a mathematics course, but rather a textbook that is dedicated to a higher level of understanding of the concepts and practical applications of statistics. In this regard, it relies on a solid grasp of concepts and structures in calculus and algebra.',\n",
       " 'With the advance and adoption of the Common Core State Standards in mathematics, we can anticipate that primary and secondary school students will experience a broader exposure to statistics through their school years. As a consequence, we will need to develop a curriculum for teachers and future teachers so that they can take content in statistics and turn that into curriculum for their students. This book can serve as a source of that content.',\n",
       " 'In addition, those engaged both in industry and in scholarly research are experiencing a surge in the need to design more complex experiments and analyze more diverse data types. Universities and industry are responding with advanced educational opportunities to extend statistics education beyond the theory of probability and statistics, linear models and design of experiments to more modern approaches that include stochastic processes, machine learning and data mining, Bayesian statistics, and statistical computing. This book can serve as an entry point for these critical topics in statistics.',\n",
       " 'An Annotated Syllabus',\n",
       " 'The four parts of the course - organizing and collecting data, an introduction to probability, estimation procedures and hypothesis testing - are the building blocks of many statistics courses. We highlight some of the particular features in this book.',\n",
       " 'Organizing and Collecting Data',\n",
       " 'Much of this is standard and essential - organizing categorical and quantitative data, appropriately displayed as contin-gency tables, bar charts, histograms, boxplots, time plots, and scatterplots, and summarized using medians, quartiles, means, weighted means, trimmed means, standard deviations, correlations and regression lines. We use this as an opportunity to introduce to the statistical software package R and to add additional summaries like the empirical cu-mulative distribution function and the empirical survival function. One example incorporating the use of this is the comparison of the lifetimes of wildtype and transgenic mosquitoes and a discussion of the best strategy to display and summarize data if the goal is to examine the differences in these two genotypes of mosquitoes in their ability to carry and spread malaria. A bit later, we will do an integration by parts exercise to show that the mean of a non-negative continuous random variable is the area under its survival function.',\n",
       " 'Collecting data under a good design is introduced early in the text and discussion of the underlying principles of experimental design is an abiding issue throughout the text. With each new mathematical or statistical concept comes an enhanced understanding of what an experiment might uncover through a more sophisticated design than what was previously thought possible. The students are given readings on design of experiment and examples using R to create a sample under variety of protocols.',\n",
       " 'Introduction to Probability',\n",
       " 'Probability theory is the analysis of random phenomena. It is built on the axioms of probability and is explored, for example, through the introduction of random variables. The goal of probability theory is to uncover properties arising from the phenomena under study. Statistics is devoted to the analysis of data. One goal of statistical science is to',\n",
       " 'viii',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '',\n",
       " 'articulate as well as possible what model of random phenomena underlies the production of the data. The focus of this section of the course is to develop those probabilistic ideas that relate most directly to the needs of statistics.',\n",
       " 'Thus, we must study the axioms and basic properties of probability to the extent that the students understand conditional probability and independence. Conditional probability is necessary to develop Bayes formula which we will later use to give a taste of the Bayesian approach to statistics. Independence will be needed to describe the likelihood function in the case of an experimental design that is based on independent observations. Densities for continuous random variables and mass function for discrete random variables are necessary to write these likelihood functions explicitly. Expectation will be used to standardize a sample sum or sample mean and to perform method of moments estimates.',\n",
       " 'Random variables are developed for a variety of reasons. Some, like the binomial, negative binomial, Poisson or the gamma random variable, arise from considerations based on Bernoulli trials or exponential waiting. The hyperge-ometric random variable helps us understand the difference between sampling with and without replacement. The F , t and chi-square random variables will later become test statistics. Uniform random variables are the ones simulated by random number generators. Because of the central limit theorem, the normal family is the most important among the list of parametric families of random variables.',\n",
       " 'The flavor of the text returns to becoming more authentically statistical with the law of large numbers and the central limit theorem. These are largely developed using simulation explorations and first applied to simple Monte Carlo techniques and importance sampling to estimate the value of an definite integrals. One cautionary tale is an example of the failure of these simulation techniques when applied without careful analysis. If one uses, for example, Cauchy random variables in the evaluation of some quantity, then the simulated sample means can appear to be converging only to experience an abrupt and unpredictable jump. The lack of convergence of an improper integral reveals the difficulty. The central object of study is, of course, the central limit theorem. It is developed both in terms of sample sums and sample means and proportions and used in relatively standard ways to estimate probabilities. However, in this book, we can introduce the delta method which adds ideas associated to the central limit theorem to the context of propagation of error.',\n",
       " 'Estimation',\n",
       " 'In the simplest possible terms, the goal of estimation theory is to answer the question: What is that number? An estimate is a statistic, i. e., a function of the data. We look to two types of estimation techniques - method of moments and maximum likelihood and several criteria for an estimator using, for example, variance and bias. Several examples including mark and recapture and the distribution of fitness effects from genetic data are developed for both types of estimators. The variance of an estimator is approximated using the delta method for method of moments estimators and using Fisher information for maximum likelihood estimators. An analysis of bias is based on quadratic Taylor series approximations and the properties of expectations. Both classes of estimators are often consistent. This implies that the bias decreases towards zero with an increasing number of observations. R is routinely used in simulations to gain insight into the quality of estimators.',\n",
       " 'The point estimation techniques are followed by interval estimation and, notably, by confidence intervals. This brings us to the familiar one and two sample t-intervals for population means and one and two sample z-intervals for population proportions. In addition, we can return to the delta method and the observed Fisher information to construct confidence intervals associated respectively to method of moment estimators and and maximum likelihood estimators. We also add a brief introduction on bootstrap confidence intervals and Bayesian credible intervals in order to provide a broader introduction to strategies for parameter estimation.',\n",
       " 'Hypothesis Testing',\n",
       " 'For hypothesis testing, we first establish the central issues - null and alternative hypotheses, type I and type II errors, test statistics and critical regions, significance and power. We then present the ideas behind the use of likelihood ratio tests as best tests for a simple hypothesis. This is motivated by a game designed to explain the importance of the Neyman Pearson lemma. This approach leads us to well-known diagnostics of an experimental design, notably, the receiver operating characteristic and power curves.',\n",
       " 'ix',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '',\n",
       " 'Extensions of the Neyman Pearson lemma form the basis for the t test for means, the chi-square test for goodness of fit, and the F test for analysis of variance. These results follow from the application of optimization techniques from calculus, including the use of Lagrange multipliers to develop goodness of fit tests. The Bayesian approach to hypothesis testing is explored for the case of simple hypothesis using morphometric measurements, in this case a butterfly wingspan, to test whether a habitat has been invaded by a mimic species.',\n",
       " 'The desire of a powerful test is articulated in a variety of ways. In engineering terms, power is called sensitivity. We illustrate this with a radon detector. An insensitive instrument is a risky purchase. This can be either because the instrument is substandard in the detection of fluctuations or poor in the statistical basis for the algorithm used to determine a change in radon level. An insensitive detector has the undesirable property of not sounding its alarm when the radon level has indeed risen.',\n",
       " 'The course ends by looking at the logic of hypotheses testing and the results of different likelihood ratio analyses applied to a variety of experimental designs. The delta method allows us to extend the resulting test statistics to multivariate nonlinear transformations of the data. The textbook concludes with a practical view of the consequences of this analysis through case studies in a variety of disciplines including, for example, genetics, health, ecology, and bee biology. This will serve to introduce us to the well known t procedure for inference of the mean, both the likelihood-based G',\n",
       " '2',\n",
       " ' test and the traditional chi-square test for discrete distributions and contingency tables, and the',\n",
       " 'test for one-way analysis of variance. We add short descriptions for the corresponding non-parametric procedures, namely, permutation, ranked-sum and signed-rank tests for quantitative data, and exact tests for categorical data',\n",
       " 'Exercises and Problems',\n",
       " 'One obligatory statement in the preface of a book such as this is to note the necessity of working problems. The mate-rial can only be mastered by grappling with the issues through the application to engaging and substantive questions. In this book, we address this imperative through exercises and through problems. The exercises, integrated into the textbook narrative, are of two basic types. The first is largely mathematical or computational exercises that are meant to provide or extend the derivation of a useful identity or data analysis technique. These experiences will prepare the student to perform the calculations that routinely occur in investigations that use statistical thinking. The second type form a collection of questions that are meant to affirm the understanding of a particular concept.',\n",
       " 'Problems are collected at the end of each of the four parts of the book. While the ordering of the problems generally follows the flow of the text, they are designed to be more extensive and integrative. These problems often incorporate several concepts and will call on a variety of problem solving strategies combining handwritten work with the use of statistical software. Without question, the best problems are those that the students chose from their own interests.',\n",
       " 'Acknowledgements',\n",
       " 'The concept that let to this book grew out of a conversation with the late Michael Wells, Professor of Biochemistry at the University of Arizona. He felt that if we are asking future life scientist researchers to take the time to learn calculus and differential equations, we should also provide a statistics course that adds value to their abilities to design experiments and analyze data while reinforcing both the practical and conceptual sides of calculus. As a consequence, course development received initial funding from a Howard Hughes Medical Institute grant (52005889). Christopher Bergevin, an HHMI postdoctoral fellow, provided a valuable initial collaboration.',\n",
       " 'Since that time, I have had the great fortune to be the teacher of many bright and dedicated students whose future contribution to our general well-being is beyond dispute. Their cheerfulness and inquisitiveness has been a source of inspiration for me. More practically, their questions and their persistence led to a much clearer exposition and the addition of many dozens of figures to the text. Through their end of semester projects, I have been introduced to many interesting questions that are intriguing in their own right, but also have added to the range of applications presented throughout the text. Four of these students - Beryl Jones, Clayton Mosher, Laurel Watkins de Jong, and Taylor Corcoran - have gone on to become assistants in the course. I am particularly thankful to these four for their contributions to the dynamical atmosphere that characterizes the class experience.',\n",
       " 'x',\n",
       " 'Part I',\n",
       " 'Organizing and Producing Data',\n",
       " '1',\n",
       " 'Topic 1',\n",
       " 'Displaying Data',\n",
       " 'There are two goals when presenting data: convey your story and establish credibility. - Edward Tufte',\n",
       " 'Statistics is a mathematical science that is concerned with the collection, analysis, interpretation or explanation, and presentation of data. Properly used statistical principles are essential in guiding any inquiry informed by data and, especially in the phase of data exploration, is routinely a fundamental source for discovery and innovation. Insights from data may come from a well conceived visualization of the data, from modern methods of statistical learning and model selection as well as from time-honored formal statistical procedures.',\n",
       " 'The first encounters one has to data are through graphical displays and numerical summaries. The goal is to find an elegant method for this presentation that is at the same time both objective and informative - making clear with a few lines or a few numbers the salient features of the data. In this sense, data presentation is at the same time an art, a science, and an obligation to impartiality.',\n",
       " 'In the section, we will describe some of the standard presentations of data and at the same time, taking the opportu-nity to introduce some of the commands that the software package R provides to draw figures and compute summaries of the data.',\n",
       " '1.1',\n",
       " '\\t',\n",
       " 'Types of Data',\n",
       " 'A data set provides information about a group of individuals. These individuals are, typically, representatives chosen from a population under study. Data on the individuals are meant, either informally or formally, to allow us to make inferences about the population. We shall later discuss how to define a population, how to choose individuals in the population and how to collect data on these individuals.',\n",
       " 'Individuals are the objects described by the data.',\n",
       " 'Variables are characteristics of an individual. In order to present data, we must first recognize the types of data under consideration.',\n",
       " '– Categorical variables partition the individuals into classes. Other names for categorical variables are levels or factors. One special type of categorical variables are ordered categorical variables that suggest a ranking, say small. medium, large or mild, moderate, severe.',\n",
       " '– Quantitative variables are those for which arithmetic operations like addition and differences make sense.',\n",
       " 'Example 1.1 (individuals and variables). We consider two populations - the first is the nations of the world and the second is the people who live in those countries. Below is a collection of variables that might be used to study these populations.',\n",
       " '3',\n",
       " 'Exercise 1.2. Classify the variables as quantitative or categorical in the example above.',\n",
       " 'The naming of variables and their classification as categorical or quantitative may seem like a simple, even trite, exercise. However, the first steps in designing an experiment and deciding on which individuals to include and which information to collect are vital to the success of the experiment. For example, if your goal is to measure the time for an animal (insect, bird, mammal) to complete some task under different (genetic, environmental, learning) conditions, then, you may decide to have a single quantitative variable - the time to complete the task. However, an animal in your study may not attempt the task, may not complete the task, or may perform the task. As a consequence, your data analysis will run into difficulties if you do not add a categorical variable to include these possible outcomes of an experiment.',\n",
       " 'Exercise 1.3. Give examples of variables for the population of vertebrates, of proteins.',\n",
       " '1.2',\n",
       " '\\t',\n",
       " 'Categorical Data',\n",
       " '1.2.1',\n",
       " '\\t',\n",
       " 'Pie Chart',\n",
       " 'A pie chart is a circular chart divided into sectors, illustrating relative magnitudes in frequencies or percents. In a pie chart, the area is proportional to the quantity it represents.',\n",
       " 'Example 1.4. As the nation debates strategies for delivering health insurance, let’s look at the sources of funds and the types of expenditures.',\n",
       " '',\n",
       " 'Figure 1.1: 2008 United States health care (a) expenditures (b) income sources, Source: Centers for Medicare and Medicaid Services, Office of the Actuary, National Health Statistics Group',\n",
       " '4',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Exercise 1.5. How do you anticipate that this pie chart will evolve over the next decade? Which pie slices are likely to become larger? smaller? On what do you base your predictions?',\n",
       " 'Example 1.6. From UNICEF, we read “The proportion of children who reach their fifth birthday is one of the most fundamental indicators of a country’s concern for its people. Child survival statistics are a poignant indicator of the priority given to the services that help a child to flourish: adequate supplies of nutritious food, the availability of high-quality health care and easy access to safe water and sanitation facilities, as well as the family’s overall economic condition and the health and status of women in the community. ”',\n",
       " '',\n",
       " 'Example 1.7. Gene Ontology (GO) project is a bioinformatics initiative whose goal is to provide unified terminology',\n",
       " 'of genes and their products. The project began in 1998 as a collaboration between three model organism databases, Drosophila, yeast, and mouse. The GO Consortium presently includes many databases, spanning repositories for',\n",
       " 'plant, animal and microbial genomes. This project is supported by National Human Genome Research Institute. See',\n",
       " 'http://www.geneontology.org/',\n",
       " '',\n",
       " 'Figure 1.2: ',\n",
       " 'The 25 most frequent Biological Process Gene Ontology (GO) terms.',\n",
       " '5',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'To make a simple pie chart in R for the proportion of AIDS cases among US males by transmission category.',\n",
       " 'males<- c(58,18,16,7,1)',\n",
       " 'pie(males)',\n",
       " 'This many be sufficient for your own personal use. However, if we want to use a pie chart in a presentation, we will have to provide some essential details. For a more descriptive pie chart, one has to become accustomed to learning to interact with the software to settle on a graph that is satisfactory to the situation.',\n",
       " 'Define some colors ideal for black and white print.',\n",
       " 'colors <- c(\"white\",\"grey70\",\"grey90\",\"grey50\",\"black\")',\n",
       " 'Calculate the percentage for each category.',\n",
       " 'male_labels <- round(males/sum(males)',\n",
       " '*',\n",
       " '100, 1)',\n",
       " 'The number 1 indicates rounded to one decimal place.',\n",
       " '> male_labels <- paste(male_labels, \"\\\\%\", sep=\" \")',\n",
       " 'This adds a space and a percent sign.',\n",
       " 'Create a pie chart with defined heading and custom colors and labels and create a legend.',\n",
       " 'pie(males, main=\"Proportion of AIDS Cases among Males by Transmission Category + Diagnosed - USA, 2005\", col=colors, labels=male_labels, cex=0.8)',\n",
       " 'legend(\"topright\", c(\"Male-male contact\",\"Injection drug use (IDU)\",',\n",
       " '\"High-risk heterosexual contact\",\"Male-male contact and IDU\",\"Other\"),',\n",
       " 'cex=0.8,fill=colors)',\n",
       " 'The entry cex=0.8 indicates that the legend has a type set that is 80% of the font size of the main title.',\n",
       " 'Proportion of AIDS Cases among Males by Transmission Category Diagnosed − USA, 2005',\n",
       " '',\n",
       " '',\n",
       " ' Male−male contact',\n",
       " '',\n",
       " ' Injection drug use (IDU)',\n",
       " '58 %                             ',\n",
       " '',\n",
       " ' High−risk heterosexual contact',\n",
       " '',\n",
       " '',\n",
       " ' Male−male contact and IDU',\n",
       " '',\n",
       " ' Other',\n",
       " '',\n",
       " ' 1 %',\n",
       " '',\n",
       " ' 7 %',\n",
       " '18%',\n",
       " '\\t',\n",
       " '16%',\n",
       " '6',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " '1.2.2',\n",
       " '\\t',\n",
       " 'Bar Charts',\n",
       " 'Because the human eye is good at judging linear measures and poor at judging relative areas, a bar chart or bar graph is often preferable to pie charts as a way to display categorical data.',\n",
       " 'To make a simple bar graph in R,',\n",
       " '> barplot(males)',\n",
       " 'For a more descriptive bar chart with information on females:',\n",
       " 'Enter the data for females and create a 5  2 array.',\n",
       " 'females <- c(0,71,27,0,2)',\n",
       " 'hiv<-array(c(males,females), dim=c(5,2))',\n",
       " 'Generate side-by-side bar graphs and create a legend,',\n",
       " 'barplot(hiv, main=\"Proportion of AIDS Cases by Sex and Transmission Category + Diagnosed - USA, 2005\", ylab= \"percent\", beside=TRUE,',\n",
       " '+ names.arg = c(\"Males\", \"Females\"),col=colors)',\n",
       " 'legend(\"topright\", c(\"Male-male contact\",\"Injection drug use (IDU)\",',\n",
       " '\"High-risk heterosexual contact\",\"Male-male contact and IDU\",\"Other\"),',\n",
       " 'cex=0.8,fill=colors)',\n",
       " '',\n",
       " 'Example 1.8. Next we examine a segmented bar plot. This shows the ancestral sources of genes for 75 populations throughout Asia. the data are based on information gathered from 50,000 genetic markers. The designations for the groups were decided by the software package STRUCTURE.',\n",
       " '1.3',\n",
       " '\\t',\n",
       " 'Two-way Tables',\n",
       " 'Relationships between two categorical variables can be shown through a two-way table (also known as a contingency table , cross tabulation table or a cross classifying table ).',\n",
       " '7',\n",
       " '',\n",
       " ' REPORTS',\n",
       " '',\n",
       " '\\n',\n",
       " '',\n",
       " 'Fig. 1. Maximum-likelihood tree of 75 populations. A hypothetical most-recent common ancestor (MRCA) composed of ancestral alleles as inferred from the genotypes of one gorilla and 21 chimpanzees was used to root the tree. Branches with bootstrap values less than 50% were condensed. Population identification numbers (IDs), sample collection locations with latitudes and longitudes, ethnicities, language spoken, and size of pop-ulation samples are shown in the table adjacent to each branch in the tree. Linguistic groups are indicated with colors as shown in the legend. All',\n",
       " '\\n',\n",
       " 'population IDs except the four HapMap samples are denoted by four characters. The first two letters indicate the country where the samples were collected or (in the case of Affymetrix) genotyped, according to the following convention: AX, Affymetrix; CN, China; ID, Indonesia; IN, India; JP, Japan; KR, Korea; MY, Malaysia; PI, the Philippines; SG, Singapore; TH, Thailand; and TW, Taiwan. The last two letters are unique IDs for the population. To the right of the table, an averaged graph of results from STRUCTURE is shown for K = 14.',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " '',\n",
       " '\\n',\n",
       " 'does not smoke',\n",
       " 'smokes',\n",
       " '',\n",
       " '2 parents',\n",
       " '\\t',\n",
       " '1 parent',\n",
       " '\\t',\n",
       " '0 parents',\n",
       " 'Example 1.9. In 1964, Surgeon General Dr. Luther Leonidas Terry published a landmark report saying that smoking may be hazardous to health. This led to many influential reports on the topic, including the study of the smoking habits of 5375 high school children in Tucson in 1967. Here is a two-way table summarizing some of the results.',\n",
       " 'The row variable is the parents smoking habits.',\n",
       " 'The column variable is the student smoking habits.',\n",
       " 'The cells display the counts for each of the categories of row and column variables.',\n",
       " 'A two-way table with r rows and c columns is often called an r by c table (written r',\n",
       " '\\t',\n",
       " 'c).',\n",
       " 'The totals along each of the rows and columns give the marginal distributions. We can create a segmented bar graph as follows:',\n",
       " 'smoking<-matrix(c(400,1380,416,1823,188,1168),ncol=3)',\n",
       " 'colnames(smoking)<-c(\"2 parents\",\"1 parent\", \"0 parents\")',\n",
       " 'rownames(smoking)<-c(\"smokes\",\"does not smoke\")',\n",
       " 'smoking',\n",
       " '> barplot(smoking,legend=rownames(smoking))',\n",
       " '9',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Example 1.10. Hemoglobin E is a variant of hemoglobin with a mutation in the globin gene causing substitution of glutamic acid for lysine at position 26 of the globin chain. HbE (E is the one letter abbreviation for glutamic acid.) is the second most common abnormal hemoglobin after sickle cell hemoglobin (HbS). HbE is common from India to Southeast Asia. The chain of HbE is synthesized at a reduced rate compare to normal hemoglobin (HbA) as the HbE produces an alternate splicing site within an exon.',\n",
       " 'It has been suggested that Hemoglobin E provides some protection against malaria virulence when heterozygous,',\n",
       " 'but is causes anemia when homozygous. The circumstance in which the heterozygotes for the alleles under considera-tion have a higher adaptive value than the homozygote is called balancing selection.',\n",
       " 'The table below gives the counts of differing hemoglobin genotypes on two Indonesian islands.',\n",
       " 'Because the heterozygotes are rare on Flores, it appears malaria is less prevalent there since the heterozygote does not provide an adaptive advantage.',\n",
       " 'Exercise 1.11. Make a segmented barchart of the data on hemoglobin genotypes. Have each bar display the distribu-tion of genotypes on the two Indonesian islands.',\n",
       " '1.4',\n",
       " '\\t',\n",
       " 'Histograms and the Empirical Cumulative Distribution Function',\n",
       " 'Histograms are a common visual representation of a quantitative variable. Histograms summarize the data using rectangles to display either frequencies or proportions as normalized frequencies. In making a histogram, we',\n",
       " 'Divide the range of data into bins of equal width (usually, but not always). Count the number of observations in each class.',\n",
       " 'Draw the histogram rectangles representing frequencies or percents by area. Interpret the histogram by giving',\n",
       " 'the overall pattern',\n",
       " '– the center',\n",
       " '– the spread',\n",
       " '– the shape (symmetry, skewness, peaks)',\n",
       " 'and deviations from the pattern',\n",
       " '– outliers',\n",
       " '– gaps',\n",
       " 'The direction of the skewness is the direction of the longer of the two tails (left or right) of the distribution.',\n",
       " 'No one choice for the number of bins is considered best. One possible choice for larger data sets is Sturges’ formula to choose b1 + log',\n",
       " '2',\n",
       " ' nc bins. (b c, the floor function, is obtained by rounding down to the next integer.)',\n",
       " 'Exercise 1.12. The histograms in Figure 1.4 shows the distribution of lengths of a normal strain and mutant strain of Bacillus subtilis. Describe the distributions.',\n",
       " 'Example 1.13. Taking the age of the presidents of the United States at the time of their inauguration and creating its histogram, empirical cumulative distribution function and boxplot in R is accomplished as follows.',\n",
       " '10',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Figure 1.4: Histogram of lengths of Bacillus subtilis. Solid lines indicate wild type and dashed line mutant strain.',\n",
       " 'age<- c(57,61,57,57,58,57,61,54,68,51,49,64,50,48,65,52,56,46,54,49,51,47,55,55, 54,42,51,56,55,51,54,51,60,61,43,55,56,61,52,69,64,46,54,47,70)',\n",
       " 'par(mfrow=c(1,2))',\n",
       " 'hist(age)',\n",
       " 'plot(ecdf(age),xlab=\"age\",main=\"Age of Presidents at the Time of Inauguaration\", sub=\"Empriical Cumulative Distribution Function\")',\n",
       " 'Histogram of age',\n",
       " '\\t',\n",
       " 'Age of Presidents at Inauguaration',\n",
       " '',\n",
       " '\\n',\n",
       " '40',\n",
       " '\\t',\n",
       " '45',\n",
       " '\\t',\n",
       " '50',\n",
       " '\\t',\n",
       " '55',\n",
       " '\\t',\n",
       " '60',\n",
       " '\\t',\n",
       " '65',\n",
       " '\\t',\n",
       " '70',\n",
       " '',\n",
       " 'age',\n",
       " 'Empriical Cumulative Distribution Function',\n",
       " 'So the age of presidents at the time of inauguration range from the early forties to the late sixties with the frequency starting their tenure peaking in the early fifties. The histogram in generally symmetric about 55 years with spread from around 40 to 70 years.',\n",
       " 'The empirical cumulative distribution function F',\n",
       " 'n',\n",
       " '(x) gives, for each value x, the fraction of the data less than or equal to x. If the number of observations is n, then',\n",
       " 'F',\n",
       " 'n',\n",
       " '(x) = ',\n",
       " 'n',\n",
       " '1',\n",
       " ' #(observations less than or equal to x):',\n",
       " '',\n",
       " '11',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Thus, F',\n",
       " 'n',\n",
       " '(x) = 0 for any value of x less than all of the observed values and F',\n",
       " 'n',\n",
       " '(x) = 1 for any x greater than all of the observed values. In between, we will see jumps that are multiples of the 1=n. For example, in the empirical cumulative distribution function for the age of the presidents, we will see a jump of size 4=45 at x = 57 to indicate the fact that 4 of the 44 presidents were 57 at the time of their inauguration.',\n",
       " 'For an alternative method to create a graph of the empirical cumulative distribution function, first place the observations in order from smallest to largest. For the age of presidents data, we can accomplish this in R by writing sort(age). Next match these up with the integral multiples of the 1 over the number of observations. In R, we enter 1:length(age)/length(age). Finally, type=\"s\" to give us the steps described above.',\n",
       " 'plot(sort(age),1:length(age)/length(age),type=\"s\",ylim=c(0,1), main = c(\"Age of Presidents at the Time of Inauguration\"), sub=(\"Empiricial Cumulative Distribution Function\"), xlab=c(\"age\"),ylab=c(\"cumulative fraction\"))',\n",
       " 'Exercise 1.14. Give the fraction of presidents whose age at inauguration was under 60. What is the range for the age at inauguration of the youngest fifth of the presidents?',\n",
       " 'Exercise 1.15. The histogram for data on the length of three bacterial strains is shown below. Lengths are given in microns. Below the histograms (but not necessarily directly below) are empirical cumulative distribution functions corresponding to these three histograms.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Match the histograms to their respective empirical cumulative distribution functions.',\n",
       " 'In looking at life span data, the natural question is “What fraction of the individuals have survived a given length of time?” The survival function S',\n",
       " 'n',\n",
       " '(x) gives, for each value x, the fraction of the data greater than or equal to x. If the number of observations is n, then',\n",
       " '12',\n",
       " '',\n",
       " '20',\n",
       " '\\t',\n",
       " '25',\n",
       " '\\t',\n",
       " '30',\n",
       " '\\t',\n",
       " '35',\n",
       " '\\t',\n",
       " '40',\n",
       " 'average age of the parent',\n",
       " '1.5',\n",
       " '\\t',\n",
       " 'Scatterplots',\n",
       " 'We now consider two dimensional data. The values of the first variable x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ' are assumed known and in an experiment and are often set by the experimenter. This variable is called the explanatory, predictor, discriptor, or input variables and in a two dimensional scatterplot of the data display its values on the horizontal axis. The values y',\n",
       " '1',\n",
       " '; y',\n",
       " '2',\n",
       " ' : : : ; y',\n",
       " 'n',\n",
       " ', taken from observations with input x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ' are called the response or target variable and its values are displayed on the vertical axis. In describing a scatterplot, take into consideration',\n",
       " 'the form, for example,',\n",
       " '– linear',\n",
       " '– curved relationships',\n",
       " '– clusters',\n",
       " 'the direction,',\n",
       " '– a positive or negative association',\n",
       " 'and the strength of the aspects of the scatterplot.',\n",
       " 'Example 1.16. Genetic evolution is based on mutation. Consequently, one fundamental question in evolutionary biology is the rate of de novo mutations. To investigate this question in humans, Kong et al, sequenced the entire genomes of 78 Icelandic trios and recorded the age of the parents and the number of de novo mutations in the offspring.',\n",
       " 'The plot shows a moderate positive linear association, children of older parent have, on average, more mutations. The number of mutations range from 40 for children of younger parents to 100 for children of older parents. We will later learn that the father is the major source of this difference with age.',\n",
       " 'Example 1.17 (Fossils of the Archeopteryx). The name Archeopteryx derives from the ancient Greek meaning “ancient feather” or “ancient wing”. Archeopteryx is generally accepted by palaeontologists as being the oldest known bird. Archaeopteryx lived in the Late Jurassic Period around 150 million years ago, in what is now southern Germany',\n",
       " 'during a time when Europe was an archipelago of islands in a shallow warm tropical sea. The first complete specimen of Archaeopteryx was announced in 1861, only two years after Charles Darwin published On the Origin of Species,',\n",
       " '13',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'and thus became a key piece of evidence in the debate over evolution. Below are the lengths in centimeters of the femur and humerus for the five specimens of Archeopteryx that have preserved both bones.',\n",
       " 'femur<-c(38,56,59,64,74)',\n",
       " 'humerus<-c(41,63,70,72,84)',\n",
       " 'plot(femur, humerus,main=c(\"Bone Lengths for Archeopteryx\"))',\n",
       " 'Unless we have a specific scientific question, we have no real reason for a choice of the explanatory variable.',\n",
       " 'Bone Lengths for Archeopteryx',\n",
       " '',\n",
       " '\\n',\n",
       " '●',\n",
       " '●',\n",
       " '●',\n",
       " '●',\n",
       " '●',\n",
       " '40',\n",
       " '\\t',\n",
       " '45',\n",
       " '\\t',\n",
       " '50',\n",
       " '\\t',\n",
       " '55',\n",
       " '\\t',\n",
       " '60',\n",
       " '\\t',\n",
       " '65',\n",
       " '\\t',\n",
       " '70',\n",
       " '\\t',\n",
       " '75',\n",
       " 'femur',\n",
       " 'Describe the scatterplot.',\n",
       " 'Example 1.18. This historical data show the 20 largest banks in 1974. Values given in billions of dollars.',\n",
       " '14',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Income vs. Assets (in billions of dollars)',\n",
       " '\\n',\n",
       " 'assets',\n",
       " 'Describe the scatterplot.',\n",
       " 'In 1972, Michele Sindona, a banker with close ties to the Mafia, along with a purportedly bogus Freemasonic lodge, and the Nixon administration purchased controlling interest in Bank 19, Long Island’s Franklin National Bank. As a result of his acquisition of a controlling stake in Franklin, Sindona had a money laundering operation to aid his alleged ties to Vatican Bank and the Sicilian drug cartel. Sindona used the bank’s ability to transfer funds, produce letters of credit, and trade in foreign currencies to begin building a banking empire in the United States. In mid-1974, management revealed huge losses and depositors started taking out large withdrawals, causing the bank to have to borrow over $1 billion from the Federal Reserve Bank. On 8 October 1974, the bank was declared insolvent due to mismanagement and fraud, involving losses in foreign currency speculation and poor loan policies.',\n",
       " 'What would you expect to be a feature on this scatterplot of a failing bank? Does the Franklin Bank have this feature?',\n",
       " '1.6',\n",
       " '\\t',\n",
       " 'Time Plots',\n",
       " 'Some data sets come with an order of events, say ordered by time.',\n",
       " 'Example 1.19. The modern history of petroleum began in the 19th century with the refining of kerosene from crude oil. The world’s first commercial oil wells were drilled in the 1850s in Poland and in Romania.The first oil well in North America was in Oil Springs, Ontario, Canada in 1858. The US petroleum industry began with Edwin Drake’s drilling of a 69-foot deep oil well in 1859 on Oil Creek near Titusville, Pennsylvania for the Seneca Oil Company. The industry grew through the 1800s, driven by the demand for kerosene and oil lamps. The introduction of the internal combustion engine in the early part of the 20th century provided a demand that has largely sustained the industry to this day. Today, about 90% of vehicular fuel needs are met by oil. Petroleum also makes up 40% of total energy consumption in the United States, but is responsible for only 2% of electricity generation. Oil use increased exponentially until the world oil crises of the 1970s.',\n",
       " '15',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Worldwide Oil Production',\n",
       " 'With the data given in two columns oil and year, the time plot plot(year,oil,type=\"b\") is given on the left side of the figure below. This uses type=\"b\" that puts both lines and circles on the plot.',\n",
       " 'World Oil Production',\n",
       " '',\n",
       " '\\n',\n",
       " '',\n",
       " 'year',\n",
       " '\\t',\n",
       " 'year',\n",
       " 'Figure 1.5: Oil production (left) and the logarithm of oil production (right) from 1880 to 1988.',\n",
       " 'Sometimes a transformation of the data can reveal the structure of the time series. For example, if we wish to examine an exponential increase displayed in the oil production plot, then we can take the base 10 logarithm of the production and give its time series plot. This is shown in the plot on the right above. (In R, we write log(x) for the natural logarithm and log(x,10) for the base 10 logarithm.)',\n",
       " 'Exercise 1.20. What happened in the mid 1970s that resulted in the long term departure from exponential growth in the use of oil?',\n",
       " '16',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Example 1.21. The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body tasked with evaluating the risk of climate change caused by human activity. The panel was established in 1988 by the World Meteorological Organization and the United Nations Environment Programme, two organizations of the United Nations. The IPCC does not perform original research but rather uses three working groups who synthesize research and prepare a report. In addition, the IPCC prepares a summary report. The Fourth Assessment Report (AR4) was completed in early 2007. The fifth was released in 2014.',\n",
       " 'Below is the first graph from the 2007 Climate Change Synthesis Report: Summary for Policymakers.',\n",
       " 'The technique used to draw the curves on the graphs is called local regression. At the risk of discussing concepts that have not yet been introduced, let’s describe the technique behind local regression. Typically, at each point in the data set, the goal is to draw a linear or quadratic function. The function is determined using weighted least squares, giving most weight to nearby points and less weight to points further away. The graphs above show the approximating curves. The blue regions show areas within two standard deviations of the estimate (called a confidence interval). The goal of local regression is to provide a smooth approximation to the data and a sense of the uncertainty of the data. In practice, local regression requires a large data set to work well.',\n",
       " '',\n",
       " '17',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " 'Example 1.22. The next figure give a time series plot of a single molecule experiment showing the movement of kinesin along a microtubule. In this case the kinesin has at its foot a glass bead and its heads are attached to a microtubule. The position of the glass bead is determined by using a laser beam and the optical properties of the bead to locate the bead and provide a force on the kinesin molecule. In this time plot, the load on the microtubule has a force of 3.5 pN and the concentration of ATP is 100 M. What is the source of fluctuations in this time series plot of bead position? How would you expect this time plot to change with changes in ATP concentration and with changes in force?',\n",
       " '',\n",
       " '1.7',\n",
       " '\\t',\n",
       " 'Answers to Selected Exercises',\n",
       " '1.11. Here are the R commands:',\n",
       " 'genotypes<-matrix(c(128,6,0,119,78,4),ncol=2)',\n",
       " 'colnames(genotypes)<-c(\"Flores\",\"Sumba\")',\n",
       " 'rownames(genotypes)<-c(\"AA\",\"AE\",\"EE\")',\n",
       " 'genotypes',\n",
       " 'Flores Sumba',\n",
       " 'AA',\n",
       " '\\t',\n",
       " '128',\n",
       " '\\t',\n",
       " '119',\n",
       " 'AE',\n",
       " '\\t',\n",
       " '6',\n",
       " '\\t',\n",
       " '78',\n",
       " 'EE',\n",
       " '\\t',\n",
       " '0',\n",
       " '\\t',\n",
       " '4',\n",
       " 'barplot(genotypes,legend=rownames(genotypes),args.legend=list(x=\"topleft\")) The legend was moved to the left side to avoid crowding with the taller bar for the data on Sumba.',\n",
       " '18',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Displaying Data',\n",
       " '',\n",
       " '\\n',\n",
       " 'EE',\n",
       " '',\n",
       " 'AE',\n",
       " 'AA',\n",
       " 'Flores',\n",
       " '\\t',\n",
       " 'Sumba',\n",
       " '1.12. The lengths of the normal strain has its center at 2.5 microns and range from 1.5 to 5 microns. It is somewhat skewed right with no outliers. The mutant strain has its center at 5 or 6 microns. Its range is from 2 to 14 microns and it is slightly skewed right. It has not outliers.',\n",
       " '1.14. Look at the graph to the point above the value 60 years. Look left from this point to note that it corresponds to a value of 0.80.',\n",
       " 'Look at the graph to the point right from the value 0.20. Look down to note that it corresponds to 49 years. .',\n",
       " '1.15. Match histogram wild1f to wilddaf. Note that both show the range is from 2 to 5 microns and that about half of the data lies between 2 and 3 microns. Match histogram wild2f with wildcf. The data is relatively uniform from 3.5 to 6.5 microns. Finally, match histogram wild3f with wildbf. The range is from 2 to 8 microns with most of the data between 3 and 6 microns. .',\n",
       " '1.22. The fluctuation are due to the many bombardments with other molecules in the cell, most frequently, water molecules.',\n",
       " 'As force increases, we expect the velocity to increase - to a point. If the force is too large, then the kinesin is ripped away from the microtubule. As ATP concentration increases, we expect the velocity to increase - again, to a point. If ATP concentration is sufficiently large, then the biochemical processes are saturated.',\n",
       " '19',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " '20',\n",
       " 'Topic 2',\n",
       " 'Describing Distributions with Numbers',\n",
       " 'There are three kinds of lies: lies, damned lies, and statistics. - Benjamin Disraeli',\n",
       " 'It is easy to lie with statistics. It is hard to tell the truth without it. - Andrejs Dunkels',\n",
       " 'We next look at quantitative data. Recall that in this case, these data can be subject to the operations of arithmetic.',\n",
       " 'In particular, we can add or subtract observation values, we can sort them and rank them from lowest to highest.',\n",
       " 'We will look at two fundamental properties of these observations. The first is a measure of the center value for the data, i.e., the median or the mean. Associated to this measure, we add a second value that describes how these observations are spread or dispersed about this given measure of center.',\n",
       " 'The median is the central observation of the data after it is sorted from the lowest to highest observations. In addition, to give a sense of the spread in the data, we often give the smallest and largest observations as well as the observed value that is 1/4 and 3/4 of the way up this list, known at the first and third quartiles. This difference, known as the interquartile range is a measure of the spread or the dispersion of the data. For the mean, we commonly use the standard deviation to describe the spread of the data.',\n",
       " 'These concepts are described in more detail in this section.',\n",
       " '2.1',\n",
       " '\\t',\n",
       " 'Measuring Center',\n",
       " '2.1.1',\n",
       " '\\t',\n",
       " 'Medians',\n",
       " 'The median take the middle value for x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ' after the data has been sorted from smallest to largest,',\n",
       " 'x',\n",
       " '(1)',\n",
       " '; x',\n",
       " '(2)',\n",
       " '; : : : ; x',\n",
       " '(n)',\n",
       " ':',\n",
       " '(x',\n",
       " '(k)',\n",
       " ' is called the k-th order statistic. Sorting can be accomplished in R by using the sort command.)',\n",
       " 'If n is odd, then this is just the value of the middle observation x',\n",
       " '((n+1)=2)',\n",
       " '. If n is even, then the two values closest to the center are averaged.',\n",
       " '1',\n",
       " '2',\n",
       " ' ',\n",
       " '(x',\n",
       " '(n=2) ',\n",
       " '+',\n",
       " ' ',\n",
       " 'x',\n",
       " '(n=2+1)',\n",
       " '):',\n",
       " '',\n",
       " 'If we store the data in R in a vector x, we can write median(x) to compute the median.',\n",
       " '2.1.2',\n",
       " '\\t',\n",
       " 'Means',\n",
       " 'For a collection of numeric data, x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ', the sample mean is the numerical average',\n",
       " '21',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'Alternatively, if the value x occurs n(x) times in the data, then use the distributive property to see that',\n",
       " 'So the mean x depends only on the proportion of observations p(x) for each value of x.',\n",
       " 'Example 2.1. For the data set f1; 2; 2; 2; 3; 3; 4; 4; 4; 5g; we have n = 10 and the sum',\n",
       " '1 + 2 + 2 + 2 + 3 + 3 + 4 + 4 + 4 + 5 = 1n(1) + 2n(2) + 3n(3) + 4n(4) + 5n(5)',\n",
       " '= 1(1) + 2(3) + 3(2) + 4(3) + 5(1) = 30',\n",
       " 'Thus, x = 30=10 = 3.',\n",
       " 'Example 2.2. For the data on the length in microns of wild type Bacillus subtilis data, we have',\n",
       " 'So the sample mean x = 2:49.',\n",
       " 'If we store the data in R in a vector x, we can write mean(x) which is equal to sum(x)/length(x) to compute the mean.',\n",
       " 'To extend this idea a bit, we can take a real-valued function h and instead consider the observations h(x',\n",
       " '1',\n",
       " '); h(x',\n",
       " '2',\n",
       " '); : : : ; h(x',\n",
       " 'n',\n",
       " '), then',\n",
       " 'Exercise 2.3. Let x',\n",
       " 'n',\n",
       " ' be the sample mean for the quantitative data x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " '. For an additional observation x',\n",
       " 'n+1',\n",
       " ', use x to give a formula for x',\n",
       " 'n+1',\n",
       " ', the mean of n + 1 observations. Generalize this formula for the case of k additional observations x',\n",
       " 'n+1',\n",
       " ' : : : ; x',\n",
       " 'n+k',\n",
       " 'Many times, we do not want to give the same weight to each observation. For example, in computing a student’s grade point average, we begin by setting values x',\n",
       " 'i',\n",
       " ' corresponding to grades ( A 7!4, B 7!3 and so on) and giving weights w',\n",
       " '1',\n",
       " '; w',\n",
       " '2',\n",
       " '; : : : ; w',\n",
       " 'n',\n",
       " ' equal to the number of units in a course. We then compute the grade point average as a weighted mean. To do this:',\n",
       " 'Multiply the value of each course by its weight x',\n",
       " 'i',\n",
       " 'w',\n",
       " 'i',\n",
       " '. This is called the number of quality points for the course. Add up the quality points:',\n",
       " 'X',\n",
       " 'n',\n",
       " 'x',\n",
       " '1',\n",
       " 'w',\n",
       " '1',\n",
       " ' + x',\n",
       " '2',\n",
       " 'w',\n",
       " '2',\n",
       " ' + : : : + x',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " ' =',\n",
       " '\\t',\n",
       " 'x',\n",
       " 'i',\n",
       " 'w',\n",
       " 'i',\n",
       " 'i=1',\n",
       " 'Add up the weights, i. e., the number of units attempted:',\n",
       " 'X',\n",
       " 'n',\n",
       " 'w',\n",
       " '1',\n",
       " ' + w',\n",
       " '2',\n",
       " ' + : : : + w',\n",
       " 'n',\n",
       " ' =',\n",
       " '\\t',\n",
       " 'w',\n",
       " 'i',\n",
       " 'i=1',\n",
       " '22',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'Figure 2.1: Empirical Survival Function for the Bacterial Data. This figure displays how the area under the survival function to the right of the y-axis and above the x-axis is the mean value x for non-negative data. For x = 1:5; 2:0; 2:5; 3:0; 3:5; 4:0, and 4.5. This area is the sum of the area of the retangles displayed. The width of each of the rectangles is x and the height is equal to p(x). Thus, the area is the product xp(x). The sum of these areas are presented in Example 2.2 to compute the sample mean.',\n",
       " 'Divide the total quality points by the number of units attempted:',\n",
       " 'X',\n",
       " 'n',\n",
       " 'p',\n",
       " 'j',\n",
       " ' = w',\n",
       " 'j',\n",
       " '=',\n",
       " '\\t',\n",
       " 'w',\n",
       " 'i',\n",
       " 'i=1',\n",
       " 'be the proportion or fraction of the weight given to the j-th observation, then we can rewrite (2.1) as X',\n",
       " 'n',\n",
       " 'x',\n",
       " 'i',\n",
       " 'p',\n",
       " 'i',\n",
       " ':',\n",
       " 'i=1',\n",
       " 'If we store the weights in a vector w, then we can compute the weighted mean using weighted.mean(x,w)',\n",
       " 'If an extremely high observation is changed to be even higher, then the mean follows this change while the median does not. For this reason, the mean is said to be sensitive to outliers while the median is not. To reduce the impact of extreme outliers on the mean as a measure of center, we can also consider a truncated mean or trimmed mean. The p trimmed mean is obtained by discarding both the lower and the upper p 100% of the data and taking the arithmetic mean of the remaining data.',\n",
       " 'In R, we write mean(x, trim = p) where p, a number between 0 and 0.5, is the fraction of observations to be trimmed from each end before the mean is computed.',\n",
       " 'Note that the median can be regarded as the 50% trimmed mean. The median does not change with a changes in the extreme observations. Such a property is called a resistant measure. On the other hand, the mean is not a resistant measure.',\n",
       " '23',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'Exercise 2.4. Give the relationship between the median and the mean for a (a) left skewed, (b) symmetric, or (c) right skewed distribution.',\n",
       " '2.2',\n",
       " '\\t',\n",
       " 'Measuring Spread',\n",
       " '2.2.1',\n",
       " '\\t',\n",
       " 'Five Number Summary',\n",
       " 'The first and third quartile, Q',\n",
       " '1',\n",
       " ' and Q',\n",
       " '3',\n",
       " ', are, respectively, the median of the lower half and the upper half of the data. The five number summary of the data are the values of the minimum, Q',\n",
       " '1',\n",
       " ', the median, Q',\n",
       " '3',\n",
       " ' and the maximum. These values, along with the mean, are given in R using summary(x). Returning to the data set on the age of presidents:',\n",
       " '> summary(age)',\n",
       " 'Min. 1st Qu. Median Mean 3rd Qu. Max. 42.00 51.00 55.00 54.98 58.00 70.00',\n",
       " 'We can display the five number summary using a boxplot.',\n",
       " '> boxplot(age, main = c(\"Age of Presidents at the Time of Inauguration\"))',\n",
       " 'Age of Presidents at the Time of Inauguration',\n",
       " '',\n",
       " 'The value Q',\n",
       " '3',\n",
       " '\\t',\n",
       " 'Q',\n",
       " '1',\n",
       " ' is called the interquartile range and is denoted by IQR. It is found in R with the command IQR.',\n",
       " 'Outliers are somewhat arbitrarily chosen to be those above Q',\n",
       " '3',\n",
       " ' + ',\n",
       " '3',\n",
       " '2',\n",
       " ' IQR and below Q',\n",
       " '1',\n",
       " ' ',\n",
       " '3',\n",
       " '2',\n",
       " ' IQR. With this criterion, the ages of Ronald Reagan and Donald Trump, considered outliers, are displayed by the two circles at the top of the boxplot. The boxplot command has the default value range = 1.5 in the choice of displaying outliers. This can be altered to loosen or tighten this criterion.',\n",
       " 'Exercise 2.5. Use the range command to create a boxplot for the age of the presidents at the time of their inaugu-ration using as outliers any value above Q',\n",
       " '3',\n",
       " ' + IQR and below Q',\n",
       " '1',\n",
       " ' IQR as the criterion for outliers. How many outliers does this boxplot have?',\n",
       " '24',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'Example 2.6. Consider a two column data set. Column 1 - MPH - gives car gas milage. Column 2 - origin - gives the country of origin for the car. We can create side by side boxplots with the command',\n",
       " '> boxplot(MPG,Origin)',\n",
       " 'to produce',\n",
       " '',\n",
       " '2.2.2',\n",
       " '\\t',\n",
       " 'Sample Variance and Standard Deviation',\n",
       " 'The sample variance averages the square of the differences from the mean',\n",
       " 'The sample standard deviation, s',\n",
       " 'x',\n",
       " ', is the square root of the sample variance. We shall soon learn the rationale for the decision to divide by n 1. However, we shall also encounter circumstances in which division by n is preferable. We will routinely drop the subscript x and write s to denote standard deviation if there is no ambiguity.',\n",
       " 'Example 2.7. For the data set on Bacillus subtilis data, we have x = 498=200 = 2:49',\n",
       " 'So the sample variance s',\n",
       " '2',\n",
       " 'x',\n",
       " ' = 90:48=199 = 0:4546734 and standard deviation s',\n",
       " 'x',\n",
       " ' = 0:6742947. To accomplish this in R',\n",
       " '25',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'bacteria<-c(rep(1.5,18),rep(2.0,71),rep(2.5,48),rep(3,37),rep(3.5,16),rep(4,6), + rep(4.5,4))',\n",
       " 'length(bacteria)',\n",
       " '[1] 200',\n",
       " 'mean(bacteria) [1] 2.49',\n",
       " 'var(bacteria) [1] 0.4546734',\n",
       " 'sd(bacteria) [1] 0.6742947',\n",
       " 'For quantitative variables that take on positive values, we can take the ratio of the standard deviation to the mean cv',\n",
       " 'x',\n",
       " ' = ',\n",
       " 's',\n",
       " 'x',\n",
       " 'x',\n",
       " ' ;',\n",
       " '',\n",
       " 'called the coefficient of variation as a measure of the relative variability of the observations. Note that cv',\n",
       " 'x',\n",
       " ' is a pure number and has no units.',\n",
       " 'For the data of bacteria lengths, the coefficient of variability is',\n",
       " 'introduce the next exercise, define the sum of squares about the value',\n",
       " '\\t',\n",
       " ',',\n",
       " 'X',\n",
       " 'n',\n",
       " 'SS( ) =   (x',\n",
       " 'i',\n",
       " '    )',\n",
       " '2',\n",
       " ':',\n",
       " 'i=1',\n",
       " 'Exercise 2.9. Flip a fair coin 16 times, recording the number of heads. Repeat this activity 20 times, giving x',\n",
       " '1',\n",
       " '; : : : ; x',\n",
       " '20',\n",
       " ' heads. Our instincts say that the mean should be 8. Compute SS(8). Next find x for the data you generated and compute SS(x). Notice that SS(8) > SS(x).',\n",
       " 'Note that in repeating the experiment of flipping a fair coin 16 times and recording the number of heads, we would like to compute the variation about 8, the value that our intuition tells us is the true mean. In many circumstances, we do not have such intuition. Thus, we doing the best we can by computing x, the mean from the data. In this case, the variation about the sample mean is smaller than the variation about what may be called a true mean. Thus, division of ',\n",
       " 'P',\n",
       " 'n',\n",
       " 'i=1',\n",
       " '(x',\n",
       " 'i',\n",
       " ' x)',\n",
       " '2',\n",
       " ' by n systematically underestimates the variance. The definition of sample variance is based on the fact that this can be compensated for this by dividing by something small than n. We will learn why the appropriate choice is n 1 when we investigate Unbiased Estimation in Topic 13.',\n",
       " 'To show that the phenomena in Exercise 2.9 is true more broadly, we next perform a little algebra. This is similar to the computation of the parallel axis theorem in physics. The parallel axis theorem is used to determine the moment of inertia of a rigid body about any axis, given the moment of inertia of the object about the parallel axis through the object’s center of mass (x) and the perpendicular distance between the axes. In this case, we a looking at the rigid motion of a finite number of equal point masses.',\n",
       " 'In the formula for SS( ), divide the difference in the value of each observation x',\n",
       " 'i',\n",
       " ' to the value into the difference to the sample mean x and then the distance from the sample mean to (i.e. x ).',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " 'SS( ) =',\n",
       " '\\t',\n",
       " '((x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x) + (x',\n",
       " '\\t',\n",
       " '))',\n",
       " '2',\n",
       " ' =\\t(x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x)',\n",
       " '2',\n",
       " ' + 2',\n",
       " '\\t',\n",
       " '(x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x)(x',\n",
       " '\\t',\n",
       " ') +\\t(x',\n",
       " '\\t',\n",
       " ')',\n",
       " '2',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " '\\t',\n",
       " 'X',\n",
       " 'n',\n",
       " '=\\t(x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x)',\n",
       " '2',\n",
       " ' +',\n",
       " '\\t',\n",
       " '(x',\n",
       " '\\t',\n",
       " ')',\n",
       " '2',\n",
       " ' =\\t(x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x)',\n",
       " '2',\n",
       " ' + n(x',\n",
       " '\\t',\n",
       " ')',\n",
       " '2',\n",
       " ':',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " '\\t',\n",
       " 'i=1',\n",
       " '26',\n",
       " 'the difference between x and the chosen value',\n",
       " '\\t',\n",
       " '. We shall see this idea of partitioning in other contexts.',\n",
       " 'Note that the minimum value of SS( ) can be obtained by minimizing the second term. This takes place at = x. Thus,',\n",
       " 'Exercise 2.10. The following formulas may be useful in aggregating data. Suppose you have data sets collected on two consecutive days with the following summary statistics.',\n",
       " 'Now combine the observations of the two days and use this to show that the combined mean',\n",
       " 'x',\n",
       " ' ',\n",
       " '=',\n",
       " ' ',\n",
       " 'n',\n",
       " '1',\n",
       " 'x',\n",
       " '1 ',\n",
       " '+',\n",
       " ' ',\n",
       " 'n',\n",
       " '2',\n",
       " 'x',\n",
       " '2',\n",
       " '',\n",
       " 'n',\n",
       " '1',\n",
       " ' + n',\n",
       " '2',\n",
       " 'and the combined variance',\n",
       " '',\n",
       " '(Hint: Use (2.2)).',\n",
       " 'Exercise 2.11. For the data set x',\n",
       " '1',\n",
       " '; x',\n",
       " '2',\n",
       " '; : : : ; x',\n",
       " 'n',\n",
       " ', let',\n",
       " 'y',\n",
       " 'i',\n",
       " ' = a + bx',\n",
       " 'i',\n",
       " ':',\n",
       " 'Give the summary statistics for the y data set given the corresponding values of the x data set. (Consider carefully the consequences of the fact that a might be less than 0.)',\n",
       " 'Among these, the quadratic identity',\n",
       " 'var(x + bx) = b',\n",
       " '2',\n",
       " 'var(x)',\n",
       " 'is one of the most frequently used and useful in all of statistics.',\n",
       " '2.3',\n",
       " '\\t',\n",
       " 'Quantiles and Standardized Variables',\n",
       " 'A single observation, say 87 on a exam, gives little information about the performance on the exam. One way to include more about this observation would be to give the value of the empirical cumulative distribution function. Thus,',\n",
       " 'F',\n",
       " 'n',\n",
       " '(87) = 0:7223',\n",
       " '27',\n",
       " 'Introduction to the Science of Statistics',\n",
       " '\\t',\n",
       " 'Describing Distributions with Numbers',\n",
       " '',\n",
       " 'tells us that about 72% of the exam scores were below 87. This is sometimes reported by saying that 87 is the 0.7223 quantile for the exam scores.',\n",
       " 'We can determine this value using the R command quantile. For the ages of presidents at inauguration, we have that the 72% quantile is 57 year old.',\n",
       " 'quantile(age,0.72)',\n",
       " '72%',\n",
       " '57',\n",
       " 'Thus, for example, for the ages of the president, we have that IQR(age) can also be computed using the command quantile(age,3/4) - quantile(age,1/4). R returns the value 7. The quantile command on its own returns the five number summary.',\n",
       " '0%\\t25%\\t50%',\n",
       " '\\t',\n",
       " '75% 100%',\n",
       " '42',\n",
       " '\\t',\n",
       " '51',\n",
       " '\\t',\n",
       " '55',\n",
       " '\\t',\n",
       " '58',\n",
       " '\\t',\n",
       " '70',\n",
       " 'Another, and perhaps more common use of the term quantiles is a general term for partitioning ranked data into equal parts. For example, quartiles partitions the data into 4 equal parts. Percentiles partitions the data into 100 equal parts. Thus, the k-th q-tile is the value in the data for which k=q of the values are below the given value. This naturally leads to some rounding issues which leads to a large variety of small differences in the definition of quantiles.',\n",
       " 'Exercise 2.12. For the example above, describe the quintile, decile, and percentile of the observation 87.',\n",
       " 'A second way to evaluate a score of 87 is to related it to the mean. Thus, if the mean x = 76. Then, we might say that the exam score is 11 points above the mean. If the scores are quite spread out, then 11 points above the mean is just a little above average. If the scores are quite tightly spread, then 11 points is quite a bit above average. Thus, for comparisons, we will sometimes use the standardized version of x',\n",
       " 'i',\n",
       " ',',\n",
       " 'z',\n",
       " 'i',\n",
       " ' ',\n",
       " '=',\n",
       " ' ',\n",
       " 'x',\n",
       " 'i',\n",
       " '\\t',\n",
       " 'x ',\n",
       " ':',\n",
       " '',\n",
       " 's',\n",
       " 'x',\n",
       " 'The observations z',\n",
       " 'i',\n",
       " ' have mean 0 and standard deviation 1. The value z',\n",
       " 'i',\n",
       " ' is also called the standard score , the z-value, the z-score, and the normal score. An individual z-score, z',\n",
       " 'i',\n",
       " ', gives the number of standard deviations an observation x',\n",
       " 'i',\n",
       " ' is above (or below) the mean.',\n",
       " 'The R command scale transforms the data to the standard score. For the ages of the presidents, we use the scale command to show the standardized ages. The head command show the first 6 rows of the output for presidents from George Washington to John Qunicy Adams.',\n",
       " 'head(data.frame(scale(age),(age-mean(age))/sd(age))) scale.age. X.age...mean.age...sd.age.',\n",
       " 'Exercise 2.13. What are the units of the standard score? What is the relationship of the standard score of an obser-vation x',\n",
       " 'i',\n",
       " ' and y',\n",
       " 'i',\n",
       " ' = ax',\n",
       " 'i',\n",
       " ' + b?',\n",
       " '2.4',\n",
       " '\\t',\n",
       " 'Quantile-Quantile Plots',\n",
       " 'In addition to side by side boxplots or histograms, we can also compare two cumulative distribution function directly with the quantile-quantile or Q-Q plot. If the quantitative data sets x and y have the same number of observations,',\n",
       " '28',\n",
       " '',\n",
       " 'Figure 2.2: age of first seizure (left) side-by-side boxplots, (center) empirical cumulative distribution functions, (right) Q-Q plot with Q',\n",
       " '1',\n",
       " ', the median, and Q',\n",
       " '3',\n",
       " ' indicated by the solid ',\n",
       " 'red',\n",
       " ' dots. The solid line on the plot has intercept 0 and slope 1. (missense age=nonsense age)',\n",
       " 'then this is simply plot(sort(x),sort(y)). In this case the Q-Q plot matches each of the quantiles for the two data sets. If the data sets have an unequal number of observations, then observations from the larger data are reduced by interpolation to create data sets of equal length and the Q-Q plot is plot(sort(xred),sort(yred)) for the reduced data sets xred and yred.',\n",
       " 'Example 2.14. Dravet syndrome, also known as Severe Myoclonic Epilepsy of Infancy (SMEI), is a rare and catas-trophic form of intractable epilepsy that begins in infancy. A recent study looks at de novo mutations in the DNA sequence SCN1A that codes for a sodium channel protein. An improperly functioning sodium channel can have severe consequences for brain function.',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics_statbook_doc['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Displaying Data',\n",
       " '1.1 Types of Data',\n",
       " '1.2 Categorical Data',\n",
       " '1.2.1 Pie Chart',\n",
       " '1.2.2 Bar Charts',\n",
       " '1.3 Two-way Tables',\n",
       " '1.4 Histograms and the Empirical Cumulative Distribution Function',\n",
       " '1.5 Scatterplots',\n",
       " '1.6 Time Plots',\n",
       " '1.7 Answers to Selected Exercises',\n",
       " 'Describing Distributions with Numbers',\n",
       " '2.1 Measuring Center',\n",
       " '2.1.1 Medians',\n",
       " '2.1.2 Means',\n",
       " '2.2 Measuring Spread',\n",
       " '2.2.1 Five Number Summary',\n",
       " '2.2.2 Sample Variance and Standard Deviation',\n",
       " '2.3 Quantiles and Standardized Variables',\n",
       " '2.4 Quantile-Quantile Plots',\n",
       " '2.5 Answers to Selected Exercises',\n",
       " 'Correlation and Regression',\n",
       " '3.1 Covariance and Correlation',\n",
       " '3.2 Linear Regression',\n",
       " '3.2.1 Transformed Variables',\n",
       " '3.3 Extensions',\n",
       " '3.3.1 Nonlinear Regression',\n",
       " '3.3.2 Multiple Linear Regression',\n",
       " '3.4 Answers to Selected Exercises',\n",
       " 'Producing Data',\n",
       " '4.1 Preliminary Steps',\n",
       " '4.2 Professional Ethics',\n",
       " '4.3 Formal Statistical Procedures',\n",
       " '4.3.1 Observational Studies',\n",
       " '4.3.2 Randomized Controlled Experiments',\n",
       " '4.3.3 Natural experiments',\n",
       " '4.4 Case Studies',\n",
       " '4.4.1 Observational Studies',\n",
       " '4.4.2 Experiments',\n",
       " 'The Basics of Probability',\n",
       " '5.2 Equally Likely Outcomes and the Axioms of Probability',\n",
       " '5.3 Consequences of the Axioms',\n",
       " '5.4 Counting',\n",
       " '5.4.1 Fundamental Principle of Counting',\n",
       " '5.4.2 Permutations',\n",
       " '5.4.3 Combinations',\n",
       " '5.5 Answers to Selected Exercises',\n",
       " 'Conditional Probability and Independence',\n",
       " '6.1 Restricting the Sample Space - Conditional Probability',\n",
       " '6.3 The Law of Total Probability',\n",
       " '6.4 Bayes formula',\n",
       " 'Random Variables and Distribution Functions',\n",
       " '7.2 Distribution Functions',\n",
       " '7.3 Properties of the Distribution Function',\n",
       " '7.3.1 Discrete Random Variables',\n",
       " '7.3.2 Continuous Random Variables',\n",
       " '7.4 Mass Functions',\n",
       " '7.5 Density Functions',\n",
       " '7.7 Joint and Conditional Distributions',\n",
       " '7.7.1 Discrete Random Variables',\n",
       " '7.7.2 Continuous Random Variables',\n",
       " '7.7.3 Independent Random Variables',\n",
       " '7.8 Simulating Random Variables',\n",
       " '7.8.1 Discrete Random Variables and the sample Command',\n",
       " '7.8.2 Continuous Random Variables and the Probability Transform',\n",
       " '7.9 Answers to Selected Exercises',\n",
       " 'The Expected Value',\n",
       " '8.1 Definition and Properties',\n",
       " '8.2 Discrete Random Variables',\n",
       " '8.5 Summary',\n",
       " '8.8.1 Equivalent Conditions for Independence',\n",
       " '8.9 Quantile Plots and Probability Plots',\n",
       " '8.10 Answers to Selected Exercises',\n",
       " 'Examples of Mass Functions and Densities',\n",
       " '9.1 Examples of Discrete Random Variables',\n",
       " '9.2 Examples of Continuous Random Variables',\n",
       " '9.3 More on Mixtures',\n",
       " '9.4 R Commands',\n",
       " '9.5 Summary of Properties of Random Variables',\n",
       " '9.5.1 Discrete Random Variables',\n",
       " '9.6 Answers to Selected Exercises',\n",
       " 'The Law of Large Numbers',\n",
       " '10.3 Importance Sampling',\n",
       " '10.4 Answers to Selected Exercises',\n",
       " 'The Central Limit Theorem',\n",
       " '11.2 The Classical Central Limit Theorem',\n",
       " '11.2.1 Bernoulli Trials and the Continuity Correction',\n",
       " '11.5 Summary of Normal Approximations',\n",
       " '11.5.1 Sample Sum',\n",
       " '11.5.3 Sample Proportion',\n",
       " '11.5.4 Delta Method',\n",
       " '11.6 Answers to Selected Exercises',\n",
       " 'Overview of Estimation',\n",
       " '12.2 Classical Statistics',\n",
       " '12.3 Bayesian Statistics',\n",
       " '12.4 Answers to Selected Exercises',\n",
       " 'Method of Moments',\n",
       " '13.2 The Procedure',\n",
       " '13.3 Examples',\n",
       " 'Unbiased Estimation',\n",
       " '14.2 Computing Bias',\n",
       " '14.3 Compensating for Bias',\n",
       " '14.4 Consistency',\n",
       " '14.5 Cramer´-Rao Bound',\n",
       " '14.6 A Note on Exponential Families and Efficient Estimators',\n",
       " '14.7 Answers to Selected Exercises',\n",
       " 'Maximum Likelihood Estimation',\n",
       " '15.2 Examples',\n",
       " '15.3 Summary of Estimators',\n",
       " '15.4 Asymptotic Properties',\n",
       " '15.5 Comparison of Estimation Procedures',\n",
       " '15.6 Multidimensional Estimation',\n",
       " '15.7 The Case of Exponential Families',\n",
       " '15.8 Choice of Estimators',\n",
       " '15.9 Technical Aspects',\n",
       " '15.10 Answers to Selected Exercises',\n",
       " 'Interval Estimation',\n",
       " '16.1 Classical Statistics',\n",
       " '16.1.1 Means',\n",
       " '16.1.2 Linear Regression',\n",
       " '16.1.3 Sample Proportions',\n",
       " '16.1.4 Summary of Standard Confidence Intervals',\n",
       " '16.1.5 Interpretation of the Confidence Interval',\n",
       " '16.1.6 Extensions on the Use of Confidence Intervals',\n",
       " '16.2 The Bootstrap',\n",
       " '16.3 Bayesian Statistics',\n",
       " '16.4 Answers to Selected Exercises',\n",
       " 'Simple Hypotheses',\n",
       " '17.1 Overview and Terminology',\n",
       " '17.2 The Neyman-Pearson Lemma',\n",
       " '17.2.1 The Receiver Operating Characteristic',\n",
       " '17.3 Examples',\n",
       " '17.4 Summary',\n",
       " '17.5 Proof of the Neyman-Pearson Lemma',\n",
       " '17.6 An Brief Introduction to the Bayesian Approach',\n",
       " '17.7 Answers to Selected Exercises',\n",
       " 'Composite Hypotheses',\n",
       " '18.1 Partitioning the Parameter Space',\n",
       " '18.2 The Power Function',\n",
       " '18.3 The p-value',\n",
       " '18.4 Distribution of p-values and the Receiving Operating Characteristic',\n",
       " '18.5 Multiple Hypothesis Testing',\n",
       " '18.5.1 Familywise Error Rate',\n",
       " '18.5.2 False Discovery Rate',\n",
       " '18.6 Answers to Selected Exercises',\n",
       " 'Extensions on the Likelihood Ratio',\n",
       " '19.1 One-Sided Tests',\n",
       " '19.2 Likelihood Ratio Tests',\n",
       " '19.3 Chi-square Tests',\n",
       " '19.4 Answers to Selected Exercises',\n",
       " 'Procedures',\n",
       " '20.1 Guidelines for Using the t Procedures',\n",
       " '20.2 One Sample t Tests',\n",
       " '20.3 Correspondence between Two-Sided Tests and Confidence Intervals',\n",
       " '20.4 Matched Pairs Procedures',\n",
       " '20.5 Two Sample Procedures',\n",
       " '20.6 Summary of Tests of Significance',\n",
       " '20.6.1 General Guidelines',\n",
       " '20.6.2 Test for Population Proportions',\n",
       " '20.6.3 Test for Population Means',\n",
       " '20.7 A Note on the Delta Method',\n",
       " '20.8 The t Test as a Likelihood Ratio Test',\n",
       " '20.9 Non-parametric alternatives',\n",
       " '20.9.1 Permutation Test',\n",
       " '20.9.2 Mann-Whitney or Wilcoxon Rank Sum Test',\n",
       " '20.9.3 Wilcoxon Signed-Rank Test',\n",
       " '20.10 Answers to Selected Exercises',\n",
       " 'Goodness of Fit',\n",
       " '21.2 Contingency tables',\n",
       " '21.3 Applicability and Alternatives to Chi-squared Tests',\n",
       " '21.4 Answer to Selected Exercise',\n",
       " 'Analysis of Variance',\n",
       " '22.1 Overview',\n",
       " '22.2 One Way Analysis of Variance',\n",
       " '22.3 Contrasts',\n",
       " '22.4 Two Sample Procedures',\n",
       " '22.5 Kruskal-Wallis Rank-Sum Test',\n",
       " '22.6 Answer to Selected Exercises']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics_statbook_doc_bigger,statistics_statbook_doc_bigger_indexes = get_txt_bigger(statistics_statbook_doc)\n",
    "chapter_statistics_statbook = find_chapters_format_statbooks(statistics_statbook_doc_bigger,key_word_='Topic')\n",
    "chapter_statistics_statbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An Introduction to the Science of Statistics:',\n",
       " 'From Theory to Implementation',\n",
       " 'Preliminary Edition',\n",
       " 'c ',\n",
       " 'Joseph C. Watkins',\n",
       " 'Contents',\n",
       " 'Preface',\n",
       " 'Who Should Use this Book',\n",
       " 'An Annotated Syllabus',\n",
       " 'Organizing and Collecting Data',\n",
       " 'Introduction to Probability',\n",
       " 'Estimation',\n",
       " 'Hypothesis Testing',\n",
       " 'Exercises and Problems',\n",
       " 'Acknowledgements',\n",
       " 'Part I',\n",
       " 'Organizing and Producing Data',\n",
       " 'Topic 1',\n",
       " 'Displaying Data',\n",
       " '1.1 Types of Data',\n",
       " '1.1',\n",
       " 'Types of Data',\n",
       " '1.2 Categorical Data',\n",
       " '1.2',\n",
       " 'Categorical Data',\n",
       " '1.2.1 Pie Chart',\n",
       " '1.2.1',\n",
       " 'Pie Chart',\n",
       " '*',\n",
       " 'Proportion of AIDS Cases among Males by Transmission Category Diagnosed − USA, 2005',\n",
       " '16%',\n",
       " '1.2.2 Bar Charts',\n",
       " '1.2.2',\n",
       " 'Bar Charts',\n",
       " '1.3 Two-way Tables',\n",
       " '1.3',\n",
       " 'Two-way Tables',\n",
       " '1.4 Histograms and the Empirical Cumulative Distribution Function',\n",
       " '1.4',\n",
       " 'Histograms and the Empirical Cumulative Distribution Function',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " '1',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " '1.5 Scatterplots',\n",
       " '1.5',\n",
       " 'Scatterplots',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'Bone Lengths for Archeopteryx',\n",
       " 'Income vs. Assets (in billions of dollars)',\n",
       " '1.6 Time Plots',\n",
       " '1.6',\n",
       " 'Time Plots',\n",
       " 'Worldwide Oil Production',\n",
       " '1.7 Answers to Selected Exercises',\n",
       " '1.7',\n",
       " 'Answers to Selected Exercises',\n",
       " 'Topic 2',\n",
       " 'Describing Distributions with Numbers',\n",
       " '2.1 Measuring Center',\n",
       " '2.1',\n",
       " 'Measuring Center',\n",
       " '2.1.1 Medians',\n",
       " '2.1.1',\n",
       " 'Medians',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '(1)',\n",
       " '(2)',\n",
       " '(n)',\n",
       " '(k)',\n",
       " '((n+1)=2)',\n",
       " '2',\n",
       " '(x',\n",
       " '+',\n",
       " 'x',\n",
       " '):',\n",
       " '2.1.2 Means',\n",
       " '2.1.2',\n",
       " 'Means',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'n+1',\n",
       " 'n+1',\n",
       " 'n+1',\n",
       " 'n+k',\n",
       " 'i',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'n',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'j',\n",
       " 'j',\n",
       " 'i',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " '2.2 Measuring Spread',\n",
       " '2.2',\n",
       " 'Measuring Spread',\n",
       " '2.2.1 Five Number Summary',\n",
       " '2.2.1',\n",
       " 'Five Number Summary',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '3',\n",
       " '2',\n",
       " '1',\n",
       " '3',\n",
       " '2',\n",
       " '3',\n",
       " '1',\n",
       " '2.2.2 Sample Variance and Standard Deviation',\n",
       " '2.2.2',\n",
       " 'Sample Variance and Standard Deviation',\n",
       " 'x',\n",
       " '2',\n",
       " 'x',\n",
       " 'x',\n",
       " 'x',\n",
       " 's',\n",
       " 'x',\n",
       " 'x',\n",
       " 'x',\n",
       " 'n',\n",
       " 'i',\n",
       " '2',\n",
       " '1',\n",
       " '20',\n",
       " 'P',\n",
       " 'n',\n",
       " 'i=1',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'i',\n",
       " '2',\n",
       " '2',\n",
       " 'i',\n",
       " '2',\n",
       " '2',\n",
       " 'x',\n",
       " '=',\n",
       " 'n',\n",
       " 'x',\n",
       " '+',\n",
       " 'n',\n",
       " 'x',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '2.3 Quantiles and Standardized Variables',\n",
       " '2.3',\n",
       " 'Quantiles and Standardized Variables',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'i',\n",
       " '=',\n",
       " 'x',\n",
       " 'x ',\n",
       " ':',\n",
       " 'x',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '2.4 Quantile-Quantile Plots',\n",
       " '2.4',\n",
       " 'Quantile-Quantile Plots',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '2.5 Answers to Selected Exercises',\n",
       " '2.5',\n",
       " 'Answers to Selected Exercises',\n",
       " 'n',\n",
       " '+ 1',\n",
       " 'n',\n",
       " '+ 1',\n",
       " 'x',\n",
       " '=',\n",
       " 'k',\n",
       " '(x',\n",
       " '+',\n",
       " '+ ',\n",
       " 'x',\n",
       " '):',\n",
       " 'n',\n",
       " '+',\n",
       " 'k',\n",
       " 'n',\n",
       " '+',\n",
       " 'k',\n",
       " '3',\n",
       " '3',\n",
       " '1;1',\n",
       " '1;2',\n",
       " '1;n',\n",
       " '2;1',\n",
       " '2;2',\n",
       " '2;n',\n",
       " '0',\n",
       " '00',\n",
       " '00',\n",
       " '1',\n",
       " 'Topic 3',\n",
       " 'Correlation and Regression',\n",
       " '3.1 Covariance and Correlation',\n",
       " '3.1',\n",
       " 'Covariance and Correlation',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " 'x',\n",
       " 'r=0.9',\n",
       " 'r=0.7',\n",
       " 'r=0.3',\n",
       " 'r=−0.8',\n",
       " '*',\n",
       " '*',\n",
       " '3.2 Linear Regression',\n",
       " '3.2',\n",
       " 'Linear Regression',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '\\\\',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " 'DAT A',\n",
       " '2',\n",
       " 'F IT',\n",
       " '2',\n",
       " 'RESID',\n",
       " '2',\n",
       " 'n',\n",
       " 'P',\n",
       " ':',\n",
       " '\\\\',\n",
       " '2',\n",
       " '3.2.1 Transformed Variables',\n",
       " '3.2.1',\n",
       " 'Transformed Variables',\n",
       " '1',\n",
       " '1',\n",
       " 'n',\n",
       " 'n',\n",
       " '5',\n",
       " '1',\n",
       " 'dy',\n",
       " 'dt',\n",
       " '0',\n",
       " 'kt',\n",
       " '0',\n",
       " '0',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " 'd',\n",
       " '[',\n",
       " 'ES',\n",
       " 'dt',\n",
       " ']',\n",
       " 'm',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " 'max',\n",
       " '2',\n",
       " '0',\n",
       " 'V',\n",
       " '1',\n",
       " '[S',\n",
       " '1',\n",
       " ']',\n",
       " 'V',\n",
       " '1',\n",
       " '[S',\n",
       " '1',\n",
       " ']',\n",
       " 'max',\n",
       " 'm',\n",
       " '3.3 Extensions',\n",
       " '3.3',\n",
       " 'Extensions',\n",
       " '0',\n",
       " 'k',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '3.3.1 Nonlinear Regression',\n",
       " '3.3.1',\n",
       " 'Nonlinear Regression',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '^',\n",
       " 'i',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " 'n',\n",
       " 'n',\n",
       " 'max',\n",
       " 'm',\n",
       " '*',\n",
       " '*',\n",
       " 'max',\n",
       " 'm',\n",
       " '3.3.2 Multiple Linear Regression',\n",
       " '3.3.2',\n",
       " 'Multiple Linear Regression',\n",
       " 'ij',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'X',\n",
       " 'c',\n",
       " 'ij',\n",
       " 'ik',\n",
       " 'kj',\n",
       " 'jj',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " 'T',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'T',\n",
       " 'n1',\n",
       " 'nk',\n",
       " '0',\n",
       " '1',\n",
       " 'k',\n",
       " 'T',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'T',\n",
       " 'T',\n",
       " 'x',\n",
       " '2',\n",
       " 'i',\n",
       " 'T',\n",
       " 'i=1',\n",
       " 'n',\n",
       " 'ij',\n",
       " 'j',\n",
       " 'i',\n",
       " 'i',\n",
       " '0',\n",
       " '1',\n",
       " 'i',\n",
       " '2',\n",
       " '2',\n",
       " 'i',\n",
       " 'k',\n",
       " 'k',\n",
       " 'i',\n",
       " 'i',\n",
       " '*',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " 'uspopulation',\n",
       " '= 3863670  10',\n",
       " '0:0147(year',\n",
       " '1790)  0:00002808(year',\n",
       " '1790)',\n",
       " '3.4 Answers to Selected Exercises',\n",
       " '3.4',\n",
       " 'Answers to Selected Exercises',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " 'x',\n",
       " '2',\n",
       " 'y',\n",
       " '2',\n",
       " 'x',\n",
       " 'x',\n",
       " 'y',\n",
       " '2',\n",
       " 'y',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " 'x',\n",
       " 'y',\n",
       " '2',\n",
       " 'x+y',\n",
       " '2',\n",
       " 'x+y',\n",
       " 'x',\n",
       " 'i',\n",
       " 's',\n",
       " 'x',\n",
       " 'x',\n",
       " 'y',\n",
       " 'i',\n",
       " 's',\n",
       " 'y',\n",
       " 'y',\n",
       " 'y',\n",
       " 'x',\n",
       " '2',\n",
       " 'x  y',\n",
       " '^',\n",
       " 'y',\n",
       " 'y',\n",
       " 'y',\n",
       " 'y',\n",
       " 'i',\n",
       " 'i',\n",
       " '6',\n",
       " '5',\n",
       " '*',\n",
       " '*',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '0',\n",
       " 'Topic 4',\n",
       " 'Producing Data',\n",
       " '4.1 Preliminary Steps',\n",
       " '4.1',\n",
       " 'Preliminary Steps',\n",
       " '4.2 Professional Ethics',\n",
       " '4.2',\n",
       " 'Professional Ethics',\n",
       " '4.3 Formal Statistical Procedures',\n",
       " '4.3',\n",
       " 'Formal Statistical Procedures',\n",
       " '4.3.1 Observational Studies',\n",
       " '4.3.1',\n",
       " 'Observational Studies',\n",
       " '4.3.2 Randomized Controlled Experiments',\n",
       " '4.3.2',\n",
       " 'Randomized Controlled Experiments',\n",
       " 'Factor B: hive temperature',\n",
       " 'Factor A:',\n",
       " 'genotype',\n",
       " '4.3.3 Natural experiments',\n",
       " '4.3.3',\n",
       " 'Natural experiments',\n",
       " '4.4 Case Studies',\n",
       " '4.4',\n",
       " 'Case Studies',\n",
       " '4.4.1 Observational Studies',\n",
       " '4.4.1',\n",
       " 'Observational Studies',\n",
       " '4.4.2 Experiments',\n",
       " '4.4.2',\n",
       " 'Experiments',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " 'Part II',\n",
       " 'Probability',\n",
       " 'Topic 5',\n",
       " 'The Basics of Probability',\n",
       " '5.1\\tIntroduction',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '5.2 Equally Likely Outcomes and the Axioms of Probability',\n",
       " '5.2',\n",
       " 'Equally Likely Outcomes and the Axioms of Probability',\n",
       " '#(',\n",
       " '#( )',\n",
       " 'A)',\n",
       " '16',\n",
       " '5',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'j',\n",
       " 'j',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'j',\n",
       " 'j',\n",
       " '1',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'j',\n",
       " 'j',\n",
       " 'j',\n",
       " '5.3 Consequences of the Axioms',\n",
       " '5.3',\n",
       " 'Consequences of the Axioms',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " '1',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " '1',\n",
       " 'c',\n",
       " 'i',\n",
       " 'c',\n",
       " '5.4 Counting',\n",
       " '5.4',\n",
       " 'Counting',\n",
       " '5.4.1 Fundamental Principle of Counting',\n",
       " '5.4.1',\n",
       " 'Fundamental Principle of Counting',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " 'k',\n",
       " '(k)',\n",
       " '5.4.2 Permutations',\n",
       " '5.4.2',\n",
       " 'Permutations',\n",
       " 'k',\n",
       " 'k',\n",
       " '*',\n",
       " 'n',\n",
       " '(n',\n",
       " 'k',\n",
       " ')!',\n",
       " ' ',\n",
       " ':',\n",
       " '5.4.3 Combinations',\n",
       " '5.4.3',\n",
       " 'Combinations',\n",
       " 'n',\n",
       " 'k',\n",
       " '3',\n",
       " 'k',\n",
       " '8',\n",
       " '3',\n",
       " '23',\n",
       " '*',\n",
       " '5.5 Answers to Selected Exercises',\n",
       " '5.5',\n",
       " 'Answers to Selected Exercises',\n",
       " '42',\n",
       " '64',\n",
       " '21',\n",
       " '32',\n",
       " 'c',\n",
       " 'c',\n",
       " 'n',\n",
       " '[',\n",
       " '!',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " '2',\n",
       " 'k',\n",
       " 'n',\n",
       " 'n',\n",
       " 'x',\n",
       " '52',\n",
       " '5',\n",
       " 'x',\n",
       " '*',\n",
       " '*',\n",
       " 'Topic 6',\n",
       " 'Conditional Probability and Independence',\n",
       " '6.1 Restricting the Sample Space - Conditional Probability',\n",
       " '6.1',\n",
       " 'Restricting the Sample Space - Conditional Probability',\n",
       " '0.2',\n",
       " '−0.6',\n",
       " 'P',\n",
       " '(AjB) =',\n",
       " '=',\n",
       " '6.2\\tThe Multiplication Principle',\n",
       " '51',\n",
       " '3',\n",
       " '52',\n",
       " '4',\n",
       " '17',\n",
       " '1',\n",
       " '13',\n",
       " '1',\n",
       " '50',\n",
       " '2',\n",
       " '51',\n",
       " '3',\n",
       " '52',\n",
       " '4',\n",
       " '25',\n",
       " '1',\n",
       " '17',\n",
       " '1',\n",
       " '13',\n",
       " '1',\n",
       " '4',\n",
       " '2',\n",
       " ':',\n",
       " '4',\n",
       " 'b',\n",
       " '\\t',\n",
       " 'g',\n",
       " '2',\n",
       " 'b+g',\n",
       " '2',\n",
       " '6.3 The Law of Total Probability',\n",
       " '6.3',\n",
       " 'The Law of Total Probability',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " '1',\n",
       " '2',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'C',\n",
       " 'c',\n",
       " 'ij',\n",
       " 'ii',\n",
       " '1',\n",
       " '2',\n",
       " '6.4 Bayes formula',\n",
       " '6.4',\n",
       " 'Bayes formula',\n",
       " ':',\n",
       " '= 30:',\n",
       " 'c',\n",
       " 'c',\n",
       " 'P',\n",
       " '(AjC)P',\n",
       " '(C)',\n",
       " '0:0009',\n",
       " 'c',\n",
       " 'j',\n",
       " 'n',\n",
       " '36',\n",
       " '1',\n",
       " '6',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " '1',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " 'j',\n",
       " 'j',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '= 0',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '= 1',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " 'p',\n",
       " '=',\n",
       " '2',\n",
       " ':',\n",
       " 'ij',\n",
       " '1',\n",
       " '2',\n",
       " 'ij',\n",
       " 'ij',\n",
       " 'ij',\n",
       " '2',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'i',\n",
       " '1',\n",
       " 'c',\n",
       " '3',\n",
       " '7',\n",
       " '9',\n",
       " '1',\n",
       " 'c',\n",
       " '3',\n",
       " '7',\n",
       " '9',\n",
       " '1',\n",
       " 'c',\n",
       " '3',\n",
       " '7',\n",
       " '9',\n",
       " '1',\n",
       " '3',\n",
       " '7',\n",
       " '9',\n",
       " 'P',\n",
       " '(BjA) =',\n",
       " '=',\n",
       " ':',\n",
       " 'P',\n",
       " '(BjA)P',\n",
       " '(A)',\n",
       " 'c',\n",
       " 'c',\n",
       " 'Topic 7',\n",
       " 'Random Variables and Distribution Functions',\n",
       " '7.1\\tIntroduction',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '7.2 Distribution Functions',\n",
       " '7.2',\n",
       " 'Distribution Functions',\n",
       " 'X',\n",
       " 'X',\n",
       " 'c',\n",
       " 'c',\n",
       " '1',\n",
       " '2',\n",
       " 'i',\n",
       " 'i',\n",
       " '1',\n",
       " 'n',\n",
       " 'i',\n",
       " 'i',\n",
       " 'X',\n",
       " 'X',\n",
       " ' ',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " '3',\n",
       " 'X',\n",
       " '7.3 Properties of the Distribution Function',\n",
       " '7.3',\n",
       " 'Properties of the Distribution Function',\n",
       " 'X',\n",
       " 'X',\n",
       " 'x!',\n",
       " 'X',\n",
       " 'x!1',\n",
       " 'X',\n",
       " 'X',\n",
       " '7.3.1 Discrete Random Variables',\n",
       " '7.3.1',\n",
       " 'Discrete Random Variables',\n",
       " 'X',\n",
       " 'X',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " 'X',\n",
       " '0',\n",
       " 'X',\n",
       " '0',\n",
       " 'X',\n",
       " '0',\n",
       " '7.3.2 Continuous Random Variables',\n",
       " '7.3.2',\n",
       " 'Continuous Random Variables',\n",
       " 'X',\n",
       " 'F',\n",
       " '2',\n",
       " '=',\n",
       " '4',\n",
       " 'X',\n",
       " '*',\n",
       " 'X',\n",
       " 'X',\n",
       " 'x',\n",
       " '7.4 Mass Functions',\n",
       " '7.4',\n",
       " 'Mass Functions',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'x',\n",
       " 'X',\n",
       " 'x',\n",
       " 'X',\n",
       " 'x',\n",
       " 'X',\n",
       " 'x',\n",
       " '2',\n",
       " 'n',\n",
       " 'n!1',\n",
       " 'n',\n",
       " 'n',\n",
       " 'b+1',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " '0',\n",
       " '7.5 Density Functions',\n",
       " '7.5',\n",
       " 'Density Functions',\n",
       " 'X',\n",
       " 'X',\n",
       " 'x',\n",
       " 'X',\n",
       " 'X',\n",
       " 'f',\n",
       " 'X',\n",
       " '(x) =',\n",
       " 'lim  ',\n",
       " 'F',\n",
       " ' ',\n",
       " '(x',\n",
       " ' ',\n",
       " '+',\n",
       " '  ',\n",
       " 'x)',\n",
       " '  ',\n",
       " 'F',\n",
       " ' ',\n",
       " '(x)',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lines_statistics_statbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_index_statistics_statbook = [new_lines_statistics_statbook.index(ch) for ch in chapter_statistics_statbook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' An Introduction to the Science of Statistics:. From Theory to Implementation. Preliminary Edition. c . Joseph C. Watkins. Contents. Preface. Who Should Use this Book. An Annotated Syllabus. Organizing and Collecting Data. Introduction to Probability. Estimation. Hypothesis Testing. Exercises and Problems. Acknowledgements. Part I. Organizing and Producing Data. Topic 1',\n",
       " ' Displaying Data',\n",
       " ' 1.1 Types of Data. 1.1. Types of Data',\n",
       " ' 1.2 Categorical Data. 1.2. Categorical Data',\n",
       " ' 1.2.1 Pie Chart. 1.2.1. Pie Chart. *. Proportion of AIDS Cases among Males by Transmission Category Diagnosed − USA, 2005. 16%',\n",
       " ' 1.2.2 Bar Charts. 1.2.2. Bar Charts',\n",
       " ' 1.3 Two-way Tables. 1.3. Two-way Tables',\n",
       " ' 1.4 Histograms and the Empirical Cumulative Distribution Function. 1.4. Histograms and the Empirical Cumulative Distribution Function. 2. n. n. n. 1. n. n. n',\n",
       " ' 1.5 Scatterplots. 1.5. Scatterplots. 1. 2. n. 1. 2. n. 1. 2. n. Bone Lengths for Archeopteryx. Income vs. Assets (in billions of dollars)',\n",
       " ' 1.6 Time Plots. 1.6. Time Plots. Worldwide Oil Production',\n",
       " ' 1.7 Answers to Selected Exercises. 1.7. Answers to Selected Exercises. Topic 2',\n",
       " ' Describing Distributions with Numbers',\n",
       " ' 2.1 Measuring Center. 2.1. Measuring Center',\n",
       " ' 2.1.1 Medians. 2.1.1. Medians. 1. 2. n. (1). (2). (n). (k). ((n+1)=2). 2. (x. +. x. ):',\n",
       " ' 2.1.2 Means. 2.1.2. Means. 1. 2. n. 1. 2. n. n. 1. 2. n. n+1. n+1. n+1. n+k. i. 1. 2. n. i. i. n. 1. 1. 2. 2. n. n. i. i. n. 1. 2. n. i. n. j. j. i. n. i. i',\n",
       " ' 2.2 Measuring Spread. 2.2. Measuring Spread',\n",
       " ' 2.2.1 Five Number Summary. 2.2.1. Five Number Summary. 1. 3. 1. 3. 3. 1. 3. 3. 2. 1. 3. 2. 3. 1',\n",
       " ' 2.2.2 Sample Variance and Standard Deviation. 2.2.2. Sample Variance and Standard Deviation. x. 2. x. x. x. s. x. x. x. n. i. 2. 1. 20. P. n. i=1. i. 2. i. n. n. n. n. i. 2. i. 2. i. 2. n. n. n. i. 2. 2. i. 2. 2. x. =. n. x. +. n. x. 1. 2. 1. 2. n. i. i. 2',\n",
       " ' 2.3 Quantiles and Standardized Variables. 2.3. Quantiles and Standardized Variables. n. i. z. i. =. x. x . :. x. i. i. i. i. i. i. i',\n",
       " ' 2.4 Quantile-Quantile Plots. 2.4. Quantile-Quantile Plots. 1. 3. 1. 3. 1. 3',\n",
       " ' 2.5 Answers to Selected Exercises. 2.5. Answers to Selected Exercises. n. + 1. n. + 1. x. =. k. (x. +. + . x. ):. n. +. k. n. +. k. 3. 3. 1;1. 1;2. 1;n. 2;1. 2;2. 2;n. 0. 00. 00. 1. Topic 3',\n",
       " ' Correlation and Regression',\n",
       " ' 3.1 Covariance and Correlation. 3.1. Covariance and Correlation. 1. 2. n. 1. 2. n. 2. i. i. 2. x. r=0.9. r=0.7. r=0.3. r=−0.8. *. *',\n",
       " ' 3.2 Linear Regression. 3.2. Linear Regression. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. \\\\. i. i. i. i. i. i. i. i. i. 2. DAT A. 2. F IT. 2. RESID. 2. n. P. :. \\\\. 2',\n",
       " ' 3.2.1 Transformed Variables. 3.2.1. Transformed Variables. 1. 1. n. n. 5. 1. dy. dt. 0. kt. 0. 0. a. i. i. 2. 0. 0. 0. 2. d. [. ES. dt. ]. m. 1. 2. 1. max. 2. 0. V. 1. [S. 1. ]. V. 1. [S. 1. ]. max. m',\n",
       " ' 3.3 Extensions. 3.3. Extensions. 0. k. i. i. i',\n",
       " ' 3.3.1 Nonlinear Regression. 3.3.1. Nonlinear Regression. n. i. i. 2. ^. i. 1. 1. 2. 2. n. n. max. m. *. *. max. m',\n",
       " ' 3.3.2 Multiple Linear Regression. 3.3.2. Multiple Linear Regression. ij. A. A. B. B. A. B. X. c. ij. ik. kj. jj. 1. 1. 1. T. 1. 1. 2. n. T. n1. nk. 0. 1. k. T. 1. 2. n. T. T. x. 2. i. T. i=1. n. ij. j. i. i. 0. 1. i. 2. 2. i. k. k. i. i. *. 0. 1. 2. 2. uspopulation. = 3863670  10. 0:0147(year. 1790)  0:00002808(year. 1790)',\n",
       " ' 3.4 Answers to Selected Exercises. 3.4. Answers to Selected Exercises. i. i. 2. x. 2. y. 2. x. x. y. 2. y. 2. n. i. 2. i. 2. i. i. x. y. 2. x+y. 2. x+y. x. i. s. x. x. y. i. s. y. y. y. x. 2. x  y. ^. y. y. y. y. i. i. 6. 5. *. *. n. i. i. 2. 0. Topic 4',\n",
       " ' Producing Data',\n",
       " ' 4.1 Preliminary Steps. 4.1. Preliminary Steps',\n",
       " ' 4.2 Professional Ethics. 4.2. Professional Ethics',\n",
       " ' 4.3 Formal Statistical Procedures. 4.3. Formal Statistical Procedures',\n",
       " ' 4.3.1 Observational Studies. 4.3.1. Observational Studies',\n",
       " ' 4.3.2 Randomized Controlled Experiments. 4.3.2. Randomized Controlled Experiments. Factor B: hive temperature. Factor A:. genotype',\n",
       " ' 4.3.3 Natural experiments. 4.3.3. Natural experiments',\n",
       " ' 4.4 Case Studies. 4.4. Case Studies',\n",
       " ' 4.4.1 Observational Studies. 4.4.1. Observational Studies',\n",
       " ' 4.4.2 Experiments. 4.4.2. Experiments. 1. 2. 1. 2. Part II. Probability. Topic 5',\n",
       " ' The Basics of Probability. 5.1\\tIntroduction. 1. 2. n',\n",
       " ' 5.2 Equally Likely Outcomes and the Axioms of Probability. 5.2. Equally Likely Outcomes and the Axioms of Probability. #(. #( ). A). 16. 5. 1. 2. n. j. j. 1. 1. 2. n. j. j. 1. n. 1. 2. n. j. j. j',\n",
       " ' 5.3 Consequences of the Axioms. 5.3. Consequences of the Axioms. c. c. c. 1. 1. 2. i. i. i. 1. 1. 2. i. i. i. i. i. 1. 1. i. i. i. 1. 1. c. i. c',\n",
       " ' 5.4 Counting. 5.4. Counting',\n",
       " ' 5.4.1 Fundamental Principle of Counting. 5.4.1. Fundamental Principle of Counting. 1. 2. 1. 2. k. (k)',\n",
       " ' 5.4.2 Permutations. 5.4.2. Permutations. k. k. *. n. (n. k. )!.  . :',\n",
       " ' 5.4.3 Combinations. 5.4.3. Combinations. n. k. 3. k. 8. 3. 23. *',\n",
       " ' 5.5 Answers to Selected Exercises. 5.5. Answers to Selected Exercises. 42. 64. 21. 32. c. c. n. [. !. i. i. 1. 2. k. n. n. x. 52. 5. x. *. *. Topic 6',\n",
       " ' Conditional Probability and Independence',\n",
       " ' 6.1 Restricting the Sample Space - Conditional Probability. 6.1. Restricting the Sample Space - Conditional Probability. 0.2. −0.6. P. (AjB) =. =. 6.2\\tThe Multiplication Principle. 51. 3. 52. 4. 17. 1. 13. 1. 50. 2. 51. 3. 52. 4. 25. 1. 17. 1. 13. 1. 4. 2. :. 4. b. \\t. g. 2. b+g. 2',\n",
       " ' 6.3 The Law of Total Probability. 6.3. The Law of Total Probability. 1. 2. n. 1. 2. n. i. i. i. i. i. i. C. c. ij. ii. 1. 2',\n",
       " ' 6.4 Bayes formula. 6.4. Bayes formula. :. = 30:. c. c. P. (AjC)P. (C). 0:0009. c. j. n. 36. 1. 6. c. c. c. c. 1. n. i. i. i. i. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. j. j. p. =. p. =. p. =. p. = 0. p. =. p. =. p. =. p. = 1. p. =. p. =. p. =. p. =. 2. :. ij. 1. 2. ij. ij. ij. 2. c. c. c. c. c. c. c. c. i. 1. c. 3. 7. 9. 1. c. 3. 7. 9. 1. c. 3. 7. 9. 1. 3. 7. 9. P. (BjA) =. =. :. P. (BjA)P. (A). c. c. Topic 7',\n",
       " ' Random Variables and Distribution Functions. 7.1\\tIntroduction. 2. 2. 2',\n",
       " ' 7.2 Distribution Functions. 7.2. Distribution Functions. X. X. c. c. 1. 2. i. i. 1. n. i. i. X. X.  . X. X. X. X. 3. X',\n",
       " ' 7.3 Properties of the Distribution Function. 7.3. Properties of the Distribution Function. X. X. x!. X. x!1. X. X',\n",
       " ' 7.3.1 Discrete Random Variables. 7.3.1. Discrete Random Variables. X. X. 0. 0. 0. 0. 0. X. 0. X. 0. X. 0',\n",
       " ' 7.3.2 Continuous Random Variables. 7.3.2. Continuous Random Variables. X. F. 2. =. 4. X. *. X. X. x',\n",
       " ' 7.4 Mass Functions. 7.4. Mass Functions. X. X. X. X. X. X. X. x. X. x. X. x. X. x. 2. n. n!1. n. n. b+1. X. X. X. X. X. X. X. X. 0',\n",
       " ' 7.5 Density Functions. 7.5. Density Functions. X. X. x. X. X. f. X. (x) =. lim  . F.  . (x.  . +.   . x).   . F.  . (x). X. X. X. X. X. R. 1. X. X. X. X. b. X. X. X. 0. 0. f. (x) =. e. if . x >.  . 0:. *. *. *. X. Y. 7.6\\tMixtures. 1. 2. 1. 2. 1. 2. i. i. X. 1. 2. 1. 2. 1. n. 1. n. 1. n. P. n. i. i. i. n. X. n. i. i. 1. 1. n. n. 1. n. 1. n. i. X. X. X. 1. 1. 1. n. n. n. 1. 1. n. n. X. i. X. X. 0. 1. 1. 0. n. n. 0. 1. 1. n. n. n. i. i. X',\n",
       " ' 7.7 Joint and Conditional Distributions. 7.7. Joint and Conditional Distributions. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2',\n",
       " ' 7.7.1 Discrete Random Variables. 7.7.1. Discrete Random Variables. X. ;X. 1. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 1. X. jX. 2. 1. 2. 2. 1. 1. X. 1. 1. X. jX. 2. 1. 2. 1. X. jX. 2. 1',\n",
       " ' 7.7.2 Continuous Random Variables. 7.7.2. Continuous Random Variables. 1. 1. 1. 1. 2. 2. 2. 2. X. ;X. 1. 1. 1. 1. 2. 2. 2. 2. X. ;X. 1. 2. 1. 2. 1. 2. 1. X. 1. X. ;X. 1. 2. 2. P. fx. 2. < X. 2. x. 2.  . +.  x. 2. j. x. 1.  < X. 1.  x. 1.  . +.  x. 1. g.  . =.  . P .  .  .  .  .  .  .  .  .  . 1. 1. 1. 1. f. (x. ; x. ). x. x. =. f. (x. ; x. ). x. 2. X. 1. 1. X. 1. 2. 2. X ;X. 1. 2. f. (x. jx. ) =. f. X. (x. 1. ). X. 1. 1. X. jX. 2. 1. 2',\n",
       " ' 7.7.3 Independent Random Variables. 7.7.3. Independent Random Variables. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 2. 2. X. ;X. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. X. 1. X. 2. X. ;X. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. X. 1. 1. X. 2. 2. X. 1. X. 2. 1. 2. 1. 2. n. 1. 2. n. 1. n',\n",
       " ' 7.8 Simulating Random Variables. 7.8. Simulating Random Variables',\n",
       " ' 7.8.1 Discrete Random Variables and the sample Command. 7.8.1. Discrete Random Variables and the sample Command',\n",
       " ' 7.8.2 Continuous Random Variables and the Probability Transform. 7.8.2. Continuous Random Variables and the Probability Transform. X. X. X. X. 1. X. X. 1. X. X. U. U. 1. 2. n. X. X. i. X. 1. i. :. X. 2. X. 1. 1. 2. n. 1. 2. n. i. i. X. 2. 1. 2. n. 1. 2. n',\n",
       " ' 7.9 Answers to Selected Exercises. 7.9. Answers to Selected Exercises. n. n. 1. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1. 2. 1. 2. c. c. c. c. c. i. i. j. i. j. i. j. i. j. X. X. i. i. >. :. *. 3. X. n. 1. 2. 1. 2. 1. 2. n. n. n. \\\\. n. 1. 2. n. 1. 2. n!1. X. n. n!1. n. 0. X. 0. 0. X. X. 0. n. 0. 1. 2. 1. 2. X. 0. n!1. X. n. n!1. n. 0. 0. X. q. 2. q. q. p. 1=2. p. 1=4. 3=4. p. Y. X. X. X. X. n. n. Y. Y. Y. 0. X. y b . 1.  . = . 1. X.  . :. a. = 1. F. a. :. i. i. i. i. i. X. 1. 2. 3. *. 1. 2. X. 1. X. 2. X. jX. 2. 1. X. 1. X. ;X. 1. 2. X. 1. X. jX. 1. 1. X. ;X. 1. 2. X. 1. 1. X. 1. X. ;X. 1. 2. X. 1. X. 2. 1. 2. n. Topic 8',\n",
       " ' The Expected Value',\n",
       " ' 8.1 Definition and Properties. 8.1. Definition and Properties. 1. 2. n. 1. 6. 1. 6. 1. 6. 1. 6. 1. 6. 1. 6. 21. 6. 7. 2. 1. 4. 1. 4. 1. 4. 12. 1. 12. 1. 12. 1. 11. 4. 2. 2. 1. 2. 1. 2. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 2. 2. R. a. b. 1. 2. 1. 2. Z. Z . Z . 1. 1. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2',\n",
       " ' 8.2 Discrete Random Variables. 8.2. Discrete Random Variables. 1. k. k+1. N. 1. i. n. k. k+1. i. i. i. k. k. +1. i. k. k+1. i. 2. 2. 2. 2. 1. 2. n. X. 1. 2. n. 8.3\\tBernoulli Trials. 1. 2. n. i. i. i. i. i. n. 1. 2. n. n. 1. 2. n. n. 3. 5. 8. 3. 3. 5. 8. n. i. i. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. x. ~. 1. X. 1. 2. n. X. 1. 2. n. 1. 2. X. ;X. 1. 2. 1. 1. 1. 2. 1. 2. X. ;X. 1. 2. 2. 1. X. 2. X. 1. 1. T. t . X. X. X. R. 0. 1. 2. Z. Z. X. 0. 0. 3. 4',\n",
       " ' 8.5 Summary. 8.5. Summary. 8.6\\tNames for Eg(X).. 2. 2. 2. 2. 2. 2. 2. 2. 2. p. \". X. #. 1. 2. d. 1. 2. d. d. i. i. d. ih. ;xi. X. ih. ;Xi. X. X. X. h. i. X. h. i. X. X. d. k. e. =. x. e. ;. k. X. 0. X. X. (k). k. X. X. 0. X. (k). k. t . :. (. X. k). k. a+bX. X. 1. 2. 1. 2. 1 1. 2 2. 1. 2. n. 1. 2. n. 1. 1. 2. 2. n. n. 2. 1. 1. 2. 2. 2. 2. n. n. 1. 2. n. 1. 2. n. 1. 2. n. 1. 2. n. 1. 2. n. n. n. 1. 1. 2. 2. n. n. i. j. i. j. i. i. i. i. j. 1. 2. n. 1. 2. n. i. j. T. T',\n",
       " ' 8.8.1 Equivalent Conditions for Independence. 8.8.1. Equivalent Conditions for Independence. 1. 2. n. 1. 2. n. 1. 1. 2. 2. n. n. 1. 1. 2. 2. n. n. f. (x. ; x. ; : : : ; x. ) =. f. (x. )f. (x. ). f. (x. ):. 1. 2. n. 1. 1. 2. 2. n. n. 1. 1. 2. 2. n. n',\n",
       " ' 8.9 Quantile Plots and Probability Plots. 8.9. Quantile Plots and Probability Plots. x. ; x. ; : : : ; x. x. 1. x',\n",
       " ' 8.10 Answers to Selected Exercises. 8.10. Answers to Selected Exercises. 2. 2. 1 . +2. 2.   . 1.  . 2.   . 1.  . 2.   . 1.  . 2.   . 1.  . 2.   . 1.  .  . 1.  .  . 91. 6. 6. 6. 6. 6. 6. 6. \\t. 6. 2. 2. 1. 4.  . 2.   . 1. 4.  . 2.   . 1. 4.  . 2.   . 12. 1.  . 2.   . 12. 1.  . 2.   . 12. 1.  .  . 1. 4.  .  . 12. 1.  .  . 119. 12. 5. 5. X. 2. 2. X. *. *. *. Z. Z. Z. X. X. X. X. R. 0. 1. X. R. b. 1. X. X. 2. 2. 2. 3. 3. 3. 3. 3. d. k. k. x. k. x  k. (k. ). Topic 9',\n",
       " ' Examples of Mass Functions and Densities. X',\n",
       " ' 9.1 Examples of Discrete Random Variables. 9.1. Examples of Discrete Random Variables. X. X. n. x.   . x.   . n  x. i. i. 1. 2. 1. 2. X. x. X. x . n. x. x. i. i. 1. 2. 1. 2. f. (xja; b) =. b. a . + 1.  . :. x. x. i. EX. =. m. +. n. :. 1. 2. k. m. +. n. 1. 2. k. 1. 2. 1. 2. x. 2. S . X.  .  .  ',\n",
       " ' 9.2 Examples of Continuous Random Variables. 9.2. Examples of Continuous Random Variables. f. X. (xj. ). P . f. x < X   x.  . +.   . x. :. f. (xja; b) =. b. a . :. X. x. T. s. T. s) =. Z. 1. x. e. dx. 0. 1. 2. n. (1). (2). (n). (k). 1. 1.   .   . 1.  . 2. p. Y. X. X. 1. dy. d. 1. Y. X. 1. dy. d. 1. f. (xj. ;. ) = . x.  . +1.  . :. 2 .  .  .  .  .  . 1. 2. 1. 2. 2. 2. 2. 2 . Y. 1. 2. y. =2;. 2. 2. 3. n. 1. ;. 1. 2',\n",
       " ' 9.3 More on Mixtures. 9.3. More on Mixtures. 1.   . n.  . 1. n. n. 1. 1. n. n. i. i. X. 1. n. i. X. i. i. i . n. i. i. i. Z',\n",
       " ' 9.4 R Commands. 9.4. R Commands',\n",
       " ' 9.5 Summary of Properties of Random Variables. 9.5. Summary of Properties of Random Variables',\n",
       " ' 9.5.1 Discrete Random Variables. 9.5.1. Discrete Random Variables. X',\n",
       " ' 9.6 Answers to Selected Exercises. 9.6. Answers to Selected Exercises. Y. y. 1. 1. n. i. 1. n. 1. p . +   + . 1.   . p.  .  . n(1.   . p). 1. 2. Y. 1. X. 1. 1. 2. 2. 1.  .  . 2. 1. p. Topic 10',\n",
       " ' The Law of Large Numbers. 10.1\\tIntroduction. 1. 2. i. n. n. p. 1. 2. 1. n. 1. n. n. n. 10.2\\tMonte Carlo Integration. 1. 2. X. Z. a. 1. 1. 2. n. 1. 2. n. p. 3. R. 0. *. 2. p. 3. p. 2. 1. Z. Z. *. *. *. *. 1. 1. 2. 2. 3. 3. i. i. i',\n",
       " ' 10.3 Importance Sampling. 10.3. Importance Sampling. 1. 2. Y. Y. Y. 1. 2. Y. f. 2. 2. Y. Y. Y. 1. 2. n. Y. Y. Y. 1. i. Y. 1. i. 1. 2. n. 1. 2. n. *. *. Y. X. *. 3=2. *. *. 1. p. x. dx',\n",
       " ' 10.4 Answers to Selected Exercises. 10.4. Answers to Selected Exercises. 1. 0. =2. p. p. 2. *. *. *. *. *. *. 3. Z. 1. e. f. X. (x). dx:. 0. X. *. 2. Topic 11',\n",
       " ' The Central Limit Theorem. x. 11.1\\tIntroduction. n. S. =. X. n. :. n. p. p',\n",
       " ' 11.2 The Classical Central Limit Theorem. 11.2. The Classical Central Limit Theorem. 1. 2. n. n. 1. 2. n. i. n. n. p. 2. n. p. p. n. n. i. 2. n. Z. n. n. n. 1 . 1 .  .  . 2. 144. S',\n",
       " ' 11.2.1 Bernoulli Trials and the Continuity Correction. 11.2.1. Bernoulli Trials and the Continuity Correction. n. X. *. *. *. m. 11.3\\tPropagation of Error. Y. Y. Y .  . Y. 3. Y. 3. Y. 2. Y. Y. 2. Y. 3. 0. 0. 1. 1. 1. 0. ^. 1. =. 0. 1. 0. :. 1. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. d. 1. 2. d. 0. 0. 0. 1. 2. 3. 0. 0. 0. ‘. w. h. 2. ‘. 2. 2. w. 2. 2. h. 2. Y. n. 1. 2. n. 1. 1. 1. 6. 1;1. 1;n. i. 1. 2. 3. 1. 2. 3. 1. n. F. p. N. 1. n. F. p. N. 2. ^ . 2. F. i. i;1. i;2. i;n. i. i. i. i;j. B;n. 2',\n",
       " ' 11.5 Summary of Normal Approximations. 11.5. Summary of Normal Approximations. Z. =. :. 1. 2',\n",
       " ' 11.5.1 Sample Sum. 11.5.1. Sample Sum. n. 1. 2. n. 2. n. n. n. 2. p. n. 2. n. S. n. n. n. n. n. S. n. n. n. x. n. n. n',\n",
       " ' 11.5.3 Sample Proportion. 11.5.3. Sample Proportion. 1. 2. n. 1. 2. n',\n",
       " ' 11.5.4 Delta Method. 11.5.4. Delta Method. 0. p. Z. n. =. jg. ( )j. =. p. n. :. 1;1. 1;2. 1;n. 1. 1. 2. 2;1. 2;2. 2;n. 2. 2. 2. @. @. y. 1. 2. σ. n. 2. 2. @. @. x. 1. 2. σ. n. 1. 1. g. g',\n",
       " ' 11.6 Answers to Selected Exercises. 11.6. Answers to Selected Exercises. n. p. n. p. n. p. n. n. \". T. n. n. n. 3. #. n. 3. 1. =2. n. 2. 3. 3. 1. i. 3. i. 2. j. i. 2. j. i. j. k. i. j. k. 1 .   . 400. n. 42000. 8. n. 840. 50. p. n. n. p. p. 1. 2. Y .  .  . y.  .  .  .  . 2.  . 2. i;j. i. j. i;i. F .   . N.  . Part III. Estimation. Topic 12',\n",
       " ' Overview of Estimation. 12.1\\tIntroduction. 2. 1. 3. i. (1). (2). (n). 1. n. 1. n. i. 1. n. 1. n. i. n. 1. 1. 2. 2. n. n. 2. 1. n',\n",
       " ' 12.2 Classical Statistics. 12.2. Classical Statistics. X. 1. n. X. n. X. X. k. X. 1. X. 2. X. n. X. ^',\n",
       " ' 12.3 Bayesian Statistics. 12.3. Bayesian Statistics. Xj. ~. jX. Xj. Xj. Xj. ~. ~. X. Xj. X. jX. Xj. jX. 1. n. X. +. k=1. x. k. 1. 2. 3. x. x. x. x. 1. 2. 3. jX. 1. 1. jX. ;X. 1. 2. 1. 2. jX. Xj. 1. 2. 3. X. j. 3. X. j. 2. X. j. 1. x. x. x. x. x. x. jX. 1. X. j. 1. 2. 3. jX. ;X. 1. 2. X. j. 2. jX. 1. 3. 1. 2. 3. i. i. i. 1. 2. 1. 2. i. i. i. i. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. P. ;P. jY. ;Y. 1. 2. 1. 2. 1. 1. 1. 2. 2. 2. 1. +x. 1. +n. y. 2. +x. 2. +n. y. 1. 2. Z Z. 1. 1. 2. 2. 1. 2. 1. 2. 1. 2. *. P. jY. ;Y. 1. 2. 22. 22 . = . 121.  . 1. 2. 1. 2. 0. Xj. 1. 1. 0. 1. 0. 1',\n",
       " ' 12.4 Answers to Selected Exercises. 12.4. Answers to Selected Exercises. Xj. X. Xj. Xj. X. 1. 2. 1. 2. i. i. 2. g. 2. (.  . ;.  . ).    .  . 2. 1. 2. 12. 1. 1. 2. n. ( ) =. e. :. jX. 1. nx.  . n.   .  .   . +nx.   . 1.  . ( +n). 1. 2. 3. 4. 5. Topic 13',\n",
       " ' Method of Moments. 13.1\\tIntroduction. 1. 2. M. 1. 2. X. i. X. i. X. X. ^. ^',\n",
       " ' 13.2 The Procedure. 13.2. The Procedure. 1. 2. 1. m. 1. 1. 1. 2. d. 2. 2. 1. 2. d. d. d. 1. 2. d. 1. 2. n. m. 1. 2 .  . 2.    . 2. 1. 1. 2. m',\n",
       " ' 13.3 Examples. 13.3. Examples. 1. 2. n. f. (x. ) =. x. +1. ;. x > . 1. :. X. +. +. +. +. +. +. +. e. +. +. e. 2. 1. +. 1. n. k. r.  . N. t. f. g. N. N. t. 1. 2. k. kt. r. *. Histogram of Nhat. 2. *. 2. 2. X. X. 3. X. X. Topic 14',\n",
       " ' Unbiased Estimation. 14.1\\tIntroduction. 1. 2. n. d. d. 1. 2. n. 2. 2. d. 2. 2. 2. d. 2',\n",
       " ' 14.2 Computing Bias. 14.2. Computing Bias. 1. 2. 2. 2. i. i. i. i. 2. 2. 2.  . u. u. 2. 2. 2. 2. 1. n. u. 2. 2. b. 2. u. 2. 2. 2. u',\n",
       " ' 14.3 Compensating for Bias. 14.3. Compensating for Bias. ^. u. 2. 2. 2. kt. 00. 2kt. 3. p. 36:02. 2. 2. n. n. 2. n. n. n. 2. n. 2. *. 2. 1). t. 1. ;. t . = 1. :. 2. : : : :. n',\n",
       " ' 14.4 Consistency. 14.4. Consistency. 1. 2. n. n. 1. 2. n. n. 1. 2. n. n. n. n. n. n. n. 1. 2. n. n. n. 1. n',\n",
       " ' 14.5 Cramer´-Rao Bound. 14.5. Cramer´-Rao Bound. 1. n. 1. n.  . k. ( ) =. E. d. (. X. ).  . @.  . ln.  . f(Xj.  . ).   . :. Z. dx. 6= 0. 0. 1. n. f. x. =. +   +. :. k. n. 0. 2. n. 1. 2. n',\n",
       " ' 14.6 A Note on Exponential Families and Efficient Estimators. 14.6. A Note on Exponential Families and Efficient Estimators. X. 1. 2. n. n. Var. n. 1. 0. X. X. @. ln. f. (x. ) =. @. ln. c( ) +. d(x). c. ( ) =.  . 1 +.  . e. p. I. ( ) =. (1 +. e. )',\n",
       " ' 14.7 Answers to Selected Exercises. 14.7. Answers to Selected Exercises. i. 2. i. u. 2. 2. u. u. u. 2. u. 2. 2. u. 2. 00. 2 .  .   .  . n. 2. n. 1. 1. 1. 1. 0. 2 . 2. 1. 1. 0. 4. 0. 2. x =2. 0. 2. p. 1. p. 1. p. p. p . =.  . 1 +.  . e. 1 +. e. @  . ln.  . c( ) =.  . 1 +.  . e.   . =.  . p. Topic 15',\n",
       " ' Maximum Likelihood Estimation. 15.1\\tIntroduction. 1. n. 1. n',\n",
       " ' 15.2 Examples. 15.2. Examples. L(pjx) =. p. x. (1. p. ).   . x.     . p. x.  . (1.   . p).   . x.  . =.  . p. x. x. (1.   . p). n.   . x. x. :. r. k  r. r. i. i. i. i. i. i. i. 2. 2. ^. i. i. i. i. i. i. i. 2. 2. 1. n. f(xj. ) =. i. i. i. (n). (n). (n). (n). (n). 1. i  n. i. n. (n). 1. i n. i. i. (n). X. i. 1. 2. n. 1. 2. n. (n). X. X. X. (n). d(X) =. n. X',\n",
       " ' 15.3 Summary of Estimators. 15.3. Summary of Estimators',\n",
       " ' 15.4 Asymptotic Properties. 15.4. Asymptotic Properties. p. n. 0. 0. n.  . @. ^',\n",
       " ' 15.5 Comparison of Estimation Procedures. 15.5. Comparison of Estimation Procedures. 1. 2. n. 0. method of moments. (. ). 0. (. ). maximum likelihood. ^. 0. 0. 0. 00. 2. 0. 0. p. 0. p. 0. 0. 0',\n",
       " ' 15.6 Multidimensional Estimation. 15.6. Multidimensional Estimation. 1. 2. n. i. Z. i;n. =. I( ). ii. =. p. n. :. .. alpha',\n",
       " ' 15.7 The Case of Exponential Families. 15.7. The Case of Exponential Families. X. (^(X)) =. nI( ). X.  . d . ( )',\n",
       " ' 15.8 Choice of Estimators. 15.8. Choice of Estimators',\n",
       " ' 15.9 Technical Aspects. 15.9. Technical Aspects. 0. 1. 0. E. ln. f(X. 1. j. ). :. 0. 0. 0. i. 0. 0. 0. 2. i. 0. 2. 0. 0. n. 0. 0. 0. 2. 0',\n",
       " ' 15.10 Answers to Selected Exercises. 15.10. Answers to Selected Exercises. X. w. i. i. 2. i. ^. n. n. n. i. i. n. i. i. Topic 16',\n",
       " ' Interval Estimation. ^',\n",
       " ' 16.1 Classical Statistics. 16.1. Classical Statistics. ^',\n",
       " ' 16.1.1 Means. 16.1.1. Means. 1. 2. n. 0. 2. X. 0 .   . 0. x. z.  . p. n. (1. ). =. 2  0.  . 1. 2. n. s=. x. n. p. n. 1;. n. 1. n. 1;. n. 1. n. 1;. n. 1;(1 )=2. 1;1. 1;n. 2;1. 2;n. 1 .  . 2.  . 1 2. d. 1;1. 1;n. 2;1. 2;n. 1. 1. 2. 2. 2. 2. 2. 1. 1 .   . 2.  . 1. 2. 1. 2.  .   . 2. 2.  . 1. 2. 2. 2. 1. 2. 2. 2. p. 1. 2. 1. 1. 2. 1. 2.  .   . 2. 2. 1. 2',\n",
       " ' 16.1.2 Linear Regression. 16.1.2. Linear Regression. 1. 1. 2. 2. n. n. i. i. i. i. var(x). (. ;. ). x. ( ). (. ;. ). (. ;. ). u. x. 3;0:025',\n",
       " ' 16.1.3 Sample Proportions. 16.1.3. Sample Proportions. (1. 2. 428 + 152. 0:01. 1. 2. 1. 2. 1. 2. p^. 1. p. ^. 2. +. :. ‘. u. ‘. u',\n",
       " ' 16.1.4 Summary of Standard Confidence Intervals. 16.1.4. Summary of Standard Confidence Intervals. 1. 2. p. u. x',\n",
       " ' 16.1.5 Interpretation of the Confidence Interval. 16.1.5. Interpretation of the Confidence Interval. ^. ‘. ^. u. 0.5. 0.4. 0.3. 0.2. 0.1. 0. −0.1. −0.2. −0.3. −0.4. −0.5. 0. 10. 20. 30. 40. 50. 60. 70. 80. 90. 100. 1. 2. 1. 2. 1. 2',\n",
       " ' 16.1.6 Extensions on the Use of Confidence Intervals. 16.1.6. Extensions on the Use of Confidence Intervals. 3. 1. 2. n. 1. 0. 0. ‘. h. =. g(‘; h) = tan. h. n. 1. 1. !. !1. n',\n",
       " ' 16.2 The Bootstrap. 16.2. The Bootstrap. 1. 2. n. 1. 2. n. 1. 2. *. ^',\n",
       " ' 16.3 Bayesian Statistics. 16.3. Bayesian Statistics. 0. x. z.  . p. n.  . :. (x). z.  . p.  . +.  . n.  . :',\n",
       " ' 16.4 Answers to Selected Exercises. 16.4. Answers to Selected Exercises. 2. 2. 2. 2. 1. 1. i. (. ;. ). i. P. n. =1. (. ;. ). i. j. 2. (. ;. ). i. ^. *. 0:025. 0:025. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. Part IV. Hypothesis Testing. Topic 17',\n",
       " ' Simple Hypotheses',\n",
       " ' 17.1 Overview and Terminology. 17.1. Overview and Terminology. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0',\n",
       " ' 17.2 The Neyman-Pearson Lemma. 17.2. The Neyman-Pearson Lemma. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. :. 0',\n",
       " ' 17.2.1 The Receiver Operating Characteristic. 17.2.1. The Receiver Operating Characteristic. 0. 1',\n",
       " ' 17.3 Examples. 17.3. Examples. 0. 1. 0. 0. 1. n. 0. 2. 1. 0. ~. 0. 1. 1 0. 0. 2. p. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1 . 1. 1. 1. 1. 1. 0 .  .   . 1.  .  .  . 1. n. 0. n. i. =1.  . i.  .  . 0.  . 0. 0. P. n. n. p^ =. i=1. X. i. ;. Z. =. p. 0. 0. 0. 1. 0:05. 0. 1. 0. 0. 1. 1. z. =. p. 0. 0. 0. 0',\n",
       " ' 17.4 Summary. 17.4. Summary. 0. 0. 1. 1. 0. 0. 0. 0',\n",
       " ' 17.5 Proof of the Neyman-Pearson Lemma. 17.5. Proof of the Neyman-Pearson Lemma. i. 0. 1. 0. 1. 0. 1. 0. 1. 0',\n",
       " ' 17.6 An Brief Introduction to the Bayesian Approach. 17.6. An Brief Introduction to the Bayesian Approach. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. X. 1. 1. c. X. 0. 0. 1. ~. 1. 0. =. x. 0. 0. I. II. 0. 0. 1. 1. 1. ‘. I. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. II. I. 0. 0. I. II. 0. 1',\n",
       " ' 17.7 Answers to Selected Exercises. 17.7. Answers to Selected Exercises. ~. 1. 0. 1  . p.  0. 0. p. 1. p. 0. 1. 0. P. 0. 0. *. *. *. *. *. *. *. *. *. *. *. *. 0. I. jI. 0. 0. 0. I. II. 0. 0. *. 0. 1. Topic 18',\n",
       " ' Composite Hypotheses. 0',\n",
       " ' 18.1 Partitioning the Parameter Space. 18.1. Partitioning the Parameter Space. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0.  .   . 1.  .  .  .   .   . 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1',\n",
       " ' 18.2 The Power Function. 18.2. The Power Function. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 2. n. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. X. 0. 0. 1. 0. 1. 0. 0. 0 .   . 0.  . 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. N. 0.  =2.  =2. 1. n. p. 0. p. 0. 0 .   . 0.  .  .  .  .    .  . 1. 2. n. 1. 2. n. L. R. 0. L. R. 1. L. R. i. R. R. 0. L. L. (n). ~. g .  .  . (n). R. i. (n). ~. g .  .  .  . n. R. L. ~. !. L. L. p',\n",
       " ' 18.3 The p-value. 18.3. The p-value. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1',\n",
       " ' 18.4 Distribution of p-values and the Receiving Operating Characteristic. 18.4. Distribution of p-values and the Receiving Operating Characteristic. 0. 0. 1. 1. S(X). 0. 0. 0. 1. 1. p. 0. S(X). 0. 1. 1. R. R. R. R. R. R. 1. 1. R. i. i. 0. 1. 0 1. 1. 0',\n",
       " ' 18.5 Multiple Hypothesis Testing. 18.5. Multiple Hypothesis Testing. 1. m',\n",
       " ' 18.5.1 Familywise Error Rate. 18.5.1. Familywise Error Rate. B. 1. m. 1. m. 1. m. i. 1. m. i. B. 1. m. B. I .    . \\t. I . m. 1=m. I .   . I.  . 1=m. 0. 1. m. 2. 2m. 1. m. *. *. *',\n",
       " ' 18.5.2 False Discovery Rate. 18.5.2. False Discovery Rate. 0. 0. U. 0. R. 0. 0. 1. 0.  . 0.  . *. *. *',\n",
       " ' 18.6 Answers to Selected Exercises. 18.6. Answers to Selected Exercises. 0. 0. 0. 0. p. 0. 1600. 1. 1. n. (n). i. i. X. (n). 1. 1. n. =. S(x). 0. S(x). 0. 1. S(x). 0. S(x). 0. S(x). 0. S(X). i. i. i. 0. R. 1. 1. 0. 1. R. 1. R. s. 1. 1. 0. 0. 1. 0. fs. <s. g. 1. 0. 0. 0. p. *. *. *. 0. 1. i. 1. m. 1. m. c. c. 1. c. m. c. 1. c. m. 1. m. I . m. S(x). 0. 0. S(x). 0. S(x). 0. S(x). 0. S(. 1. x). 0. F. (F. S(. 1. x). (u. ). ) =. u;. S(x). 0. Topic 19',\n",
       " ' Extensions on the Likelihood Ratio. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. ~. 0',\n",
       " ' 19.1 One-Sided Tests. 19.1. One-Sided Tests. 0. 2. 0. 0. 1. 1. 1 . 0. 1 .   . 0. 0. ~. 1 .   . 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. n. 0. 0. 1. z. =. p. 0. 0. 0. *. *. *. *. *. *. *. *. *. *. *. *. *. *. *. 0. 1. 0. 0',\n",
       " ' 19.2 Likelihood Ratio Tests. 19.2. Likelihood Ratio Tests. 0. 0. 0. 0. 1. n. 0. 2. 0. 0. 0. 0. 0. 0. 1. 0. ln. (. x. ). 2. :. 0. 0. 0. 1. 2. 1. 1. 2. 1. 1;1. i;2. 1;n. 2. 2;1. i;2. 2;n. i. 0. 1. 2. 0. 1. 2. 1. 2. 1. 2. *. *',\n",
       " ' 19.3 Chi-square Tests. 19.3. Chi-square Tests. 0. 0. i. i. 1. 1. 1. 0. n. 2. k.  .  .  . 0. 0. 0 . 1. 1. k. k. k. 0. m. f. 1. m. f. m. f. m. f. m .   . f.  .   . 0. m. f. m. f. m. f. m. f. f. f. m. m. *. *. *. *. m. f. 0. 0. 1. 0. 2 .   . 0.  .  .  . n. 0. 1. 0',\n",
       " ' 19.4 Answers to Selected Exercises. 19.4. Answers to Selected Exercises. r. k  r. 2. 1. 2. 1. =. 1. 2. 1. 2. 1. p. ^. p. *. L(pjx) = (1. p. ). n.   . x. x. p. x. x.  . :. 0. 0. (1. p. ). n. p. p. np^. (1. p^). p^. 0. 0. 1. 0. i. i. i;1. i;2. i;n. 1. 2. p. 0. x. x. x. x. (1. p. 0. ). n.   . x.   . x.  . n.   . x.   . x.  . p. n. 0. p^. +n. p^. (1. p. 0. ). n.   . p. n.   .   . p. 0. 0. L(^p. 0. jx. 1. ;. x. 2. ) =. p^. n. 0. p^. (1. p. ^. ).   . p^.  . (1.   . p^. ).   . 1. 2. *. *. c. t. 0. t. c. 1. t. c. 10. 0. 2. ^. ). 2.  . d.  . 2.  .  . ^.  . 1. m. f. m. f. m. m. m .  . f.  . f. f . m . m.   . m.  . *. *. *. *. 1. n. *. *. *. *. *. *. *. *. n. n. 0. 2. *. *. *. *. *. *. *. *. *. *. *. *. 0.  . Topic 20',\n",
       " ' Procedures. x. =. n. s=. x. n. p',\n",
       " ' 20.1 Guidelines for Using the t Procedures. 20.1. Guidelines for Using the t Procedures',\n",
       " ' 20.2 One Sample t Tests. 20.2. One Sample t Tests. 0. 0. 1. 0. 1. n. 2. n. 1; =2. n. 1; =2. 2. 0. 1. 0. x. p. n.  . t. :. 1. n. ~. p. =. n. 0. n. 1; =2. 0. 0. n. 1; =2. 0',\n",
       " ' 20.3 Correspondence between Two-Sided Tests and Confidence Intervals. 20.3. Correspondence between Two-Sided Tests and Confidence Intervals. 0. 0. 0',\n",
       " ' 20.4 Matched Pairs Procedures. 20.4. Matched Pairs Procedures. 1. 2. n. 1. 2. n. F. H. 0. 1. 0. F. \\t. H. 1. F. \\t. H ',\n",
       " ' 20.5 Two Sample Procedures. 20.5. Two Sample Procedures. 1. 2. n. 1. 2. n. X. Y. X. X. 2. Y. Y. 2. 2. X. 2. Y. x. Y. t. c. 0. c. \\t. t. 1. c .   . t. t. =. q. = 1. :. 604. :. 10. +. 11. t. wt. 0. wt. \\t. t. 1. wt .   . t. 0. f. \\t. m. 1. f .   . m. m. m. 2. x. x. 2. x. 2. x. 2. x. x. 2. x. 2. x. 2',\n",
       " ' 20.6 Summary of Tests of Significance. 20.6. Summary of Tests of Significance. 0. 0. 1. 1. 0. 0. 1',\n",
       " ' 20.6.1 General Guidelines. 20.6.1. General Guidelines. 0. 1. 0. 0. 0',\n",
       " ' 20.6.2 Test for Population Proportions. 20.6.2. Test for Population Proportions. 0. 0. 1. 2. 1. 2. 1. 2. i. i',\n",
       " ' 20.6.3 Test for Population Means. 20.6.3. Test for Population Means. 1. 2. 1. 2',\n",
       " ' 20.7 A Note on the Delta Method. 20.7. A Note on the Delta Method. X. Y',\n",
       " ' 20.8 The t Test as a Likelihood Ratio Test. 20.8. The t Test as a Likelihood Ratio Test. 1. n. 2. n. 1; =2. 2. 0. x. s=. n. 0. p. n. 1; =2',\n",
       " ' 20.9 Non-parametric alternatives. 20.9. Non-parametric alternatives',\n",
       " ' 20.9.1 Permutation Test. 20.9.1. Permutation Test. 1. 2. 1. 2. 1. 1. 2',\n",
       " ' 20.9.2 Mann-Whitney or Wilcoxon Rank Sum Test. 20.9.2. Mann-Whitney or Wilcoxon Rank Sum Test. 1. 2. n. 1. 2. n. X. Y. 0. 2. X. Y. y. y;i. y. y. y. x. x;j. x. y. y. y. x. x. y',\n",
       " ' 20.9.3 Wilcoxon Signed-Rank Test. 20.9.3. Wilcoxon Signed-Rank Test. x. y. 1. 2. n. 1. 2. n. F. H. 0. F. H. 1. F. H. i. i. i. i',\n",
       " ' 20.10 Answers to Selected Exercises. 20.10. Answers to Selected Exercises. 1. 2. n. Z. Z. i. m. f. 1. n. 1. n. k. k. k. k. *. 5. 5. Topic 21',\n",
       " ' Goodness of Fit. 21.1\\tFit of a Distribution. O. A. B. AB. O. A. B. AB. 1. k. i. 1. k. k. 1. k. i. i. i. 1. k. 0. 0. 1. n. i. i. 1. 2. k. X. i. i. i. X. i. 1. k. p^. 0. 0. 0. p^. p^. @. p^. n. p. i. i. i. 1. 2. n. i. j. 1. k. i. i. 0. i. i. 2. 2. 0. O. A. B. AB. 1. i. i. 0. 2. *. *. *. 1. 2. 3. 4. 1. 2. 3. 4. 1. 2. 1. 2. 2. 2. 11. 12. 22. 11. 12. 22. 0. 1. 2. 1. 2. 1. 2. 0. 1. 2. 11. 2. 1. 12. 1. 2. 22. 2. 2. 2',\n",
       " ' 21.2 Contingency tables. 21.2. Contingency tables. 1. r. 1. c. ij. i. j. ij. i. j. ij. i. j. E. =. O. O. :. E. ij. =. :. 11. 12. 1;c. 1. 1;c. 1. j. 2. 0. E. 11. =. =. = 332:49:. +. +. +. 0. ij. ij. ij. 2 . 2',\n",
       " ' 21.3 Applicability and Alternatives to Chi-squared Tests. 21.3. Applicability and Alternatives to Chi-squared Tests. 1. 2. 1. 2. 1. 2. 1. 2. 1. 1. 2. 1. 2. 1. 1. 11. 1. 2. 2. 12. 1. =. 2 ',\n",
       " ' 21.4 Answer to Selected Exercise. 21.4. Answer to Selected Exercise. i. i. i. i. i. *. *. *. *. *. ij. n. n. ij. i. j. r. c. 0;ij. i. j. ij. n. n. ij. n. i. n. j. r. c. n. i. n. j. i. j. 0. i. j. 0. 0. i. i. i. p^. 0. =. 1. 2. Topic 22',\n",
       " ' Analysis of Variance',\n",
       " ' 22.1 Overview. 22.1. Overview. 11. 1n. 21. 2n. 31. 2n. 2. between. 2. residual. 0',\n",
       " ' 22.2 One Way Analysis of Variance. 22.2. One Way Analysis of Variance. 0. j .   . k.  .  .   . 1. j .   . k.  .  . ij. j. i. 1. q. ij. j .  . ij. ij. 2. j. j. 0. i. i. j .  . 1. total. total. 2. 1. total. j. SS. =. SS. +. SS. 2. residuals. residuals. residual. j . 0:025;30. residual. 0. low .   . med.  .   . high.    .   . 1.  .   . low.   . med.   . high.  . low. med. high. 7. between. *. *. *. residual. 2. resid. resid. med',\n",
       " ' 22.3 Contrasts. 22.3. Contrasts. *. *',\n",
       " ' 22.4 Two Sample Procedures. 22.4. Two Sample Procedures. ij. j .  . ij. ij. j. j. 1j. n. j. 2. 0. 1 .   . 2.    .   . 1.  .   . 1.  .   . 2. 1. 2. 2. 1. 2. 2. 0. 1. 2. 2. 1. 2.   . 2.  .  . 11. n. 1. 12. n. 2. 1. 2. j. j. 0. 0. 0 . total. residuals. total. between. 1. 1. 2. 2. 2. 2. residuals. SS. =. SS. +. SS. (y) =. residual. between. =. 1 + . SS. between.     . (n. +n. )=2. SS. SS. 0 . jT. (y)j. > t. :. p. between. residuals. p. 7',\n",
       " ' 22.5 Kruskal-Wallis Rank-Sum Test. 22.5. Kruskal-Wallis Rank-Sum Test. ij. j. i. ij. j. 1. q. n. 1. n. 1. n. +. 2. 1. i. 2. q. 1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_corpus = []\n",
    "chapter_corpus = \"\"\n",
    "chapter_index = 0 \n",
    "for i,line in enumerate(new_lines_statistics_statbook):\n",
    "    \n",
    "    if chapter_index == len(chapter_index_statistics_statbook):\n",
    "        break\n",
    "    \n",
    "    # if we reach to a new chapter \n",
    "    if i == chapter_index_statistics_statbook[chapter_index]:\n",
    "        total_corpus.append(chapter_corpus[1:])\n",
    "        chapter_corpus = \"\"\n",
    "        chapter_index+=1\n",
    "    chapter_corpus = chapter_corpus + \". \" + line\n",
    "total_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
